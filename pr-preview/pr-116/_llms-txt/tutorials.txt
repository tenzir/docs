<SYSTEM>Tutorials</SYSTEM>

# Learn idiomatic TQL

This tutorial teaches you how to write TQL that is clear, efficient, and maintainable. It assumes you already know basic TQL syntax and operators. You’ll learn the patterns and practices that make TQL code idiomatic—the way experienced TQL developers write it. ## What makes TQL idiomatic? [Section titled “What makes TQL idiomatic?”](#what-makes-tql-idiomatic) Idiomatic TQL follows consistent patterns that leverage the language’s strengths: * **Vertical clarity**: Pipelines flow top-to-bottom for readability * **Explicit data contracts**: Clear about what data should vs. might exist * **Domain-aware types**: Uses IP addresses, not strings; durations, not integers * **Composition over complexity**: Small, focused operators that combine well * **Performance-conscious**: Filters early, aggregates late This tutorial shows you these patterns through concrete examples, comparing idiomatic approaches with common pitfalls. ## Pipeline structure [Section titled “Pipeline structure”](#pipeline-structure) ### Vertical vs horizontal: choosing the right style [Section titled “Vertical vs horizontal: choosing the right style”](#vertical-vs-horizontal-choosing-the-right-style) TQL offers two ways to chain statements: a newline `\n` (vertical) or pipe `|` (horizontal). While both are valid, each has its place. #### Always use vertical structure in files [Section titled “Always use vertical structure in files”](#always-use-vertical-structure-in-files) ✅ Idiomatic vertical structure: ```tql let $ports = [22, 443] from "/tmp/logs.json" where port in $ports select src_ip, dst_ip, bytes summarize src_ip, total=sum(bytes) ``` Benefits of vertical structure: * **Readability**: Easy to scan and understand data flow * **Debugging**: Simple to comment out individual operators * **Modification**: Easy to insert or remove pipeline stages * **Version Control**: Clear diffs when pipelines change #### Use horizontal structure only for command-line [Section titled “Use horizontal structure only for command-line”](#use-horizontal-structure-only-for-command-line) ✅ Appropriate for one-liners: ```tql tenzir 'from "logs.json" | where severity == "high" | summarize count()' ``` The horizontal approach is ideal for: * **Command-line usage**: Quick ad-hoc queries in the terminal * **API requests**: Single-line strings in JSON payloads * **Shell scripts**: Embedding TQL in bash scripts * **Interactive exploration**: Building pipelines in a REPL #### Never mix styles [Section titled “Never mix styles”](#never-mix-styles) ❌ Avoid hybrid approaches: ```tql let $ports = [22, 443] from "/tmp/logs.json" | where port in $ports | select src_ip, dst_ip, bytes | summarize src_ip, total=sum(bytes) ``` This Kusto-like style makes code harder to read and maintain, especially with nested pipelines that increase indentation. ### Trailing commas in vertical structures [Section titled “Trailing commas in vertical structures”](#trailing-commas-in-vertical-structures) When writing vertical structures, use trailing commas consistently to improve maintainability. #### Lists and records [Section titled “Lists and records”](#lists-and-records) ✅ Vertical structures with trailing commas: ```tql let $ports = [ 22, 80, 443, 3306, ] let $config = { threshold: 100, timeout: 30s, enabled: true, } ``` Benefits: * Add new items without modifying existing lines * Reorder items without worrying about comma placement * Get cleaner diffs in version control * Avoid syntax errors when adding/removing items ✅ Horizontal structures without trailing commas: ```tql let $ports = [22, 80, 443, 3306] let $config = {threshold: 100, timeout: 30s} ``` ❌ Never use trailing commas horizontally: ```tql let $ports = [22, 80, 443, 3306,] // Wrong! let $config = {threshold: 100, timeout: 30s,} // Wrong! ``` ❌ No trailing comma after an operator argument sequence: ```tql from {status: 200, path: "/api"}, {status: 404, path: "/missing"} // No comma here ``` ## Field management [Section titled “Field management”](#field-management) ### Use `move` expressions to prevent field duplication [Section titled “Use move expressions to prevent field duplication”](#use-move-expressions-to-prevent-field-duplication) ✅ Clean field transfers with move: ```tql // Moving fields during transformation normalized.src_ip = move raw.source_address normalized.dst_port = move raw.destination.port normalized.severity = move alert.level ``` ❌ Avoid copy-then-drop pattern: ```tql normalized.src_ip = raw.source_address normalized.dst_port = raw.destination.port normalized.severity = alert.level drop raw.source_address, raw.destination.port, alert.level ``` ### Be intentional about field preservation [Section titled “Be intentional about field preservation”](#be-intentional-about-field-preservation) ✅ Move fields that are transformed: ```tql ocsf.activity_id = move http_methods[method]? else 99 ``` ✅ Drop only static metadata fields: ```tql drop event_kind // Same across all events ``` ❌ Don’t leave transformed data in original location: ```tql ocsf.src_ip = original.ip // Bad: original.ip still exists ``` When normalizing data (e.g., to OCSF format): * Use `move` for fields being transformed to prevent duplication * Ensure transformed values don’t appear in both old and new locations * Only drop fields you’re certain are constant across events * Verify no critical data ends up in `unmapped` fields * Treat all input-derived values as dynamic, not constants * Don’t hardcode field values based on example data ### Use meaningful names for computed fields [Section titled “Use meaningful names for computed fields”](#use-meaningful-names-for-computed-fields) ✅ Clear intent: ```tql summarize \ src_ip, total_traffic=sum(bytes), avg_response=mean(response_time), error_rate=count(status >= 400) / count() ``` ❌ Unclear: ```tql summarize src_ip, sum(bytes), mean(response_time) ``` ## Type awareness [Section titled “Type awareness”](#type-awareness) ### Leverage TQL’s domain-specific types [Section titled “Leverage TQL’s domain-specific types”](#leverage-tqls-domain-specific-types) ✅ Use native types: ```tql let $timestamp = now() let $weekend = $timestamp.day_of_week() in ["Saturday", "Sunday"] where src_ip in 10.0.0.0/8 where duration > 5min ``` ⚠️ Less expressive and error-prone: ```tql let $timestamp = now() let $weekend = $timestamp_day in [0, 6] // What do 0 and 6 mean? where src_ip.string().starts_with("10.") where duration_ms > 300000 ``` ## Performance considerations [Section titled “Performance considerations”](#performance-considerations) ### Place filters early in pipelines [Section titled “Place filters early in pipelines”](#place-filters-early-in-pipelines) ✅ Filter first, reduce data volume: ```tql from "large_dataset.json" where severity == "critical" // Reduce early where timestamp > now() - 1h // Further reduction select relevant_fields // Drop unnecessary data summarize ... // Aggregate reduced dataset ``` ❌ Process everything, then filter: ```tql from "large_dataset.json" select all_fields summarize ... where result > threshold // Filter after expensive operation ``` ## Composition patterns [Section titled “Composition patterns”](#composition-patterns) ### Use constants for reusable values [Section titled “Use constants for reusable values”](#use-constants-for-reusable-values) ✅ Maintainable and self-documenting: ```tql let $internal_net = 10.0.0.0/8 let $critical_ports = [22, 3389, 5432] // SSH, RDP, PostgreSQL let $high_risk_threshold = 0.8 where src_ip in $internal_net where dst_port in $critical_ports where risk_score > $high_risk_threshold ``` ❌ Magic numbers scattered throughout: ```tql where src_ip in 10.0.0.0/8 where dst_port in [22, 3389, 5432] where risk_score > 0.8 // What does 0.8 mean? ``` ## Record constants and mappings [Section titled “Record constants and mappings”](#record-constants-and-mappings) ### Use record constants for mappings instead of if-else chains [Section titled “Use record constants for mappings instead of if-else chains”](#use-record-constants-for-mappings-instead-of-if-else-chains) ✅ Clean record-based mappings with else fallback: ```tql let $http_methods = { CONNECT: 1, DELETE: 2, GET: 3, HEAD: 4, OPTIONS: 5, POST: 6, PUT: 7, TRACE: 8, PATCH: 9, } let $activity_names = [ "Unknown", "Connect", "Delete", "Get", "Head", "Options", "Post", "Put", "Trace", "Patch", ] let $dispositions = { OBSERVED: {id: 15, name: "Detected"}, LOGGED: {id: 17, name: "Logged"}, ALLOWED: {id: 1, name: "Allowed"}, BLOCKED: {id: 2, name: "Blocked"}, DENIED: {id: 2, name: "Blocked"}, } // Use record indexing with else for fallback values activity_id = $http_methods[method]? else 99 activity_name = $activity_names[activity_id]? else "Other" disposition = $dispositions[action]? else {id: 0, name: "Unknown"} ``` ❌ Complex if-else chains: ```tql // Hard to maintain and extend if method == "GET" { activity_id = 3 } else if method == "POST" { activity_id = 6 } else if method == "PUT" { activity_id = 7 } else if method == "DELETE" { activity_id = 2 } else { activity_id = 99 } // Error-prone string building if activity_id == 1 { activity_name = "Connect" } else if activity_id == 2 { activity_name = "Delete" } else if activity_id == 3 { activity_name = "Get" } // ... many more conditions ``` ❌ Inline nested if-else expressions are especially problematic: ```tql // TERRIBLE: Unreadable chain of inline conditionals activity_id = 3 if method == "GET" else 6 if method == "POST" else 7 if method == "PUT" else 2 if method == "DELETE" else 99 // AWFUL: Complex nested logic impossible to follow severity_level = "critical" if score > 90 else "high" if score > 70 else "medium" if score > 50 else "low" if score > 30 else "info" // UNMAINTAINABLE: Mixed logic with nested conditions result = "blocked" if is_malicious else "allowed" if is_trusted else "quarantine" if risk > 0.8 else "review" if risk > 0.5 else "log" ``` These inline chains are a serious anti-pattern because: * **Unreadable**: Eyes can’t parse the logic flow easily * **Error-prone**: Easy to mix up conditions and values * **Unmaintainable**: Adding/removing conditions requires rewriting the entire expression * **Debugging nightmare**: Can’t set breakpoints or log intermediate values * **Performance issues**: Every condition is evaluated sequentially ✅ Always use record constants instead: ```tql // Clean, maintainable lookups with record indexing activity_id = $http_methods[method]? else 99 activity_name = $activity_names[activity_id]? else "Other" disposition = $dispositions[action]? else {id: 0, name: "Unknown"} ``` The `else` keyword provides a fallback value when: * A field doesn’t exist (`field? else default`) * An array index is out of bounds (`array[index]? else default`) * A record key doesn’t exist (`record[key]? else default`) This pattern is particularly powerful for: * Normalizing data to standard formats (like OCSF) * Mapping between different naming conventions * Providing sensible defaults for missing data * Creating reusable transformation logic ## Writing comments [Section titled “Writing comments”](#writing-comments) Good comments explain the reasoning, business logic, or non-obvious decisions behind code. The code itself should show what it does; comments should explain why it does it. ❌ Bad; explains what (redundant): ```tql // Increment counter by 1 set counter = counter + 1 ``` ✅ Good; Explains why: ```tql /* * Binary field curves are deprecated due to: * 1. Weak reduction polynomials in some cases * 2. Complex implementation leading to side-channel vulnerabilities * 3. Patent concerns that historically limited adoption * 4. Generally slower performance compared to prime field curves * 5. Less scrutiny from cryptographic community * RFC 8422 deprecates these for TLS 1.3. */ let $weak_prime_curves = [ "secp160k1", // 160-bit curves "secp160r1", "secp160r2", "secp192k1", // 192-bit curves "secp224k1", // 224-bit curves "secp224r1", // NIST P-224 ] ``` ## Data quality [Section titled “Data quality”](#data-quality) TQL’s diagnostic system helps you maintain data quality by distinguishing between expected variations and genuine problems. Understanding how to work with warnings intentionally is key to building robust pipelines. In TQL, warnings are not annoyances to suppress—they’re signals about your data’s health. The language provides tools to express your expectations clearly: * **No `?`**: Field should exist; warning indicates a problem * **With `?`**: Field naturally varies; absence is normal * **`assert`**: Enforce invariants with warnings * **`strict`**: Escalate warnings to errors when quality matters ### Be deliberate about optional field access [Section titled “Be deliberate about optional field access”](#be-deliberate-about-optional-field-access) The `?` operator controls whether missing fields trigger warnings. Use it to express your data contract clearly. ✅ Clear data expectations in transformations: ```tql // Required field - warning if missing result = {id: event_id, severity: severity} // Optional field - no warning if missing result = {id: event_id, customer: customer_id?} ``` ✅ Express expectations in selections: ```tql // Required field - warning if missing select event_id, timestamp // Mix required and optional fields select event_id, customer_id? ``` ❌ Suppressing warnings on required fields: ```tql // Bad: This hides data quality problems select event_id? // Should warn if missing! ``` ### Enforce invariants with `assert` [Section titled “Enforce invariants with assert”](#enforce-invariants-with-assert) Use `assert` when specific conditions must be true for your pipeline to work correctly. Unlike `where`, which silently filters, `assert` emits warnings when invariants are violated. ✅ Use `assert` for data quality checks: ```tql // Ensure critical field has valid values assert severity in ["low", "medium", "high", "critical"] // Verify schema expectations subscribe "ocsf" assert @name == "ocsf.network_activity" // Wrong event type = warning ``` ✅ Combine `assert` with filtering: ```tql // First assert invariant (with warning) assert src_ip != null // Then filter normally (silent) where src_ip.is_private() ``` ❌ Don’t use `assert` for normal filtering: ```tql // Wrong: This creates unnecessary warnings assert severity == "critical" // Right: Use where for filtering where severity == "critical" ``` ### Treat warnings as errors with `strict` [Section titled “Treat warnings as errors with strict”](#treat-warnings-as-errors-with-strict) The [`strict`](/reference/operators/strict) operator escalates all warnings to errors within its scope, stopping the pipeline when data quality issues occur. ✅ Use `strict` for critical data processing: ```tql // Stop pipeline if any required field is missing strict { select transaction_id // Warning → Error if missing } ``` ✅ Combine with `assert` for comprehensive checks: ```tql strict { // Assertion becomes fatal if violated assert amount > 0 // Missing field also becomes fatal select customer_id } ``` ### Choose the right quality control [Section titled “Choose the right quality control”](#choose-the-right-quality-control) | Tool | Use When | Behavior | | ------------ | -------------------- | ------------------ | | `field` | Field must exist | Warning if missing | | `field?` | Field is optional | Silent if missing | | `where` | Filtering data | Silent filter | | `assert` | Enforcing invariants | Warning + filter | | `strict { }` | Zero tolerance | Warnings → Errors | ✅ Production pipeline with layered quality control: ```tql // Constants for validation let $valid_severities = ["low", "medium", "high", "critical"] let $required_fields = ["event_id", "timestamp", "source"] // Strict mode for critical path strict { subscribe "prod" // Assertions for data integrity assert severity in $valid_severities assert timestamp > 2024-01-01 // Required field access (warnings → errors) where event_id != null and source != null // Normal processing context::enrich "geo", key=source } // Optional enrichment (outside strict) where geo?.country? == "US" // No warning if geo missing ``` This layered approach ensures critical data meets requirements while allowing flexibility for optional enrichments.

# Map data to OCSF

In this tutorial you’ll learn how to **map events to [Open Cybersecurity Schema Framework (OCSF)](https://schema.ocsf.io)**. We walk you through an example of events from a network monitor and show how you can use Tenzir pipelines to transform them into OCSF-compliant events. ![OCSF Pipeline](/pr-preview/pr-116/_astro/ocsf-pipeline.DpXOPlHZ_19DKCs.svg) The diagram above illustrates the data lifecycle and shows where the OCSF mapping takes place: you collect data from various data sources, each of which has a different shape, and then convert them to a standardized representation. Normalization decouples data acquisition from downstream analytics, so you can scale each process independently. ## OCSF Primer [Section titled “OCSF Primer”](#ocsf-primer) OCSF is a vendor-agnostic event schema (aka. “taxonomy”) that defines structure and semantics for security events. Here are some key terms you need to know to map events: * **Attribute**: a unique identifier for a specific type, e.g., `parent_folder` of type `String` or `observables` of type `Observable Array`. * **Event Class**: a description of an event that uses specific attributes, e.g., `HTTP Activity` and `Detection Finding`. * **Category**: a group of event classes, e.g., `System Activity` or `Findings`. The diagram below illustrates how subsets of attributes form an event class: ![OCSF Event Classes](/pr-preview/pr-116/_astro/ocsf-event-classes.BlDtprEG_19DKCs.svg) The **Base Event Class** is a special event class that appears in every event class. Think of it as a mixin of attributes that OCSF automatically includes: ![OCSF Base Event Class](/pr-preview/pr-116/_astro/ocsf-base-event-class.Cn0E_XRl_19DKCs.svg) For this tutorial, we look at OCSF from the perspective of the *mapper* persona, i.e., as someone who converts existing events into the OCSF schema. OCSF also defines three other personas, author, producer, and analyst. This tutorial doesn’t cover them. Our mission as mapper is to study the event semantics of the data source we want to map, and translate the event to the appropriate OCSF event class. ## Case Study: Zeek Logs [Section titled “Case Study: Zeek Logs”](#case-study-zeek-logs) Let’s map some [Zeek](https://zeek.org) logs to OCSF! Zeek generates logs in tab-separated values (TSV) or JSON format. Here’s an example of a connection log in TSV format: conn.log (TSV) ```text #separator \x09 #set_separator , #empty_field (empty) #unset_field - #path conn #open 2023-03-07-10-23-46 #fields ts uid id.orig_h id.orig_p id.resp_h id.resp_p id.vlan id.vlan_inner proto service duration orig_bytes resp_bytes conn_state local_orig local_resp missed_bytes history orig_pkts orig_ip_bytes resp_pkts resp_ip_bytes tunnel_parents vlan inner_vlan orig_l2_addr resp_l2_addr geo.orig.country_code geo.orig.region geo.orig.city geo.orig.latitude geo.orig.longitude geo.resp.country_code geo.resp.region geo.resp.city geo.resp.latitude geo.resp.longitude community_id #types time string addr port addr port int int enum string interval count count string bool bool count string count count count count set[string] int int string string string string string double double string string string double double string 1637155963.237882 CZwqhx3td8eTfCSwJb 128.14.134.170 57468 198.71.247.91 80 - - tcp http 5.162805 205 278 SF - - 0 ShADadfF 6 525 5 546 - - - 64:9e:f3:be:db:66 00:16:3c:f1:fd:6d US CA Los Angeles 34.0544 -118.2441 US VA Ashburn 39.0469 -77.4903 1:YXWfTYEyYLKVv5Ge4WqijUnKTrM= 1637157758.165570 CnrwFesjfOhI3fuu1 45.137.23.27 47958 198.71.247.91 53 - - udp dns - - - S0 - - 0 D 1 58 0 0 - - - 64:9e:f3:be:db:66 00:16:3c:f1:fd:6d BD - - 23.7018 90.3742 US VA Ashburn 39.0469 -77.4903 1:0nZC/6S/pr+IceCZ04RjDZbX+KI= 1637229399.549141 CBTne9tomX1ktuCQa 10.4.21.101 53824 107.23.103.216 587 - - tcp smtp 606.747526 975904 11950 SF - - 0 ShAdDaTtTfF 1786 1069118 1070 55168 - - - 00:08:02:1c:47:ae 20:e5:2a:b6:93:f1 - - - - - US VA Ashburn 39.0469 -77.4903 1:I6VoTvbCqaKvPrlFnNbRRbjlMsc= ``` You can also [download this sample](/packages/zeek/tests/inputs/conn.log) to avoid dealing with tabs and spaces in the snippet above. ### Step 1: Parse the input [Section titled “Step 1: Parse the input”](#step-1-parse-the-input) We first parse the log file into a structured form so that we can work with the individual fields. The [`read_zeek_tsv`](/reference/operators/read_zeek_tsv) operator parses the above structure out of the box: ```sh tenzir 'read_zeek_tsv' < conn.log ``` Now that we have decomposed the data into its atomic values, we can map them to the corresponding OCSF fields. ### Step 2: Map to OCSF [Section titled “Step 2: Map to OCSF”](#step-2-map-to-ocsf) To map fields, you must first identify the appropriate OCSF event class. In our example, the corresponding event class in OCSF is [Network Activity](https://schema.ocsf.io/1.6.0/classes/network_activity). We use OCSF v1.6.0 throughout this tutorial. To make the mapping process more organized, we map per *attribute group*. The schema has four groups: 1. **Classification**: Important for the taxonomy and schema itself 2. **Occurrence**: Temporal characteristics about when the event happened 3. **Context**: Auxiliary information about the event 4. **Primary**: Defines the key semantics of the given event Here’s a template for the mapping pipeline: ```tql // --- Preamble --------------------------------- // Move the original event into a dedicated field that we pull our values from. // We recommend naming the field so that it represents the respective data // source. this = { zeek: this } // (2) Populate the OCSF event. Style-wise, we recommend using one coherent // block of TQL per OCSF attribute group to provide a bit of structure for the // reader. // --- OCSF: classification attributes ---------- ocsf.activity_id = 6 ocsf.class_uid = 4001 // ...fill out remaining classification attributes. // --- OCSF: occurence attributes --------------- ocsf.time = move zeek.ts // 👈 remove source field while mapping ocsf.duration = move zeek.duration ocsf.end_time = ocsf.time + ocsf.duration ocsf.start_time = ocsf.time // ...fill out remaining occurence attributes. // --- OCSF: context attributes ----------------- ocsf.metadata = { product: { name: "Zeek", }, uid: move zeek.uid, version: "1.6.0", } // ...fill out remaining context attributes. // --- OCSF: primary attributes ----------------- ocsf.src_endpoint = { ip: zeek.id.orig_h, port: zeek.id.orig_p, } // ...fill out remaining primary attributes. drop zeek.id // 👈 remove source field after mapping // --- Finalize --------------------------------- // (3) Hoist all `ocsf` attributes into the root and declare the remaining fields in // `zeek` as `unmapped`. this = {...ocsf, unmapped: zeek} // (4) Assign metadata, such as a name for easier filtering downstream. @name = "ocsf.network_activity" ``` Let’s unpack this: 1. With `this = { zeek: this }` we move the original event into the field `zeek`. This approach also avoids name clashes when we create new fields in the next steps. Because we are mapping Zeek logs, we chose `zeek` as a name to make the subsequent mappings almost self-explanatory. 2. The main work takes place here. Our approach is structured: for every field in the source event, (1) map it, and (2) remove it. Ideally, use the `move` keyword to perform (1) and (2) together, e.g., `ocsf.x = move source.y`. If a field needs to be used multiple times in the same expression, use the [`drop`](/reference/operators/drop) afterwards. 3. The assignment `this = {...ocsf, unmapped: zeek}` means that we move all fields from the `ocsf` record into the top-level record (`this`), and at the same time add a new field `unmapped` that contains everything that we didn’t map. This is why it’s important to remove the fields from the source as we perform the mapping. If you forget to map a field, it simply lands in the `unmapped` catch-all record and you can tweak your mapping later. 4. We give the event a new schema name so that we can easily filter by its shape in further pipelines. Now that we have a template, let’s get our hands dirty and go deep into the actual mapping. #### Classification Attributes [Section titled “Classification Attributes”](#classification-attributes) The classification attributes are important for the schema. Mapping them is mechanical and mostly involves reviewing the schema docs. ```tql ocsf.activity_id = 6 ocsf.activity_name = "Traffic" ocsf.category_uid = 4 ocsf.category_name = "Network Activity" ocsf.class_uid = 4001 ocsf.class_name = "Network Activity" ocsf.severity_id = 1 ocsf.severity = "Informational" ocsf.type_uid = ocsf.class_uid * 100 + ocsf.activity_id ``` Note that computing the field `type_uid` requires simple arithmetic. You can also rely on [`ocsf::derive`](/reference/operators/ocsf/derive) to populate sibling fields—an optimization to write more concise mappings. #### Occurrence Attributes [Section titled “Occurrence Attributes”](#occurrence-attributes) Let’s tackle the occurrence group. These attributes are all about time. ```tql ocsf.time = move zeek.ts ocsf.duration = move zeek.duration ocsf.end_time = ocsf.time + ocsf.duration ocsf.start_time = ocsf.time ``` Using `+` with a value of type `time` and `duration` yields a new `time` value, just as you’d expect. #### Context Attributes [Section titled “Context Attributes”](#context-attributes) The context attributes provide auxiliary information. Most notably, the `metadata` attribute holds data-source specific information. Even though `unmapped` belongs to this group, we deal with it at the very end. ```tql ocsf.metadata = { log_name: "conn.log", logged_time: move zeek._write_ts?, product: { name: "Zeek", vendor_name: "Zeek", cpe_name: "cpe:2.3:a:zeek:zeek", }, uid: move zeek.uid, version: "1.6.0", } drop zeek._path? // implied in metadata.log_name ocsf.app_name = move zeek.service ``` We use `?` when accessing fields that are not always present, e.g., `zeek._write_ts?`. If you omit the `?`, the pipeline emits a warning when `_write_ts` is missing. #### Primary Attributes [Section titled “Primary Attributes”](#primary-attributes) The primary attributes define the semantics of the event class itself. This is where the core value of the data is, as we are mapping the most event-specific information. ```tql ocsf.src_endpoint = { ip: zeek.id.orig_h, port: zeek.id.orig_p, } ocsf.dst_endpoint = { ip: zeek.id.resp_h, port: zeek.id.resp_p, } // Here, we use `drop` because we simply want to get rid of the intermediate // `id` record that we already mapped above. drop zeek.id // Locality of reference: we define the protocol numbers close where they are // used for easier readability. let $proto_nums = { tcp: 6, udp: 17, icmp: 1, icmpv6: 58, ipv6: 41, } ocsf.connection_info = { community_uid: move zeek.community_id?, protocol_name: move zeek.proto, protocol_num: $proto_nums[zeek.proto]? else -1 } // If we cannot use static records, branch with if/else statements. if ocsf.src_endpoint.ip.is_v6() or ocsf.dst_endpoint.ip.is_v6() { ocsf.connection_info.protocol_ver_id = 6 } else { ocsf.connection_info.protocol_ver_id = 4 } if zeek.local_orig and zeek.local_resp { ocsf.connection_info.direction = "Lateral" ocsf.connection_info.direction_id = 3 } else if zeek.local_orig { ocsf.connection_info.direction = "Outbound" ocsf.connection_info.direction_id = 2 } else if zeek.local_resp { ocsf.connection_info.direction = "Inbound" ocsf.connection_info.direction_id = 1 } else { ocsf.connection_info.direction = "Unknown" ocsf.connection_info.direction_id = 0 } drop zeek.local_orig, zeek.local_resp // The `status` attribute in OCSF is a success indicator. While we could use // `zeek.conn_state` to extract success/failure, this would go beyond the // tutorial. ocsf.status_id = 99 ocsf.status = "Other" ocsf.status_code = move zeek.conn_state ocsf.traffic = { bytes_in: zeek.resp_bytes, bytes_out: zeek.orig_bytes, packets_in: zeek.resp_pkts, packets_out: zeek.orig_pkts, total_bytes: zeek.orig_bytes + zeek.resp_bytes, total_packets: zeek.orig_pkts + zeek.resp_pkts, } drop zeek.resp_bytes, zeek.orig_bytes, zeek.resp_pkts, zeek.orig_pkts ``` Here’s what happens here: * The expression `$proto_nums[zeek.proto]` takes the value of Zeek’s `proto` field (e.g., `tcp`) and uses it as an index into a static record `$proto_nums`. Add a `?` at the end to avoid warnings when the lookup returns `null`, and use the inline `else` expression for the fallback value. * To check whether we have an IPv4 or an IPv6 connection, we call [`is_v6()`](/reference/functions/is_v6) on the IPs of the connection record. TQL comes with numerous other domain-specific [functions](/reference/functions) that make mapping security data a breeze. #### Putting it together [Section titled “Putting it together”](#putting-it-together) When we combine all TQL snippets from above, we get the following output: ```tql { activity_id: 6, activity_name: "Traffic", category_uid: 4, category_name: "Network Activity", class_uid: 4001, class_name: "Network Activity", severity_id: 1, severity: "Informational", type_uid: 400106, time: 2021-11-17T13:32:43.237881856Z, duration: 5.162805s, end_time: 2021-11-17T13:32:48.400686856Z, start_time: 2021-11-17T13:32:43.237881856Z, metadata: { log_name: "conn.log", logged_time: null, product: { name: "Zeek", vendor_name: "Zeek", cpe_name: "cpe:2.3:a:zeek:zeek", }, uid: "CZwqhx3td8eTfCSwJb", version: "1.6.0", }, app_name: "http", src_endpoint: { ip: 128.14.134.170, port: 57468, }, dst_endpoint: { ip: 198.71.247.91, port: 80, }, connection_info: { community_uid: "1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=", flag_history: "ShADadfF", protocol_name: "tcp", protocol_num: 6, protocol_ver_id: 4, }, status: "Other", status_code: "SF", status_id: 99, traffic: { bytes_in: 278, bytes_out: 205, packets_in: 5, packets_out: 6, total_bytes: 483, total_packets: 11, }, unmapped: { missed_bytes: 0, orig_ip_bytes: 525, resp_ip_bytes: 546, tunnel_parents: null, vlan: null, inner_vlan: null, orig_l2_addr: "64:9e:f3:be:db:66", resp_l2_addr: "00:16:3c:f1:fd:6d", geo: { orig: { country_code: "US", region: "CA", city: "Los Angeles", latitude: 34.0544, longitude: -118.2441, }, resp: { country_code: "US", region: "VA", city: "Ashburn", latitude: 39.0469, longitude: -77.4903, }, }, }, } ``` There are still several fields that we can map to the schema, but we’ll leave this as an exercise for the reader. #### Recap: Understand the OCSF pipeline architecture [Section titled “Recap: Understand the OCSF pipeline architecture”](#recap-understand-the-ocsf-pipeline-architecture) Most pipelines (1) onboard data from a source, (2) transform it, and (3) send it somewhere. This tutorial focuses on the middle piece, with transformation being the mapping to OCSF. ![Zeek OCSF Pipeline](/pr-preview/pr-116/_astro/ocsf-zeek-pipeline.tOw32iXh_19DKCs.svg) We’ve addressed data *onboarding* by reading a log file and decomposing the unstructured Zeek TSV contents into a structured record. The built-in [`read_zeek_tsv`](/reference/operators/read_zeek_tsv) operator made this trivial. But it often requires a lot more elbow grease to get there. Check out our extensive [guide on extracting structured data from text](/guides/data-shaping/extract-structured-data-from-text/) for more details. We haven’t yet addressed the other end of the pipeline: data *offboarding*. Our examples run `tenzir` on the command line, relying on an implicit output operator that writes the result of the last transformation to the terminal. In other words, we have a sandwich structure in our pipeline. To make it explicit: ```tql // (1) Onboard data (explicit) load_stdin read_zeek_tsv // (2) Map to OCSF // ... // Lots of TQL here! // ... // (3) Offboard data (implicit) write_tql save_stdout ``` Such a pipeline is impractical because data may arrive via multiple channels: log files, Kafka messages, or via Syslog over the wire. Even the encoding may vary. Zeek TSV is one way you can configure Zeek, but JSON output is another format. Similarly, users may want to consume the data in various ways. In theory, we’re done now. We have a working mapping. It’s just not yet very (re)usable. For maximum flexibility, we want to split the pipeline into independently usable snippets. The next section describes how to achieve this. ### Step 3: Package the mapping [Section titled “Step 3: Package the mapping”](#step-3-package-the-mapping) To make our OCSF mapping more reusable, we extract it as **user-defined operator** and put it into a [package](/explanations/packages). There is an entire [tutorial on writing packages](/tutorials/write-a-package), but all you need to know right now that packages are one-click installable bundles that you can flexibly deploy. After installation, you can call the newly introduced mapping operators from any pipeline. #### Break down complexity with user-defined operators [Section titled “Break down complexity with user-defined operators”](#break-down-complexity-with-user-defined-operators) Let’s work towards a package that comes with a user-defined operator called `zeek::ocsf::map` that maps Zeek connection logs to OCSF: ```tql load_stdin read_zeek_tsv zeek::ocsf::map // 👈 Maps Zeek logs to OCSF with a user-defined operator. write_tql save_stdout ``` All you have to do to get there is create a package with following directory structure: Notice how `zeek::ocsf::map` has two modules that are colon-separated: `zeek` and `ocsf`. The directory structure in the package determines the module hierarchy: Since operators fully compose, you can implement `zeek::ocsf::map` as a combination of other operators, e.g., one operator per log type: zeek/operators/ocsf/map.tql ```tql // Dispatch mappings based on schema name. This assumes that you've taken apart // the input logs and added the appropriate @name based on the log type. The // read_zeek_tsv operator does this automatically. You could also dispatch based // on any other field, such as _path, event_type, etc. if @name == "zeek.conn" { // Map "conn.log" events zeek::ocsf::logs::conn } else if @name == "zeek.dns" { // Map "dns.log" events zeek::ocsf::logs::dns } else { // In the future, raise a warning here or put the event into a DLQ. pass } ``` In this layout, you’d put the mapping operators in the following directories: #### Write tests for prod-grade reliability [Section titled “Write tests for prod-grade reliability”](#write-tests-for-prod-grade-reliability) To achieve production-grade quality of your mappings, you must ensure that they do what they promise. In practice, this means shipping tests along the mappings: given a mapping, test whether a provided input produces a valid output. This is where our [test framework](/reference/test-framework) comes into play. Put your test scenarios in `tests/` and sample data into `tests/inputs`: Here’s an example of the test: zeek/tests/map\_one.tql ```tql from_file f"{env("TENZIR_INPUTS")}/conn.log" { read_zeek_tsv } zeek::ocsf::map head 1 ``` Now run the test framework in the package directory: ```sh uvx tenzir-test ``` ```txt i executing project: zeek (.) i running 1 tests (44 jobs) in project . i 1× tenzir (v5.16.0+gc0a0c3ba49) ✘ tests/map_one.tql └─▶ Failed to find ref file: "../docs/public/packages/zeek/tests/map_one.txt" i ran 1 test: 0 passed (0%) / 1 failed (100%) passed (100%) / 0 failed (0%) ``` There is no baseline for the test yet, let’s generate it via `--update` ```sh uvx tenzir-test --update ``` Now there’s a \*.txt file next to the test scenario. Verify it that it has the expected output. Alternatively, use `uvx tenzir-test --passthrough` to print the output to the terminal for inline inspection. After running `uvx tenzir-test` again without any flags, you get the following output: ```txt i executing project: zeek (.) i running 1 tests (44 jobs) in project .; update i 1× tenzir (v5.16.0+gc0a0c3ba49) ✔ tests/map_one.tql i ran 1 test: 1 passed (100%) / 0 failed (0%) ``` Perfect. Now proceed with all log types and you have production-grade package. ### Step 4: Install and use the package [Section titled “Step 4: Install and use the package”](#step-4-install-and-use-the-package) After you have fleshed out the complete package, [install it](/guides/basic-usage/install-a-package), either interactively via [`package::add`](/reference/operators/package/add), or IaC-style by putting it into a git repo and pointing the config option `tenzir.package-dirs` to it. ## Summary [Section titled “Summary”](#summary) This tutorial showed you how to map security data to OCSF using TQL pipelines. You learned: * How OCSF events look like in high-level terms * How to structure your mapping pipeline using OCSF attribute groups (classification, occurrence, context, and primary) * How to use TQL operators and expressions to transform raw events into OCSF-compliant records * How to package your mappings as reusable operators in a Tenzir package The key to successful OCSF mapping is systematic organization: move your source event into a dedicated field, map attributes group by group while removing source fields as you go, and collect any unmapped fields in a catch-all record. This approach ensures you don’t miss any fields and makes your mappings easy to maintain and extend. For more examples and ready-to-use OCSF mappings, check out the [Tenzir Community Library](https://github.com/tenzir/library).

# Plot data with charts

In this tutorial, you will learn how to **use pipelines to plot data as charts**. The Tenzir Query Language (TQL) excels at slicing and dicing even the most complex shapes of data. But turning tabular results into actionable insights often calls for visualization. This is where charts come into play. ![Charts](/pr-preview/pr-116/_astro/charts.yAPl35Lg_Z2fad56.svg) ## Available chart types [Section titled “Available chart types”](#available-chart-types) Tenzir supports four types of charts, each with a dedicated operator: 1. **Pie**: [`chart_pie`](/reference/operators/chart_pie) 2. **Bar**: [`chart_bar`](/reference/operators/chart_bar) 3. **Line**: [`chart_line`](/reference/operators/chart_line) 4. **Area**: [`chart_area`](/reference/operators/chart_area) ## How to plot data [Section titled “How to plot data”](#how-to-plot-data) Plotting data in the Explorer involves three steps: 1. [Run a pipeline](/guides/usage/basics/run-pipelines) to prepare the data. 2. Add a `chart_*` operator to render the plot. 3. View the chart below the Editor. ![Pipeline to Chart](/pr-preview/pr-116/_astro/pipeline-to-chart.0tOnE2lJ_Z1V1cUL.svg) After generating a chart, you can **download it** or **add it to a dashboard** to make it permanent refresh it periodically. **Example**: Here's how you generate a pie chart that shows the application breakdown from Zeek connection logs: ```tql export where @name == "zeek.conn" where service != null top service head chart_pie label=service, value=count ``` ### Add a chart to a dashboard [Section titled “Add a chart to a dashboard”](#add-a-chart-to-a-dashboard) To make a chart permanent: 1. Click the **Dashboard** button. ![Add Chart to Dashboard](/pr-preview/pr-116/_astro/add-chart-to-dashboard-1.CKWKUeyV_Z18cI3e.webp) 2. Enter a title for the chart, then click **Add to Dashboard**. ![Add Chart Title](/pr-preview/pr-116/_astro/add-chart-to-dashboard-2.CH4qklWh_2mxpxW.webp) 3. View the chart in your dashboard. ![Chart in Dashboard](/pr-preview/pr-116/_astro/add-chart-to-dashboard-3.CbiY8Gpy_Z1K1cfx.webp) 🎉 Congratulations! Your chart is now saved and will automatically reload when you open the dashboard. ### Download a chart [Section titled “Download a chart”](#download-a-chart) Download a chart in the Explorer as follows: 1. Click the download button in the top-right corner. ![Download Chart](/pr-preview/pr-116/_astro/download-chart-explorer.BaT4xIUq_ZIpXxc.webp) 2. Choose **PNG** or **SVG** to save the chart as an image. You can also download a chart on a dashboard: 1. Click the three-dot menu in the top-right corner of the chart. 2. Click **Download** ![Download Chart](/pr-preview/pr-116/_astro/download-chart-dashboard.CDK7X36c_Z1KaSo7.webp) 3. Choose **PNG** or **SVG** to save the chart as an image. ⬇️ You have now successfully save the chart to your computer. Enjoy. ## Master essential charting techniques [Section titled “Master essential charting techniques”](#master-essential-charting-techniques) Now that you know how to create charts, let us explore some common techniques to enhance your charting skills. ### Plot counters as bar chart [Section titled “Plot counters as bar chart”](#plot-counters-as-bar-chart) A good use case for bar charts is visualization of counters of categorical values, because comparing bar heights is an effective way to gain a relative understanding of the data at hand. 1. **Shape your data**: Suppose you want to create a bar chart showing the outcomes of coin flips. First, generate a few observations: ```tql from {} repeat 20 set outcome = "heads" if random().round() == 1 else "tails" summarize outcome, n=count() ``` Sample output: ```tql {outcome: "tails", n: 9} {outcome: "heads", n: 11} ``` 2. **Plot the data**: Add the [`chart_bar`](/reference/operators/chart_bar) operator to visualize the counts. Map the outcome and count fields to the x-axis and y-axis: ```tql from {outcome: "tails", n: 9}, {outcome: "heads", n: 11} chart_bar x=outcome, y=n ``` ![Bar chart](/pr-preview/pr-116/_astro/chart-bar.DEYyg_Q2_2dYaw5.webp) ##### Group and stack bars [Section titled “Group and stack bars”](#group-and-stack-bars) Sometimes, your data has a third dimension. You can **group** multiple series into a single plot. Example with a `time` dimension: ```tql from ( {outcome: "tails", n: 9, time: "Morning"}, {outcome: "heads", n: 11, time: "Morning"}, {outcome: "tails", n: 14, time: "Afternoon"}, {outcome: "heads", n: 15, time: "Afternoon"}, {outcome: "tails", n: 4, time: "Evening"}, {outcome: "heads", n: 12, time: "Evening"}, ) chart_bar x=outcome, y=n, group=time ``` ![Grouped bar chart](/pr-preview/pr-116/_astro/chart-bar-grouped.CFNoOv9R_1zSNcF.webp) To **stack** the grouped bars, add `position="stacked"`: ```tql from ( {outcome: "tails", n: 9, time: "Morning"}, {outcome: "heads", n: 11, time: "Morning"}, {outcome: "tails", n: 14, time: "Afternoon"}, {outcome: "heads", n: 15, time: "Afternoon"}, {outcome: "tails", n: 4, time: "Evening"}, {outcome: "heads", n: 12, time: "Evening"}, ) chart_bar x=outcome, y=n, group=time, position="stacked" ``` ![Stacked bar chart](/pr-preview/pr-116/_astro/chart-bar-stacked.D14MtCVs_Zk0nsT.webp) #### Scale the y-axis logarithmically [Section titled “Scale the y-axis logarithmically”](#scale-the-y-axis-logarithmically) If your data spans several orders of magnitude, **log scaling** can make smaller values visible. Example without log scaling: ```tql from ( {outcome: "A", n: 3}, {outcome: "B", n: 5}, {outcome: "C", n: 10}, {outcome: "D", n: 21}, {outcome: "E", n: 10000}, ) chart_bar x=outcome, y=n ``` ![Unscaled bar chart](/pr-preview/pr-116/_astro/chart-bar-nolog.Bbkxdwf4_Z2mhied.webp) The large value (`E`) dominates the chart, hiding the smaller categories. Enable log scaling via `y_log=true` to reveal them: ```tql from ( {outcome: "A", n: 3}, {outcome: "B", n: 5}, {outcome: "C", n: 10}, {outcome: "D", n: 21}, {outcome: "E", n: 10000}, ) chart_bar x=outcome, y=n, y_log=true ``` ![Log-scaled bar chart](/pr-preview/pr-116/_astro/chart-bar-log.DkLhynLc_Z2yD6g.webp) Now, you can clearly see all the values! 👀 Interpreting Log-Scaled Plots Log scaling removes linearity. Comparing bar heights no longer reflects a simple numeric ratio. Stacked values are not additive anymore. ### Plot compositions as pie chart [Section titled “Plot compositions as pie chart”](#plot-compositions-as-pie-chart) Pie charts are well-understood and frequently occur in management dashboards. Let's plot some synthetic data with the [`chart_pie`](/reference/operators/chart_pie) operator: ```tql from ( {category: "A", percentage: 40}, {category: "B", percentage: 25}, {category: "C", percentage: 20}, {category: "D", percentage: 10}, {category: "E", percentage: 5}, ) chart_pie label=category, value=percentage ``` ![Pie chart](/pr-preview/pr-116/_astro/chart-pie.DZN_Mgb1_1ICXF9.webp) To provide a consistent user experience across all chart types, `chart_pie` treats `label` and `x` as interchangeable, as well as `value` and `y`. This mapping makes intuitive sense when you consider a pie chart as a bar chart rendered in a radial coordinate system. ### Plot metrics as line chart [Section titled “Plot metrics as line chart”](#plot-metrics-as-line-chart) Line charts come in handy when visualizing data trends over a continuous scale, such as time series data. 1. **Shape your data**: For our line chart demo, we'll use some internal node metrics provided by the [`metrics`](/reference/operators/metrics) operator. Let's look at the RAM usage of the node: ```tql metrics "process" drop swap_space, open_fds head 3 ``` ```plaintext {timestamp: 2025-04-27T18:16:17.692Z, current_memory_usage: 2363461632, peak_memory_usage: 4021136} {timestamp: 2025-04-27T18:16:18.693Z, current_memory_usage: 2366595072, peak_memory_usage: 4021136} {timestamp: 2025-04-27T18:16:19.694Z, current_memory_usage: 2385154048, peak_memory_usage: 4021136} ``` 2. **Plot the data**: Add the [`chart_line`](/reference/operators/chart_line) operator to visualize the time series. We are going to plot the memory usage within the last day: ```tql metrics "process" where timestamp > now() - 1d chart_line x=timestamp, y=current_memory_usage ``` ![Line chart](/pr-preview/pr-116/_astro/chart-line.BGCcmM4p_3j7Fx.webp) 3. **Aggregate to reduce the resolution**: Plotting metrics with a 1-second granularity over the course of a full day can make a line chart very noisy. In fact, we have a total of 86,400 samples in our plot. This can make a line chart quickly illegible. Let's reduce the noise by aggregating the samples into 15-min buckets: ```tql metrics "process" where timestamp > now() - 1d set timestamp = timestamp.round(15min) summarize timestamp, mem=mean(current_memory_usage) chart_line x=timestamp, y=mem ``` ![Line chart with summarize](/pr-preview/pr-116/_astro/chart-line-summarize.C-kldtdX_Z1eCx27.webp) This looks a lot smoother! **Pro tip**: you can even further [optimize the above pipeline](#optimize-plotting-with-inline-expressions) by using additional operator arguments. #### Compare multiple series [Section titled “Compare multiple series”](#compare-multiple-series) Our metrics data not only includes the current memory usage but also peak usage. Comparing the these two in the same chart helps us understand potentially dangerous spikes. Let's add that second series to the y-axis by upgrading from a single value to a record that represents the series. ```tql metrics "process" where timestamp > now() - 1d chart_line ( x=timestamp, y={current: mean(current_memory_usage), peak: max(peak_memory_usage * 1Ki)}, resolution=15min ) ``` ![Line chart with multiple series](/pr-preview/pr-116/_astro/chart-line-multiple.Dp7WqfNy_ZSeETJ.webp) Because `current_memory_usage` comes in gigabytes and `peak_memory_usage` in megabytes, we cannot compare them directly. Hence we normalized the peak usage to gigabytes to make them comparable in a single plot. If you cannot enumerate the series to plot statically in a record, use the `group` option to specify a field that contains a unique identifier per series. Here's an example that plots the number of events per pipeline: ```tql metrics "publish" chart_line ( x=timestamp, y=sum(events), x_min=now()-1d, group=pipeline_id, resolution=30min, ) ``` ### Plot distributions as area chart [Section titled “Plot distributions as area chart”](#plot-distributions-as-area-chart) Area charts are fantastic for visualizing quantities that accumulate over a continuous variable, such as time or value ranges. They are similar to line charts but emphasize the volume underneath the line. In the above section about line charts, you can exchange every call to [`chart_line`](/reference/operators/chart_line) with [`chart_area`](/reference/operators/chart_area) and will get a working plot. ```tql from ( {time: 1, a: 10, b: 20}, {time: 2, a: 8, b: 25}, {time: 3, a: 14, b: 30}, {time: 4, a: 10, b: 25}, {time: 5, a: 18, b: 40}, ) chart_area x=time, y={a: a, b: b} ``` ![Area chart](/pr-preview/pr-116/_astro/chart-area.qpx_UOJY_Z25z6KF.webp) The area under the curve gives you a strong visual impression of the total event volume over time. #### Stack multiple series [Section titled “Stack multiple series”](#stack-multiple-series) Like bar charts, area charts can display **stacked series**. This means that the values of the series add up, helping you *compare contributions* from different groups while still highlighting the overall cumulative shape. Pass `position="stacked" to see the difference`: ```tql from ( {time: 1, a: 10, b: 20}, {time: 2, a: 8, b: 25}, {time: 3, a: 14, b: 30}, {time: 4, a: 10, b: 25}, {time: 5, a: 18, b: 40}, ) chart_area x=time, y={a: a, b: b}, position="stacked" ``` ![Stacked area chart](/pr-preview/pr-116/_astro/chart-area-stacked.D1LdGZR3_1mFoEj.webp) Notice the difference in the y-axis interpretation: * Without stacking, the areas *overlap* each other. * With stacking, the areas become *disjoint* and *cumulatively add up* to the total height. ## Optimize Plotting with Inline Expressions [Section titled “Optimize Plotting with Inline Expressions”](#optimize-plotting-with-inline-expressions) While it's intuitive to *first* prepare your data and *then* start thinking about how to parameterize your chart operator, this leaves some opportunities for optimization and better results on the table. That's why the `chart_*` operators offer additional options for inline filtering, rounding, and summarization. These options allow you often to immediately jump to charting without having to think too much about data prepping. To appreciate these optimization, let's start with our metrics pipeline from the above line chart example: ```tql metrics "process" where timestamp > now() - 1d // filtering set timestamp = timestamp.round(15min) // rounding summarize timestamp, mem=mean(current_memory_usage) // aggregation chart_line x=timestamp, y=mem ``` ![Line chart with summarize](/pr-preview/pr-116/_astro/chart-line-summarize.C-kldtdX_Z1eCx27.webp) You can push the filtering, rounding, and aggregation into the chart operator: ```tql metrics "process" chart_line ( x=timestamp, y=mean(current_memory_usage), // aggregation resolution=15min, // rounding (= flooring) x_min=now() - 1d, // filtering ) ``` ![Line chart with resolution](/pr-preview/pr-116/_astro/chart-line-resolution.CA1-QTst_2R3uq.webp) Note how this make the pipeline more succinct by removing the extra `where`, `set`, and `summarize` operators. Here are some important details to remember: * The `x_min`/`x_max` (also `y_min`/`y_max`) options set the visible axis domain to fixed interval. Use these options to crop or expand the viewport. * When using the `x_min` or `x_max` options, the chart operator implicitly filters your data for the correct time range, just as if you had specified `where x >= floor(x_min, resolution) and x < ceil(x_max, resolution)`. This avoids needing to specify the ranges twice, and makes sure that the resolution is taken into account correctly. * Specifying a duration with `resolution` option creates nicer buckets than naïve rounding, as it uses a dynamic floor for the minimum and a ceiling for the maximum value in the respective bucket. This results in nicer axis ticks that align on the bucket boundary, e.g., hourly or daily. * The combination of `resolution` with an [aggregation function](/reference/functions#aggregation) for the `y` series is equivalent to the manual `summarize`. * When you use `resolution`, you can additionally use the `fill` option to patch up missing values with a provided value, e.g., `fill=0` replaces otherwise empty buckets with a data point in the plot.

# Write a package

This tutorial walks you through building a package for an SSL blacklist. Packages bundle pipelines, operators, contexts, and examples. You can [install packages](/guides/basic-usage/install-a-package) from the [Tenzir Library](https://app.tenzir.com/library) or deploy them as code. ## Map the use case [Section titled “Map the use case”](#map-the-use-case) We’ll pick an example from the SecOps space: detect malicious certificates listed on the [SSLBL blocklist](https://sslbl.abuse.ch/). This involves three primary actions: 1. Build a lookup table of SHA-1 hashes that mirror SSLBL data. 2. Extract SHA1 hashes of certificates in OCSF Network Activity events and compare them against the lookup table. 3. Tag matching events with the OCSF OSINT profile, so that downstream tools can escalate the match into an alert or detection finding. We’ll begin with the managing the lookup table. But first we need to get the package scaffolding in place. ## Create the package scaffold [Section titled “Create the package scaffold”](#create-the-package-scaffold) Create a directory named `sslbl` and add the standard package layout: ## Add the package manifest [Section titled “Add the package manifest”](#add-the-package-manifest) The [`package.yaml`](/packages/sslbl/package.yaml) is the **package manifest**. I contains descriptive metadata, but also the definitions of contexts and inputs, as we shall see below. ### Add descriptive metadata [Section titled “Add descriptive metadata”](#add-descriptive-metadata) sslbl/package.yaml ```yaml name: SSLBL author: Tenzir author_icon: https://github.com/tenzir.png package_icon: | https://raw.githubusercontent.com/tenzir/library/main/sslbl/package.svg description: | The [SSLBL](https://sslbl.abuse.ch/) package provides a lookup table with SHA1 hashes of blacklisted certificates for TLS monitoring use cases. ``` ### Define the lookup table context [Section titled “Define the lookup table context”](#define-the-lookup-table-context) Next, add a lookup table context to the manifest. The node creates the context when you install the package. sslbl/package.yaml ```yaml contexts: sslbl: type: lookup-table description: | A table keyed by SHA1 hashes of SSL certificates on the SSL blocklist. ``` ## Add user-defined operators [Section titled “Add user-defined operators”](#add-user-defined-operators) Packages give you the ability to implement **user-defined operators** that live right next to Tenzir’s [built-in operators](/reference/operators). These custom operators are an essential capability to scale your data processing, as you can break down complex operations into smaller, testable building blocks. ### Create the user-defined operators [Section titled “Create the user-defined operators”](#create-the-user-defined-operators) First, we create an operator that fetches the latest SSL blocklist from the [SSLBL](https://sslbl.abuse.ch/) website. The operator in (/packages/sslbl/operators/fetch.tql) looks as follows: operators/fetch.tql ```tql from_http "https://sslbl.abuse.ch/blacklist/sslblacklist.csv" { read_csv comments=true, header="timestamp,SHA1,reason" } ``` The relative path in the packagage defines the operator name. After installing the package, you can call this operator via `sslbl::fetch`. It will produce events of this shape: ```tql { timestamp: 2014-05-04T08:09:56Z, SHA1: "b08a4939fb88f375a2757eaddc47b1fb8b554439", reason: "Shylock C&C", } ``` Let’s create another operator to map this data to [OSINT objects](https://schema.ocsf.io/1.6.0/objects/osint)—the standardized representation of an indicators of compromise (IOCs) in OCSF. operators/ocsf/to\_osint.tql ```tql confidence = "High" confidence_id = 3 created_time = move timestamp malware = [{ classification_ids: [3], classifications: ["Bot"], // The source lists the name as "$NAME C&C" and we drop the C&C suffix. name: (move reason).split(" ")[0], }] value = move SHA1 type = "Hash" type_id = 4 ``` This pipeline translates the original feed into this shape: ```tql { confidence: "High", confidence_id: 3, created_time: 2014-05-04T08:09:56Z, malware: [ { classification_ids: [ 3, ], classifications: [ "Bot", ], name: "Shylock", }, ], value: "b08a4939fb88f375a2757eaddc47b1fb8b554439", type: "Hash", type_id: 4, } ``` We’re not done yet. Let’s create one final operator that wraps a single fetch into an OCSF event that describes a single collection of IoCs: the [OSINT Inventory Info](https://schema.ocsf.io/1.6.0/classes/osint_inventory_info) event. operators/ocsf/to\_osint\_inventory\_info.tql ```tql // Collect a single fetch into an array of OSINT objects. summarize osint=collect(this) // Categorization activity_id = 2 activity_name = "Collect" category_uid = 5 category_name = "Discovery" class_uid = 5021 class_name = "OSINT Inventory Info" severity_id = 1 severity = "Informational" type_uid = class_uid * 100 + activity_id // Additional context attributes actor = { app_name: "Tenzir" } metadata = { product: { name: "SSLBL SSL Certificate Blacklist", vendor_name: "abuse.ch", }, version: "1.6.0", } // Occurence attributes time = now() // Apply Tenzir event metadata @name = "ocsf.osint_inventory_info" ``` We can now call all three operators in one shot to construct an OCSF event: ```tql sslbl::fetch sslbl::ocsf::to_osint sslbl::ocsf::to_osint_inventory_info ``` Now that we have building blocks, let’s combine them into something meaningful. ## Add deployable pipelines [Section titled “Add deployable pipelines”](#add-deployable-pipelines) With our user-defined operators, we get building blocks, but not yet entire pipelines. We now create a pipeline that downloads SSLBL data and updates the context periodically so that we always have the latest version of the SSLBL data for enrichment. The `sslbl::fetch` operator just downloads the blacklist entries once. But the remote data source changes periodically, and we want to always work with the latest version. So we turn the one-shot download into a continuous data feed using the [`every`](/reference/operators/every) operator: sslbl/pipelines/publish-as-ocsf.tql ```tql every 1h { sslbl::fetch } sslbl::ocsf::to_osint sslbl::ocsf::to_osint_inventory_info publish "ocsf" ``` This is a closed pipeline, meaning, it has an input operator ([`every`](/reference/operators/every)) and an output operator ([`publish`](/reference/operators/publish)). The pipeline produces a new OCSF Inventory Info event every hour and publishes it to the `ocsf` topic so that other pipelines in the same node can consume it. This is a best-practice design pattern to expose data that you may reuse multiple times. But instead of publishing the data as OCSF events and subscribing to it afterwards, we can directly update the lookup table from the plain OSINT objects: sslbl/pipelines/update-lookup-table.tql ```tql every 1h { sslbl::fetch } sslbl::ocsf::to_osint context::update "sslbl", key=value ``` Thanks to our user-defined operators, implementing these two different pipelines doesn’t take much effort. ## Add examples [Section titled “Add examples”](#add-examples) To illustrate how others can use the package, we encourage package authors to add a few TQL snippets to the `examples` directory in the package. ### Example 1: One-shot lookup table update [Section titled “Example 1: One-shot lookup table update”](#example-1-one-shot-lookup-table-update) Here’s a snippet that perform a single fetch followed by an update of the lookup table: examples/one-shot-update.tql ```tql --- description: | This example demonstrates how to fetch SSLBL data, convert it to OSINT format, and update the context with the new data. --- sslbl::fetch sslbl::ocsf::to_osint context::update "sslbl", key=value ``` ### Example 2: Enrich with the context [Section titled “Example 2: Enrich with the context”](#example-2-enrich-with-the-context) What do we do with feed of SHA1 hashes that correspond to bad certificates? One natural use case is to look at TLS traffic and compare these values with the SHA1 hashes in the feed. Here’s a pipeline for this: examples/enrich-network-activity.tql ```tql --- description: | Subscribe to all OCSF events and extract those network events containing SHA-1 certificate hashes. Correlate those events with the SSLBL database and attach the OSINT profile to matching events. --- subscribe "ocsf" where category_uid == 4 // Filter out network events that have SHA1 certificate hashes. where not tls?.certificate?.fingerprints?.where(x => x.algorithm_id == 2).is_empty() // Convert the list of SHA1 hashes into a record for enrichment. In the future, // we'd want to enrich also within arrays. When we unroll we unfortunately lose // all other certificate hash values, so this is sub-optimal. unroll tls.certificate.fingerprints enrich "sslbl", key=tls.certificate.fingerprints.value, into=_tmp // Slap OSINT profile onto the event on match. if _tmp != null { osint.add(move _tmp) metadata.profiles?.add("osint") } else { drop _tmp } publish "ocsf-osint" ``` This pipelines hones in on OCSF Network Activity events (`category_uid == 4`) that come with a SHA1 TLS certificate fingerprint (`algorithm_id == 2`). If we have a matche, we add the `osint` profile to the event and publish it to separate topic `ocsf-osint` for further processing. ### Example 3: Show a summary of the dataset [Section titled “Example 3: Show a summary of the dataset”](#example-3-show-a-summary-of-the-dataset) examples/top-malware.tql ```tql --- description: | Inspect all data in the context and count the different malware types, rendering the result as a pie chart. --- context::inspect "sslbl" select malware = value.malware[0].name top malware chart_pie x=malware, y=count ``` ## Make your package configurable [Section titled “Make your package configurable”](#make-your-package-configurable) To make a package more reusable, **inputs** offer a simple templating mechanism to replace variables with user-provided values. For example, to replace the hard-coded 1-hour refresh cadence with an input, replace the value with `{{ inputs.refresh_interval }}` in the above pipeline: sslbl/pipelines/update-as-ocsf.tql ```tql every {{ inputs.refresh_interval }} { sslbl::fetch } sslbl::ocsf::to_osint context::update "sslbl", key=value ``` Then add the input to your `package.yaml`: sslbl/package.yaml ```yaml inputs: refresh_interval: name: Time between context updates description: | How often the pipeline refreshes the SSLBL lookup table. default: 1h ``` Users can accept the default or override the value [during installation](/guides/basic-usage/install-a-package), e.g., when using [`package::add`](/reference/operators/package/add): ```tql package::add "/path/to/sslbl", inputs={refresh_interval: 24h} ``` ## Test your package [Section titled “Test your package”](#test-your-package) Testing ensures that you always have a working package during development. The earlier you start, the better! ### Add tests for your operators [Section titled “Add tests for your operators”](#add-tests-for-your-operators) Since our package ships with user-defined operators, we highly recommend to write tests for them, for the following reasons: 1. You help users gain confidence in the functionality. 2. You provide illustrative input-output pairs. 3. You evolve faster with less regressions. Yes, writing tests is often boring and cumbersome. But it doesn’t have to be that way! With our purpose-built [test framework](/reference/test-framework) for the Tenzir ecosystem, it is actually fun. 🕺 Let’s bootstrap the `tests` directory: ```sh mkdir tests mkdir tests/inputs # files we reference from tests ``` We’ll put a trimmed version of <https://sslbl.abuse.ch/blacklist/sslblacklist.csv> into `tests/inputs/`: tests/inputs/sslblacklist.csv ```csv ################################################################ # abuse.ch SSLBL SSL Certificate Blacklist (SHA1 Fingerprints) # # Last updated: 2025-10-08 06:32:12 UTC # # # # Terms Of Use: https://sslbl.abuse.ch/blacklist/ # # For questions please contact sslbl [at] abuse.ch # ################################################################ # # Listingdate,SHA1,Listingreason 2025-10-08 06:32:12,e8f4490420d0b0fc554d1296a8e9d5c35eb2b36e,Vidar C&C 2025-10-08 06:12:48,d9b07483491c0748a479308b29c5c754b92d6e06,ACRStealer C&C 2025-10-08 06:10:17,31740cfc82d05c82280fd6cce503543a150e861f,Rhadamanthys C&C 2025-10-08 06:09:30,302ed4eeb28e1e8ca560c645b8eb342498300134,Rhadamanthys C&C 2025-10-08 06:08:57,eaf3a17e7f86a626afcfce9f4a85ac20a7f62a67,Rhadamanthys C&C 2025-10-07 18:25:40,5b7f9db9187f5ccfaf1bcdb49f9d1db0396dabde,ACRStealer C&C 2025-10-07 18:24:31,70cf9d9812b38101361bd8855274bf1766840837,OffLoader C&C 2025-10-07 18:24:30,6e4ea638522aa0f5f6e7f14571f5ff89826f6e07,OffLoader C&C 2025-10-07 06:15:35,fc0afaa2e30b121c2d929827afd69e4c63669ac3,ACRStealer C&C 2025-10-07 06:14:59,7948950f56e714a3ecf853664ffa65ae86a70051,QuasarRAT C&C 2025-10-07 06:14:51,1fcb051a4dfa5999fee4877bbd4141f22b3a4074,AsyncRAT C&C 2025-10-07 05:53:50,e5632523b028288923c241251c9f1b6275e3db61,OffLoader C&C 2025-10-07 05:53:49,8b6d220268c0c6206f4164fb8422bac2653924b4,OffLoader C&C 2025-10-07 05:53:07,9d686b89970266a03647f2118f143ecb47f59188,Rhadamanthys C&C 2025-10-07 05:52:18,89e31fdaeefdaddcf0554e3b3e2d48c1b683abbd,Rhadamanthys C&C 2025-10-07 05:51:35,79a805627bc3b86aa42272728209fc28019de02f,Rhadamanthys C&C 2025-10-06 11:01:21,fc55fdafe2f70b2f409bbf7f89613ca2ca915181,QuasarRAT C&C 2025-10-06 06:44:36,6f932e3a0bf05164eb2bf02cfb5a29c1b210ebb2,Mythic C&C 2025-10-06 06:44:31,8f442060bb10d59eca5d8c9b7cd6d8653a3c3ac8,Vidar C&C 2025-10-06 06:44:27,7cd14588ba5bbeb56ce42f05023bd2f5159d8f19,Vidar C&C 2025-10-06 06:42:52,b36f4c106bf048b68b43b448835f5dab1c982083,QuasarRAT C&C 2025-10-04 10:46:00,bf0b77de0046e53200e5287be0916f6921e86336,ACRStealer C&C 2025-10-04 10:44:54,e33db1f571fdb786ecc8e5a4b43131fdcbe2c0d1,ACRStealer C&C 2025-10-04 10:44:38,c75f347253ebf8a4c26053d2db7ce5bf3e1417f5,QuasarRAT C&C 2025-10-04 10:43:27,fc216972dbec1903de5622107dc97b6c33535074,QuasarRAT C&C 2025-10-04 10:43:21,2e14c326f940b962e2ecefe69a5ee4fc12d26cab,AsyncRAT C&C ``` Let’s test the operator that maps our input to OCSF OSINT objects: tests/ocsf/to\_osint.tql ```tql from_file f"{env("TENZIR_INPUTS")}/sslblacklist.csv" { read_csv comments=true, header="timestamp,SHA1,reason" } sslbl::ocsf::to_osint head 1 ``` We first watch the terminal output it in passthrough mode: ```sh uvx tenzir-test --passthrough ``` tests/ocsf/to\_osint.txt ```tql { confidence: "High", confidence_id: 3, created_time: 2025-10-08T06:32:12Z, malware: [ { classification_ids: [ 3, ], classifications: [ "Bot", ], name: "Vidar", }, ], value: "e8f4490420d0b0fc554d1296a8e9d5c35eb2b36e", type: "Hash", type_id: 4, } ``` As expected, a valid OCSF OSINT object. Let’s make confirm this as our new baseline: ```sh uvx tenzir-test --update ``` This created a [`to_osint.txt`](/packages/sslbl/tests/ocsf/to_osint.txt) file next to the [`to_osint.tql`](/packages/sslbl/tests/ocsf/to_osint.tql) file. Future runs will use this baseline for comparisons. Continue to test the remaining operators, or add additional tests for some examples. ### Test contexts and node interaction [Section titled “Test contexts and node interaction”](#test-contexts-and-node-interaction) Our package defines a context that lives in a node. Writing tests with nodes is a bit more involved than writing tests for stateless operators that we can simply run through the `tenzir` binary. To test node interactions, we need to use the **suite** concept from the test framework, which spins up a single fixture and then executes a series of tests sequentially against that fixture. With that capability, we can finally test the multi-stage process of updating the context, inspecting it, and using it for an enrichment. Defining a suite doesn’t take much, just add a `test.yaml` file to a sub-directory that represents the suite of tests. We’ll do this: Here’s the test suite definition: tests/context/test.yaml ```yaml suite: context fixtures: [node] timeout: 30 ``` Let’s take a look at our tests: tests/context/01-context-list.tql ```tql // Ensure the package installed the context properly. context::list ``` Then load our input into the node’s lookup table: tests/context/02-context-update.tql ```tql from_file f"{env("TENZIR_INPUTS")}/sslblacklist.csv" { read_csv comments=true, header="timestamp,SHA1,reason" } context::update "sslbl", key=SHA1, value={time: timestamp, malware: reason} ``` Finally ensure that the lookup table has the expected values: tests/context/03-context-inspect.tql ```tql // Summarize the context. context::inspect "sslbl" select malware = value.malware top malware ``` ## Share and contribute [Section titled “Share and contribute”](#share-and-contribute) Phew, you made it! You now have a reusable package. 🎉 Now that you have a package, what’s next? 1. Join our [Discord server](/discord) and showcase the package in the `show-and-tell` channel to gather feedback. 2. If you deem it useful for everyone, open a pull request in our [Community Library on GitHub](https://github.com/tenzir/library). Packages from this library appear automatically in the [Tenzir Library](https://app.tenzir.com/library). 3. Spread the word on social media and tag us so we can amplify it.