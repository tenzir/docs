# Generated by Claude Opus 4 from ../src/content/docs

SUMMARY = """

# Tenzir Documentation Summary

## Overview

**Tenzir** is a data pipeline engine designed specifically for security teams. It provides capabilities to collect, shape, normalize, optimize, enrich, store, replay, and route telemetry data through a powerful pipeline-based architecture.

## Core Components

### 1. Architecture

Tenzir's architecture consists of three primary abstractions:

#### **Pipelines**
- Chains of operators that represent dataflows
- Three types of operators:
  - **Inputs**: Produce data (void-to-events or void-to-bytes)
  - **Outputs**: Consume data (events-to-void or bytes-to-void)
  - **Transformations**: Both consume and produce data
- Support multi-schema dataflows with automatic batching
- Built-in networking capabilities for creating distributed data fabrics
- Support both live stream processing and historical queries seamlessly

#### **Nodes**
- Running processes that manage and execute pipelines
- Can run pipelines as separate subprocesses (default) or within the same process
- Provide REST API for pipeline management
- Include lightweight storage engine with catalog and sparse indexes
- Can operate standalone or connect to the platform

#### **Platform**
- Higher-level management layer for fleet management
- Provides web interface (app.tenzir.com)
- Features user/workspace administration, IdP authentication, and dashboards
- Available in different editions (Community, Professional, Enterprise, Sovereign)

### 2. Tenzir Query Language (TQL)

TQL is the language for writing pipelines with a Unix-like philosophy of composing operators. The language uses `|` (pipe) to chain operators together, similar to Unix shell commands.

#### Basic Syntax and Pipeline Structure

**Pipeline Composition:**
```tql
from "source" | operator1 | operator2 | to "destination"
```

Pipelines follow a data flow pattern:
- **Input** operators produce data (from sources)
- **Transformation** operators modify data
- **Output** operators consume data (to destinations)

#### Key Language Features:

**Expressions:**
- Rich literal types including IP addresses, subnets, durations, and timestamps
- Format strings (f-strings) for flexible string construction
- Field access with dot notation and optional chaining
- Arithmetic, logical, and relational operations
- Lambda expressions for functional operations
- Conditional expressions with `if/else`

**Statements:**
- Operator invocations with positional and named arguments
- Assignment statements for field manipulation
- `if` statements for conditional routing
- `let` statements for defining constants

**Type System:**
- Superset of JSON with additional types
- Basic types: bool, int64, uint64, double, duration, time, string, blob, ip, subnet, secret
- Complex types: enum, list, record
- All types are nullable by default
- Lossless mapping to Apache Arrow format

#### TQL Examples and Common Patterns

**1. Basic Data Filtering and Transformation**
```tql
// Read JSON logs and filter by severity
from_file "logs/*.json"
| read_json
| where severity >= 4
| select timestamp, host, message, severity
```

**2. Field Manipulation and Assignment**
```tql
// Enrich events with calculated fields
from_kafka topic="events"
| read_json
| category = "security"
| risk_score = severity * impact
| formatted_message = f"[{severity}] {host}: {message}"
```

**3. Conditional Processing**
```tql
// Route events based on conditions
from_syslog "514/udp"
| if severity >= 5 {
    alert_type = "critical"
    to_splunk "critical-events"
  } else if severity >= 3 {
    alert_type = "warning"
    to_s3 "s3://logs/warnings/"
  } else {
    drop alert_type
    to_file "low-priority.json"
  }
```

**4. Aggregation and Analytics**
```tql
// Count events by host and calculate statistics
from_http "0.0.0.0:8080"
| read_json
| summarize 
    count = count(),
    avg_response_time = mean(response_time),
    max_response_time = max(response_time)
  by host, status_code
| where count > 100
| sort count desc
```

**5. Time-based Operations**
```tql
// Use time functions and constants
let $start = now() - 1h
let $window = 5min

from "s3://logs/today/"
| read_parquet
| where timestamp >= $start
| timeshift timestamp, -$window
| summarize events=count() by slot=floor(timestamp, $window)
```

**6. Working with Complex Data Types**
```tql
// Handle nested structures and lists
from_file "events.json"
| read_json
| unroll tags                    // Expand list into individual events
| where tags.name == "suspicious"
| user_info = {                  // Create nested record
    name: user.first + " " + user.last,
    email: user.email,
    roles: user.permissions.map(p => p.name)
  }
```

**7. String Operations and Parsing**
```tql
// Parse and manipulate strings
from_tcp "0.0.0.0:5514"
| read_lines
| message = parse_kv(raw_line)   // Parse key=value pairs
| url_parts = url.split("/")
| domain = url_parts[2]
| where domain.ends_with(".suspicious.com")
```

**8. Multi-Source Data Collection**
```tql
// Combine data from multiple sources
publish "normalized" {
  from_kafka topic="app_logs" 
  | read_json 
  | set source="application"
}

publish "normalized" {
  from_syslog "514/udp" 
  | set source="syslog"
}

subscribe "normalized"
| where timestamp > now() - 10min
| to_opensearch "https://elastic:9200"
```

**9. Real-time Enrichment**
```tql
// Create lookup table and enrich events
context::create_lookup_table "threat_intel"

// Populate context
from_http "https://api.threats.com/indicators"
| read_json
| context::update "threat_intel", key=indicator, value=threat_data

// Enrich incoming events
from_kafka topic="network_logs"
| read_json
| context::enrich "threat_intel", key=dest_ip
| where threat_data != null
| to_splunk "threats"
```

**10. Detection with External Rules**
```tql
// Apply Sigma rules for detection
from_file "/var/log/security/*.json"
| read_json
| sigma "/rules/persistence/"
| where rule.level == "high"
| alert = f"Detection: {rule.title} on {hostname}"
| to_email "soc@company.com", subject=alert
```

### 3. Data Processing Features

#### **Multi-Schema Support**
- Single pipeline can handle events with different schemas
- Automatic batching optimizations for performance
- "Ordering pushdown" for efficient processing

#### **Enrichment Framework**
- **Contexts**: Stateful objects for enrichment
- Three context types:
  - **Lookup Tables**: Hash tables with advanced features
    - Longest-prefix subnet matching
    - Per-key expiration timeouts (create/write/read)
    - Aggregation functions as values
  - **Bloom Filters**: Space-efficient set membership testing
  - **GeoIP Database**: IP-to-location mapping using MMDB format
- In-band enrichment mode (with out-of-band and hybrid planned)

#### **Edge Storage**
- Built-in storage engine at nodes
- Parquet/Feather file formats
- Sparse indexes for query optimization
- Predicate pushdown for efficient querying
- Import/export operators for data persistence

### 4. Configuration & Deployment

#### **Configuration Hierarchy** (in order of precedence):
1. Command-line arguments
2. Environment variables (TENZIR_* prefix)
3. Configuration files (YAML format)
4. Compile-time defaults

#### **Plugin Architecture**
- Dynamic plugins (shared libraries) and static plugins (compiled-in)
- Configurable plugin loading and blocking
- Plugin-specific configuration support

#### **Packages**
- Sets of pipelines and contexts deployed as a unit
- Include metadata, inputs (template variables), pipelines, contexts, and examples
- Support for versioning and configuration overrides
- Infrastructure as Code (IaC) style deployment

## Key Differentiators

1. **Unified Processing**: Same pipeline syntax works for both streaming and historical data
2. **Multi-Schema Flexibility**: Handle heterogeneous data in a single pipeline
3. **Security-Focused**: Purpose-built for security telemetry with relevant operators
4. **Distributed Architecture**: Built-in networking for creating data fabrics
5. **Rich Type System**: Native support for security-relevant types (IP, subnet, etc.)
6. **Flexible Enrichment**: Powerful context system with dynamic updates

## Documentation Structure

The documentation is organized into four main sections:
- **Guides**: Step-by-step instructions for specific tasks
- **Reference**: Technical specifications of operators, functions, and APIs
- **Explanations**: Conceptual overviews of Tenzir components
- **Tutorials**: Learning-oriented lessons for getting started

## Operator and Function Reference

### Operator Categories

Tenzir provides over 200 operators organized into these categories:

1. **Data Input/Output**: Cloud storage, message queues, network protocols, files, and integrations (S3, Kafka, HTTP, Splunk, OpenSearch, etc.)
2. **Parsing and Printing**: Format converters for JSON, CSV, YAML, Parquet, PCAP, security formats (CEF, LEEF, Suricata, Zeek)
3. **Data Transformation**: Field manipulation (select, drop, set), filtering (where, head, tail), analytics (summarize, sort, top)
4. **Flow Control**: Pipeline routing (fork, load_balance), timing (delay, throttle), execution context (local, remote, buffer)
5. **Context Operations**: Enrichment with lookup tables, bloom filters, and GeoIP databases
6. **Detection and Security**: Integration with Sigma and YARA rules
7. **Pipeline Management**: Lifecycle control, monitoring, and package management
8. **Inter-Pipeline Communication**: Pub/sub with publish and subscribe operators

### Function Categories

Tenzir provides over 200 functions organized by purpose:

1. **Aggregation**: Statistical functions (count, sum, mean, median), advanced analytics (quantile, entropy)
2. **String Operations**: Transformation, trimming, splitting, regex operations, validation
3. **List and Record**: Manipulation, access, and transformation of complex data structures
4. **Time & Date**: Extraction, conversion, formatting, and special time functions
5. **Type Conversion**: Casting between types including security-specific types (IP, subnet)
6. **Encoding/Hashing**: Base64, hex, URL encoding, and cryptographic hashing
7. **Network/IP**: Classification, analysis, and security functions for IP addresses
8. **Parsing/Printing**: Format-specific parsing and printing functions
9. **System and Utility**: Environment access, file operations, UUID generation

For complete documentation, see:
- [Operator Reference](/reference/operators)
- [Function Reference](/reference/functions)

## Available Resources

- **Quickstart**: Interactive demo node at /guides/quickstart
- **Community Library**: Free packages at github.com/tenzir/library
- **Discord Community**: Support and discussion forum
- **Hosted Platform**: app.tenzir.com (with self-hosting options)

"""