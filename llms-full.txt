<SYSTEM>This is the full developer documentation for Tenzir</SYSTEM>

# Tenzir Documentation

> The data pipeline engine for security teams

**Tenzir** is the data pipeline engine for security teams. Our pipelines collect, shape, normalize, optimize, enrich, store, replay, and route your telemetry data.

![End-to-end pipeline](/_astro/end-to-end-pipeline.CicYXk_o_QxI6y.svg)

## How are the docs organized?

[Section titled “How are the docs organized?”](#how-are-the-docs-organized)

The Tenzir documentation consists of four different types of materials:

<!-- The SVG image -->

![Documentation structure](/docs-structure.svg)

<!-- Clickable overlay areas -->

<!-- Tutorials (top-left) -->

[](/tutorials/)

<!-- Guides (top-right) -->

[](/guides/)

<!-- Explanations (bottom-left) -->

[](/explanations/)

<!-- Reference (bottom-right) -->

[](/reference/)

* **Guides**: Practical step-by-step explanation to help you achieve a specific goal. Most useful when you're trying to get something done.
* **Reference**: Nitty-gritty technical descriptions of how Tenzir works. Most useful when you need detailed information about Tenzir's building blocks.
* **Explanations**: Big-picture explanations of higher-level Tenzir concepts. Most useful for building understanding of a particular topic.
* **Tutorial**: Learning-oriented lesson that take you through a series of steps to complete a project. Most useful when you want to get started with Tenzir.

## How do I start?

[Section titled “How do I start?”](#how-do-i-start)

Ready to dive in?

Get your fingers dirty and explore the demo node with just a few clicks and a guided tour. Security data has never been easier.

[Quickstart](guides/quickstart)

New to Tenzir?

Learn the basics of pipelines and get familiar with the Tenzir Query Language (TQL). You'll master security data in no time.

[Learn TQL](explanations/architecture/pipeline)

More questions? Need answers?

Tenzir is a community-driven open-source project. Join our friendly Discord server to chat with security data enthusiasts and the Tenzir team.

[Join Discord](/discord)

# Changelog

Welcome to the Tenzir changelog hub! Here you can find release notes, feature updates, and behind-the-scenes improvements across our two main products.

We maintain separate changelogs for each of our offerings to help you stay up to date with what matters most to you:

Tenzir Node

The core dataflow engine that powers your pipelines. We track all enhancements, bug fixes, and breaking changes in its dedicated changelog.

[Tenzir Node Changelog](/changelog/node)

Tenzir Platform

Our centralized control plane and web interface for managing your nodes and pipelines. Its changelog covers UI improvements, deployment updates, and platform-level capabilities.

[Tenzir Platform Changelog](/changelog/platform)

***

For general release announcements and deeper dives into selected features, check out our [blog](https://tenzir.com/blog) or join the conversation on our [Discord](https://tenzir.com/discord).

# Explanations

**Explanations** are big-picture explanations of higher-level Tenzir concepts. They are most useful for building understanding of a particular topic.

<!-- The SVG image -->

![Documentation structure](/explanations.svg)

<!-- Clickable overlay areas -->

<!-- Tutorials (top-left) -->

[](/tutorials/)

<!-- Guides (top-right) -->

[](/guides/)

<!-- Explanations (bottom-left) -->

<!-- Reference (bottom-right) -->

[](/reference/)

# Overview

Tenzir’s architecture consists three primary abstractions:

1. [**Pipeline**](/explanations/architecture/pipeline): A sequence of operators responsible for loading, parsing, transforming, and routing data. Pipelines are the core mechanism for data processing.
2. [**Node**](/explanations/architecture/node): A running process that manages and executes pipelines.
3. [**Platform**](/explanations/architecture/platform): A higher-level management layer that provides oversight and control over multiple nodes.

Here’s how they relate schematically:

![Pipelines, Nodes, Platform](/_astro/pipelines-nodes-platform.tEUv8QT0_19DKCs.svg)

When a node starts, it will automatically attempt to connect to the platform, giving you a seamless way to manage and deploy pipelines through a web interface. However, using the platform is optional—pipelines can also be controlled directly via the node’s API with a CRUD-style approach.

The platform, beyond pipeline management, offers user and workspace administration, authentication support via external identity providers (IdP), and dashboards that consists of charts.

Tenzir hosts one instance of the platform at [app.tenzir.com](https://app.tenzir.com), but you can also [deploy the platform on premises](/guides/platform-setup) in fully air-gapped environments.

# Node

A **node** is a running process that manages and executes pipelines.

When a node starts, it will automatically attempt to connect to the [platform](/explanations/architecture/platform), giving you a seamless way to manage and deploy pipelines through a web interface. However, using the platform is optional—you can also be manually manage pipelines via the node’s [REST API](/reference/node/api).

## Standalone vs. Managed Pipeline Execution

[Section titled “Standalone vs. Managed Pipeline Execution”](#standalone-vs-managed-pipeline-execution)

To understand the benefits of a node, let’s first consider how you can run pipelines without one. You run a single pipeline directly from the command line using the `tenzir` binary:

![Standalone Execution](/_astro/standalone-execution.D-GySmuS_19DKCs.svg)

This *standalone execution* mode of a pipeline is ideal for ad-hoc data transformations akin to how one would use `jq`, but with much broader data handling capabilities.

For continuous and more dependable data processing, you will quickly realize that you also need scheduled execution, automatic restarting, monitoring of warnings/errors, and more advanced execution capabilities, like real-time enrichment with contextual data or correlation with historical data.

This is where a node comes into play, offering a vehicle to execute one or more pipelines in a managed fashion. You can spawn a node with the `tenzir-node` binary or by running the Docker container that contains this binary:

![Managed Execution](/_astro/managed-execution.C42wjIQe_19DKCs.svg)

## Pipeline Subprocesses

[Section titled “Pipeline Subprocesses”](#pipeline-subprocesses)

A node can execute pipelines in two different modes: as separate subprocesses or within the same `tenzir-node` process. By default, pipelines run as separate subprocesses. Each approach offers distinct advantages and trade-offs.

![Pipeline Subprocesses](/_astro/pipeline-subprocesses.BBG1UNAC_19DKCs.svg)

When pipelines run as separate subprocesses, they don’t share fate with each other. If one pipeline crashes, it won’t affect other running pipelines, which improves overall reliability. This isolation also enables better scaling across available CPU cores since each pipeline can utilize cores independently. Vertically scaling a node becomes more efficient as this approach reduces pressure on the node’s internal scheduler and improves overall system responsiveness. However, subprocess execution comes with costs: each process requires its own memory space and system resources, and the system must serialize data when crossing process boundaries.

In contrast, when pipelines run within the same `tenzir-node` process, they operate with less OS pressure due to running as a single process with a fixed number of threads and efficient user-level task scheduling. Pipelines can pass data directly without serialization at their boundaries, which can noticeably lower overall resource consumption. The trade-off here involves shared fate—a critical error in one pipeline could potentially affect the entire node process. Additionally, all pipelines must share the same process resources, which can create bottlenecks under heavy load and limit parallelism.

Choose between subprocess and in-process execution based on your specific requirements for reliability, performance, and resource efficiency. You can [configure this behavior](/guides/node-setup/configure-a-node#configure-pipeline-subprocesses) in your node settings.

# Pipeline

A Tenzir **pipeline** is a chain of **operators** that represents a dataflow. Operators are the atomic building blocks that produce, transform, or consume data. Think of them as Unix or Powershell commands where the result from one command is feeding into the next:

![Pipeline Chaining](/_astro/operator-chaining.CjiL40XZ_19DKCs.svg)

Our pipelines have 3 types of operators: **inputs** that produce data, **outputs** that consume data, and **transformations** that do both:

![Pipeline Structure](/_astro/operator-types.B4v1eusl_19DKCs.svg)

You write pipelines in the [Tenzir Query Language (TQL)](/explanations/language), a language that we developed from the ground up to concisely describe such dataflows.

Learn TQL

Head over to our [language documentation](/explanations/language) for an in-depth explanation of how TQL works. We’re continuing here with high-level architectural aspects of the pipeline execution model.

## Typed Operators

[Section titled “Typed Operators”](#typed-operators)

Tenzir pipelines operate both ond unstructured stream of bytes and typed event streams. The execution model ensures type safety while maintaining high performance through batching and parallel processing.

An operator has an **upstream** and **downstream** type:

![Upstream and Downstream Types](/_astro/operator-table.ChkIKyOz_19DKCs.svg)

This typing ensures pipelines are well-formed. Adjacent operators must have matching types: the downstream type of one operator must match the upstream type of the next, i.e., upstream/downstream types of adjacent operators have to match. Otherwise the pipeline is malformed.

With these operators as building blocks, you can create all kinds of pipelines, as long as they follow the two principal rules of (1) sequencing inputs, transformations, and outputs, and (2) ensuring that operator upstream/downstream types match. Here are examples of other valid pipeline variations:

![Operator Composition Examples](/_astro/operator-composition-variations.8EYlYgMP_19DKCs.svg)

## Multi-Schema Dataflows

[Section titled “Multi-Schema Dataflows”](#multi-schema-dataflows)

As mentioned above, pipelines can transport both *bytes* and *events*. Let’s go deeper into the details of Tenzir represents events. Every event that flows through a pipeline is part of a *data frame* with a schema. Internally, these data frames are represented as Apache Arrow record batches, encoding potentially of tens of thousands of events in a single block of data. This innate batching is the reason why the pipelines can achieve high throughput.

Unique about Tenzir’s pipeline executor is that a single pipeline can process events with *multiple schemas*. When you typically work with data frames, your workload runs on input with a fixed schema, e.g., when you query a database table. In Tenzir, schemas can change dynamically during the execution of a pipeline, much like document-oriented engines that work on JSON or have one-event-at-a-time processing semantics. Tenzir is unique in that it gives the user the feeling of operating on a single event at a time while hiding the structured data frame batching behind the scenes. Thus, Tenzir combines the performance of structured query engines with the flexibility of document-oriented engines, making it perfect fit for processing *semi-structured data* at scale:

![Structured vs document-oriented engines](/_astro/document-vs-structured.y0QQZq3T_19DKCs.svg)

The schema variance begins early in the data flow, where parsers emit events with changing schemas as they encounter changing fields. If an operator detects a schema changes, it creates a new batch of events. In terms of performance, the worst case for Tenzir is a ordered stream of schema-switching events, with every event having a new schema than the previous one. But even for those scenarios operators can efficiently build homogeneous batches when the inter-event order does not matter. Similar to predicate pushdown, Tenzir operators support *ordering pushdown* to signal to upstream operators that the event order only matters intra-schema but not inter-schema. In this case the operator transparently “demultiplex” a heterogeneous event stream into N homogeneous streams. The [`sort`](/reference/operators/sort) operator is an example of such an operator; it pushes its ordering requirements upstream, allowing parsers to efficiently create multiple streams events in parallel.

![Multi-schema Example](/_astro/multi-schema-example.Ddg0yxO5_19DKCs.svg)

Some operators only work with exactly one instance per schema internally, such as [`write_csv`](/reference/operators/write_csv), which first writes a header and then all subsequent rows have to adhere to the emitted schema. Such operators cannot handle events with changing schemas.

It’s important to mention that most of the time you don’t have to worry about schemas. They are there for you when you want to work with them, but it’s often enough to just specified the fields that you want to work with, e.g., `where id.orig_h in 10.0.0.0/8`, or `select src_ip, dest_ip, proto`. Schemas are inferred automatically in parsers, but you can also seed a parser with a schema that you define explicitly.

## Unified Live Stream Processing and Historical Queries

[Section titled “Unified Live Stream Processing and Historical Queries”](#unified-live-stream-processing-and-historical-queries)

Tenzir’s execution engine transparently processes both historical data and real-time event streams within a single, unified pipeline model. [TQL](/explanations/language) empowers you to switch between these workloads by simply changing the data source at the start of your pipeline.

![Unified Processing](/_astro/unified-processing.l9So0oFr_19DKCs.svg)

This design lets you reuse the same logic for exploring existing data and for deploying it on live streams, which streamlines the entire analytics workflow.

Each Tenzir Node includes a lightweight **edge storage** engine for efficient local data persistence. You interact with this storage engine using just two dedicated operators to store and retrieve data. The retrievial goes much beyond replay.

A naive interpretation would be that [`export`](/reference/operators/export) first retrieves all its data, which subsequent operators then filter. However, Tenzir actively optimizes this process using **predicate pushdown**. Before a pipeline runs, Tenzir pushes filter conditions from later stages down to the initial storage source. This allows the source to intelligently fetch only the necessary data, often using fast index lookups and avoiding costly full scans.

Tenzir’s unique edge storage engine enables this powerful optimization. The diagram below illustrates how the engine works:

![Database Architecture](/_astro/storage-engine-architecture.DQ2lar3W_19DKCs.svg)

The edge storage engine is not a traditional database but a lightweight **catalog** that maintains a thin indexing layer over immutable Apache Parquet and Feather files. It maintains **sparse indexes**, such as min-max synopses and Bloom filters, that act as a table of contents. These indexes allow the engine to quickly rule out data partitions that do not match a query’s filter, avoiding unnecessary scans. The catalog also tracks evolving schemas and provides a transactional interface for partition operations.

Because the engine handles these optimizations automatically, the same pipeline logic can be seamlessly repurposed. A pipeline developed for historical analysis can be deployed on a live data stream by simply exchanging the historical data source for a streaming one. This unified model streamlines the path from interactive exploration to production deployment.

Federated Search

The Tenzir pipeline execution engine leverages powerful optimizations, such as predicate, limit, and ordering pushdowns. These optimizations are propagated to any pipeline source, including operators that fetch data from remote storage layers, databases, or SIEMs. This process enables efficient **federated search** across distributed systems and is a transparent, fundamental capability of the engine.

# Platform

The **platform** provides *fleet management* for [nodes](/explanations/architecture/node). With an API and web interface, the platform offers user and workspace administration, authentication via external identity providers (IdP), and dashboards consisting of pipeline-powered charts.

There exist three primary entities in the platform:

1. **Users**: Authenticated by an Identity Provider (IdP)
2. **Organizations**: Manage billing/licencse, members, and workspaces
3. **Workspace**: A logical grouping of nodes, secrets, and dashboards.

The diagram below illustrates their relationship.

![Platform Components](/_astro/platform-components.Bpqj86Ga_19DKCs.svg)

A user inside an organization is called a **member**. The organization configures what members have access to what workspaces.

Organizations

Organizations are not available in the free **Community Edition**. Please see [tenzir.com/pricing](https://tenzir.com/pricing) for a detailed feature comparison.

## Data Model

[Section titled “Data Model”](#data-model)

The following diagram visualizes the platform’s data model (highlighted) and how the entities relate to each other with respect to their multiplicities.

![Platform Data Model](/_astro/platform-data-model.BNks0SgU_19DKCs.svg)

It’s important to note that a node can only be part of one workspace. There is no support for “multi-homing” as it would create non-trivial questions about how to reconcile secrets and permissions from multiple workspaces.

## Deployment Modes

[Section titled “Deployment Modes”](#deployment-modes)

Based on the [Edition](https://tenzir.com/pricing) of Tenzir, you have different deployment modes of the platform. The below diagram illustrates the variants.

![Deployment Modes](/_astro/deployment-modes.CUEE2kAI_19DKCs.svg)

* **Community Edition**: geared towards single-user deployments, the Community Edition only associates a personal workspace with every user.
* **Professional Edition**: geared towards small-business deployments, the Professional Edition features organizations for allowing multiple users to collaborate.
* **Enterprise Edition**: geared towards large enterprise deployments, the Enterprise Edition supports multiple additional enterprise features like external secrets management, RBAC, and audit logs.
* **Sovereign Edition**: geared towards on-prem deployments, the Sovereign Edition allows full control over all aspects of the Tenzir Platform, including multiple platform instances, multiple organizations within each platform, and integration with existing on-prem infrastructure.

The Sovereign Edition is best suited for service providers that need strict data segregation, either by deploying one platform instance per customer or by instantiating one organization per customer. Dedicated platforms per customer provide physical data separation at the cost of higher management overhead, whereas an organization-based multi tenancy approach is a logical separation method with shared underlying resources, yet easier to manage.

# Configuration

There exist multiple options to configure the behavior of the Tenzir pipeline executor (`tenzir`) and Tenzir Node (`tenzir-node`) that ship with a Tenzir package:

1. Command-line arguments
2. Environment variables
3. Configuration files
4. Compile-time defaults

The options are sorted by precedence, i.e., command-line arguments override environment variables, which override configuration file settings. Compile-time defaults can only be changed by rebuilding Tenzir from source.

Let's discuss the first three options in more detail.

## Command Line Arguments

[Section titled “Command Line Arguments”](#command-line-arguments)

The command line arguments of the executables have the following synopsis:

```plaintext
tenzir [opts] <pipeline>
tenzir-node [opts]
```

We have both long `--long=X` and short `-s X` options. Boolean options do not require explicit specification of a value, and it suffices to write `--long` and `-s` to set an option to true.

## Environment Variables

[Section titled “Environment Variables”](#environment-variables)

You can use environment variables as an alternative method to passing command line options. This comes in handy when working with non-interactive deployments where the command line is hard-coded, such as in Docker containers.

An environment variable has the form `KEY=VALUE`, and we describe the format of `KEY` and `VALUE` below. Tenzir processes only environment variables that have the form `TENZIR_{KEY}=VALUE`. For example, `TENZIR_ENDPOINT=1.2.3.4` translates to the command line option `--endpoint=1.2.3.4` and YAML configuration `tenzir.endpoint: 1.2.3.4`.

### Keys

[Section titled “Keys”](#keys)

There exists a one-to-one mapping from configuration file keys to environment variable names. Here are two examples:

* `tenzir.import.batch-size` 👈 configuration file key
* `TENZIR_IMPORT__BATCH_SIZE` 👈 environment variable

A hierarchical key of the form `tenzir.x.y.z` maps to the environment variable `TENZIR_X__Y__Z`. More generally, the `KEY` in `TENZIR_{KEY}=VALUE` adheres to the following rules:

1. Double underscores map to the `.` separator of YAML dictionaries.

2. Single underscores `_` map to a `-` in the corresponding configuration file key. This is unambiguous because Tenzir does not have any options that include a literal underscore.

From the perspective of the command line, setting the `--foo` option via `tenzir --foo` or `tenzir-node --foo` maps onto the environment variable `TENZIR_FOO` and the configuration file key `tenzir.foo`. Here are two examples with identical behavior:

```sh
TENZIR_ENDPOINT=0.0.0.0:42000 tenzir-node
tenzir-node --endpoint=0.0.0.0:42000
```

CAF and Plugin Settings

To provide [CAF](https://github.com/actor-framework/actor-framework) and plugin settings, which have the form `caf.x.y.z` and `plugins.name.x.y.z` in the configuration file, the environment variable must have the form `TENZIR_CAF__X__Y__Z` and `TENZIR_PLUGINS__NAME__X__Y__Z` respectively.

The configuration file is an exception in this regard: `tenzir.caf.` and `tenzir.plugins.` are invalid key prefixes. Instead, CAF and plugin configuration file keys have the prefixes `caf.` and `plugins.`, i.e., they are hoisted into the global scope.

### Values

[Section titled “Values”](#values)

While all environment variables are strings on the shell, Tenzir parses them into a typed value internally. In general, parsing values from the environment follows the same syntactical rules as command line parsing.

In particular, this applies to lists. For example, `TENZIR_PLUGINS="foo,bar"` is equivalent to `--plugins=foo,bar`.

Tenzir ignores environment variables with an empty value because the type cannot be inferred. For example, `TENZIR_PLUGINS=` will not be considered.

## Configuration files

[Section titled “Configuration files”](#configuration-files)

Tenzir's configuration file is in YAML format. On startup, Tenzir attempts to read configuration files from the following places, in order:

1. `/opt/tenzir/etc/tenzir/tenzir.yaml` for system-wide configuration, where `/opt/tenzir` is the default install prefix and `/etc/tenzir` is the default sysconfdir.

2. `~/.config/tenzir/tenzir.yaml` for user-specific configuration. Tenzir respects the XDG base directory specification and its environment variables.

3. A path to a configuration file passed via `--config=/path/to/tenzir.yaml`.

If there exist configuration files in multiple locations, options from all configuration files are merged in order, with the latter files receiving a higher precedence than former ones. For lists, merging means concatenating the list elements.

Example configuration

Check out the [example configuration](/reference/node/configuration) for a fully documented `tenzir.yaml` that shows all available settings.

### Plugin Configuration Files

[Section titled “Plugin Configuration Files”](#plugin-configuration-files)

In addition to `tenzir/tenzir.yaml`, Tenzir loads `tenzir/plugin/<plugin>.yaml` for plugin-specific configuration for a given plugin named `<plugin>`. The same rules apply as for the regular configuration file directory lookup.

### Bare Mode

[Section titled “Bare Mode”](#bare-mode)

Sometimes, users may wish to run Tenzir without side effects, e.g., when wrapping Tenzir in their own scripts. Run with `--bare-mode` to disable looking at all system- and user-specified configuration paths.

## Plugins

[Section titled “Plugins”](#plugins)

Tenzir's plugin architecture allows for flexible replacement and enhancement of functionality at various pre-defined customization points. There exist **dynamic plugins** that ship as shared libraries and **static plugins** that are compiled into libtenzir.

### Install plugins

[Section titled “Install plugins”](#install-plugins)

Dynamic plugins are just shared libraries and can be placed at a location of your choice. We recommend putting them into a single directory and add the path to the `tenzir.plugin-dirs` configuration option..

Static plugins do not require installation since they are compiled into Tenzir.

### Load plugins

[Section titled “Load plugins”](#load-plugins)

The configuration key `tenzir.plugins` specifies the list of plugins that should load at startup. The `all` plugin name is reserved. When `all` is specified Tenzir loads all available plugins in the configured plugin directories. If no `tenzir.plugins` key is specified, Tenzir will load `all` plugins by default. To load no plugins at all, specify a `tenzir.plugins` configuration key with no plugin values, e.g. the configuration file entry `plugins: []` or launch parameter `--plugins=`.

Since dynamic plugins are shared libraries, they must be loaded first into the running Tenzir process. At startup, Tenzir looks for the `tenzir.plugins` inside the `tenzir.plugin-dirs` directories configured in `tenzir.yaml`. For example:

```yaml
tenzir:
  plugin-dirs:
    - .
    - /opt/foo/lib
  plugins:
    - example
    - /opt/bar/lib/libtenzir-plugin-example.so
```

Before executing plugin code, Tenzir loads the specified plugins via `dlopen(3)` and attempts to initialize them as plugins. Part of the initialization is passing configuration options to the plugin. To this end, Tenzir looks for a YAML dictionary under `plugins.<name>` in the `tenzir.yaml` file. For example:

```yaml
plugins:
  example:
    option: 42
```

Alternatively, you can specify a `plugin/<plugin>.yaml` file. The example configurations above and below are equivalent. This makes plugin deployments easier, as plugins can be installed and uninstalled alongside their respective configuration.

```yaml
option: 42
```

After initialization with the configuration options, the plugin is fully operational and Tenzir will call its functions at the plugin-specific customization points.

### List plugins

[Section titled “List plugins”](#list-plugins)

You can get the list of available plugins using the [`plugins`](/reference/operators/plugins) operator:

```bash
tenzir 'plugins'
```

### Block plugins

[Section titled “Block plugins”](#block-plugins)

As part of your Tenzir deployment, you can selectively disable plugins by name. For example, if you do not want the `shell` operator and the `kafka` connector to be available, set this in your configuration:

```yaml
tenzir:
  disable-plugins:
    - shell
    - kafka
```

# Enrichment

Enrichment means adding contextual data to events. The purpose of this added context is to allow for making better decisions, e.g., to triage alerts and weed out false positive, to leverage country information to classify logins as malicious, or to flag a sighting of an indicator of compromise.

Tenzir comes a flexible enrichment framework where the central abstraction is a **context**: a stateful object that can be updated with pipelines and used for enrichment in other pipelines:

![Context update & enrich](/_astro/context-update-enrich.s9uj4OUs_19DKCs.svg)

The update and enrich operations can occur concurrently. This allows for creating highly dynamic use cases where context state rapidly changes, such as when modelling the threat landscape or internal network infrastructure.

Reusing pipelines as mechanism for context updates (as opposed to other systems that, say, offer a separate interface to only load static CSV files) has the benefit that we can leverage the full power of TQL. In other words, we can reuse *all* existing connectors, formats, periodic scheduling, and more.

## Enrichment Modes

[Section titled “Enrichment Modes”](#enrichment-modes)

In general, we distinguish three different contextualization modes:

![Contextualization Modes](/_astro/contextualization-modes.D2N6G2W8_19DKCs.svg)

1. **In-band**. The context data is co-located with the pipeline that enriches the dataflow. For high-velocity pipelines with thousands of events per second, this is of often the only way to enrich.
2. **Out-of-band**. The context data is outside of the to-be-contextualized dataflow. The most common example of this kind are REST APIs. Enrichment then means performing one API call per event, waiting for the result, and then merging it into the event to continue processing. For public APIs, latencies are in the tens to hundreds of milliseconds, making this mode suitable for low-velocity.
3. **Hybrid**. An excerpt of the context sits inline and the bulk of it remotely. When both performance matters and state is not possible to ship to the contextualization point itself, then a hybrid approach can be a viable middle ground. [Google Safe Browsing](https://security.googleblog.com/2022/08/how-hash-based-safe-browsing-works-in.html) is an example of this kind, where the Chrome browser keeps a subset of context state that represents threat actor URLs in the form of partial hashes, and when a users visits a URL where a partial match occurs, Chrome performs a candidate check using an API call. More than 99% of checked URLs never make it to the remote API, making this approach scalable. Note that the extra layer of hashing also protects the privacy of the entity performing the context lookup.

To date, Tenzir only supports in-band enrichment, but out-of-band and hybrid modes are already planned.

## Context Types

[Section titled “Context Types”](#context-types)

Tenzir features several context types:

* **Lookup table**: a hash table that associates arbitrary information with a given enrichment key.
* **GeoIP database**: a special-purpose table for attaching geographic information to IP addresses.
* **Bloom filter**: a compact representation of a sets that allows for membership tests only, with the space efficiency coming at the cost a false positives during lookup.

![Context types](/_astro/context-types.B_enb-EP_19DKCs.svg)

If these built-in types do not suffice for your needs, you can also write your own C++ context plugin.

### Lookup Table

[Section titled “Lookup Table”](#lookup-table)

Tenzir’s lookup table context is a hash table that can have both keys and values of different types. For example, you can have IP addresses, subnets, and numbers as table keys all in the same table, and every value can have a different type.

There are three particularly powerful features of lookup tables that we describe next.

#### 1. Longest-Prefix Subnet Match for Subnet Keys

[Section titled “1. Longest-Prefix Subnet Match for Subnet Keys”](#1-longest-prefix-subnet-match-for-subnet-keys)

When table keys are of type `subnet`, you can probe the lookup table with values of type `ip` in addition. For example, if you have a key `10.0.0.0/8` you can perform a lookup with `10.0.0.1`, `10.1.1.1`, etc. The lookup table will always return the value associated with `10.0.0.0/8`.

![Subnet Keys](/_astro/lookup-table-subnet-keys.GRqTyUEi_19DKCs.svg)

When a table contains subnet keys with overlapping ranges, such as `10.0.0.0/22` and `10.0.0.0/24`, a lookup returns the entry with the *longest-prefix match* of the key. For example, probing the table with `10.0.0.1` returns the value associated with the `/24` subnet, because it is a longer match (24 > 22).

Modeling CMDB & Asset Inventory

Subnet key matching comes in handy when you are building lookup tables that represent your infrastructure. We often see giant static spreadsheets as “network plans” that (alas often only partially) describe roles of certain network segments. This information can be gold from a security perspective, when it comes to triaging alerts, e.g., to assess whether the crown jewels are affected, or when detecting dubious network traffic zone transitions, e.g., during lateral movement of attackers.

#### 2. Per-Key Create/Write/Read Expiration

[Section titled “2. Per-Key Create/Write/Read Expiration”](#2-per-key-createwriteread-expiration)

Every table entry has three optional expiration timeouts that evict the entry. These timeout are compounding, so it suffices if *one* of them fires to trigger eviction.

![Expiration](/_astro/lookup-table-expiration.4H8rBJ1d_19DKCs.svg)

The three timeout types are:

1. **Create timeout**: Whenever a new entry is created in the table, e.g., through `context::update`, this timeout starts counting down. This happens exactly once and the timer cannot be reset. Therefore, a create timeout is an *upper bound* on the lifetime of an entry.
2. **Write timeout**: Whenever an update modifies a table entry by writing a new value for a given key with `context::update`, this timeout resets.
3. **Read timeout**: Whenever an entry is accessed, which happens during `context::enrich`, this timeout resets.

All three timers start at table entry creation, i.e., creating an entry is the “0th” write and read.

Modeling Threat Intelligence Decay

Per-key timeouts come in handy when you want to associate lifetimes with observables or indicators of compromise. Many indicators have a short half-life and most are obsolete a couple of weeks. To avoid accumulating “garbage” that produces false positives, use a combination of timeouts to automatically remove stale entries. Many threat feeds and threat intelligence platforms provide this information already to generate more actionable enrichments.

#### 3. Aggregation Functions as Values

[Section titled “3. Aggregation Functions as Values”](#3-aggregation-functions-as-values)

Lookup tables offer more than just entries with static values. You can also aggregate into values with [aggregation functions](/reference/functions#aggregation). In this case an update of a table entry does not write the new value directly, but rather hands that value to the configured aggregation function, which in turn updates the table value.

For example, the `min` aggregation function computes the minimum over its values. Consider a sequence of context updates with values `3`, `4`, `2`, `3`, `1` for the updated key. Then the value after each update would be `3`, `3`, `2`, `2`, `1`. The example below uses `min` and `max` to implement a first-seen and last-seen timestamps—a common pattern during entity tracking.

![Aggregation](/_astro/lookup-table-aggregation.CIhz1i3Q_19DKCs.svg)

Passive Inventorization

Aggregating into table values instead of just overwriting the value has a plethora of use cases. A popular example involves computing first/last/times-seen statistics to build passive DNS tables. But also extracting IP-to-MAC, host-to-username, or any other mappings available in telemetry provide valuable context for entity-based reasoning. Our [Suricon 2024 talk](https://docs.google.com/presentation/d/1SnWQMBN7HQ4ASckUgy_kfQyWzHSnrN9zl-m4ptMVolQ/present) has more examples.

### Bloom Filter

[Section titled “Bloom Filter”](#bloom-filter)

[Bloom filters](https://en.wikipedia.org/wiki/Bloom_filter) are a space-efficient representation of a set. In case you have a massive number of elements but only need to check for set membership. However, the compact spatial representation comes at a cost of a false probability during lookup. You can think of it as a [lookup table](#lookup-table) without values—just keys, but where looking up a key may say “yes” even though the key doesn’t actually exist. The probability of this happening is fortunately configurable.

A Bloom filter has two tuning knobs:

1. **Capacity**: the maximum number of items in the filter.
2. **False-positive probability**: the chance of reporting an item not in the filter.

These two parameters dictate the space usage of the Bloom filter. Consult Thomas Hurst’s [Bloom Filter Calculator](https://hur.st/bloomfilter/) for finding the optimal configuration for your use case.

Tenzir’s Bloom filter implementation is a C++ rebuild of DCSO’s [bloom](https://github.com/DCSO/bloom) library. It is binary-compatible and uses the exact same method for FNV1 hashing and parameter calculation, making it a drop-in replacement for `bloom` users.

Large Observable Sets

Let’s say you have a 10 billion SHA256 hash digests of malware samples. Your endpoint telemetry provides a SHA256 along with every process creation event. You’d like to check whether the newly created process is in the malware set. With a lookup table, you would need at least 256 / 8 \* 100^6 = 320 GB of memory. With a Bloom filter that has a 1% false positive rate, you can represent the same set with XXX memory. A lot less!

### GeoIP Database

[Section titled “GeoIP Database”](#geoip-database)

The GeoIP context provides geo-spatial information about a given IP address. Conceptually, it is a lookup table with IP addresses as keys that maps to country codes/names, region codes/names, cities, zip codes, and geographic coordinates.

Since a naive table representation would explode in memory, [MaxMind](https://www.maxmind.com/) came up with a custom binary [MMDB format](https://maxmind.github.io/MaxMind-DB/) along with a [corresponding library](https://github.com/maxmind/libmaxminddb) to perform IP lookups. This format is the de-facto standard for geo-spatial IP address enrichment today. Tenzir supports it natively.

Detecting Anomalous Logins

The textbook example for a GeoIP use case involves performing anomaly detection on user login events. When a user logs into a system from previously unknown country, a detection engine often raises an alert. Assuming your endpoint telemetry only provides you with the IP address of the device that performed the login, you can use a GeoIP context to add the missing country information that a downstream detector can then evaluate.

# FAQs

This page provides answers to frequently asked questions (FAQs) about Tenzir.

## What is Tenzir?

[Section titled “What is Tenzir?”](#what-is-tenzir)

Tenzir is the data pipeline engine for security teams, enabling you to collect, parse, shape, normalize, aggregate, route, store, and query your security telemetry at ease.

Tenzir is also the name of the startup behind the product.

## What part of Tenzir is open and what part is closed source?

[Section titled “What part of Tenzir is open and what part is closed source?”](#what-part-of-tenzir-is-open-and-what-part-is-closed-source)

To create a healthy open core business with a thriving open source foundation, we aim to find the right balance between enabling open source enthusiast whilst offering a commercial package for those seeking a turn-key solution:

![Open source vs. closed source](/_astro/open-vs-closed-source.B517vmB2_19DKCs.svg)

There exist three moving parts:

1. **Executor**: The pipeline executor is open source and available under a permissive BSD 3-clause licence at [GitHub](https://github.com/tenzir/tenzir).
2. **Node**: The node makes pipeline management easy. A node orchestrates multiple pipelines and offers additional features, such as contexts for enrichment and an indexed storage engine.
3. **Platform**: The platform is the control plane for managing nodes and offers a web-based interface.

You can either [build the open source version](/guides/development/build-from-source) yourself and [add your own plugins](/explanations/architecture), or use our compiled binary packages that include the command line tool and the node. We offer the platform as Docker Compose files. We host one platform instance at [app.tenzir.com](https://app.tenzir.com).

Tenzir comes in severals editions, including a free [Community Edition](https://tenzir.com/pricing). If you have any questions, don’t hesitate to [reach out](https://tenzir.com/contact) what best suits your needs.

## Can Tenzir see my data?

[Section titled “Can Tenzir see my data?”](#can-tenzir-see-my-data)

*No*, but let us explain.

A Tenzir deployment consists of *nodes* that you manage, and a *platform* available as SaaS from us or operated by you. The *app* runs in your browser to access the platform. All computation and storage takes place at your nodes. The platform acts as rendezvous point that connects two TLS-encrypted channels, one from the node to the platform, and one from the browser to the platform:

![Platform Connections](/_astro/platform-connections.BdzkU-en_19DKCs.svg)

We connect these two channels at the platform. Therefore, whoever operates the platform *could* interpose on data that travels from the nodes to the app. In the [Professional Edition](https://tenzir.com/pricing) and [Enterprise Edition](https://tenzir.com/pricing), we run the platform. However, we emphasize that data privacy is of utmost importance to us and our customers. As a mission-driven company with strong ethics, our engineering follows state-of-the-art infrastructure-as-code practices and we are performing security audits to ensure that our code quality meets the highest standards.

We have plans to make this a single, end-to-end encrypted channel, so that we no longer have the theoretical ability to interpose on the data transfer between app and node.

If you have more stringent requirements, you can also run the platform yourself with the [Sovereign Edition](https://tenzir.com/pricing).

## Does Tenzir run on-premise?

[Section titled “Does Tenzir run on-premise?”](#does-tenzir-run-on-premise)

Yes, Tenzir can run on premise and supports fully air-gapped environments. The [Sovereign Edition](https://tenzir.com/pricing) allows you to [deploy the entire platform](/guides/platform-setup) in a Dockerized environment.

The [Community Edition](https://tenzir.com/pricing), [Professional Edition](https://tenzir.com/pricing) and [Enterprise Edition](https://tenzir.com/pricing) are backed by a Tenzir-hosted instance of the platform in the public cloud (AWS in Europe).

Read more on [how Tenzir works](/explanations/architecture) to understand what component of Tenzir runs where.

## Does Tenzir offer cloud-native nodes?

[Section titled “Does Tenzir offer cloud-native nodes?”](#does-tenzir-offer-cloud-native-nodes)

Tenzir currently does not offer cloud-hosted nodes. You can only run nodes in your own environment, including your cloud environment.

However, we offer a cloud-native *demo node* that you can deploy as part of every account.

## Why Did You Create a New Query Language? Why Not SQL?

[Section titled “Why Did You Create a New Query Language? Why Not SQL?”](#why-did-you-create-a-new-query-language-why-not-sql)

We opted for our own language—the [**Tenzir Query Language**](/explanations/language) or **TQL**— for several reasons that we outline below.

At Tenzir, we have a clear target audience: security practitioners. They are rarely data engineers fluent in SQL or experienced with lower-level data tools. Rather, they identify as blue/purple teamers, incident responders, threat hunters, detection engineers, threat intelligence analysts, and other domain experts.

**Why Not Stick with SQL?** SQL, while powerful and pervasive, comes with significant usability challenges. Despite decades of success, SQL’s syntax can be hard to learn, read, and modify, even for experts. Some of the key limitations include:

* **Rigid Clause Order**: SQL’s fixed structure (e.g., `SELECT ... FROM ... WHERE ...`) forces users to think in an “inside-out” manner that doesn’t match the natural flow of data transformations. This often leads to complex subqueries and redundant clauses.
* **Complex Subqueries**: Expressing multi-level aggregations or intermediate transformations typically requires deeply nested subqueries, which hurt readability and make edits labor-intensive.
* **Difficult Debugging**: SQL’s non-linear data flow makes tracing logic through large queries cumbersome, impeding efficient debugging.

These challenges make SQL difficult for security practitioners who need to focus on quick, intuitive data analysis without getting bogged down by the intricacies of query structuring.

**Why a Pipeline Language?** We chose a pipeline-based approach for our query language because it enhances user experience and addresses the pain points of SQL. Here’s how:

* **Sequential and Intuitive Data Flow**: Pipeline syntax expresses data operations in a top-to-bottom sequence, reflecting the logical order of data transformations. This makes it easier to follow and understand, especially for complex queries.
* **Simplified Query Construction**: With a pipeline language, operations can be chained step-by-step without requiring nested subqueries or repetitive constructs. This improves readability and allows users to build and modify queries incrementally.
* **Easier Debugging**: Each stage in a pipeline can be isolated and inspected, simplifying the process of identifying issues or making adjustments. This is in stark contrast to SQL, where changes often ripple through multiple interconnected parts of a query.

**Lessons from Other Languages.** We spoke to numerous security analysts with extensive experience using SIEMs. Splunk’s Search Processing Language (SPL), for instance, has set a high standard in user experience, catering well to its non-engineer user base. This inspired us to create a language with:

* The *familiarity* of [Splunk](https://splunk.com)
* The *power* of [Kusto](https://github.com/microsoft/Kusto-Query-Language)
* The *flexibility* of [jq](https://stedolan.github.io/jq/)
* The *clarity* of [PRQL](https://prql-lang.org/)
* The *expressiveness* of [dplyr](https://dplyr.tidyverse.org/)
* The *ambition* of [SuperSQL](https://superdb.org/)
* The *composability* of [Nu](https://www.nushell.sh/)

Even Elastic recognized the need for more intuitive languages by introducing **ES|QL**, which leans into a pipeline style. Nearly every major SIEM and observability tool has adopted some version of a pipeline language, underscoring the trend toward simplified, step-by-step data handling.

**Balancing Streaming and Batch Workloads.** One of our key design goals was to create a language that effortlessly handles both streaming and batch processing. By allowing users to switch between historical and live data inputs with minimal change to the pipeline, our language maintains flexibility without introducing complexity.

Our decision to develop a new language was not taken lightly. We aimed to build on the strengths of SQL while eliminating its weaknesses, creating an intuitive, powerful tool tailored for security practitioners. By combining insights from existing successful pipeline languages and leveraging modern data standards, we offer a user-friendly and future-proof solution for security data analysis.

## What database does Tenzir use?

[Section titled “What database does Tenzir use?”](#what-database-does-tenzir-use)

Tenzir does not rely on a third-party database.

Tenzir nodes include a light-weight storage engine on top of Feather or Parquet files, accessible via the [`import`](/reference/operators/import) and [`export`](/reference/operators/export) operators.

The storage engine comes with a catalog that tracks schema meta data and a thin layer of indexing to accelerate queries.

Our [tuning guide](/guides/node-setup/tune-performance) has further details on the inner workings.

## Does a Tenzir node run on platform *X*?

[Section titled “Does a Tenzir node run on platform X?”](#does-a-tenzir-node-run-on-platform-x)

We support the platforms that we mention in our [deployment instructions](/guides/node-setup/provision-a-node).

For any other platform, the answer is most likely *no*. Please [talk to us](/discord) to let us know what is missing, or dive right in by contributing to our [open source repository](https://github.com/tenzir/tenzir).

## Do you have an integration for *X*?

[Section titled “Do you have an integration for X?”](#do-you-have-an-integration-for-x)

Tenzir has multiple layers where integrations can occur. If you cannot find *X* in the list of [existing integration](/integrations), .

1. **Application**. If *X* does not require a built-in integration, our [Community Library](https://github.com/tenzir/library) may contain a package for it. Many integration to specific tools are just thin API wrappers that only require composing a few operators.
2. **Format**. If your *X* is a wire format, either text-based like JSON or binary like PCAP, then look for [parsing operators](/reference/operators#parsing) and [printing operators](/reference/operators#printing). Those start with `read_*` and `write_*`, respectively. Similarly, there exist [parsing functions](/reference/functions#parsing) and [printing functions](/reference/functions#printing) that start with `parse_*` and `print_*`.
3. **Fluent Bit**. Sometimes we can compensate for the lack of an existing integration by going through one level of indirection. Tenzir ships with all of Fluent Bit’s [inputs](https://docs.fluentbit.io/manual/pipeline/inputs/) and [outputs](https://docs.fluentbit.io/manual/pipeline/outputs/), because the Fluent Bit library is baked into every Tenzir binary. Use the [`from_fluent_bit`](/reference/operators/from_fluent_bit) operator to get events in via Fluent Bit and the [`to_fluent_bit`](/reference/operators/to_fluent_bit) operator to send events via Fluent Bit.
4. **Escape Hatches**. As last resort, you can bring in Shell and Python scripts to make up for native support for *X*. The [`shell`](/reference/operators/shell) operator brings byte streams via standard input and output into a pipeline, and the [`python`](/reference/operators/python) operator allows you to perform arbitrary event-to-event transformation using the full power of Python.
5. **Community**. Still unlucky? Then please let us know in our friendly [Discord server](/discord). Perhaps we are already working on an integration for *X* or it is somewhere on the roadmap. If you’re feeling adventurous and want to contribute to our open source core, let us know beforehand! We’re happy to guide you such that your contribution gets successfully into our code base.

Don’t be shy!

Please do not hesitate to reach out to us if you think something is missing, by [opening a GitHub Discussion](https://github.com/orgs/tenzir/discussions/new/choose) or asking us directly in our [Discord server](/discord). All questions are welcome.

# Glossary

This page defines central terms in the Tenzir ecosystem.

Missing term?

If you are missing a term, please open a [GitHub Discussion](https://github.com/orgs/tenzir/discussions/new?category=questions-answers) or ping us in our [Discord server](/discord).

## App

[Section titled “App”](#app)

Web user interface to access [platform](#platform) at [app.tenzir.com](https://app.tenzir.com).

The app is a web application that partially runs in the user’s browser. It is written in [Svelte](https://svelte.dev/).

## Catalog

[Section titled “Catalog”](#catalog)

Maintains [partition](#partition) ownership and metadata.

The catalog is a component in the [node](#node) that owns the [partitions](#partition), keeps metadata about them, and maintains a set of sparse secondary indexes to identify relevant partitions for a given query. It offers a transactional interface for adding and removing partitions.

## Connector

[Section titled “Connector”](#connector)

Manages chunks of raw bytes by interacting with a resource.

A connector is either a *loader* that acquires bytes from a resource, or a *saver* that sends bytes to a resource. Loaders are implemented as ordinary [operators](/reference/operators) prefixed with `load_*` while savers are prefixed with `save_*`.

## Context

[Section titled “Context”](#context)

A stateful object used for in-band enrichment.

Contexts come in various types, such as a lookup table, Bloom filter, and GeoIP database. They live inside a node and you can enrich with them in other pipelines.

* Read more about [enrichment](/explanations/enrichment)

## Destination

[Section titled “Destination”](#destination)

An pipeline ending with an [output](#output) operator preceded by a [`subscribe`](/reference/operators/subscribe) input operator.

* Learn more about [pipelines](/explanations/architecture/pipeline)

## Edge Storage

[Section titled “Edge Storage”](#edge-storage)

The indexed storage that pipelines can use at the [node](#node). Every node has a light-weight storage engine for importing and exporting events. You must mount the storage into the node such that it can be used from [pipelines](#pipeline) using the [`import`](/reference/operators/import) and [`export`](/reference/operators/export) [operators](#operator). The storage cengine comes with a [catalog](#catalog) that tracks [partitions](#partition) and keeps sparse [indexes](#index) to accelerate historical queries.

* [Ingest data into the node’s edge storage](/guides/edge-storage/import-into-a-node)
* [Query the node’s edge storage](/guides/edge-storage/export-from-a-node)

## Event

[Section titled “Event”](#event)

A record of typed data. Think of events as JSON objects, but with a richer [type system](/explanations/language/types) that also has timestamps, durations, IP addresses, and more. Events have fields and can contain numerous shapes that describe its types (= the [schema](#schema)).

* Learn more about [pipelines](/explanations/architecture/pipeline)

## Format

[Section titled “Format”](#format)

Translates between bytes and events.

A format is either supported by a *parser* that converts bytes to events, or a *printer* that converts events to bytes. Example formats are JSON, CEF, or PCAP.

* See available [operators for parsing](/reference/operators#parsing)
* See available [operators for printing](/reference/operators#printing)
* See available [functions for parsing](/reference/functions#parsing)
* See available [functions for printing](/reference/functions#printing)

## Function

[Section titled “Function”](#function)

Computes something over a value in an [event](#event). Unlike operators that work on streams of events, functions can only act on single values.

* See available [functions](/reference/functions)

## Index

[Section titled “Index”](#index)

Optional data structures for accelerating queries involving the node’s [edge storage](#edge-storage).

Tenzir featres in-memory *sparse* indexes that point to [partitions](#partition).

* [Configure the catalog](/guides/node-setup/tune-performance#configure-the-catalog)

## Input

[Section titled “Input”](#input)

An [operator](#operator) that only producing data, without consuming anything.

* Learn more about [pipelines](/explanations/architecture/pipeline)

## Integration

[Section titled “Integration”](#integration)

A set of pipelines to integrate with a third-party product.

An integration describes use cases in combination with a specific product or tool. Based on the depth of the configuration, this may require configuration on either end.

* List of [all integrations](/integrations)
* [Does Tenzir have an integration for *X*?](/explanations/faqs#do-you-have-an-integration-for-x)

## Library

[Section titled “Library”](#library)

A collection of [packages](#package).

Our community library is [freely available at GitHub](https://github.com/tenzir/library).

## Loader

[Section titled “Loader”](#loader)

A connector that acquires bytes.

A loader is the dual to a [saver](#saver). It has a no input and only performs a side effect that acquires bytes. Use a loader implicitly with the [`from`](/reference/operators/from) operator or explicitly with the `load_*` operators.

* Learn more about [pipelines](/explanations/architecture/pipeline)

## Node

[Section titled “Node”](#node)

A host for [pipelines](#pipeline) and storage reachable over the network.

The `tenzir-node` binary starts a node in a dedicated server process that listens on TCP port 5158.

* [Deploy a node](/guides/node-setup/provision-a-node)
* Use the [REST API](/reference/node/api) to manage a node
* [Import into a node](/guides/edge-storage/import-into-a-node)
* [Export from a node](/guides/edge-storage/export-from-a-node)

## Metrics

[Section titled “Metrics”](#metrics)

Runtime statistics about the node and pipeline execution.

* [Collect metrics](/guides/basic-usage/collect-metrics)

## OCSF

[Section titled “OCSF”](#ocsf)

The [Open Cybersecurity Schema Framework (OCSF)](https://schema.ocsf.io) is a cross-vendor schema for security event data. Our [community library](#library) contains packages that map data sources to OCSF.

* [Map data to OCSF](/tutorials/map-data-to-ocsf)

## Operator

[Section titled “Operator”](#operator)

The building block of a [pipeline](#pipeline).

An operator is an [input](#input), a [transformation](#transformation), or an [output](#output).

* See all available [operators](/reference/operators)

## Output

[Section titled “Output”](#output)

An [operator](#operator) consuming data, without producing anything.

* Learn more about [pipelines](/explanations/architecture/pipeline)

## PaC

[Section titled “PaC”](#pac)

The acronym PaC stands for *Pipelines as Code*. It is meant as an adaptation of [Infrastructure as Code (IaC)](https://en.wikipedia.org/wiki/Infrastructure_as_code) with pipelines represent the (data) infrastructure that is provisioning as code.

* Learn how to provision [piplines as code](/guides/basic-usage/run-pipelines#as-code).

## Package

[Section titled “Package”](#package)

A collection of [pipelines](#pipeline) and [contexts](#context).

* Read more about [packages](/explanations/packages)
* [Learn how to write a package](/tutorials/write-a-package)

## Parser

[Section titled “Parser”](#parser)

A bytes-to-events operator.

A parser is the dual to a [printer](#printer). You use a parser implicitly in the [`from`](/reference/operators/from) operator, or via the `read_*` operators. There exist also [functions](#function) for applying parsers to string values.

* Learn more about [pipelines](/explanations/architecture/pipeline)
* See available [operators for parsing](/reference/operators#parsing)
* See available [functions for parsing](/reference/functions#parsing)

## Partition

[Section titled “Partition”](#partition)

The horizontal scaling unit of the storage attached to a [node](#node).

A partition contains the raw data and optionally a set of indexes. Supported formats are [Parquet](https://parquet.apache.org) or [Feather](https://arrow.apache.org/docs/python/feather.html).

## Pipeline

[Section titled “Pipeline”](#pipeline)

Combines a set of [operators](#operator) into a dataflow graph.

* Learn more about [pipelines](/explanations/architecture/pipeline)
* [Run a pipeline](/guides/basic-usage/run-pipelines)

## Platform

[Section titled “Platform”](#platform)

The control plane for nodes and pipelines, accessible at [app.tenzir.com](https://app.tenzir.com).

* Understand the [Tenzir architecture](/explanations/architecture)

## Printer

[Section titled “Printer”](#printer)

An events-to-bytes operator.

A [format](#format) that translates events into bytes.

A printer is the dual to a [parser](#parser). Use a parser implicitly in the [`to`](/reference/operators/to) operator.

* Learn more about [pipelines](/explanations/architecture/pipeline)
* See available [operators for printing](/reference/operators#printing)
* See available [functions for printing](/reference/functions#printing)

## Saver

[Section titled “Saver”](#saver)

A [connector](#connector) that emits bytes.

A saver is the dual to a [loader](#loader). It has a no output and only performs a side effect that emits bytes. Use a saver implicitly with the [`to`](/reference/operators/to) operator or explicitly with the `save_*` operators.

* Learn more about [pipelines](/explanations/architecture/pipeline)

## Schema

[Section titled “Schema”](#schema)

A top-level record type of an event.

* [Show available schemas in the edge storage](/guides/edge-storage/show-available-schemas)

## Source

[Section titled “Source”](#source)

An pipeline starting with an [input](#input) operator followed by a [`publish`](/reference/operators/publish) output operator.

* Learn more about [pipelines](/explanations/architecture/pipeline)

## TQL

[Section titled “TQL”](#tql)

An acronym for *Tenzir Query Language*.

TQL is the language in which users write [pipelines](#pipeline).

* Learn more about the [language](/explanations/language)

## Transformation

[Section titled “Transformation”](#transformation)

An [operator](#operator) consuming both input and producing output.

* Learn more about [pipelines](/explanations/architecture/pipeline)

# Overview

The **Tenzir Query Language (TQL)** is a dataflow language designed for processing of unstructured byte-streams and semi-structured events.

TQL is built on a powerful streaming execution engine, but it shields you from the complexity of low-level data processing. It provides a rich set of building blocks to create intricate [pipelines](/explanations/architecture/pipeline) that collect, transform, and route data. You can also embed your TQL programs into reusable [packages](/explanations/packages) to create one-click deployable use cases.

![TQL Layers](/_astro/tql-layers.CMohzmcx_19DKCs.svg)

## Why TQL?

[Section titled “Why TQL?”](#why-tql)

Security practitioners need to collect, transform, and analyze telemetry without the complexity of general-purpose data engineering tools. We created TQL to meet this need by synthesizing the best ideas from languages that security teams already use:

* **Splunk SPL’s familiarity**: Operators that security analysts recognize
* **Kusto’s power**: Rich aggregation and time-series capabilities
* **Unix pipes’ composability**: Small, focused operators that chain together
* **jq’s flexibility**: Powerful transformations on semi-structured data

TQL combines the power of a streaming execution engine with an intuitive pipeline syntax that mirrors how practitioners think about data processing. This approach is validated by nearly every modern SIEM (Splunk SPL, Elastic ES|QL, Microsoft KQL) and offers several advantages over traditional query languages like SQL.

Why not SQL?

While SQL is the standard for relational databases, its design creates friction in security operations:

* **Inside-out thinking**: SQL’s rigid `SELECT ... FROM ... WHERE` structure forces you to specify the output format before defining transformations, leading to complex nested subqueries.
* **Debugging complexity**: Tracing data flow through nested CTEs and subqueries is cumbersome when investigating incidents under time pressure.
* **Schema rigidity**: SQL assumes uniform, predefined schemas—a poor fit for the heterogeneous mix of logs, alerts, and telemetry that security teams process daily.

The pipeline model offers a more natural workflow:

* **Sequential reasoning**: Data flows top-to-bottom, matching your mental model.
* **Incremental construction**: Build queries step-by-step, testing at each stage.
* **Isolated debugging**: Inspect intermediate results by commenting out downstream operators.
* **Composability**: Combine simple operators into sophisticated workflows.

## Core Concepts

[Section titled “Core Concepts”](#core-concepts)

### Unified streaming and batch processing

[Section titled “Unified streaming and batch processing”](#unified-streaming-and-batch-processing)

TQL seamlessly handles both real-time and historical analysis. Unlike traditional tools that require separate codebases for streaming and batch workflows, TQL uses the same pipeline logic for both.

Process archived data from a data lake:

```tql
from_file "s3://bucket/logs/2024-01/*.parquet"
where timestamp > 2024-01-15T00:00:00
```

Or monitor a live stream from a message bus:

```tql
from "kafka://topic:9092"
where timestamp > now() - 1h
```

TQL draws inspiration from Unix pipes, where data flows through a sequence of transformations. But unlike shell pipelines that primarily work on text, TQL operates on both unstructured data (bytes) and structured data (events).

### Multi-schema philosophy

[Section titled “Multi-schema philosophy”](#multi-schema-philosophy)

Unlike traditional databases that require strict schemas, TQL embraces **heterogeneous data** as a first-class concept. Real-world data pipelines process multiple event types simultaneously—firewall logs, DNS queries, authentication events—each with different schemas.

TQL’s operators are **polymorphic**, adapting to different schemas at runtime:

```tql
// Single pipeline processing multiple event types
from "mixed_security_logs.json"
where timestamp > now() - 1h
where severity? == "high" or risk_score? > 0.8
select \
  timestamp,
  event_type=@name,            // Capture the schema name
  message,                     // Common field
  src_ip?,                     // Present in network events
  username?,                   // Present in auth events
  dns_query?                   // Present in DNS events
```

This philosophy enables powerful patterns:

* **Type-aware aggregation**: Group and aggregate across different schemas
* **Unified processing**: Apply common transformations to diverse data
* **Schema evolution**: Handle changing schemas without pipeline updates
* **Mixed-source correlation**: Join events from different systems

## Language Structure

[Section titled “Language Structure”](#language-structure)

The language documentation is organized into four main sections:

* [**Types**](/explanations/language/types): TQL’s type system with domain-specific types for security and network data
* [**Expressions**](/explanations/language/expressions): The computational core of TQL, from literals to complex evaluations
* [**Statements**](/explanations/language/statements): Control and structure with bindings, operators, and control flow
* [**Programs**](/explanations/language/programs): Complete data processing workflows and execution model

Learn Idiomatic TQL

Ready to get hands-on? Read the [tutorial on learning idiomatic TQL](/tutorials/learn-idiomatic-tql), with concrete examples and best practices.

# Expressions

Expressions form the computational core of TQL. They range from simple literals to complex evaluations.

## Types and Operations

[Section titled “Types and Operations”](#types-and-operations)

Each type in TQL provides specific operations, starting from the simplest and building up to more complex types. For functions that work with these types, see the [functions reference](/reference/functions).

### Null

[Section titled “Null”](#null)

The `null` type represents absent or invalid values using the literal `null`:

```tql
from {
  value: null,
  is_null: null == null,
  has_value: 42 != null,
}
```

```tql
{
  value: null,
  is_null: true,
  has_value: true,
}
```

The `else` operator provides null coalescing:

```tql
from {
  result: null else "default",
}
```

```tql
{
  result: "default",
}
```

### Boolean

[Section titled “Boolean”](#boolean)

Boolean values (`bool`) support logical operations `and`, `or`, and `not`:

```tql
from {
  x: true and false,
  y: true or false,
  z: not true,
}
```

```tql
{
  x: false,
  y: true,
  z: false,
}
```

TQL implements *short-circuit evaluation*: it stops evaluating once it determines the result.

### String

[Section titled “String”](#string)

Strings support several formats:

* **Regular strings**: `"hello\nworld"` (with escape sequences)
* **Raw strings**: `r"C:\path\to\file"` (no escape processing)
* **Raw strings with quotes**: `r#"They said "hello""#` (allows quotes inside)

Strings support concatenation via `+` and substring checking via `in`:

```tql
from {
  name: "World",
  greeting: "Hello, " + name + "!",
  has_hello: "Hello" in greeting,
}
```

```tql
{
  name: "World",
  greeting: "Hello, World!",
  has_hello: true,
}
```

#### Format Strings (f-strings)

[Section titled “Format Strings (f-strings)”](#format-strings-f-strings)

Format strings provide a concise way to build dynamic strings using embedded expressions. They’re much more readable than string concatenation. For example, instead of:

```tql
percent = round(found / total * 100).string()
message = "Found " + found.string() + "/" + total.string() + ": " + percent + "%"
```

You can simply write:

```tql
message = f"Found {found}/{total}: {round(found / total * 100)}%"
```

To include literal braces, double them:

```tql
from {
  name: "TQL",
  template: f"Use {{braces}} in {name} like this: {{example}}",
}
```

```tql
{
  name: "TQL",
  template: "Use {braces} in TQL like this: {example}",
}
```

### Blob

[Section titled “Blob”](#blob)

Blobs represent raw binary data. Use them for handling non-textual data like network packets, encrypted payloads, or file contents.

Read [why TQL has binary blob types](/explanations/language/types/#why-binary-blob-types) for details.

### Numbers

[Section titled “Numbers”](#numbers)

Numeric literals can include magnitude suffixes for readability:

* **Power-of-ten suffixes**: `k` (1,000), `M` (1,000,000), `G`, `T`, `P`, `E`
* **Power-of-two suffixes**: `Ki` (1,024), `Mi` (1,048,576), `Gi`, `Ti`, `Pi`, `Ei`

For example, `2k` equals `2000` and `2Ki` equals `2048`.

All numeric types support standard arithmetic operations:

```tql
from {
  sum: 10 + 5,
  diff: 10 - 5,
  product: 10 * 5,
  quotient: 10 / 5,
}
```

```tql
{
  sum: 15,
  diff: 5,
  product: 50,
  quotient: 2.0,
}
```

#### Type Coercion

[Section titled “Type Coercion”](#type-coercion)

When mixing numeric types, TQL automatically coerces to the type that can hold the most values:

| Left Type |   Operator  | Right Type | Result Type |
| :-------- | :---------: | :--------- | :---------- |
| `int64`   | +, -, \*, / | `int64`    | `int64`     |
| `int64`   | +, -, \*, / | `uint64`   | `int64`     |
| `int64`   | +, -, \*, / | `double`   | `double`    |
| `uint64`  | +, -, \*, / | `uint64`   | `uint64`    |
| `uint64`  | +, -, \*, / | `double`   | `double`    |
| `double`  | +, -, \*, / | `double`   | `double`    |

#### Overflow and Error Handling

[Section titled “Overflow and Error Handling”](#overflow-and-error-handling)

TQL handles numeric errors gracefully by emitting warnings in the following cases:

* **Overflow/Underflow**: Returns `null` (no wrapping)
* **Division by zero**: Returns `null`
* **Invalid operations**: Returns `null`

This design prevents silent data corruption and makes errors explicit in your data.

Example:

```tql
let $x = 42 / 0
from {
  x: $x,
}
```

This emits the following warning:

```plaintext
warning: division by zero
 --> <input>:1:10
  |
1 | let $x = 42 / 0
  |          ~~~~~~
  |
```

### Duration

[Section titled “Duration”](#duration)

Create durations using time unit suffixes:

* Nanoseconds: `ns`
* Microseconds: `us`
* Milliseconds: `ms`
* Seconds: `s`
* Minutes: `min`
* Hours: `h`
* Days: `d`
* Weeks: `w`
* Months: `mo`
* Years: `y`

Example: `30s`, `5min`, `2h30min`

Durations support arithmetic operations for time calculations:

```tql
from {
  total: 1h + 30min,
  doubled: 30min * 2,
  half: 2h / 4,
  ratio: 30min / 1h,  // 0.5
}
```

```tql
{
  total: 1h30min,
  doubled: 1h,
  half: 30min,
  ratio: 0.5,
}
```

### Time

[Section titled “Time”](#time)

Write dates and timestamps using the [ISO 8601 standard](https://en.wikipedia.org/wiki/ISO_8601):

* Date only: `2024-10-03`
* Full timestamp: `2024-10-03T14:30:00Z`
* With timezone offset: `2024-10-03T14:30:00+02:00`

Time points represent specific moments and support arithmetic with durations:

```tql
from {
  start: 2024-01-01T00:00:00Z,
  one_day_later: start + 24h,
  one_hour_earlier: start - 1h,
}
```

```tql
{
  start: 2024-01-01T00:00:00Z,
  one_day_later: 2024-01-02T00:00:00Z,
  one_hour_earlier: 2023-12-31T23:00:00Z,
}
```

Calculating elapsed time is a common operation that converts two time points into a duration via subtraction:

```tql
from {
  start: 2024-01-01T00:00:00Z,
  end: 2024-01-01T12:30:00Z,
  elapsed: end - start,
}
```

```tql
{
  start: 2024-01-01T00:00:00Z,
  end: 2024-01-01T12:30:00Z,
  elapsed: 12h30min,
}
```

### IP and Subnet

[Section titled “IP and Subnet”](#ip-and-subnet)

The `ip` type handles both IPv4 and IPv6 addresses.

* IPv4: `192.168.1.1`, `10.0.0.1`
* IPv6: `::1`, `2001:db8::1`

This also applies to subnets: both `10.0.0.0/8` and `2001:db8::/32` are valid subnets.

IP addresses and subnets support membership and containment testing:

```tql
let $ip = 192.168.1.100;
let $network = 10.1.0.0/24;
from {
  ip: $ip,
  network: $network,
  is_private: $ip in 192.168.0.0/16,
  is_loopback: $ip in 127.0.0.0/8,
  contains_ip: 10.1.0.5 in $network,
  contains_subnet: 10.1.0.0/28 in $network,
}
```

```tql
{
  ip: 192.168.1.100,
  network: 10.1.0.0/24,
  is_private: true,
  is_loopback: false,
  contains_ip: true,
  contains_subnet: true,
}
```

### Secret

[Section titled “Secret”](#secret)

[Secrets](/explanations/secrets) protect sensitive values like authentication tokens and passwords. The `secret` type contains only a secret’s name, not its actual value, which is resolved asynchronously when needed.

Create secrets using the [`secret`](/reference/functions/secret) function or pass string literals directly to operators that accept secrets:

```tql
// Using managed secret
auth_header = "Bearer " + secret("api-token")


// Using format string (produces a secret)
connection = f"https://{secret("user")}:{secret("pass")}@api.example.com"
```

Secrets support concatenation with `+` and can be used in format strings. When a format string contains a secret, the result is also a secret. Converting a secret to a string yields a masked value (`"***"`) to prevent accidental exposure.

### List

[Section titled “List”](#list)

TQL has *typed* lists, which means that the type of the elements in a list is fixed and must not change per element. Lists use brackets to sequence data. `[]` denotes the empty list. Specify items with comma-delimited expressions:

```tql
let $ports = [80, 443, 8080]
let $mixed = [1, 2+3, foo()]  // Can contain expressions
```

Lists support indexing with `[]` and membership testing with `in`, with negative indices counting from the end of the list (`-1` refers to the last element):

```tql
let $items = [10, 20, 30]
from {
  first: $items[0],
  last: $items[-1],
  has_twenty: 20 in $items,
}
```

```tql
{
  first: 10,
  last: 30,
  has_twenty: true,
}
```

Use `?` for safe indexing that returns `null` instead of generating a warning:

```tql
from {
  items: [1, 2],
  third: items[2]? else 0,
}
```

```tql
{
  items: [
    1,
    2,
  ],
  third: 0,
}
```

The spread operator `...` expands lists into other lists:

```tql
let $base = [1, 2]
let $extended = [...$base, 3]  // Results in [1, 2, 3]
```

### Records

[Section titled “Records”](#records)

Records use braces to structure data. `{}` denotes the empty record. Specify fields using identifiers followed by a colon and an expression. Use quoted strings for invalid field names. For example:

```tql
let $my_record = {
  name: "Tom",
  age: 42,
  friends: ["Jerry", "Brutus"],
  "detailed summary": "Jerry is a cat."  // strings for invalid identifiers
}
```

The spread operator `...` expands records into other records:

Lifting nested fields

```tql
from {
  type: "alert",
  context: {
    severity: "high",
    source: 1.2.3.4,
  }
}
this = {type: type, ...context}
```

```tql
{
  type: "alert",
  severity: "high",
  source: 1.2.3.4,
}
```

Fields must be unique, and later values overwrite earlier ones.

The spread operator `...` expands records:

```tql
let $base = {a: 1, b: 2}
from {
  extended: {...$base, c: 3},
}
```

```tql
{
  extended: {
    a: 1,
    b: 2,
    c: 3,
  },
}
```

## Field Access

[Section titled “Field Access”](#field-access)

TQL provides multiple ways to access and manipulate fields within records and events.

### Basic Access

[Section titled “Basic Access”](#basic-access)

Use a single identifier to refer to a top-level field:

```tql
from {
  name: "Alice",
  age: 30,
}
adult = age >= 18
```

Chain identifiers with dots to access nested fields:

```tql
from {
  user: {
    profile: {
      name: "Alice"
    }
  },
}
username = user.profile.name
```

```tql
{
  user: {
    profile: {
      name: "Alice"
    }
  },
  username: "Alice",
}
```

### The `this` Keyword

[Section titled “The this Keyword”](#the-this-keyword)

`this` references the entire top-level event:

```tql
from {
  x: 1,
  y: 2,
}
z = this
```

```tql
{
  x: 1,
  y: 2,
  z: {
    x: 1,
    y: 2,
  },
}
```

You can also overwrite the entire event:

```tql
this = {transformed: true, data: this}
```

### Non-existent Fields

[Section titled “Non-existent Fields”](#non-existent-fields)

Trying to access a field that does not exist in an event will raise a warning and evaluate to `null`.

### Optional Access with `?`

[Section titled “Optional Access with ?”](#optional-access-with)

The optional field access operator (`?`) suppresses warnings when accessing non-existent fields:

Processing events with optional fields

```tql
from {event: "logon", user: {id: 123, name: "John Doe"}},
     {event: "logon", user: {id: 456}},
     {event: "logoff", user: {id: 123}}
select event, user_id=user.id, name=user.name?
```

```tql
{event: "logon", user_id: 123, name: "John Doe"}
{event: "logon", user_id: 456, name: null}  // No warning for missing `user.name`
{event: "logoff", user_id: 123, name: null} // No warning for missing `user.name`
```

Optional access also works on nested paths:

```tql
from {
  user: {address: {city: "NYC"}},
}
city = user.address?.city?  // No warning if `address` or `address.city` do not exist.
```

### Fallback with `else`

[Section titled “Fallback with else”](#fallback-with-else)

The `else` keyword provides default values when used with `?`:

```tql
from \
  { severity: 10, priority: null }, \
  { severity: null, priority: null }
severity_level = severity? else "unknown" // If `severity` is `null`, use `"unknown"` instead
priority =  priority? else 3              // if `priority` is `null`, default it to `3`
```

Without `else`, the `?` operator returns `null` when the field doesn’t exist. With `else`, you get a sensible default value instead:

```tql
from {
  foo: 1,
  bar: 2,
}
select
  value = missing?,           // null
  with_default = missing? else "default"  // "default"
```

### Indexing

[Section titled “Indexing”](#indexing)

Both lists and records support indexing operations to access their elements.

#### List Indexing

[Section titled “List Indexing”](#list-indexing)

Access list elements using integral indices, starting with `0`:

```tql
let $my_list = ["Hello", "World"]
first = $my_list[0]    // "Hello"
second = $my_list[1]   // "World"
```

Use `?` to handle out-of-bounds access:

```tql
let $ports = [80, 443]
third = $ports[2]? else 8080  // Fallback when index doesn't exist
```

#### Record Indexing

[Section titled “Record Indexing”](#record-indexing)

Bracket notation accesses fields with special characters or runtime values:

```tql
let $answers = {"the ultimate question": 42}
result = $answers["the ultimate question"]
```

Access fields based on runtime values:

```tql
let $severity_to_level = {"ERROR": 1, "WARNING": 2, "INFO": 3}
from {
  severity: "ERROR",
}
level = $severity_to_level[severity]  // Dynamic field access
```

Indexing expressions (see next section below) support numeric indices for records:

Accessing a field by position

```tql
from {
  foo: "Hello",
  bar: "World",
}
select first_field = this[0]  // "Hello"
```

### Moving Fields

[Section titled “Moving Fields”](#moving-fields)

The `move` expression transfers a field’s value and removes the original field in one atomic operation. Use the `move` keyword in front of a field to relocate it as part of an assignment:

```tql
from {foo: 1, bar: 2}
qux = move bar + 2
```

```tql
{foo: 1, qux: 4}  // Note: bar is gone
```

Use `move` in assignments to avoid separate delete operations:

```tql
// Clean approach
new_field = move old_field


// Instead of verbose
new_field = old_field
drop old_field
```

In addition to the `move` keyword, there exists a [`move`](/reference/operators/move) operator that is a convenient alternative when relocating multiple fields. For example, this sequence of assignments with the `move` keyword:

```tql
x = move foo
y = move bar
z = move baz
```

can be rewritten succinctly with the [`move`](/reference/operators/move) operator:

```tql
move x=foo, y=bar, z=baz
```

Key points

* Only usable in expression position (right side of `=`)
* Only works with fields, not arbitrary expressions
* Different from the [`move`](/reference/operators/move) operator that is a statement

### Metadata

[Section titled “Metadata”](#metadata)

Events carry both data and metadata. Access metadata fields using the `@` prefix. For instance, `@name` holds the name of the event.

Currently, available metadata fields include `@name`, `@import_time`, and `@internal`. Future updates may allow defining custom metadata fields.

```tql
from {
  event_name: @name,           // The schema name
  import_time: @import_time,   // When the event was imported
}
```

## Additional Operations

[Section titled “Additional Operations”](#additional-operations)

Beyond type-specific operations, TQL provides general-purpose operators for working with data.

### Unary Operations

[Section titled “Unary Operations”](#unary-operations)

TQL supports unary operators:

* `-` for numbers and durations (negation)
* `not` for boolean values (logical NOT)

```tql
from {
  value: 42,
  flag: true,
}
negative = -value
inverted = not flag
```

```tql
{
  value: 42,
  flag: true,
  negative: -42,
  inverted: false,
}
```

### Binary Operations

[Section titled “Binary Operations”](#binary-operations)

Binary operators work on two operands. The supported operations depend on the data types involved.

#### Arithmetic Operations Summary

[Section titled “Arithmetic Operations Summary”](#arithmetic-operations-summary)

| Operation      | Example | Behavior                         |
| -------------- | ------- | -------------------------------- |
| Addition       | `a + b` | Type coercion to wider type      |
| Subtraction    | `a - b` | Returns null on underflow        |
| Multiplication | `a * b` | Returns null on overflow         |
| Division       | `a / b` | Returns null on division by zero |

#### Time and Duration Arithmetic Summary

[Section titled “Time and Duration Arithmetic Summary”](#time-and-duration-arithmetic-summary)

| Operation             | Result Type | Example                 |
| --------------------- | ----------- | ----------------------- |
| `time + duration`     | `time`      | `now() + 5min`          |
| `time - duration`     | `time`      | `timestamp - 1h`        |
| `time - time`         | `duration`  | `end_time - start_time` |
| `duration + duration` | `duration`  | `5min + 30s`            |
| `duration * number`   | `duration`  | `5min * 3`              |
| `duration / number`   | `duration`  | `1h / 2`                |
| `duration / duration` | `double`    | `30min / 1h` → `0.5`    |

For detailed type coercion rules and more examples, see the specific type sections above.

#### Comparison

[Section titled “Comparison”](#comparison)

All types support equality comparison (`==`, `!=`). Additionally, ordered types support relational comparisons (`<`, `<=`, `>`, `>=`):

```tql
from {
  a: 5,
  b: 10,
}
set equal = a == b
set not_equal = a != b
set less = a < b
set less_equal = a <= b
set greater = a > b
set greater_equal = a >= b
```

```tql
{
  a: 5,
  b: 10,
  equal: false,
  not_equal: true,
  less: true,
  less_equal: true,
  greater: false,
  greater_equal: false,
}
```

**Comparison rules by type:**

* **All types**: Can compare equality with themselves and with `null`
* **Numeric types**: Can compare across different numeric types; ordered by magnitude
* **Strings**: Compare lexicographically (dictionary order)
* **IP addresses**: Ordered by their IPv6 bit pattern
* **Subnets**: Ordered by their IPv6 bit pattern
* **Times**: Chronologically ordered
* **Durations**: Ordered by length

#### Logical

[Section titled “Logical”](#logical)

Combine boolean expressions with `and` and `or`:

```tql
where timestamp > now() - 1d and severity == "critical"
where port == 22 or port == 3389
```

Short-circuit Evaluation

The `and` and `or` operators use short-circuit evaluation: they evaluate the second operand only if necessary. This means that if the first operand is sufficient to determine the result, the second operand is not evaluated.

#### Membership Testing (`in`)

[Section titled “Membership Testing (in)”](#membership-testing-in)

The `in` operator tests containment across different types:

| Expression            | Checks if…                              |
| :-------------------- | :-------------------------------------- |
| `value in list`       | List contains the value                 |
| `substring in string` | String contains the substring           |
| `ip in subnet`        | IP address is within the subnet range   |
| `subnet in subnet`    | First subnet is contained in the second |

```tql
from {
  ip: 10.0.0.5,
  port: 443,
  message: "connection error",
}
in_private = ip in 10.0.0.0/8
is_https = port in [443, 8443]
has_error = "error" in message
```

```tql
{
  ip: 10.0.0.5,
  port: 443,
  message: "connection error",
  in_private: true,
  is_https: true,
  has_error: true,
}
```

To negate membership tests, use `not in` or `not (value in container)`.

### Operator Precedence

[Section titled “Operator Precedence”](#operator-precedence)

Operations follow standard precedence rules:

| Precedence  | Operators                                | Associativity |
| ----------- | ---------------------------------------- | ------------- |
| 1 (highest) | Method call, field access, `[]` indexing | -             |
| 2           | Unary `+`, `-`                           | -             |
| 3           | `*`, `/`                                 | Left          |
| 4           | Binary `+`, `-`                          | Left          |
| 5           | `==`, `!=`, `<`, `<=`, `>`, `>=`, `in`   | Left          |
| 6           | `not`                                    | -             |
| 7           | `and`                                    | Left          |
| 8 (lowest)  | `or`                                     | Left          |

Expressions like `1 - 2 * 3 + 4` follow these precedence and associativity rules. The expression evaluates as `(1 - (2 * 3)) + 4`. Example: `1 + 2 * 3` evaluates as `1 + (2 * 3)` = 7

## Conditional Expressions

[Section titled “Conditional Expressions”](#conditional-expressions)

### Python-style Conditionals

[Section titled “Python-style Conditionals”](#python-style-conditionals)

TQL uses Python-style conditional expressions, i.e., `x if condition else y` where `x`, `y`, and `condition` are expressions.

Use conditionals in assignments and format strings:

```tql
from {
  response_code: 200,
  success: true,
}
status = "OK" if response_code == 200 else "ERROR"
message = f"Status: {'✓' if success else '✗'}"
```

Chaining is allowed but discouraged for readability:

```tql
from {
  severity: "high",
}
priority = 1 if severity == "critical" else 2 if severity == "high" else 3
```

### Standalone `if`

[Section titled “Standalone if”](#standalone-if)

`if` acts as a guard, returning `null` when false:

```tql
from {
  performance: "good",
  should_compute: false,
}
bonus = 1000 if performance == "excellent"  // null otherwise
result = now() if should_compute            // null since should_compute is false
```

```tql
{
  performance: "good",
  should_compute: false,
  bonus: null,
  result: null,
}
```

### Standalone `else`

[Section titled “Standalone else”](#standalone-else)

`else` performs null coalescing:

```tql
from {
  field: null,
}
value = field else "default"  // Use "default" if field is null
```

## Functions and Methods

[Section titled “Functions and Methods”](#functions-and-methods)

Functions and take positional and/or named arguments, producing a value as a result of their computation.

Call **free functions** with parentheses and comma-delimited arguments:

```tql
from {
  result: sqrt(16),
  rounded: round(3.7, 1),
  current: now(),
}
```

Call **methods** using dot notation:

```tql
from {
  text: "  hello  ",
  message: "world",
}
trimmed = text.trim()
length = message.length()
```

### Uniform Function Call Syntax (UFCS)

[Section titled “Uniform Function Call Syntax (UFCS)”](#uniform-function-call-syntax-ufcs)

TQL supports the [uniform function call syntax (UFCS)](https://en.wikipedia.org/wiki/Uniform_Function_Call_Syntax), which allows you to interchangeably call a function with at least one argument either as free function or method. For example, `length(str)` and `str.length()` resolve to the identical function call. The latter syntax is particularly suitable for function chaining, e.g., `x.f().g().h()` reads left-to-right as “start with `x`, apply `f()`, then `g()` and then `h()`,” compared to `h(g(f(x)))`, which reads “inside out.”

* Free function

  ```tql
  from {input: "  hello  "}
  output = capitalize(trim(input))
  ```

  ```tql
  {
    input: "  hello  ",
    output: "Hello",
  }
  ```

* Method

  ```tql
  from {input: "  hello  "}
  output = input.trim().capitalize()
  ```

  ```tql
  {
    input: "  hello  ",
    output: "Hello",
  }
  ```

Note the improved readability of function chaining:

* Free function

  ```tql
  from {message: "  HELLO world  "}
  message = replace(to_lower(trim(message)), " ", "_")
  ```

  ```tql
  {
    message: "hello_world",
  }
  ```

* Method

  ```tql
  from {message: "  HELLO world  "}
  message = message
    .trim()                       // Remove whitespace
    .to_lower()                   // Normalize case
    .replace(" ", "_")            // Replace spaces
  ```

  ```tql
  {
    message: "hello_world",
  }
  ```

Prefer method style

While both styles are valid, we prefer the method syntax for:

* **Readability**: Left-to-right data flow is easier to follow
* **Discoverability**: IDEs can better suggest available methods
* **Consistency**: Aligns with common programming patterns
* **Chaining**: Natural for multi-step transformations

For a comprehensive list of functions, see the [functions reference](/reference/functions).

## Advanced Expressions

[Section titled “Advanced Expressions”](#advanced-expressions)

### Lambda Expressions

[Section titled “Lambda Expressions”](#lambda-expressions)

Some operators and functions accept **lambda expressions** of the form `arg => expr`:

```tql
let $list = [1, 2, 3, 4, 5]
let $data = [{value: 1}, {value: 2}]
from {
  threshold: 3,
}
doubled = [1, 2, 3].map(x => x * 2)
filtered = $list.where(x => x > threshold)
transformed = $data.map(item => item.value * 100)
```

```tql
{
  threshold: 3,
  doubled: [
    2,
    4,
    6,
  ],
  filtered: [],
  transformed: [
    100,
    200,
  ],
}
```

The input gets an explicit name and the expression evaluates for each element.

### Pipeline Expressions

[Section titled “Pipeline Expressions”](#pipeline-expressions)

Some operators accept pipeline expressions as arguments, written with braces:

```tql
every 10s {
  from_http "https://api.example.com/"
  select id, data
}
fork {
  to_hive "s3://bucket/path/", partition_by=[id], format="json"
}
```

If the pipeline expression is the last argument, omit the preceding comma. Braces can contain multiple statements separated by newlines.

### Let Substitution

[Section titled “Let Substitution”](#let-substitution)

Reference previously defined [`let`](/explanations/language/statements#let) bindings using `$`-prefixed names:

```tql
let $pi = 3.14159
let $radius = 5
from {
  area: $radius * $radius * $pi,
}
```

```tql
{
  area: 78.53975,
}
```

Evaluation Time

Constants evaluate *once* at pipeline start and remain available throughout.

## Expression Evaluation

[Section titled “Expression Evaluation”](#expression-evaluation)

TQL expressions can be evaluated at different times: at pipeline start (constant) or per event (runtime).

### Constant Expressions

[Section titled “Constant Expressions”](#constant-expressions)

A **constant expression** evaluates to a constant when the pipeline containing it starts. Many pipeline operators require constant arguments:

```tql
head 5           // Valid: 5 is constant
head count       // Invalid: count depends on events
```

Functions like `now()` and `random()` can be constant-evaluated:

```tql
let $start_time = now()        // Evaluated once at pipeline start
where timestamp > $start_time - 1h
```

They are evaluated once at pipeline start, and the result is treated as a constant.

### Runtime Evaluation

[Section titled “Runtime Evaluation”](#runtime-evaluation)

Most expressions evaluate per event at runtime:

```tql
// These evaluate for each event
score = impact * likelihood
is_recent = timestamp > now() - 5min
formatted = f"Alert: {severity} at {timestamp}"
```

# Programs

TQL **programs** compose [statements](/explanations/language/statements) into complete data processing workflows that can execute. Valid TQL programs adhere to the following rules:

1. Adjacent operators must have identical types.
2. A pipeline must be **closed**, i.e., begin with void input and end with void output.

Pipeline Auto-Completion

When a pipeline is not closed, Tenzir attempts to *auto-complete* it. On the [command line](/guides/basic-usage/run-pipelines/#on-the-command-line), it suffices to write a sequence of transformations because Tenzir automatically adds a JSON input operator at the beginning and TQL output operator at the end. In the [web inteface](/guides/basic-usage/run-pipelines/#in-the-platform), auto-completetion takes place with an output operator: The web app appends [`serve`](/reference/operators/serve) to turn the dataflow into a REST API, allowing your browser to access it by routing the data through the platform.

## Statement chaining

[Section titled “Statement chaining”](#statement-chaining)

You chain statements with either a newline (`\n`) or pipe symbol (`|`). We purposefully offer choice to cater to two primary styles:

1. Vertical structuring with newlines for full-text editing
2. Horizontal inline pipe composition for command-line usage

Prefer the vertical approach for readability in files and documentation. Throughout this documentation, we only use the vertical style for clarity and consistency.

Let’s juxtapose the two styles. Here’s a vertical TQL program:

```tql
let $ports = [22, 443]


from_file "/tmp/logs.json"
where port in $ports
select src_ip, dst_ip, bytes
summarize src_ip, total=sum(bytes)
```

And here a horziontal one:

```tql
let $ports = [22, 443] | from "/tmp/logs.json" | where port in $ports | select src_ip, dst_ip, bytes | summarize src_ip, total=sum(bytes)
```

In theory, you can combine pipes and newlines to write programs that resemble Kusto and similar languages. However, we discourage this practice because it can make the code harder to read and maintainespecially when adding nested pipelines that increase the level of indentation.

## Diagnostics

[Section titled “Diagnostics”](#diagnostics)

TQL’s diagnostic system is designed to give you insights into what happens during data processing. There exist two types of diagnostics:

1. **Errors**: Stop pipeline execution immediately (critical failures)
2. **Warnings**: Signal data quality issues but continue processing

When a pipeline emits an error, it stops execution. Unless you configured the pipeline to restart on error, it now requires human intervention to resolve the issue and resume execution.

Warnings do not cause a screeching halt of the pipeline. They are useful for identifying potential issues that may impact the quality of the processed data, such as missing or unexpected values.

Best Practices

We have a dedicated [section on warnings and errors](/tutorials/learn-idiomatic-tql/#data-quality) in our [learning idiomatic TQL tutorial](/tutorials/learn-idiomatic-tql).

## Pipeline nesting

[Section titled “Pipeline nesting”](#pipeline-nesting)

Operators can contain entire subpipelines that execute based on the operator’s semantics. For example, the [`every`](/reference/operators/every) operator executes its subpipeline at regular intervals:

```tql
every 1h {
  from_http "api.example.com"
  select domain, risk
  context::update "domains", key=domain, value=risk
}
```

You define subpipelines syntactically within a block of curly braces (`{}`).

Some operators require that you define a closed (void-to-void) pipeline, whereas others exhibit parsing (bytes-to-events) or printing (events-to-bytes) semantics.

## Comments

[Section titled “Comments”](#comments)

Comments make implicit choices and assumptions explicit. They have no semantic effect and the compiler ignores them during parsing.

TQL features C-style comments, both single and multi-line.

### Single-line comments

[Section titled “Single-line comments”](#single-line-comments)

Use a double slash (`//`) to comment until the end of the line.

Here’s an example where a comment spans a full line:

```tql
// the app only supports lower-case user names
let $user = "jane"
```

Here’s an example where a comment starts in the middle of a line:

```tql
let $users = [
  "jane", // NB: also admin!
  "john", // Been here since day 1.
]
```

### Multi-line comments

[Section titled “Multi-line comments”](#multi-line-comments)

Use a slash-star (`/*`) to start a multi-line comment and a star-slash (`*/`) to end it.

Here’s an example where a comment spans multiple lines:

```tql
/*
 * User validation logic
 * ---------------------
 * Validate user input against a set of rules.
 * If any rule fails, the user is rejected.
 * If all rules pass, the user is accepted.
 */
let $user = "jane"
```

## Execution Model

[Section titled “Execution Model”](#execution-model)

TQL pipelines execute on a streaming engine that processes data incrementally. Understanding the execution model helps you write efficient pipelines and predict performance characteristics.

Key execution principles:

* **Stream processing by default**: Data flows through operators as it arrives
* **Lazy evaluation**: Operations execute only when data flows through them
* **Back-pressure handling**: Automatic flow control prevents memory exhaustion
* **Network transparency**: Pipelines can span multiple nodes seamlessly

### Streaming vs blocking

[Section titled “Streaming vs blocking”](#streaming-vs-blocking)

Understanding operator behavior helps write efficient pipelines:

**Streaming operators** process events incrementally:

* [`where`](/reference/operators/where): Filters one event at a time
* [`select`](/reference/operators/select): Transforms fields immediately
* [`drop`](/reference/operators/drop): Removes fields as events flow

**Blocking operators** need all input before producing output:

* [`sort`](/reference/operators/sort): Must see all events to order them
* [`summarize`](/reference/operators/summarize): Aggregates across the entire stream
* [`reverse`](/reference/operators/reverse): Needs complete input to reverse order

 Efficient: streaming operations first:

```tql
from "large_file.json"
where severity == "critical"    // Streaming: reduces data early
select relevant_fields          // Streaming: drops unnecessary data
sort timestamp                  // Blocking: but on reduced dataset
```

L Less efficient: blocking operation on full data:

```tql
from "large_file.json"
sort timestamp                  // Blocking: processes everything
where severity == "critical"    // Then filters
```

### Constant vs runtime evaluation

[Section titled “Constant vs runtime evaluation”](#constant-vs-runtime-evaluation)

Understanding when expressions evaluate helps write efficient pipelines:

Constants: evaluated once at pipeline start

```tql
let $threshold = 1Ki
let $start_time = 2024-01-15T09:00:00  // Would be now() - 1h in real usage
let $config = {
  ports: [80, 443, 8080],
  networks: [10.0.0.0/8, 192.168.0.0/16],
}


// Runtime: evaluated per event
from {bytes: 2Ki, timestamp: 2024-01-15T09:30:00},
     {bytes: 512, timestamp: 2024-01-15T09:45:00},
     {bytes: 3Ki, timestamp: 2024-01-15T10:00:00}
where bytes > $threshold            // Constant comparison
where timestamp > $start_time       // Constant comparison
current_time = 2024-01-15T10:30:00  // Would be now() in real usage
age = current_time - timestamp      // Runtime calculation
```

### Network transparency

[Section titled “Network transparency”](#network-transparency)

TQL pipelines can span network boundaries seamlessly. For example, the [`import`](/reference/operators/import) operator implicitly performs a network connection based on where it runs. If the `tenzir` binary executes the pipeline, the executor establishesa transparent network connection. If the pipeline runs within a node, the executor passes the data directly to the next operator in the same process.

# Statements

TQL programs are a sequence of statements. Operator statements perform various actions on data streams. Each operator statement can be thought of as a modular unit that processes data and can be combined with other operators to create complex dataflows.

[Statements](/explanations/language/statements) provide control and structure with bindings, operators, assignments, and control-flow primitives.

## Operator

[Section titled “Operator”](#operator)

Operator statements consist of the operator name, followed by an arbitrary number of arguments. Arguments are delimited by commas and may optionally be enclosed in parentheses. If the last argument is a pipeline expression, the preceding comma can be omitted for brevity.

Arguments can be of two kinds:

* **Positional**, where the order matters
* **Named**, where each argument is explicitly associated with a parameter name.

Additionally, arguments can be either:

* **Required**, meaning they must be provided
* **Optional**, which means they do not necessarily need to be provided, usually meaning they have a default value.

Finally, some operators require [constant arguments](/explanations/language/expressions/#constant-expressions), while others can take expressions, which are evaluated per event.

```tql
select foo, bar.baz
drop qux
head 42
sort abs(x)
```

Operators read, transform, and write data:

```tql
where src_endpoint.port in $critical_ports
```

Operators have an *upstream* and *downstream* type, which can be:

* **void**: No data (used at pipeline boundaries)
* **bytes**: Unstructured binary data (files, network streams)
* **events**: Structured, typed records (the primary data model)

The diagram below illustrates the cross-product of upstream and downstream types:

![Upstream and Downstream Types](/_astro/operator-table.ChkIKyOz_19DKCs.svg)

Here are visual examples that illustrate the upstream and downstream operator types.

```tql
from "/path/to/file.json"
where src_ip in 10.0.0.0/8
to "s3://bucket/dir/file.parquet"
```

This pipeline consists of three operators:

![Operator Composition Example 1](/_astro/operator-composition-example-1.CA4c2kgf_19DKCs.svg)

Let’s break it down:

1. [`from`](/reference/operators/from): A void-to-events input operator that reads events from a URI.
2. [`where`](/reference/operators/where): An events-to-events transformation operator that filters events matching a predicate.
3. [`to`](/reference/operators/to): An events-to-void output operator the writes to the specified URI.

The [`from`](/reference/operators/from) and [`to`](/reference/operators/to) operators perform a bit “magic” in that they also infer the format of the data being read or written, i.e., JSON due to the `.json` extension and Parquet due to the `.parquet` extension. You can also write the specific operators for these operations yourself:

```tql
load_kafka "topic"
read_ndjson
select host, message
write_yaml
save_zmq "tcp://1.2.3.4"
```

![Operator Composition Example 2](/_astro/operator-composition-example-2.C_LbiE3g_19DKCs.svg)

Here, we use a separate set of operators that go through bytes explicitly. Let’s break it down as well:

1. [`load_kafka`](/reference/operators/load_kafka): A void-to-events input operator that reads from a Kafka topic.
2. [`read_ndjson`](/reference/operators/read_ndjson): An bytes-to-events transformation operator (aka. *parser*) that reads newline-delimited JSON.
3. [`select`](/reference/operators/select): An events-to-events transformation operator that selects specific fields from events.
4. [`write_yaml`](/reference/operators/write_yaml): An events-to-bytes transformation operator that turns events to YAML foramt.
5. [`save_zmq`](/reference/operators/save_zmq): A bytes-to-void output operator that writes bytes to a ZeroMQ socket.

## Assignment

[Section titled “Assignment”](#assignment)

An assignment statement in TQL is structured as `<place> = <expression>`, where `<place>` typically refers to a field or item of a list. If the specified place already exists, the assignment will overwrite its current value. If it does not exist, a new field will be created.

The `<place>` can also reference a field path. For example, the statement `foo.bar = 42` assigns the value 42 to the field `bar` within the record `foo`. If `foo` is not a record or does not exist before, it will be set to a record containing just the field `bar`.

```tql
category_name = "Network Activity"
type_uid = class_uid * 100 + activity_id
traffic.bytes_out = event.sent_bytes
```

Assignments modify fields:

```tql
risk_score = bytes / 1Ki * severity_weight
```

When you write an assignment outside an explicit operator context, it implicitly uses the [`set`](/reference/operators/set) operator:

```tql
severity = "high"
// ...is actually shorthand for:
set severity = "high"
```

This design keeps pipelines concise while maintaining clarity about what’s happening.

## `let`

[Section titled “let”](#let)

The `let` statement binds a constant to a specific name within the pipeline’s scope. The syntax for a `let` statement is `let $<identifier> = <expression>`. For instance, `let $meaning = 42` creates a constant `$meaning` that holds the value 42.

More complex expressions can also be assigned, such as `let $start = now() - 1h`, which binds `$start` to a value representing one hour before the pipeline was started.

Constants defined with `let` can be referenced in subsequent statements, including other `let` statements. For example, `let $end = $start + 30min` can be used to define `$end` depending on the value of `$start`.

```tql
let $meaning = 42
let $start = now() - 1h
let $end = $start + 30min
```

A `let` statement introduces a constant that gets substituted during [expression evaluation](/explanations/language/expressions/#let-substitution).

## `if`

[Section titled “if”](#if)

The `if` statement is a primitive designed to route data based on a predicate. Its typical usage follows the syntax `if <expression> { … } else { … }`, where two subpipelines are specified within the braces. When its expression evaluates to `true`, the first pipeline processes the event. Conversely, when it evaluates to `false`, it is routed through the second one.

After the `if` statement the event flow from both pipelines is joined together. The `else` clause can be omitted, resulting in the syntax `if <expression> { … }`, which has the same behavior as `if <expression> { … } else {}`. Additionally, the `else` keyword can be followed by another `if` statement, allowing for chained `if` statements. This chaining can be repeated, enabling complex conditional logic to be implemented.

```tql
if score < 100 {
  severity = "low"
  drop details
} else if score < 200 {
  severity = "medium"
} else {
  severity = "high"
}
```

The `if` statement allows for branching into different statements:

```tql
if src_ip.is_private() {
  zone = "internal"
} else {
  zone = "external"
}
```

# Type System

TQL balances type safety with practical flexibility:

* **Strong typing prevents errors**: Can’t accidentally compare IPs as strings
* **Automatic inference**: Types detected from data, no declarations needed
* **Null semantics**: Every type is nullable (real data has gaps)
* **Domain operations**: Types come with relevant methods

Type safety in action

```tql
where src_ip in 10.0.0.0/8           // Type-checked subnet membership
where duration > 5min                // Type-checked duration comparison
where timestamp.hour() >= 9          // Extract hour from timestamp
where severity?.to_upper() == "HIGH" // Safe navigation with type conversion
```

## Available Types

[Section titled “Available Types”](#available-types)

The diagram below illustrates the type system at a glance:

![Type System](/_astro/type-system.BJr7NUfp_19DKCs.svg)

Tenzir’s type system is a superset of JSON: Every valid JSON object is a valid Tenzir value, but there also additional types available, such as `ip` and `subnet`.

### Basic Types

[Section titled “Basic Types”](#basic-types)

Basic types are stateless types with a static structure. The following basic types exist:

| Type       | Description                           | Example Expression Literal             |
| ---------- | ------------------------------------- | -------------------------------------- |
| `null`     | Denotes an absent or invalid value    | `null`                                 |
| `bool`     | A boolean value                       | `true`, `false`                        |
| `int64`    | A 64-bit signed integer               | `42`, `-100`, `1k`, `2Ki`              |
| `uint64`   | A 64-bit unsigned integer             | `42`, `100`, `1M`, `2Gi`               |
| `double`   | A 64-bit double (IEEE 754)            | `3.14`, `-0.5`, `1.23e-4`, `2.5k`      |
| `duration` | A time span (nanosecond granularity)  | `5s`, `10min`, `1h`, `2d`, `100ms`     |
| `time`     | A time point (nanosecond granularity) | `2024-01-15T10:30:00`, `2024-01-15`    |
| `string`   | A UTF-8 encoded string                | `"hello"`, `"world"`, `r"C:\path"`     |
| `blob`     | An arbitrary sequence of bytes        | `b"\x00\x01\x02"`, `b"raw bytes"`      |
| `ip`       | An IPv4 or IPv6 address               | `192.168.1.1`, `::1`, `fe80::1`        |
| `subnet`   | An IPv4 or IPv6 subnet                | `10.0.0.0/8`, `192.168.0.0/16`, `::/0` |
| `secret`   | A secret value                        | `secret("API_KEY")`                    |

#### Secrets

[Section titled “Secrets”](#secrets)

The `secret` type is a special type created by the [`secret`](/reference/functions/secret) function. Secrets can only be used as arguments for operators that accept them and only support a limited set of operations, such as concatenation.

See the [explanation page for secrets](/explanations/secrets) for more details.

### Complex Types

[Section titled “Complex Types”](#complex-types)

Complex types are stateful types that carry additional runtime information.

#### List

[Section titled “List”](#list)

The `list` type is an ordered sequence of values with a fixed element type.

Lists have zero or more elements.

#### Record

[Section titled “Record”](#record)

The `record` type consists of an ordered sequence *fields*, each of which have a name and type. Records must have at least one field.

The field name is an arbitrary UTF-8 string.

The field type is any Tenzir type.

## Optionality

[Section titled “Optionality”](#optionality)

All types are optional in that there exists an additional `null` data point in every value domain. Consequently, Tenzir does not have a special type to indicate optionality.

## Attributes

[Section titled “Attributes”](#attributes)

Every type has zero or more **attributes** that are free-form key-value pairs to enrich types with custom semantics.

## Why TQL has more types

[Section titled “Why TQL has more types”](#why-tql-has-more-types)

TQL extends JSON’s type system because **security and network data has specific patterns** that generic JSON cannot express efficiently or safely.

Example:

```tql
from {
  // JSON-compatible types
  event_id: 42,                      // number → int64
  is_alert: true,                    // bool
  message: "Connection established", // string
  tags: ["network", "established"],  // array → list
  metadata: {                        // object → record
    source: "firewall",
    version: 2
  },


  // TQL-specific types for security/network data
  src_ip: 192.168.1.100,                   // IP address
  network: 192.168.1.0/24,                 // subnet
  timestamp: 2024-01-15T10:30:00,          // time point
  duration: 250ms,                         // duration
  api_key: secret("sk_live_..."),          // secret (never logged)
  packet_data: b"\x00\x01\x02"             // blob (binary data)
}
```

### Why IP address types?

[Section titled “Why IP address types?”](#why-ip-address-types)

IP addresses aren’t just strings—they have structure and semantics:

✅ Native IP type operations:

```tql
where src_ip in 10.0.0.0/8      // Clear, type-safe subnet check
where src_ip.is_private()       // Built-in IP operations
where src_ip > 192.168.1.1      // Ordered comparison
```

❌ Without IP type (plain strings):

```tql
where src_ip.starts_with("10.") // Error-prone, doesn't handle all cases
where regex_match(src_ip, "^(10\\.|192\\.168\\.|...)")  // Complex and slow
// No way to do proper IP comparisons or subnet matching
```

### Why duration and time types?

[Section titled “Why duration and time types?”](#why-duration-and-time-types)

Time calculations are fundamental to log analysis:

✅ Native time type operations:

```tql
where timestamp > now() - 1h    // Intuitive time arithmetic
where response_time > 500ms     // Clear units
let $window = 5min              // Self-documenting
```

❌ Without time types (using milliseconds):

```tql
where timestamp_ms > current_time_ms - 3600000  // What unit is this?
where response_time_ms > 500    // Milliseconds? Seconds?
let $window = 300000            // Five minutes... or is it?
```

### Why subnet types?

[Section titled “Why subnet types?”](#why-subnet-types)

Network segmentation is core to security analysis:

✅ Subnet type operations:

```tql
let $internal = 10.0.0.0/8
let $dmz = 192.168.100.0/24
where src_ip in $internal and dst_ip not in $internal  // Outbound traffic
```

❌ Without subnet type:

```tql
// Would need complex IP range calculations and bit manipulation
// Error-prone and hard to maintain
```

### Why binary blob types?

[Section titled “Why binary blob types?”](#why-binary-blob-types)

Security and network data often contains raw binary content that needs special handling:

✅ Blob type for binary data:

```tql
// Handle packet captures, certificates, encrypted payloads
let $open_packet = b"\x00\x01\x02\x03"      // Blob literals
packet_data = decode_base64(encoded_packet) // Returns blob
where packet_data != $open_packet           // Comparing bytes directly
payload_hex = encode_hex(packet_data)       // blob → hex string
hash = hash_sha256(packet_data)             // Direct hashing of binary
```

❌ Without blob type (using strings):

```tql
// Binary data corrupts when treated as text
packet_data = "\xFF\xFE\x00\x00"  // Invalid UTF-8 sequence
// No safe way to handle non-UTF8 sequences
// Length calculations are wrong for multi-byte encodings
```

Blobs preserve exact byte sequences for forensics, packet analysis, and cryptographic operations.

### Why secret types?

[Section titled “Why secret types?”](#why-secret-types)

Credentials and sensitive data need protection from accidental exposure:

✅ Secret type for sensitive values:

```tql
// Secrets are never logged or displayed
let $api_key = secret("API_TOKEN")
auth_header = f"Bearer {$api_key}"


// Use secrets without exposing them
http "api.example.com", headers={Auth: $api_key}  // Use secret safely
to "debug.json"  // Secret value is not written to file
```

❌ Without secret type (using strings):

```tql
// Dangerous: credentials visible in logs
api_key = "sk_live_abc123..."   // Shows up in debug output


// No protection against exposure
to "debug.json"  // Oops, secret written to file
```

The secret type ensures sensitive data is never accidentally exposed in logs, outputs, or debugging.

## Comparison to Arrow

[Section titled “Comparison to Arrow”](#comparison-to-arrow)

All Tenzir types have a lossless mapping to [Arrow](http://arrow.apache.org) types, however, not all Arrow types have a Tenzir equivalent. As a result, it is currently not yet possible to import arbitrary Arrow data. In the future, we plan to extend our support for Arrow-native types and also offer conversion options for seamless data handover.

Tenzir has a few domain-specific types that map to Arrow [extension types](https://arrow.apache.org/docs/format/Columnar.html#extension-types). These are currently `enum`, `ip`, and `subnet`. Tenzir and Arrow attach type metadata to different entities: Tenzir attaches metadata to a type instance, whereas Arrow attaches metadata to a schema or record field.

# Packages

A **package** is a 1-click deployable unit that implements a specific use case. It contains pipelines, operators, contexts, examples, and tests. A templating mechanism makes packages customizable for a variety of deployment scenarios.

## Anatomy of a package

[Section titled “Anatomy of a package”](#anatomy-of-a-package)

A package comes in the form of a directory with the following structure:

* pkg/

  * examples/ self-contained examples the user can run

    * snippet.tql

  * operators/ user-defined operators (UDO)

    * ocsf/

      * foo.tql
      * bar.tql

    * map.tql

  * pipelines/ fully deployable pipelines

    * use-case-1.tql
    * use-case-2.tql

  * tests/ integration tests

    * inputs/

      * log.json
      * download.csv

    * test-ocsf.tql

    * test-map.tql

  * package.yaml package metadata, contexts, and inputs

Let’s discuss each component in detail.

### `examples`: Snippets to run

[Section titled “examples: Snippets to run”](#examples-snippets-to-run)

The `examples` directory contains self-contained code snippets that demonstrate how to use the package. These snippets exemplify the package’s features and provide runnable TQL code that users can execute after installing the package.

You can run these examples with a single click from [app.tenzir.com](https://app.tenzir.com).

### `operators`: User-defined operators (UDOs)

[Section titled “operators: User-defined operators (UDOs)”](#operators-user-defined-operators-udos)

The `operators` directory contains **user-defined operators (UDOs)**: reusable building blocks that you can use in your pipelines.

Tenzir names operators using the convention `<package>::[dirs...]::<basename>`. In the example above, the package defines `pkg::ocsf::foo`, `pkg::ocsf::bar`, and `pkg::map`.

You can use these operators in any pipeline:

```tql
from_file "sample.json"
pkg::ocsf::map // 👈 user-defined operator (UDO) defined in /pkg/ocsf/map.tql
to_file "ocsf.json"
```

The `tests/` directory can include tests for these operators.

### `pipelines`: End-to-end deployable TQL

[Section titled “pipelines: End-to-end deployable TQL”](#pipelines-end-to-end-deployable-tql)

The `pipelines` directory contains fully deployable TQL pipelines. Unlike UDOs, pipelines are complete units that must begin with an [input operator](/explanations/architecture/pipeline) and end with an [output operator](/explanations/architecture/pipeline). These pipelines often use UDOs defined in the same package.

You can configure pipelines using frontmatter at the beginning of the TQL file. The following options are available:

* `restart-on-error`: Configures automatic restart behavior when the pipeline encounters an error. By default, pipelines stop running and show an error state. This option causes pipelines to restart automatically instead.

  * Omit the option, or set it to `null` or `false` to disable automatic restarts.
  * Set it to `true` to enable restarts with a default delay of 1 minute.
  * Set it to a valid duration to enable restarts with a custom delay.

* `disabled`: Set to `true` to disable the pipeline. Defaults to `false`.

* `unstoppable`: Set to `true` to make the pipeline run automatically and indefinitely. You cannot pause or stop unstoppable pipelines manually. If they complete, they end up in a failed state. If you enable `restart-on-error`, they restart after the specified duration. Defaults to `false`.

Example:

```tql
---
restart-on-error: 1m
disabled: false
unstoppable: true
---


// TQL here
```

### `tests`: Integration tests

[Section titled “tests: Integration tests”](#tests-integration-tests)

The `tests` directory contains deterministic integration tests, primarily for UDOs. These tests leverage the [Test Framework](/reference/test-framework) to verify that operators behave correctly.

### `package.yaml`: Metadata

[Section titled “package.yaml: Metadata”](#packageyaml-metadata)

The `package.yaml` file serves as the **package manifest**. It contains metadata about a package and acts as a marker to identify a directory as a package. The file is required for every package.

#### Package description

[Section titled “Package description”](#package-description)

The beginning of `package.yaml` provides descriptive metadata:

```yaml
# The unique ID of the package. (required)
id: example


# The display name of the package and a path to an icon for the package.
name: Example
package_icon: https://github.com/tenzir.png


# The display name of the package author and a path to a profile picture.
author: Tenzir
author_icon: https://github.com/tenzir.png


# A user-facing description of the package.
description: |
  **Lorem ipsum** dolor sit amet, consectetur adipiscing elit. Nullam suscipit
  lacus felis, ac lacinia nibh pretium ut. Curabitur congue aliquam neque.
  Vivamus in magna non turpis malesuada volutpat ut a felis. Ut lorem eros,
  vulputate eget finibus ut, posuere sed leo. Vestibulum porta laoreet
  venenatis. Curabitur aliquet semper sem, et tincidunt metus cursus at. Nulla
  dapibus nibh vel faucibus commodo. Sed euismod eu sapien ut dictum. Phasellus
  tincidunt venenatis semper.
```

#### Inputs

[Section titled “Inputs”](#inputs)

The `inputs` section contains template variables that Tenzir replaces when you install the package. This allows the package definition to remain independent of the deployed environment.

```yaml
# Define user inputs to customize the package installation.
inputs:
  # Every input must have a unique id.
  refresh-rate:
    # A user-facing name for the input (required).
    name: Refresh Rate
    # A user-facing description of the input.
    description: |
      The interval at which we refresh our example context.
      Defaults to refreshing every second.
    # An (optional) default value for the input. The input is required if there
    # is no input value.
    default: 1s
```

You can reference inputs in pipeline and example definitions, and in context arguments using the syntax `{{ inputs.input-name }}`. Tenzir replaces these references with their configured values when installing the package. For example, with the input configured as above, the pipeline `every {{ inputs.refresh-rate }} { version }` would print the version once per second by default.

To write double curly braces literally, use the syntax `{{ '{{' }}` to produce the literal string enclosed inside the single quotes.

### Contexts

[Section titled “Contexts”](#contexts)

The `contexts` section defines contexts for [enrichment](/explanations/enrichment).

Here is an example context definition:

```yaml
# Define any number of contexts.
contexts:
  # A unique name for the context that's used in the context::* operators to
  # refer to the context.
  example:
    # The type of the context (required).
    type: lookup-table
    # An optional user-facing description of the context.
    description: |
      **Lorem ipsum** dolor sit amet, consectetur adipiscing elit. Nullam
      suscipit lacus felis, ac lacinia nibh pretium ut. Curabitur congue aliquam
      neque. Vivamus in magna non turpis malesuada volutpat ut a felis. Ut lorem
      eros, vulputate eget finibus ut, posuere sed leo. Vestibulum porta laoreet
      venenatis. Curabitur aliquet semper sem, et tincidunt metus cursus at.
      Nulla dapibus nibh vel faucibus commodo. Sed euismod eu sapien ut dictum.
      Phasellus tincidunt venenatis semper.


    # Arguments for creating the context, depending on the type. Refer to the
    # documentation of the individual context types to see the arguments they
    # require. Note that changes to these arguments do not apply to any
    # contexts that were previously created.
    args: {}
    # Disables the context.
    disabled: false
```

## Configuration during installation

[Section titled “Configuration during installation”](#configuration-during-installation)

During installation, Tenzir merges the package definition with a configuration object. This can happen in three ways:

1. In the [Tenzir Library](https://app.tenzir.com/library), you provide inputs that Tenzir converts into a `config` object.
2. Using the [`package::add`](/reference/operators/package/add) operator, you construct a `config` record explicitly.
3. Using IaC-style installation, you provide a `config.yaml` next to the `package.yaml` manifest.

Refer to the [package installation guide](/guides/basic-usage/install-a-package) for details on how each method works.

config.yaml

```yaml
# The equivalent of `package::add inputs={...}`.
inputs:
  filename: /opt/example/data.tsv
```

Tenzir replaces [inputs](/explanations/packages/#inputs), such as `from_file "{{ inputs.filename }}"`, with their configured values when installing a package. You must explicitly provide values for inputs that do not have a default value by specifying them in your `config.yaml`:

config.yaml

```yaml
inputs:
  filename: /opt/example/data.tsv
```

# Secrets

Operators accept secrets as parameters for sensitive values, such as authentication tokens, passwords, or even URLs.

Security Design

Secrets are designed to protect against *accidentally* compromising the secret value. You should assume that anyone with access to a workspace will be able to obtain the true value.

Read more in the [Security Design](#security-design) section.

## Usage in TQL

[Section titled “Usage in TQL”](#usage-in-tql)

You can use secret values only with operators that accept secrets. Operators generally do not document that they accept a secret, but they will accept secrets where appropriate.

You have two ways to pass an argument to an operator that expects a secret. The following examples use the [`to_splunk`](/reference/operators/to_splunk) operator, which expects a HEC-token for authentication:

* Provide a plain `string` ([Ad-hoc Secret](#ad-hoc-secrets)):

  ```tql
  to_splunk "https://localhost:8088", hec_token="my-plaintext-token"
  ```

  This creates a “secret” containing the string literal `my-plaintext-token`.

* Use the [`secret`](/reference/functions/secret) function ([Managed Secret](#managed-secrets)):

  ```tql
  to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token")
  ```

  The operator fetches the secret named `splunk-hec-token` to authenticate with the Splunk endpoint.

## The `secret` type

[Section titled “The secret type”](#the-secret-type)

Tenzir’s [type system](/explanations/language/types) includes secrets as a special type. You can access secrets only with the [`secret`](/reference/functions/secret) function.

### Internals

[Section titled “Internals”](#internals)

A value of type `secret` contains only the secret’s name, not the secret value itself. When a pipeline operator uses a secret, it resolves the name asynchronously. For ad-hoc secrets created from a string literal, name and value of the secret are identical, so no lookup occurs.

### Supported Operations

[Section titled “Supported Operations”](#supported-operations)

#### Concatenation

[Section titled “Concatenation”](#concatenation)

You can concatenate secrets with other secrets or strings using the `+` operator:

```tql
auth = "Bearer " + secret("my-secret")
url = $base_url + secret("user-name") + ":" + secret("password")
```

#### Format Strings

[Section titled “Format Strings”](#format-strings)

Secrets can be used in [format strings](/explanations/language/expressions#format-strings-f-strings). Unlike other types, which create strings with values formatted as-if using the `string` function, a format string containing a secret yields a secret.

The above concatenations can be written using format strings, like so:

```tql
auth = f"Bearer {secret("my-secret")}"
url = f"{$base_url}{secret("user-name")}:{secret("password")}"
```

A format string will turn secrets nested in a structured value (`record`, `list`) into the string `"***"`.

#### Encoding & Decoding

[Section titled “Encoding & Decoding”](#encoding--decoding)

In general, Tenzir does not assume that a secret is valid UTF-8, although most operators will make that constraint. Conversely some secret stores may require your secrets to be UTF-8, while some operators expect a binary secret.

To bridge this gap, you can base64-decode secrets using the [`decode_base64`](/reference/functions/decode_base64) function and base64-encode them using the [`encode_base64`](/reference/functions/encode_base64) function:

```tql
let $binary_secret = secret("my-encoded-secret").decode_base64()
```

You can also encode (or decode) the result of a concatenation or format string, which is useful for some APIs:

```tql
let $headers = {
  auth: f"{secret("user")}:{secret("password")}".encode_base64()
}
```

#### Turning Secrets into Strings

[Section titled “Turning Secrets into Strings”](#turning-secrets-into-strings)

You cannot turn a secret into a string. Any such attempt will simply produce the string `"***"`.

#### Python

[Section titled “Python”](#python)

Since secrets can also be values in a pipeline, they can also be passed *through* the [`python`](/reference/operators/python) operator. You must not modify a secret in the [`python`](/reference/operators/python) operator.

## Ad-hoc Secrets

[Section titled “Ad-hoc Secrets”](#ad-hoc-secrets)

**Ad-hoc secrets** are secrets implicitly created from a `string` within [TQL](/explanations/language). This happens when you provide a `string` to an operator that expects a `secret`.

Providing plain string values can help when you develop pipelines and do not want to add the secret to the configuration or a secret store.

This approach is also useful for arguments that you do not consider a secret, so you don’t have to create a managed secret.

However, secrets created from plain `string`s do not enjoy the same security as managed secrets. Their value appears directly in the TQL pipeline definition, as well as in the compiled and executed representation. As such, the Tenzir Node may persist the value.

## Managed Secrets

[Section titled “Managed Secrets”](#managed-secrets)

Managed secrets are identified by their name and can come from the following sources, in descending precedence:

1. The environment of the Tenzir Node
2. The configuration of the Tenzir Node
3. The Tenzir Platform secret store for the workspace the Tenzir Node belongs to

![Resolution](/_astro/secret-resolution.a_JW2CI2_19DKCs.svg)

You use a managed secrets value using the [`secret`](/reference/functions/secret).

The Tenzir Node looks up the secret’s actual value only when an operator requires it. It first checks the config, with environment variables taking precedence over configuration file entries. If the secret is not found there, a request is made to the Tenzir Platform.

If the value is transferred over any network connection, it additionally be encrypted using [ECIES](https://en.wikipedia.org/wiki/Integrated_Encryption_Scheme) with a one-time, per-secret key. The value remains encrypted throughout the transfer until the final usage site.

A `tenzir` client process can use managed secrets only if it is able to connect to a Tenzir Node.

### Configuration Secrets

[Section titled “Configuration Secrets”](#configuration-secrets)

You can specify secrets in the `tenzir.yaml` config file, under the path `tenzir.secrets`:

tenzir.yaml

```yaml
tenzir:
  secrets:
    # Add your secrets here.
    geheim: 1528F9F3-FAFA-45B4-BC3C-B755D0E0D9C2
```

Since you can also set Tenzir’s configuration options as environment variables, you can define secrets in the environment as well. The above secret could also be defined via the environment variable `TENZIR_SECRETS__GEHEIM`. An environment variable takes precedence over an equivalent key in the configuration file.

See the [configuration reference](/reference/node/configuration) for more details.

Tenzir hides the `tenzir.secrets` section from the [`config()`](/reference/functions/config) function.

### Platform Secrets

[Section titled “Platform Secrets”](#platform-secrets)

The Tenzir Platform stores a separate set of secrets for every workspace. All Tenzir Nodes connected to that workspace can access these secrets.

Read about how to configure the platform secret store in the [guides section](/guides/platform-setup/configure-secret-store#configuring-the-platform-secret-store).

#### External Secret Stores

[Section titled “External Secret Stores”](#external-secret-stores)

You can configure the Tenzir Platform to provide access to secrets stored in an external secret store instead of using it own store. This access is read-only.

Read more about how to configure an external secret store in the [guides section](/guides/platform-setup/configure-secret-store#configuring-external-secret-stores).

## Legacy Model

[Section titled “Legacy Model”](#legacy-model)

You can use the configuration option `tenzir.legacy-secret-model` to change the behavior of the `secret` function so that it returns a `string` instead of a `secret`.

When you use the legacy model, you can only use secrets from the Tenzir Node’s configuration. You cannot use secrets from the Tenzir Platform’s secret store.

We do not recommend enabling this option. It exists as a transition option and will be deprecated and removed in some future version.

## Security Design

[Section titled “Security Design”](#security-design)

We designed secrets to protect against *accidentally* compromising the secret value by revealing it in a log file, showing it in a UI element, or committing it to a code repository as part of a pipeline.

However, anyone with access to a workspace can access secret values if sufficiently motivated.

For example, you could use a simple HTTP API that echoes requests to extract secret values:

```tql
from_http "echo.api.com",
  headers = { leaked: secret("key") },
  metadata_field=metadata
select metadata.headers.leaked
```

```tql
{ leaked: "secret-value" }
```

### Secrets in Diagnostics

[Section titled “Secrets in Diagnostics”](#secrets-in-diagnostics)

We take care not to leak managed secrets in our diagnostic messages. However, numerous of our integrations rely on third party libraries. Those libraries may produce error messages which are outside of our control, but that we forward to the user to help understand an issue.

As a remedy for this, an operator censors the values of managed secrets it has used in all diagnostics. Please note that we only censor an occurrence of the full value, not parts of it. This means that a third party diagnostic forwarded to the user may still contain part of a secret value.

Use smaller secrets

Try and keep your managed secrets to the logical unit that is a secret and use multiple secrets instead of one big one.

For example and S3 URI may contain multiple keys that should be kept as separate secrets, instead of a single one:

```tql
let $url = f"s3://{secret("access-key")}:{secret("secret-key")}@bucket/path/to/file"
```

# Guides

**Guides** are practical step-by-step explanation to help you achieve a specific goal. They are most useful when you're trying to get something done.

<!-- The SVG image -->

![Documentation structure](/guides.svg)

<!-- Clickable overlay areas -->

<!-- Tutorials (top-left) -->

[](/tutorials/)

<!-- Guides (top-right) -->

<!-- Explanations (bottom-left) -->

[](/explanations/)

<!-- Reference (bottom-right) -->

[](/reference/)

# Account Creation

Get started with Tenzir by creating an account:

1. Go to [app.tenzir.com](https://app.tenzir.com). ![Landing page](/_astro/signin.D-Kjtq1Z_12o64U.webp)

2. Log in with your identity provider. This creates an account implicitly. ![IdP](/_astro/signin-choice.CDMmwGmU_1kGjAa.webp)

3. Can't use an identity provider? Click **Sign up** below the **Continue** button. We recommend using a [passkey](https://www.passkeys.com/what-are-passkeys.html) as a safer alternative to traditional passwords. ![Create passkey](/_astro/signin-passkey.AZS33VhO_1Cr2tF.webp)

4. If you created an account with us, you'll receive an email with a link to verify your email address. Click the link to complete the registration process.

🎉 Congratulations, you now have an account and can can freely use the Tenzir [Community Edition](https://tenzir.com/pricing).

## Delete an Account

[Section titled “Delete an Account”](#delete-an-account)

Delete your account as follows:

1. Go to the [Account](https://app.tenzir.com/account) page.
2. Click *Delete Account*.
3. (Optionally) Leave a note explaining why you delete your account.

Caution

Deleting your account will remove all data about you from our cloud platform. You will also lose the ability to manage pipelines on your node.

If you decide to come back, just re-create an account as described above.

# Collect metrics

Tenzir keeps track of metrics about node resource usage, pipeline state, and runtime performance.

Metrics are stored as internal events in the node’s storage engine, allowing you to work with metrics just like regular data. Use the [`metrics`](/reference/operators/metrics) input operator to access the metrics. The operator documentation lists [all available metrics](/reference/operators/metrics#schemas) in detail.

The `metrics` operator provides a *copy* of existing metrics. You can use it multiple time to reference the same metrics feed.

## Write metrics to a file

[Section titled “Write metrics to a file”](#write-metrics-to-a-file)

Export metrics continuously to a file via `metrics --live`:

```tql
metrics live=true
write_ndjson
save_file "metrics.json", append=true
```

This attaches to incoming metrics feed, renders them as NDJSON, and then writes the output to a file. Without the `live` option, the `metrics` operator returns the snapshot of all historical metrics.

## Summarize metrics

[Section titled “Summarize metrics”](#summarize-metrics)

You can [shape](/guides/data-shaping/shape-data) metrics like ordinary data, e.g., write aggregations over metrics to compute runtime statistics suitable for reporting or dashboarding:

```tql
metrics "operator"
where sink == true
summarize runtime=sum(duration), pipeline_id
sort -runtime
```

The above example computes the total runtime over all pipelines grouped by their unique ID.

# Install a package

[Packages](/explanations/packages) provide a flexible approach for combining operators, pipelines, contexts, and examples into a unified deployable unit.

Write your own package

Want to create your own package? Check out our [package development tutorial](/tutorials/write-a-package).

## Install from the Tenzir Library

[Section titled “Install from the Tenzir Library”](#install-from-the-tenzir-library)

The most convenient way to install a package is through the [Tenzir Library](https://app.tenzir.com/library):

1. Click on a package
2. Select the *Install* tab
3. Define your inputs (optional)
4. Click the *Install* button in the bottom right

## Install with the package operator

[Section titled “Install with the package operator”](#install-with-the-package-operator)

To install a package interactively in TQL, use the [`package::add`](/reference/operators/package/add) operator:

```tql
package::add "/path/to/pkg"
```

This installs the package from the directory `/path/to/pkg`. Pass an `inputs` record to adjust the package configuration and replace the package’s templates with concrete values:

```tql
package::add "package.yaml", inputs={
  endpoint: "localhost:42000",
  policy: "block",
}
```

Your package now appears when you list all installed packages:

```tql
package::list
```

```tql
{
  id: "your-package",
  install_status: "installed",
  // …
}
```

To uninstall a package interactively, use [`package::remove`](/reference/operators/package/remove) and pass the package ID.

```tql
package::remove "your-package"
```

## Install with Infrastructure as Code (IaC)

[Section titled “Install with Infrastructure as Code (IaC)”](#install-with-infrastructure-as-code-iac)

For IaC-style deployments, you can install packages *as code* by putting them next to your `tenzir.yaml` configuration file:

* /opt/tenzir/etc/tenzir/

  * packages/

    * your-package/

      * operators/

        * …

      * pipelines/

        * …

      * config.yaml The configuration for the package

      * package.yaml The package manifest with metadata

  * tenzir.yaml

Inside the `packages` directory, each installed package has its own directory. The directory name matches the package ID.

The node searches for packages in the following locations:

1. The `packages` directory in all [configuration directories](/explanations/configuration).
2. All directories specified in the `tenzir.package-dirs` configuration option.

To provide inputs in IaC-mode, place a `config.yaml` file next to the `package.yaml` file. For example, this configuration sets the inputs `endpoint` and `policy`:

config.yaml

```yaml
inputs:
  endpoint: localhost:42000
  policy: block
```

# Manage a pipeline

A pipeline can be in one of the following **states** after you [run it](/guides/basic-usage/run-pipelines):

* **Created**: the pipeline has just been deployed.
* **Running**: the pipeline is actively processing data.
* **Completed**: there is no more data to process.
* **Failed**: an error occurred.
* **Paused**: the user interrupted execution, keeping in-memory state.
* **Stopped**: the user interrupted execution, resetting all in-memory state.

The [app](https://app.tenzir.com/) or [API](/reference/node/api) allow you to manage the pipeline lifecycles.

## Change the state of a pipeline

[Section titled “Change the state of a pipeline”](#change-the-state-of-a-pipeline)

In the [app](https://app.tenzir.com/overview), an icon visualizes the current pipeline state. Change a state as follows:

1. Click the checkbox on the left next to the pipeline, or the checkbox in the column header to select all pipelines.
2. Click the button corresponding to the desired action, i.e., *Start*, *Pause*, *Stop*, or *Delete*.
3. Confirm your selection.

For the [API](/reference/node/api), use the following endpoints based on the desired actions:

* Start, pause, and stop: `/pipeline/update`
* Delete: `/pipeline/delete`

## Understand pipeline state transitions

[Section titled “Understand pipeline state transitions”](#understand-pipeline-state-transitions)

The diagram below illustrates the various states, where circles correspond to states and arrows to state transitions:

![Pipeline States](/_astro/pipeline-states.KFiJ59Ps_19DKCs.svg)

The grey buttons indicate the actions you, as a user, can take to transition into a different state. The orange arrows are transitions that take place automatically based on system events.

# Run pipelines

You can run a [pipeline](/explanations/architecture/pipeline) via the [platform](https://app.tenzir.com), on the command line using the `tenzir` binary, or as code via the configuration file.

## In the platform

[Section titled “In the platform”](#in-the-platform)

Run a pipeline by writing typing it in the editor and hitting the *Run* button.

The following invariants apply:

1. You must start with an input operator
2. The browser is always the output operator

The diagram below illustrates these mechanics:

![Pipeline in the Browser](/_astro/pipeline-browser.B2LCkA6F_19DKCs.svg)

For example, write `from {x: 42}` and click *Run* to see a single event show up.

## On the command line

[Section titled “On the command line”](#on-the-command-line)

On the command line, run `tenzir <pipeline>` where `<pipeline>` is the definition of the pipeline.

If the pipeline expects events as its input, an implicit `load_stdin | read_json` will be prepended. If it expects bytes instead, only `load_stdin` is prepended. Likewise, if the pipeline outputs events, an implicit `write_json | save_stdout` will be appended. If it outputs bytes instead, only `save_stdout` is appended.

The diagram below illustrates these mechanics:

![Pipeline on the command line](/_astro/pipeline-cli.BkLsRsJs_19DKCs.svg)

For example, run `tenzir 'version | drop dependencies'` to see a single event in the terminal:

```tql
{
  version: "5.0.1+g847fcc6334",
  tag: "g847fcc6334",
  major: 5,
  minor: 0,
  patch: 1,
  features: [
    "chart_limit",
    "modules",
    "tql2_from",
    "exact_schema",
    "tql2_only",
  ],
  build: {
    type: "Release",
    tree_hash: "ef28a81eb124cc46a646250d1fb17390",
    assertions: false,
    sanitizers: {
      address: false,
      undefined_behavior: false,
    },
  },
}
```

You could also render the output differently by choosing a different format:

```sh
tenzir 'version | drop dependencies | write_csv'
tenzir 'version | drop dependencies | write_ssv'
tenzir 'version | drop dependencies | write_parquet | save_file "version.parquet'
```

Instead of passing the pipeline description to the `tenzir` executable, you can also load the definition from a file via `-f`:

```sh
tenzir -f pipeline.tql
```

This will interpret the file contents as pipeline and run it.

## As Code

[Section titled “As Code”](#as-code)

In addition to running pipelines interactively, you can also deploy *pipelines as code (PaC)*. This infrastructure-as-code-like method differs from the app-based deployment in two ways:

1. Pipelines deployed as code always start with the Tenzir node, ensuring continuous operation.
2. To safeguard them, deletion via the user interface is disallowed.

Here’s a an example of deploying a pipeline through your configuration:

\<prefix>/etc/tenzir/tenzir.yaml

```yaml
tenzir:
  pipelines:
    # A unique identifier for the pipeline that's used for metrics, diagnostics,
    # and API calls interacting with the pipeline.
    suricata-over-tcp:
      # An optional user-facing name for the pipeline. Defaults to the id.
      name: Onboard Suricata from TCP
      # An optional user-facing description of the pipeline.
      description: |
        Onboards Suricata EVE JSON from TCP port 34343.
      # The definition of the pipeline. Configured pipelines that fail to start
      # cause the node to fail to start.
      definition: |
        load_tcp "0.0.0.0:34343"
        read_suricata
        publish "suricata"
      # Pipelines that encounter an error stop running and show an error state.
      # This option causes pipelines to automatically restart when they
      # encounter an error instead. The first restart happens immediately, and
      # subsequent restarts after the configured delay, defaulting to 1 minute.
      # The following values are valid for this option:
      # - Omit the option, or set it to null or false to disable.
      # - Set the option to true to enable with the default delay of 1 minute.
      # - Set the option to a valid duration to enable with a custom delay.
      restart-on-error: 1 minute
      # Add a list of labels that are shown in the pipeline overview page at
      # app.tenzir.com.
      labels:
        - Suricata
        - Onboarding
      # Disable the pipeline.
      disabled: false
      # Pipelines that are unstoppable will run automatically and indefinitely.
      # They are not able to pause or stop.
      # If they do complete, they will end up in a failed state.
      # If `restart-on-error` is enabled, they will restart after the specified
      # duration.
      unstoppable: true
```

# Build Environment

## Use Nix as reproducible development environment

[Section titled “Use Nix as reproducible development environment”](#use-nix-as-reproducible-development-environment)

We use [Nix](https://nixos.org) for reproducible Tenzir Node builds.

Fetch the dependencies for a dynamic build by running `nix develop` from the topmost directory in the `tenzir/tenzir` source tree.

You can automatically add the dependencies to your shell environment when you cd into the source directory via [direnv](https://direnv.net). Create an `.envrc` with the content:

```plaintext
use flake
```

If you want to silence the messages about binary caches you can use a variation of `.envrc` that invokes `nix` with a lower verbosity setting:

```sh
use_flake2() {
  watch_file flake.nix
  watch_file flake.lock
  mkdir -p "$(direnv_layout_dir)"
  eval "$(nix --quiet --quiet print-dev-env --profile "$(direnv_layout_dir)/flake-profile" "$@")"
}


use_flake2
```

The `tenzir/tenzir` repository comes with a set of CMake configure and build presets that can be used in this environment:

* `nix-clang-debug`
* `nix-clang-redeb`
* `nix-clang-release`
* `nix-gcc-debug`
* `nix-gcc-redeb`
* `nix-gcc-release`

Note

This build environment is currently only tested on Linux.

### Compile static binaries

[Section titled “Compile static binaries”](#compile-static-binaries)

Static binaries require a that the dependencies were built in static mode as well. That means we need to use a different environment; you can enter it with:

```sh
nix develop .#tenzir-static
```

The CMake presets for that mode are:

* `nix-gcc-static-debug`
* `nix-gcc-static-redeb`
* `nix-gcc-static-release`

# Code of Conduct

## Our Pledge

[Section titled “Our Pledge”](#our-pledge)

In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.

## Our Standards

[Section titled “Our Standards”](#our-standards)

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others’ private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities

[Section titled “Our Responsibilities”](#our-responsibilities)

Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.

## Scope

[Section titled “Scope”](#scope)

This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.

## Enforcement

[Section titled “Enforcement”](#enforcement)

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by [contacting the project team](https://docs.tenzir.com/discord). All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.

## Attribution

[Section titled “Attribution”](#attribution)

This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org), version 1.4, available at <https://www.contributor-covenant.org/version/1/4/code-of-conduct.html>.

For answers to common questions about this code of conduct, see <https://www.contributor-covenant.org/faq>.

# Coding Style

This page documents the coding style for the languages we use.

## Documentation

[Section titled “Documentation”](#documentation)

When documenting bugs, deficiencies, future tasks, or noteworthy things in the code, we use two keywords that most editors and tools recognize: `FIXME:` and `TODO:`. We use `FIXME` for a *known bug* and `TODO` for everything else. The subsequent `:` is important for tooling, such as syntax highlighters. Here are two examples:

```cpp
// FIXME: this currently fails on FreeBSD.
// FIXME: this algorithms is broken for i < 0.
```

A typical `TODO` could be:

```python
# TODO: refactor this code to separate mechanism from policy.
# TODO: add another argument to process user-defined tags.
```

## EditorConfig

[Section titled “EditorConfig”](#editorconfig)

* Some projects in the Tenzir organization provide `.editorconfig` files. Please respect the settings defined in these. For many editors, plugins exist to automatically apply EditorConfig files.

## Scripting Languages

[Section titled “Scripting Languages”](#scripting-languages)

* Scripts are executables (`chmod +x path/to/your-script`) and words in their names are separated using dashes (`your-script` over `your_script`).

* The first line of a script should be a shebang, e.g., `'#!/bin/sh'` or `#!/usr/bin/env python3`.

* The second line is empty.

* Starting at the third line, write a comment detailing usage instructions, and a short and concise description of the script.

### Shell Scripts

[Section titled “Shell Scripts”](#shell-scripts)

* Prefer to use POSIX sh when possible.

* Tenzir uses [ShellCheck](https://github.com/koalaman/shellcheck) for linting. Pull request review feedback for shell scripts is in parts based on ShellCheck.

### Python

[Section titled “Python”](#python)

* We use Python 3, with no special restrictions for newer features. Specify the minimum required version in the shebang, e.g. `#!/usr/bin/env python3.6`.

* Use [black](https://github.com/psf/black) for linting. Black is a heavily opinionated tool for both formatting and linting, and we found its opinion to be a good standard for us to use.

## Web Development

[Section titled “Web Development”](#web-development)

* All web-based projects in the Tenzir organization define style checkers and linters in their respective configuration files, so they are automatically applied.

## CMake

[Section titled “CMake”](#cmake)

CMake is the build scaffold of Tenzir.

### General

[Section titled “General”](#general)

* Prefer targets and properties over variables.

* Don’t use global *include\_directories*.

* Export consumable targets to both build and install directories.

* Assign sensible export names for your targets, the `tenzir::` namespace is implicitly prefixed.

### Formatting

[Section titled “Formatting”](#formatting)

* The cmake files are formatted with [cmake-format](https://github.com/cheshirekow/cmake_format).

## C++

[Section titled “C++”](#c)

Tenzir’s core is written in C++. We follow a style based on STL, [Google style](http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml), and [CAF style](https://github.com/actor-framework/actor-framework/blob/master/CONTRIBUTING.md) guidelines.

### General

[Section titled “General”](#general-1)

* Minimize vertical whitespace within functions. Use comments to separate logical code blocks.

* The `const` keyword precedes the type, e.g., `const T&` as opposed to `T const&`.

* `*` and `&` bind to the *type*, e.g., `T* arg` instead of `T *arg`.

* When declaring variables and functions, provide the [storage class specifier](https://en.cppreference.com/w/cpp/language/storage_duration) (`extern`, `static`, `thread_local`, `mutable`) first, followed by the [declaration specifiers](https://en.cppreference.com/w/cpp/language/declarations#Specifiers) in order of `friend`, `inline`, `virtual`, `explicit`, `constexpr`, `consteval`, and `constinit`.

- Always use `auto` to declare a variable unless you cannot initialize it immediately or if you actually want a type conversion. In the latter case, provide a comment why this conversion is necessary.

- Never use unwrapped, manual resource management such as `new` and `delete`.

- Never use `typedef`; always write `using T = X` in favor of `typedef X T`.

- Keywords are always followed by a whitespace: `if (...)`, `template <...>`, `while (...)`, etc.

- Do not add whitespace when negating an expression with `!`:

  ```cpp
  if (!sunny())
    stay_home()
  ```

- Opening braces belong onto the same line:

  ```cpp
  struct foo {
    void f() {
    }
  };
  ```

- Use inline functions for trivial code, such as getters/setters or straight-forward logic that does not span more than 3 lines.

### Header

[Section titled “Header”](#header)

* Header filenames end in `.hpp` and implementation filenames in `.cpp`.

* All header files should use `#pragma once` to prevent multiple inclusion.

* Don’t use `#include` when a forward declarations suffices. It can make sense to outsource forward declarations into a separate file per module. The file name should be `<MODULE>/fwd.h`.

* Include order is from high-level to low-level headers, e.g.,

  ```cpp
  // iff a matching header exists
  #include "tenzir/matching_header.hpp"


  #include "tenzir/logger.hpp"


  #include <3rd/party.hpp>


  #include <memory>


  #include <sys/types.h>
  ```

  `clang-format` is configured to automatically change the include order accordingly. Includes separated by preprocessor directives need to be sorted manually.

  Within each section, the order should be alphabetical. Tenzir includes should always be in double quotes and relative to the source directory, whereas system-wide includes in angle brackets. See below for an example on how to structure includes in unit tests.

* As in the standard library, the order of parameters when declaring a function is: inputs, then outputs. API coherence and symmetry trumps this rule, e.g., when the first argument of related functions model the same concept.

### Classes

[Section titled “Classes”](#classes)

* Use the order `public`, `protected`, `private` for functions and members in classes.

* Mark single-argument constructors as `explicit` to avoid implicit conversions; use `explicit(false)` to indicate that a non-explicit constructor is intentional.

* The order of member functions within a class is: constructors, operators, mutating members, accessors.

* Friends first: put friend declaration immediate after opening the class.

* Put declarations (and/or definitions) of assignment operators right after the constructors, and all other operators at the bottom of the public section.

* Use structs for state-less classes or when the API is the struct’s state.

* Prefer types with value semantics over reference semantics.

* Use the [rule of zero or rule of five](http://en.cppreference.com/w/cpp/language/rule_of_three).

* When providing a move constructor and move-assignment operator, declare them as `noexcept`.

* Use brace-initialization for member construction when possible. Only use parenthesis-initialization to avoid calling a `std::initializer_list` overload.

### Naming

[Section titled “Naming”](#naming)

* Class names, constants, and function names are lowercase with underscores.

* Template parameter types should be written in CamelCase.

* Types and variables should be nouns, while functions performing an action should be “command” verbs. Getter and setter functions should be nouns. We do not use an explicit `get_` or `set_` prefix. Classes used to implement metaprogramming functions also should use verbs, e.g., `remove_const`.

* All library macros should start with `TENZIR_` to avoid potential clashes with external libraries.

* Names of (i) classes/structs, (ii) functions, and (iii) enums should be lower case and delimited by underscores.

* Put non-API implementation into namespace `detail`.

* Member variables have an underscore (`_`) as suffix, unless they constitute the public interface. Getters and setters use the same member name without the suffix.

* Put static non-const variables in an anonymous namespace.

* Name generic temporary or input variables `x`, `y`, and `z`. If such variables represent a collection of elements, use their plural form `xs`, `ys`, and `zs`.

* Prefix counter variables with `num_`.

* If a function has a return value, use `result` as variable name.

### Breaking

[Section titled “Breaking”](#breaking)

* Break constructor initializers after the comma, use two spaces for indentation, and place each initializer on its own line (unless you don’t need to break at all):

  ```cpp
  my_class::my_class()
    : my_base_class{some_function()},
      greeting_{"Hello there! This is my_class!"},
      some_bool_flag_{false} {
    // ok
  }


  other_class::other_class() : name_{"tommy"}, buddy_{"michael"} {
    // ok
  }
  ```

* Break function arguments after the comma for both declaration and invocation:

  ```cpp
  a_rather_long_return_type f(const std::string& x,
                              const std::string& y) {
    // ...
  }
  ```

  If that turns out intractable, break directly after the opening parenthesis:

  ```cpp
  template <typename T>
  black_hole_space_time_warp f(
    typename const T::gravitational_field_manager& manager,
    typename const T::antimatter_clustear& cluster) {
    // ...
  }
  ```

* Break template parameters without indentation:

  ```cpp
  template <class T>
  auto identity(T x) {
    return x;
  }
  ```

* Break trailining return types without indentation if they cannot fit on the same line:

  ```cpp
  template <class T>
  auto compute_upper_bound_on_compressed_data(T x)
  -> std::enable_if_t<std::is_integral_v<T>, T> {
    return detail::bound(x);
  }
  ```

* Break before binary and ternary operators:

  ```cpp
  if (today_is_a_sunny_day()
      && it_is_not_too_hot_to_go_swimming()) {
    // ...
  }
  ```

### Template Metaprogramming

[Section titled “Template Metaprogramming”](#template-metaprogramming)

* Use the `typename` keyword only to access dependent types. For general template parameters, use `class` instead:

  ```cpp
  template <class T>
  struct foo {
    using type = typename T::type;
  };
  ```

* Use `T` for generic, unconstrained template parameters and `x` for generic function arguments. Suffix both with `s` for template parameter packs:

  ```cpp
  template <class T, class... Ts>
  auto f(T x, Ts... xs) {
    // ...
  }
  ```

* Break `using name = ...` statements always directly after `=` if they do not fit in one line.

* Use one level of indentation per “open” template and place the closing `>`, `>::type` or `>::value` on its own line. For example:

  ```cpp
  using optional_result_type =
    typename std::conditional<
      std::is_same<result_type, void>::value,
      bool,
      optional<result_type>
    >::type;
  ```

* When dealing with “ordinary” templates, use indentation based on the position of the last opening `<`:

  ```cpp
  using type = quite_a_long_template_which_needs_a_break<std::string,
                                                         double>;
  ```

* When adding new type traits, also provide `*_t` and/or `*_v` helpers:

  ```cpp
  template <class T>
  using my_trait_t = typename my_trait<T>::type;


  template <class T>
  constexpr auto my_trait_v = my_trait<T>::value;
  ```

### Logging

[Section titled “Logging”](#logging)

* Available log levels are `ERROR`, `WARN`, `INFO`, `VERBOSE`, `DEBUG` and `TRACE`.

* Messages can be sent by using the `TENZIR_<level>` macros.

* Try to restrict usage of the `TENZIR_INFO` message type to the main actors. Info is the chattiest level that most users will see, so it should require no or only little understanding of Tenzir’s system architecture for the reader to understand.

* Use the `TENZIR_TRACE_SCOPE` macro to elicit an additional message at the exit of the current scope. The trace level can be used to create a trace of the call stack with fine grained control over its depth. Since the size of trace messages can quickly go out of hand, omit trace messages from helper functions and generic algorithm implementations.

### Comments

[Section titled “Comments”](#comments)

* Doxygen comments start with `///`.

* Use Markdown instead of Doxygen formatters.

* Use `@cmd` rather than `\cmd`.

* Document pre- and post-conditions with `@pre` and `@post` (where appropriate).

* Reference other parameters with emphasis:

  ```cpp
  /// @param x A number between 0 and 1.
  /// @param y Scales *x* by a constant factor.
  ```

* Use `@tparam` to document template parameters.

* For simple getters or obvious functions returning a value, use a one-line `@returns` statement:

  ```cpp
  /// @returns The answer.
  int f();
  ```

* Use `//` or `/*` and `*/` to define basic comments that should not be swallowed by Doxygen.

### External Files

[Section titled “External Files”](#external-files)

When integrating 3rd-party code into the code base, use the following scaffold:

```cpp
//    _   _____   __________
//   | | / / _ | / __/_  __/     Visibility
//   | |/ / __ |_\ \  / /          Across
//   |___/_/ |_/___/ /_/       Space and Time
//
// SPDX-FileCopyrightText: (c) 2022 The Tenzir Contributors
// SPDX-License-Identifier: BSD-3-Clause
//
// This file comes from a 3rd party and has been adapted to fit into the Tenzir
// code base. Details about the original file:
//
// - Repository: https://github.com/Microsoft/GSL
// - Commit:     d6b26b367b294aca43ff2d28c50293886ad1d5d4
// - Path:       GSL/include/gsl/gsl_byte
// - Author:     Microsoft
// - Copyright:  (c) 2015 Microsoft Corporation. All rights reserved.
// - License:    MIT


(code here)
```

### Unit Tests

[Section titled “Unit Tests”](#unit-tests)

* Every new feature must come with unit tests.

* The filename and path should mirror the component under test. For example, the component `tenzir/detail/feature.hpp` should have a test file called `test/detail/feature.cpp`.

* The include order in unit tests resembles the order for standard headers, except that unit test includes and the suite definition comes at the top.

* Make judicious use of *fixtures* for prepping your test environment.

* The snippet below illustrates a simple example for a new component `tenzir/foo.hpp` that would go into `test/foo.cpp`.

  ```cpp
  //    _   _____   __________
  //   | | / / _ | / __/_  __/     Visibility
  //   | |/ / __ |_\ \  / /          Across
  //   |___/_/ |_/___/ /_/       Space and Time
  //
  // SPDX-FileCopyrightText: (c) 2022 The Tenzir Contributors
  // SPDX-License-Identifier: BSD-3-Clause


  #define SUITE foo


  #include "tenzir/foo.hpp" // Unit under test


  #include "test.hpp"       // Unit test framework and scaffolding


  #include <iostream>       // standard library includes


  #include <caf/...>        // CAF includes


  #include "tenzir/..."     // Tenzir includes


  using namespace tenzir;


  namespace {


  struct fixture {
    fixture() {
      // Setup
      context = 42;
    }


    ~fixture() {
      // Teardown
      context = 0;
    }


    int context;
  };


  } // namespace <anonymous>


  FIXTURE_SCOPE(foo_tests, fixture)


  TEST(construction) {
    MESSAGE("default construction");
    foo x;
    MESSAGE("assignment");
    x = 42;
    CHECK_EQUAL(x, context);
  }


  FIXTURE_SCOPE_END()
  ```

### Continuous Integration

[Section titled “Continuous Integration”](#continuous-integration)

We use GitHub Actions to build and test each commit. Merging a pull request requires that all checks pass for the latest commit in the branch. GitHub displays the status of the individual checks in the pull request.

### Code Coverage

[Section titled “Code Coverage”](#code-coverage)

The GitHub Actions workflow [Analysis](https://github.com/tenzir/tenzir/actions/workflows/analysis.yaml) contains a *Code Coverage* job that runs unit tests for libtenzir and bundled plugins, and integration tests for Tenzir with bundled plugins to create a detailed line coverage report. The CI creates and uploads reports as an artifact in the Analysis workflow as part of every pull request and for every merge to master.

In addition to the local report, the workflow uploads the coverage report to [Codecov](https://app.codecov.io/gh/tenzir/tenzir), which offers a visual interface for seeing coverage changes introduced by code changes:

[![Codecov Report](https://codecov.io/gh/tenzir/tenzir/branch/master/graphs/tree.svg?token=T9JgpY4KHO)](https://app.codecov.io/gh/tenzir/tenzir)

Each block represents a single file in the project. The size and color of each block is represented by the number of statements and the coverage, respectively.

Codecov offers also a [sunburst](https://codecov.io/gh/tenzir/tenzir/branch/master/graphs/sunburst.svg?token=T9JgpY4KHO) and [icicle](https://codecov.io/gh/tenzir/tenzir/branch/master/graphs/sunburst.svg?token=T9JgpY4KHO) graph, visualizing the same data with a different approach.

To generate a coverage report locally, create a new Debug build of Tenzir with the CMake option `-D TENZIR_ENABLE_CODE_COVERAGE=ON` and run the `ccov` build target. This creates a coverage report in `<path/to/build-dir>/ccov`.

# Documentation

The source code of the Tenzir documentation is at <https://github.com/tenzir/docs>. We use [Astro](https://astro.build) with [Starlight](https://starlight.astro.build) as our site framework.

## Quick Reference

[Section titled “Quick Reference”](#quick-reference)

### Essential Commands

[Section titled “Essential Commands”](#essential-commands)

| Command        | Action                                     |
| :------------- | :----------------------------------------- |
| `pnpm install` | Install dependencies                       |
| `pnpm dev`     | Start local dev server at `localhost:4321` |
| `pnpm build`   | Build production site to `./dist/`         |
| `pnpm preview` | Preview build locally before deploying     |

### Development Commands

[Section titled “Development Commands”](#development-commands)

| Command                | Action                                           |
| :--------------------- | :----------------------------------------------- |
| `pnpm lint:markdown`   | Lint all Markdown files                          |
| `pnpm lint:linkcheck`  | Validate all internal and external links         |
| `pnpm lint:prettier`   | Check code formatting                            |
| `pnpm astro ...`       | Run CLI commands like `astro add`, `astro check` |
| `pnpm astro -- --help` | Get help using the Astro CLI                     |

For anything beyond editing Markdown content, check out [Starlight’s docs](https://starlight.astro.build/) or read [Astro’s documentation](https://docs.astro.build).

For git workflow, branching strategy, and commit message conventions, see our [Git and GitHub Workflow](/guides/contribution/workflow) guide.

## Local Development

[Section titled “Local Development”](#local-development)

### First-Time Setup

[Section titled “First-Time Setup”](#first-time-setup)

1. **Clone the repository**:

   ```bash
   git clone https://github.com/tenzir/docs.git
   cd docs
   ```

2. **Install dependencies**:

   ```bash
   pnpm install
   ```

3. **Start development server**:

   ```bash
   pnpm dev
   ```

4. **View the site**: Browse to `http://localhost:4321/`

The development server includes:

* 🔥 Hot module reloading for instant updates
* 📝 Live Markdown rendering
* 🔍 Error reporting in the browser

### Production Build

[Section titled “Production Build”](#production-build)

Create and preview a production build:

```bash
# Build the site
pnpm build


# Preview the build
pnpm preview
```

Then browse to `http://localhost:4321/` to view the production site.

CI/CD Pipeline

Production builds are automatically created in CI. Check out our [GitHub workflow](https://github.com/tenzir/docs/blob/main/.github/workflows/docs.yaml) for implementation details.

## Link Checking

[Section titled “Link Checking”](#link-checking)

The documentation includes automated link validation to ensure all internal and external links work correctly:

### Local Link Checking

[Section titled “Local Link Checking”](#local-link-checking)

Before submitting changes, run link checking locally to catch broken links:

```bash
pnpm lint:linkcheck
```

This will build the site with link validation enabled and report any broken links found.

### CI Integration

[Section titled “CI Integration”](#ci-integration)

Link checking runs automatically in our CI pipeline:

* **All builds**: Link validation is enabled for production builds
* **Pull requests**: Link checking runs as part of the lint workflow
* **Scheduled maintenance**: Weekly link checks run every Sunday to catch broken external links

### How It Works

[Section titled “How It Works”](#how-it-works)

The link checker validates:

* Internal page references (e.g., `/guides/quickstart`)
* Anchor links within pages (e.g., `/reference/operators#aggregate`)
* External URLs (with appropriate timeout and retry logic)
* Relative links between documentation files

### Fixing Broken Links

[Section titled “Fixing Broken Links”](#fixing-broken-links)

When the link checker finds issues:

1. **Invalid internal links**: Update the link to point to the correct page path
2. **Missing anchor references**: Ensure the target heading or element exists
3. **Broken external links**: Update URLs or remove outdated references
4. **False positives**: Add exclusions to the `starlightLinksValidator` configuration in `astro.config.mjs`

The link checker will cause builds to fail if broken links are detected, ensuring the documentation maintains high quality.

## Optimize Images

[Section titled “Optimize Images”](#optimize-images)

To keep the repository size manageable, always optimize image files *before* committing them. This is especially important for formats like PNG, which can contain unnecessary metadata or use inefficient compression.

### PNG Images

[Section titled “PNG Images”](#png-images)

We recommend using [pngquant](https://pngquant.org/), a command-line utility for lossy compression of PNG files. It significantly reduces file size while preserving image quality.

To compress a PNG file in-place:

```bash
pngquant --ext .png --force --quality=65-80 image.png
```

### JPEG Images

[Section titled “JPEG Images”](#jpeg-images)

Use [jpegoptim](https://github.com/tjko/jpegoptim), a utility for optimizing JPEGs without perceptible quality loss:

```bash
jpegoptim --strip-all --max=80 image.jpg
```

Alternatively, you can use [mozjpeg](https://github.com/mozilla/mozjpeg) for even better compression ratios.

### SVG Images

[Section titled “SVG Images”](#svg-images)

Use [svgo](https://github.com/svg/svgo), a Node-based tool to optimize SVG files by removing unnecessary data:

```bash
svgo image.svg
```

This automatically rewrites the file in-place with redundant code removed and optimized structure.

## Auto-Updated Files

[Section titled “Auto-Updated Files”](#auto-updated-files)

**Important**: Some files in this repository are automatically updated by CI and should **never** be manually edited. Manual changes to these files will be overwritten the next time the [update workflow](https://github.com/tenzir/docs/blob/main/.github/workflows/update.yaml) runs.

### Automatically Updated Files

[Section titled “Automatically Updated Files”](#automatically-updated-files)

The following files are synchronized from upstream repositories:

**Tenzir Node Documentation:**

* `src/content/docs/changelog/node` is generated from individual changelog entries
* `src/content/docs/reference/functions` is synced from [tenzir/tenzir](https://github.com/tenzir/tenzir) at `docs/functions/`
* `src/content/docs/reference/operators` is synced from [tenzir/tenzir](https://github.com/tenzir/tenzir) at `docs/operators/`
* `src/content/docs/reference/functions.mdx` is generated from individual function files
* `src/content/docs/reference/operators.mdx` is generated from individual operator files
* `src/content/apis/openapi.node.yaml` is the API specification for Tenzir Node and generated from the node’s source code
* `tenzir.yaml.example` is the node’s example configuration file

**Tenzir Platform Documentation:**

* `src/content/docs/changelog/platform` is generated from individual changelog entries
* `src/content/docs/reference/platform-cli.mdx` is synced from [tenzir/platform](https://github.com/tenzir/platform) at `docs/platform-cli.mdx`

### Making Changes to Auto-Updated Content

[Section titled “Making Changes to Auto-Updated Content”](#making-changes-to-auto-updated-content)

If you need to update content in these files:

1. **For Functions/Operators**: Make changes in the [tenzir/tenzir](https://github.com/tenzir/tenzir) repository
2. **For Platform CLI**: Make changes in the [tenzir/platform](https://github.com/tenzir/platform) repository
3. **For generation logic**: Modify the scripts in `scripts/` or the update workflow

Our CI will automatically prevent pull requests that modify auto-updated files. If you encounter this error, revert your changes and make them in the appropriate upstream repository instead.

## Edit Diagrams

[Section titled “Edit Diagrams”](#edit-diagrams)

We use [Excalidraw](https://excalidraw.com) as primary tool to create sketches of architectural diagrams.

When exporting Excalidraw scenes, always **use light mode** and **choose SVG** as export format, as we have some CSS magic in place that automatically inverts colors SVGs so that they also look nicely when using dark mode.

Tenzir developers have access to our Excalidraw [Documentation collection](https://app.excalidraw.com/o/6dBWEFf9h1l/9RErQkL9e2v). For everyone else, please reach out to us on our [Discord server](/discord) to request changes to existing SVGs.

## Writing Style and Formatting

[Section titled “Writing Style and Formatting”](#writing-style-and-formatting)

This section covers the editorial and technical standards for contributing to the Tenzir documentation.

### Writing Guidelines

[Section titled “Writing Guidelines”](#writing-guidelines)

We follow the [Google Style Guide](https://developers.google.com/style) for clear and consistent technical documentation. Most notably:

* Use **active voice** in general.
* Avoid anthropomorphic language—don’t attribute human qualities to software or hardware.
* Include definite and indefinite articles (a, an, and the) in your writing. Don’t skip articles for brevity, including in headings and titles.
* In document titles and headings, use **sentence case**. Capitalize only the first word in the title, the first word in a subheading after a colon, and any proper nouns or other terms that are always capitalized a certain way.
* Capitalize product names.
* Write documentation in an **informal tone**—use common two-word contractions such as “you’re,” “don’t,” and “there’s.”
* **Define technical terms and acronyms** on first use. Don’t assume readers know specialized vocabulary.
* **Put the most important information first**. Don’t bury key details in the middle of paragraphs.
* **Use consistent terminology** throughout. Don’t use different words for the same concept (e.g., don’t alternate between “node” and “instance”).
* **Avoid unclear pronouns** like “it,” “this,” or “that” without clear antecedents. Be explicit about what you’re referring to.
* **Choose strong, specific verbs** over weak ones. Use “use” instead of “utilize,” “help” instead of “facilitate,” and “show” instead of “demonstrate.”
* **Eliminate redundant phrases**. Write “to” instead of “in order to,” “now” instead of “at this point in time,” and “because” instead of “due to the fact that.”
* **Avoid vague qualifiers** like “simply,” “just,” “easily,” or “obviously.” These don’t add clarity and may frustrate readers who find the task difficult.
* **Provide context** for why something matters. Don’t just explain what to do— explain when and why to do it.
* **Use hyperlinks judiciously**. Link to external tools, products, or resources when first mentioned, but avoid overlinking within the same document.

### Formatting Standards

[Section titled “Formatting Standards”](#formatting-standards)

Follow these conventions to maintain consistency across all documentation files.

#### General

[Section titled “General”](#general)

* Every file must end with a newline character, but avoid empty lines at the end of a file.

#### Markdown Content

[Section titled “Markdown Content”](#markdown-content)

* Break lines at **80 characters**.
* When editing Markdown, run `pnpm lint:markdown:fix` and `pnpm lint:prettier:fix` when you’re done.

#### Code

[Section titled “Code”](#code)

* Avoid empty lines within functions.
* When editing source code (`.js`, `.jsx`, `.ts`, `.tsx`, `.astro` files), run `pnpm lint:eslint:fix` when you’re done.

# Security Policy

Security is a serious matter for us. We want to ensure and maintain a secure environment for our customers and the open-source community.

## Reporting a Vulnerability

[Section titled “Reporting a Vulnerability”](#reporting-a-vulnerability)

We are eager to work with the community to resolve security vulnerabilities within our tech stack in a timely manner and to properly acknowledge the contributor(s). Please do not publicly disclose a vulnerability until we have an opportunity to review and address the issue. Follow these steps to report a vulneratbility:

1. [Open a security advisory](https://help.github.com/en/articles/creating-a-maintainer-security-advisory), which is visible to project maintainers only. Please do *not* submit a normal issue or pull request in our public repositories.
2. We will confirm the receipt of the report within two business days. (It make take additional time time to resolve the issue.)
3. If you already have a patch, we will review it and approve it privately; once merged it will be publicly disclosed. We will acknowledge you in our changelog.
4. In case we need additional information during the investigation, we will be actively reaching out.

Please do not publicly mention the security issue until after we have updated the public repository so that other downstream users have an opportunity to patch their software.

## Contact

[Section titled “Contact”](#contact)

If you have any questions, please contact us directly at [security@tenzir.com](mailto://security@tenzir.com).

# Git and GitHub Workflow

The following diagram visualizes our branching model:

![Git Branching Model](/_astro/git-branching-model.DUtyVqtp_19DKCs.svg)

Our git workflow looks as follows:

* The `main` branch reflects the latest state of development, and should always compile.

* In case we need to release a hotfix, we use dedicated patch release branches.

* The `latest` branch always points to the latest release that is not a release candidate. It exists so support a streamlined workflow for some packaging tools (e.g., Nix).

* For new features or fixes, use *topic branches* that branch off `main` with a naming convention of `topic/description`. After completing work in a topic branch, check the following steps to prepare for a merge back into `main`:

  * Squash your commits such that each commit reflects a self-contained change. You can interactively rebase all commits in your current pull request with `git rebase --interactive $(git merge-base origin/main HEAD)`.

  * Create a pull request to `main` on GitHub.

  * Wait for the results of continuous integration tools and fix any reported issues.

  * Ask a maintainer to review your work when your changes merge cleanly. If you don’t want a specific maintainer’s feedback, ask for a team review from [tenzir/engineering](https://github.com/orgs/tenzir/teams/engineering).

  * Address the feedback articulated during the review.

  * A maintainer will merge the topic branch into `main` after it passes the code review.

* Similarly, for features or fixes relating to a specific GitHub issue, use *topic branches* that branch off `main` with a naming convention of `topic/XXX`, replacing XXX with a short description of the issue.

## Commit Messages

[Section titled “Commit Messages”](#commit-messages)

Commit messages are formatted according to [this git style guide](https://github.com/agis/git-style-guide).

* The first line succinctly summarizes the changes in no more than 50 characters. It is capitalized and written in and imperative present tense: e.g., “Fix a bug” as opposed to “Fixes a bug” or “Fixed a bug”. As a mnemonic, prepend “When applied, this commit will” to the commit summary and check if it builds a full sentence.

* The first line does not contain a dot at the end. (Think of it as the header of the following description).

* The second line is empty.

* Optional long descriptions as full sentences begin on the third line, indented at 72 characters per line, explaining *why* the change is needed, *how* it addresses the underlying issue, and what *side-effects* it might have.

# Fetch data from APIs

This guide covers how to fetch data from HTTP APIs using the [`from_http`](/reference/operators/from_http) and [`http`](/reference/operators/http) operators. Whether you make simple GET requests, handle authentication, or implement pagination, the operators provide flexible HTTP client capabilities for API integration.

## Basic API Requests

[Section titled “Basic API Requests”](#basic-api-requests)

Start with these fundamental patterns for making HTTP requests to APIs.

### Simple GET Requests

[Section titled “Simple GET Requests”](#simple-get-requests)

To fetch data from an API endpoint, pass the URL as the first parameter to the `from_http` operator:

```tql
from_http "https://api.example.com/data"
```

The operator makes a GET request by default and forwards the response as an event. The `from_http` operator is an input operator, i.e., it starts a pipeline. The companion operator `http` is a transformation, allowing you to specify the URL as a field by referencing an event field that contains the URL:

```tql
from {url: "https://api.example.com/data"}
http url
```

This pattern is useful when processing multiple URLs or when URLs are generated dynamically. Most of our subsequent examples use `from_http`, as the operator options are very similar.

### Parsing the HTTP Response Body

[Section titled “Parsing the HTTP Response Body”](#parsing-the-http-response-body)

The `from_http` and `http` operators automatically determine how to parse the HTTP response body using multiple methods:

1. **URL-based inference**: The operators first check the URL’s file extension to infer both the format (JSON, CSV, Parquet, etc.) and compression type (gzip, zstd, etc.). This works just like the generic `from` operator.

2. **Header-based inference**: If the format cannot be determined from the URL, the operators fall back to using the HTTP `Content-Type` and `Content-Encoding` response headers.

3. **Manual specification**: You can always override automatic inference by providing a parsing pipeline.

#### Automatic Format and Compression Inference

[Section titled “Automatic Format and Compression Inference”](#automatic-format-and-compression-inference)

When the URL contains a recognizable file extension, the operators automatically handle decompression and parsing:

```tql
from_http "https://example.org/data/events.csv.zst"
```

This automatically infers `zstd` compression and `CSV` format from the file extension, decompresses, and parses accordingly.

For URLs without clear extensions, the operators use HTTP headers:

```tql
from_http "https://example.org/download"
```

If the server responds with `Content-Type: application/json` and `Content-Encoding: gzip`, the operator will decompress and parse as JSON.

#### Manual Format Specification

[Section titled “Manual Format Specification”](#manual-format-specification)

You can manually override the parser for the response body by specifying a parsing pipeline, i.e., a pipeline that transforms bytes to events. For example, if an API returns CSV data without a proper `Content-Type`, you can specify the parsing pipeline as follows:

```tql
from_http "https://api.example.com/users" {
  read_csv
}
```

This parses the response from CSV into structured events that you can process further.

Similarly, if you need to handle specific compression and format combinations that aren’t automatically detected:

```tql
from_http "https://example.org/archive" {
  decompress_gzip
  read_json
}
```

This explicitly specifies to decompress gzip and then parse as JSON, regardless of the URL or HTTP headers.

### POST Requests with Data

[Section titled “POST Requests with Data”](#post-requests-with-data)

Send data to APIs by specifying the `method` parameter as “post” and providing the request body in the `body` parameter:

```tql
from_http "https://api.example.com/users",
  method="post",
  body={"name": "John", "email": "john@example.com"}
```

Similarly, with the `http` operator you can also parameterize the entire HTTP request using event fields by referencing field values for each parameter:

```tql
from {
  url: "https://api.example.com/users",
  method: "post",
  data: {
    name: "John",
    email: "john@example.com"
  }
}
http url, method=method, body=data
```

The operators automatically use POST method when you specify a body.

## Request Configuration

[Section titled “Request Configuration”](#request-configuration)

Configure requests with headers, authentication, and other options for different API requirements.

### Adding Headers

[Section titled “Adding Headers”](#adding-headers)

Include custom headers by providing the `headers` parameter as a record containing key-value pairs:

```tql
from_http "https://api.example.com/data", headers={
    "Authorization": "Bearer " + secret("YOUR_BEARER_TOKEN")
  }
```

Headers help you authenticate with APIs and specify request formats. Use the [`secret`](/reference/functions/secret) function to retrieve sensitive API tokens, as in the above example.

### TLS and Security

[Section titled “TLS and Security”](#tls-and-security)

Enable TLS by setting the `tls` parameter to `true` and configure client certificates using the `certfile` and `keyfile` parameters:

```tql
from_http "https://secure-api.example.com/data",
  tls=true,
  certfile="/path/to/client.crt",
  keyfile="/path/to/client.key"
```

Use these options when APIs require client certificate authentication.

### Timeout and Retry Configuration

[Section titled “Timeout and Retry Configuration”](#timeout-and-retry-configuration)

Configure timeouts and retry behavior by setting the `connection_timeout`, `max_retry_count`, and `retry_delay` parameters:

```tql
from_http "https://api.example.com/data",
  timeout=10s,
  max_retries=3,
  retry_delay=2s
```

These settings help handle network issues and API rate limiting gracefully.

## Data Enrichment

[Section titled “Data Enrichment”](#data-enrichment)

Use HTTP requests to enrich existing data with information from external APIs.

### Preserving Input Context

[Section titled “Preserving Input Context”](#preserving-input-context)

Keep original event data while adding API responses by specifying the `response_field` parameter to control where the response is stored:

```tql
from {
  domain: "example.com",
  severity: "HIGH",
  api_url: "https://threat-intel.example.com/lookup",
  response_field: "threat_data",
}
http f"{api_url}?domain={domain}", response_field=response_field
```

This approach preserves your original data and adds API responses in a specific field.

### Adding Metadata

[Section titled “Adding Metadata”](#adding-metadata)

Capture HTTP response metadata by specifying the `metadata_field` parameter to store status codes and headers separately from the response body:

```tql
from_http "https://api.example.com/status", metadata_field=http_meta
```

The metadata includes status codes and response headers for debugging and monitoring.

## Pagination and Bulk Processing

[Section titled “Pagination and Bulk Processing”](#pagination-and-bulk-processing)

Handle APIs that return large datasets across multiple pages.

### Simple Pagination

[Section titled “Simple Pagination”](#simple-pagination)

Implement automatic pagination by providing a lambda function to the `paginate` parameter that extracts the next page URL from the response:

```tql
from_http "https://api.example.com/search?q=query",
  paginate=(response => "next_page_url" if response.has_more)
```

The operator continues making requests as long as the pagination lambda function returns a valid URL.

### Complex Pagination Logic

[Section titled “Complex Pagination Logic”](#complex-pagination-logic)

Handle APIs with custom pagination schemes by building pagination URLs dynamically using expressions that reference response data:

```tql
let $base_url = "https://api.example.com/items"
from_http f"{$base_url}?page=1",
  paginate=(x => f"{$base_url}?page={x.page + 1}" if x.page < x.total_pages),
```

This example builds pagination URLs dynamically based on response data.

### Rate Limiting

[Section titled “Rate Limiting”](#rate-limiting)

Control request frequency by configuring the `paginate_delay` parameter to add delays between requests and the `parallel` parameter to limit concurrent requests:

```tql
from {
  url: "https://api.example.com/data",
  paginate_delay: 500ms,
  parallel: 2
}
http url,
  paginate="next_url" if has_next,
  paginate_delay=paginate_delay,
  parallel=parallel
```

Use `paginate_delay` and `parallel` to manage request rates appropriately.

HTTP Pipelining

The `parallel` option effectively controls HTTP request pipelining. It exists only for the `http` operator, because unlike `from_http`, it can receive several events from its upstream operator, each of which fire off a new HTTP request. The corresponding responses may still be in transit before the next request arrives. With the `parallel` option, you specify the exact number of in-flight responses.

## Practical Examples

[Section titled “Practical Examples”](#practical-examples)

These examples demonstrate typical use cases for API integration in real-world scenarios.

### API Monitoring

[Section titled “API Monitoring”](#api-monitoring)

Monitor API health and response times:

```tql
from_http "https://api.example.com/health", metadata_field=metadata
select date=metadata.headers.Date.parse_time("%a, %d %b %Y %H:%M:%S %Z")
latency = now() - date
```

The above example parses the `Date` header from the HTTP response via [`parse_time`](/reference/functions/parse_time) into a timestamp and then compares it to the current wallclock time using the [`now`](/reference/functions/now) function.

Nit: `%T` is a shortcut for `%H:%M:%S`.

## Error Handling

[Section titled “Error Handling”](#error-handling)

Handle API errors and failures gracefully in your data pipelines.

### Retry Configuration

[Section titled “Retry Configuration”](#retry-configuration)

Configure automatic retries by setting the `max_retry_count` parameter to specify the number of retry attempts and `retry_delay` to control the time between retries:

```tql
from_http "https://unreliable-api.example.com/data",
  max_retries=5,
  retry_delay=2s
```

### Status Code Handling

[Section titled “Status Code Handling”](#status-code-handling)

Check HTTP status codes by capturing metadata and filtering based on the `code` field to handle different response types:

```tql
from_http "https://api.example.com/data", metadata_field=metadata
where metadata.code >= 200 and metadata.code < 300
```

## Best Practices

[Section titled “Best Practices”](#best-practices)

Follow these practices for reliable and efficient API integration:

1. **Use appropriate timeouts**. Set a reasonable `connection_timeout` for your use case.
2. **Implement retry logic**. Configure `max_retry_count` and `retry_delay` for handling transient failures.
3. **Respect rate limits**. Use `parallel` and `paginate_delay` to control request rates.
4. **Handle errors gracefully**. Check status codes in metadata (`metadata_field`) and implement fallback logic.
5. **Secure credentials**. Access API keys and tokens via [secrets](/explanations/secrets), not in code.
6. **Monitor API usage**. Track response times and error rates for performance.
7. **Leverage automatic format inference**. Use descriptive file extensions in URLs when possible to enable automatic format and compression detection.

# Read and watch files

This guide covers how to read files and monitor directories for new files using the [`from_file`](/reference/operators/from_file) operator. Whether you process individual files, batch process directories, or set up real-time file monitoring, [`from_file`](/reference/operators/from_file) provides a unified approach to file-based data ingestion.

## Basic File Reading

[Section titled “Basic File Reading”](#basic-file-reading)

The [`from_file`](/reference/operators/from_file) operator handles various file types and formats. Start with these fundamental patterns for reading individual files.

### Single Files

[Section titled “Single Files”](#single-files)

To read a single file, specify the path to the [`from_file`](/reference/operators/from_file) operator:

```tql
from_file "/path/to/file.json"
```

The operator automatically detects the file format from the file extension. This works for all supported formats including JSON, CSV, Parquet, and others.

### Compressed Files

[Section titled “Compressed Files”](#compressed-files)

The operator handles compressed files automatically. You need no additional configuration:

```tql
from_file "/path/to/file.csv.gz"
```

Supported compression formats include gzip, bzip2, and Zstd.

### Custom Parsing

[Section titled “Custom Parsing”](#custom-parsing)

When automatic format detection doesn’t suffice, specify a custom [parsing](/reference/operators/#parsing) pipeline:

```tql
from_file "/path/to/file.log" {
  read_syslog
}
```

The parsing pipeline runs on the file content and must return events.

## Directory Processing

[Section titled “Directory Processing”](#directory-processing)

You can process multiple files efficiently using glob patterns. This section covers batch processing and recursive directory operations.

### Processing Multiple Files

[Section titled “Processing Multiple Files”](#processing-multiple-files)

Use glob patterns to process multiple files at once:

```tql
from_file "/path/to/directory/*.csv.zst"
```

This example processes all Zstd-compressed CSV files in the specified directory.

You can also use glob patterns to consume files regardless of their format:

```tql
from_file "~/data/**"
```

This processes all files in the `~/data` directory and its subdirectories, automatically detecting and parsing each file format.

### Recursive Directory Processing

[Section titled “Recursive Directory Processing”](#recursive-directory-processing)

Use `**` to match files recursively through subdirectories:

```tql
from_file "/path/to/directory/**.csv"
```

### Custom Parsing for Multiple Files

[Section titled “Custom Parsing for Multiple Files”](#custom-parsing-for-multiple-files)

When you process multiple files with custom parsing, the pipeline runs separately for each file:

```tql
from_file "/path/to/directory/*.log" {
  read_lines
}
```

## File Monitoring

[Section titled “File Monitoring”](#file-monitoring)

Set up real-time file processing by monitoring directories for changes. These features enable continuous data ingestion workflows.

### Watch for New Files

[Section titled “Watch for New Files”](#watch-for-new-files)

Use the `watch` parameter to monitor a directory for new files:

```tql
from_file "/path/to/directory/*.csv", watch=true
```

This sets up continuous monitoring, processing new files as they appear in the directory.

### Remove Files After Processing

[Section titled “Remove Files After Processing”](#remove-files-after-processing)

Combine watching with automatic file removal using the `remove` parameter:

```tql
from_file "/path/to/directory/*.csv", watch=true, remove=true
```

This approach helps you implement file-based queues where the system should automatically clean up processed files.

## Cloud Storage Integration

[Section titled “Cloud Storage Integration”](#cloud-storage-integration)

Access files directly from cloud storage providers using their native URLs. The operator supports major cloud platforms transparently.

### Amazon S3

[Section titled “Amazon S3”](#amazon-s3)

Access S3 buckets directly using `s3://` URLs:

```tql
from_file "s3://bucket/path/to/file.csv"
```

Glob patterns work with S3 as well:

```tql
from_file "s3://bucket/data/**/*.parquet"
```

### Google Cloud Storage

[Section titled “Google Cloud Storage”](#google-cloud-storage)

Access GCS buckets using `gs://` URLs:

```tql
from_file "gs://bucket/path/to/file.csv"
```

Cloud storage integration uses Apache Arrow’s filesystem APIs and supports the same glob patterns and options as local files, including recursive globbing across cloud storage hierarchies.

## Common Patterns

[Section titled “Common Patterns”](#common-patterns)

These examples demonstrate typical use cases that combine multiple features of the [`from_file`](/reference/operators/from_file) operator for real-world scenarios.

### Real-time Log Processing

[Section titled “Real-time Log Processing”](#real-time-log-processing)

Monitor a log directory and process files as they arrive:

```tql
from_file "/var/log/application/*.log", watch=true {
  read_lines
  parse_json
}
```

### Batch Data Processing

[Section titled “Batch Data Processing”](#batch-data-processing)

Process all files in a data directory:

```tql
from_file "/data/exports/**.parquet"
```

### Archive Processing with Cleanup

[Section titled “Archive Processing with Cleanup”](#archive-processing-with-cleanup)

Process archived data and remove files after successful ingestion:

```tql
from_file "/archive/*.csv.gz", remove=true
```

## Migration Notes

[Section titled “Migration Notes”](#migration-notes)

Transitioning from Legacy Operators

We designed the [`from_file`](/reference/operators/from_file) operator to replace the existing [`load_file`](/reference/operators/load_file), [`load_s3`](/reference/operators/load_s3), and [`load_gcs`](/reference/operators/load_gcs) operators. While we still support these legacy operators, [`from_file`](/reference/operators/from_file) provides a more unified and feature-rich approach to file ingestion.

We plan to add some advanced features from the legacy operators (such as file tailing, anonymous S3 access, and Unix domain socket support) in future releases of [`from_file`](/reference/operators/from_file).

# Aggregate and summarize data

Aggregation transforms streams of events into meaningful summaries. Whether you’re calculating statistics, counting occurrences, or finding extremes, the [`summarize`](/reference/operators/summarize) operator combined with aggregation functions provides powerful data analysis capabilities.

## Understanding the summarize operator

[Section titled “Understanding the summarize operator”](#understanding-the-summarize-operator)

The `summarize` operator groups events and applies aggregation functions. Its syntax is:

```plaintext
summarize <aggregation>, <aggregation>, ..., <group>, <group>, ...
```

Where:

* Aggregations are expressions like [`sum()`](/reference/functions/sum), [`count()`](/reference/functions/count), [`mean()`](/reference/functions/mean), etc.
* Groups are field names to group by

## Basic aggregations

[Section titled “Basic aggregations”](#basic-aggregations)

Start with fundamental aggregation functions on event streams.

### Count events

[Section titled “Count events”](#count-events)

Count total events and unique values with [`count()`](/reference/functions/count) and [`count_distinct()`](/reference/functions/count_distinct):

```tql
from {product: "apple", price: 100, category: "fruit"},
     {product: "banana", price: 250, category: "fruit"},
     {product: "carrot", price: 175, category: "vegetable"},
     {product: "apple", price: 120, category: "fruit"},
     {product: "banana", price: 225, category: "fruit"}
summarize total_count = count(), unique_products = count_distinct(product)
```

```tql
{
  total_count: 5,
  unique_products: 3
}
```

### Sum and average

[Section titled “Sum and average”](#sum-and-average)

Calculate totals and averages:

```tql
from {product: "apple", price: 100, quantity: 2},
     {product: "banana", price: 250, quantity: 1},
     {product: "carrot", price: 175, quantity: 3},
     {product: "apple", price: 120, quantity: 2},
     {product: "banana", price: 225, quantity: 1}
summarize total_revenue = sum(price * quantity), avg_price = mean(price), total_quantity = sum(quantity)
```

```tql
{
  total_revenue: 1440,
  avg_price: 174.0,
  total_quantity: 9
}
```

### Min and max

[Section titled “Min and max”](#min-and-max)

Find extreme values with [`min()`](/reference/functions/min) and [`max()`](/reference/functions/max):

```tql
from {sensor: "A", temperature: 72, timestamp: 2024-01-15T10:00:00},
     {sensor: "B", temperature: 68, timestamp: 2024-01-15T10:05:00},
     {sensor: "A", temperature: 75, timestamp: 2024-01-15T10:10:00},
     {sensor: "B", temperature: 82, timestamp: 2024-01-15T10:15:00},
     {sensor: "A", temperature: 71, timestamp: 2024-01-15T10:20:00}
summarize min_temp = min(temperature), max_temp = max(temperature), earliest = min(timestamp), latest = max(timestamp)
```

```tql
{
  min_temp: 68,
  max_temp: 82,
  earliest: 2024-01-15T10:00:00.000000,
  latest: 2024-01-15T10:20:00.000000
}
```

## Grouping data

[Section titled “Grouping data”](#grouping-data)

Group events by one or more fields to calculate aggregations per group.

### Group by single field

[Section titled “Group by single field”](#group-by-single-field)

Calculate statistics per category:

```tql
from {product: "apple", price: 100, category: "fruit"},
     {product: "banana", price: 250, category: "fruit"},
     {product: "carrot", price: 175, category: "vegetable"},
     {product: "lettuce", price: 125, category: "vegetable"},
     {product: "orange", price: 225, category: "fruit"}
summarize avg_price = mean(price), item_count = count(), category
```

```tql
{
  avg_price: 191.66666666666666,
  item_count: 3,
  category: "fruit",
}
{
  avg_price: 150.0,
  item_count: 2,
  category: "vegetable",
}
```

### Group by multiple fields

[Section titled “Group by multiple fields”](#group-by-multiple-fields)

Group by multiple dimensions:

```tql
from {user: "alice", action: "login", duration: 45, date: "2024-01-15"},
     {user: "bob", action: "login", duration: 38, date: "2024-01-15"},
     {user: "alice", action: "view", duration: 12, date: "2024-01-15"},
     {user: "alice", action: "login", duration: 52, date: "2024-01-16"},
     {user: "bob", action: "edit", duration: 89, date: "2024-01-16"}
summarize avg_duration = mean(duration), action_count = count(), user, action
```

```tql
{
  avg_duration: 38.0,
  action_count: 1,
  user: "bob",
  action: "login",
}
{
  avg_duration: 89.0,
  action_count: 1,
  user: "bob",
  action: "edit",
}
{
  avg_duration: 48.5,
  action_count: 2,
  user: "alice",
  action: "login",
}
{
  avg_duration: 12.0,
  action_count: 1,
  user: "alice",
  action: "view",
}
```

## Statistical functions

[Section titled “Statistical functions”](#statistical-functions)

Use statistical aggregation functions for deeper analysis.

### Percentiles and median

[Section titled “Percentiles and median”](#percentiles-and-median)

Calculate distribution statistics with [`quantile()`](/reference/functions/quantile):

```tql
from {endpoint: "/api/users", latency: 120},
     {endpoint: "/api/users", latency: 135},
     {endpoint: "/api/users", latency: 110},
     {endpoint: "/api/orders", latency: 245},
     {endpoint: "/api/orders", latency: 225},
     {endpoint: "/api/orders", latency: 280}
summarize p50 = quantile(latency, q=0.5),
          p90 = quantile(latency, q=0.9),
          p95 = quantile(latency, q=0.95),
          endpoint
```

```tql
{
  p50: 245.0,
  p90: 280.0,
  p95: 280.0,
  endpoint: "/api/orders",
}
{
  p50: 120.0,
  p90: 135.0,
  p95: 135.0,
  endpoint: "/api/users",
}
```

### Standard deviation and variance

[Section titled “Standard deviation and variance”](#standard-deviation-and-variance)

Measure data spread with [`stddev()`](/reference/functions/stddev) and [`variance()`](/reference/functions/variance):

```tql
from {server: "web1", cpu: 45},
     {server: "web1", cpu: 52},
     {server: "web1", cpu: 48},
     {server: "web2", cpu: 85},
     {server: "web2", cpu: 92},
     {server: "web2", cpu: 88}
summarize avg_cpu = mean(cpu),
          cpu_stddev = stddev(cpu),
          cpu_variance = variance(cpu),
          server
```

```tql
{
  avg_cpu: 48.333333333333336,
  cpu_stddev: 2.8674417556808622,
  cpu_variance: 8.222222222222145,
  server: "web1",
}
{
  avg_cpu: 88.33333333333333,
  cpu_stddev: 2.8674417556810217,
  cpu_variance: 8.22222222222306,
  server: "web2",
}
```

### Mode and distinct values

[Section titled “Mode and distinct values”](#mode-and-distinct-values)

Find most common values and collect unique values with [`mode()`](/reference/functions/mode), [`distinct()`](/reference/functions/distinct), and [`count_if()`](/reference/functions/count_if):

```tql
from {user: "alice", browser: "chrome", action: "login"},
     {user: "bob", browser: "firefox", action: "view"},
     {user: "alice", browser: "chrome", action: "edit"},
     {user: "charlie", browser: "chrome", action: "login"},
     {user: "alice", browser: "safari", action: "login"}
summarize most_common_browser = mode(browser),
          unique_browsers = distinct(browser),
          login_count = count_if(action, x => x == "login")
```

```tql
{
  most_common_browser: "chrome",
  unique_browsers: [
    "chrome",
    "firefox",
    "safari",
  ],
  login_count: 3,
}
```

### Value frequencies and entropy

[Section titled “Value frequencies and entropy”](#value-frequencies-and-entropy)

Analyze value distributions with [`value_counts()`](/reference/functions/value_counts) and [`entropy()`](/reference/functions/entropy):

```tql
from {category: "A", value: 10},
     {category: "B", value: 20},
     {category: "A", value: 15},
     {category: "B", value: 25},
     {category: "C", value: 30}
summarize frequencies = value_counts(category),
          info_entropy = entropy(category)
```

```tql
{
  frequencies: [
    {
      value: "A",
      count: 2,
    },
    {
      value: "B",
      count: 2,
    },
    {
      value: "C",
      count: 1,
    },
  ],
  info_entropy: 1.0549201679861442,
}
```

## Collecting values

[Section titled “Collecting values”](#collecting-values)

Use [`collect()`](/reference/functions/collect) and [`distinct()`](/reference/functions/distinct) to gather values:

```tql
from {user: "alice", action: "login", timestamp: 2024-01-15T10:00:00},
     {user: "bob", action: "view", timestamp: 2024-01-15T10:01:00},
     {user: "alice", action: "edit", timestamp: 2024-01-15T10:02:00},
     {user: "charlie", action: "login", timestamp: 2024-01-15T10:03:00},
     {user: "alice", action: "logout", timestamp: 2024-01-15T10:04:00}
summarize all_actions = collect(action),
          unique_users = distinct(user),
          event_count = count()
```

```tql
{
  all_actions: [
    "login",
    "view",
    "edit",
    "login",
    "logout",
  ],
  unique_users: [
    "alice",
    "charlie",
    "bob",
  ],
  event_count: 5,
}
```

### First and last values

[Section titled “First and last values”](#first-and-last-values)

Get boundary values with [`first()`](/reference/functions/first) and [`last()`](/reference/functions/last):

```tql
from {sensor: "temp1", reading: 72, time: 2024-01-15T09:00:00},
     {sensor: "temp1", reading: 75, time: 2024-01-15T10:00:00},
     {sensor: "temp1", reading: 78, time: 2024-01-15T11:00:00},
     {sensor: "temp2", reading: 68, time: 2024-01-15T09:00:00},
     {sensor: "temp2", reading: 71, time: 2024-01-15T10:00:00}
summarize first_reading = first(reading),
          last_reading = last(reading),
          avg_reading = mean(reading),
          sensor
```

```tql
{
  first_reading: 72,
  last_reading: 78,
  avg_reading: 75.0,
  sensor: "temp1",
}
{
  first_reading: 68,
  last_reading: 71,
  avg_reading: 69.5,
  sensor: "temp2",
}
```

## Boolean aggregations

[Section titled “Boolean aggregations”](#boolean-aggregations)

Use [`all()`](/reference/functions/all) and [`any()`](/reference/functions/any) for boolean checks:

```tql
from {test: "unit", passed: true, duration: 45},
     {test: "integration", passed: true, duration: 120},
     {test: "e2e", passed: false, duration: 300},
     {test: "performance", passed: true, duration: 180}
summarize all_passed = all(passed),
          any_failed = any(not passed),
          total_duration = sum(duration)
```

```tql
{
  all_passed: false,
  any_failed: true,
  total_duration: 645
}
```

## Practical examples

[Section titled “Practical examples”](#practical-examples)

### Analyze API response times

[Section titled “Analyze API response times”](#analyze-api-response-times)

```tql
from {
  requests: [
    {endpoint: "/api/users", method: "GET", duration: 45, status: 200},
    {endpoint: "/api/users", method: "POST", duration: 120, status: 201},
    {endpoint: "/api/orders", method: "GET", duration: 89, status: 200},
    {endpoint: "/api/users", method: "GET", duration: 38, status: 200},
    {endpoint: "/api/orders", method: "GET", duration: 156, status: 500}
  ]
}
unroll requests
summarize endpoint=requests.endpoint,
          count=count(),
          avg_duration=mean(requests.duration)
```

```tql
{
  endpoint: "/api/orders",
  count: 2,
  avg_duration: 122.5,
}
{
  endpoint: "/api/users",
  count: 3,
  avg_duration: 67.66666666666667,
}
```

### Calculate sales metrics

[Section titled “Calculate sales metrics”](#calculate-sales-metrics)

```tql
from {
  sales: [
    {date: "2024-01-01", amount: 1200, region: "North"},
    {date: "2024-01-01", amount: 800, region: "South"},
    {date: "2024-01-02", amount: 1500, region: "North"},
    {date: "2024-01-02", amount: 950, region: "South"},
    {date: "2024-01-03", amount: 1100, region: "North"}
  ]
}
// Calculate totals by date
unroll sales
summarize date=sales.date, total=sum(sales.amount)
```

```tql
{date: "2024-01-01", total: 2000}
{date: "2024-01-02", total: 2450}
{date: "2024-01-03", total: 1100}
```

### Monitor system health

[Section titled “Monitor system health”](#monitor-system-health)

```tql
from {
  metrics: [
    {timestamp: "10:00", cpu: 45, memory: 62, disk: 78},
    {timestamp: "10:01", cpu: 52, memory: 64, disk: 78},
    {timestamp: "10:02", cpu: 89, memory: 71, disk: 79},
    {timestamp: "10:03", cpu: 67, memory: 68, disk: 79},
    {timestamp: "10:04", cpu: 48, memory: 65, disk: 80}
  ]
}
cpu_alert = metrics.map(m => m.cpu > 80).any()
avg_memory = metrics.map(m => m.memory).mean()
disk_trend = metrics.last().disk - metrics.first().disk
health_summary = {
  cpu_max: metrics.map(m => m.cpu).max(),
  memory_avg: avg_memory,
  disk_growth: disk_trend,
  critical: cpu_alert
}
```

```tql
{
  metrics: [ ... ],
  cpu_alert: true,
  avg_memory: 66.0,
  disk_trend: 2,
  health_summary: {
    cpu_max: 89,
    memory_avg: 66.0,
    disk_growth: 2,
    critical: true,
  },
}
```

## Complex aggregations

[Section titled “Complex aggregations”](#complex-aggregations)

Combine multiple aggregation functions for comprehensive analysis:

```tql
from {method: "GET", endpoint: "/api/users", status: 200, duration: 45},
     {method: "POST", endpoint: "/api/users", status: 201, duration: 120},
     {method: "GET", endpoint: "/api/orders", status: 200, duration: 89},
     {method: "GET", endpoint: "/api/users", status: 200, duration: 38},
     {method: "GET", endpoint: "/api/orders", status: 500, duration: 156},
     {method: "DELETE", endpoint: "/api/users/123", status: 204, duration: 67}
summarize request_count = count(),
          avg_duration = mean(duration),
          error_count = count_if(status, s => s >= 400),
          unique_endpoints = count_distinct(endpoint),
          method
error_rate = error_count / request_count
```

```tql
{
  request_count: 1,
  avg_duration: 120.0,
  error_count: 0,
  unique_endpoints: 1,
  method: "POST",
  error_rate: 0.0,
}
{
  request_count: 1,
  avg_duration: 67.0,
  error_count: 0,
  unique_endpoints: 1,
  method: "DELETE",
  error_rate: 0.0,
}
{
  request_count: 4,
  avg_duration: 82.0,
  error_count: 1,
  unique_endpoints: 2,
  method: "GET",
  error_rate: 0.25,
}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Choose appropriate functions**: Use `mean()` for averages, `median()` for skewed data
2. **Handle empty collections**: Check if lists are empty before aggregating
3. **Consider memory usage**: Large collections can consume significant memory
4. **Combine aggregations**: Calculate multiple statistics in one pass for efficiency

## Related guides

[Section titled “Related guides”](#related-guides)

* [Transform collections](/guides/data-shaping/transform-collections) - Work with lists and records
* [Filter and select data](/guides/data-shaping/filter-and-select-data) - Filter before aggregating
* [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations

# Convert data formats

Data comes in many formats. Converting between formats is essential for integration, export, and interoperability. This guide shows you how to transform data between JSON, CSV, YAML, and other common formats using TQL’s print functions.

## Print to JSON

[Section titled “Print to JSON”](#print-to-json)

JSON is the most common data exchange format. Use [`print_json()`](/reference/functions/print_json) to convert any data to JSON strings:

```tql
from {
  user: {
    name: "Alice",
    age: 30,
    roles: ["admin", "user"],
    email: null,
    metadata: {}
  }
}
json_string = user.print_json()
```

```tql
{
  user: {
    name: "Alice",
    age: 30,
    roles: ["admin", "user"],
    email: null,
    metadata: {}
  },
  json_string: "{\n  \"name\": \"Alice\",\n  \"age\": 30,\n  \"roles\": [\n    \"admin\",\n    \"user\"\n  ],\n  \"email\": null,\n  \"metadata\": {}\n}",
  json_stripped: "{\n  \"name\": \"Alice\",\n  \"age\": 30,\n  \"roles\": [\n    \"admin\",\n    \"user\"\n  ]\n}"
}
```

Print & Write Siblings

Many `print_*` functions have a `write_*` sibling operator that operate on entire events instead of values within events. The `write_*` operators return *bytes* instead of events. As such, you need to pair it with an [output operator that accepts bytes](/reference/operators/#outputs).

The [`print_json()`](/reference/functions/print_json) function has [`write_json`](/reference/operators/write_json) as sibling operator to format the entire event stream as JSON:

```plaintext
from {
  user: {
    name: "Alice",
    age: 30,
    roles: ["admin", "user"],
    email: null,
    metadata: {}
  }
}
write_json
```

```json
{
  "user": {
    "name": "Alice",
    "age": 30,
    "roles": [
      "admin",
      "user"
    ],
    "email": null,
    "metadata": {}
  }
}
```

### Print newline-delimited JSON

[Section titled “Print newline-delimited JSON”](#print-newline-delimited-json)

For streaming data, use [`print_ndjson()`](/reference/functions/print_ndjson):

```tql
from {
  events: [
    {type: "login", user: "alice"},
    {type: "view", user: "bob"},
    {type: "logout", user: "alice"}
  ]
}
ndjson = events.print_ndjson()
```

```tql
{
  events: [
    {
      type: "login",
      user: "alice",
    },
    {
      type: "view",
      user: "bob",
    },
    {
      type: "logout",
      user: "alice",
    },
  ],
  ndjson: "[{\"type\":\"login\",\"user\":\"alice\"},{\"type\":\"view\",\"user\":\"bob\"},{\"type\":\"logout\",\"user\":\"alice\"}]",
}
```

And with the [`write_ndjson`](/reference/operators/write_ndjson) dual:

```tql
from {
  events: [
    {type: "login", user: "alice"},
    {type: "view", user: "bob"},
    {type: "logout", user: "alice"}
  ]
}
write_ndjson
```

```json
{"events":[{"type":"login","user":"alice"},{"type":"view","user":"bob"},{"type":"logout","user":"alice"}]}
```

## Print to CSV

[Section titled “Print to CSV”](#print-to-csv)

Convert tabular data to CSV with [`print_csv()`](/reference/functions/print_csv):

```tql
from {name: "Alice", age: 30, city: "NYC"},
     {name: "Bob", age: 25, city: "SF"},
     {name: "Charlie", age: 35, city: "LA"}
csv_data = this.print_csv()
```

```tql
{
  name: "Alice",
  age: 30,
  city: "NYC",
  csv_data: "Alice,30,NYC",
}
{
  name: "Bob",
  age: 25,
  city: "SF",
  csv_data: "Bob,25,SF",
}
{
  name: "Charlie",
  age: 35,
  city: "LA",
  csv_data: "Charlie,35,LA",
}
```

You get an extra header with [`write_csv`](/reference/operators/write_csv) dual:

```tql
from {name: "Alice", age: 30, city: "NYC"},
     {name: "Bob", age: 25, city: "SF"},
     {name: "Charlie", age: 35, city: "LA"}
csv_data = this.print_csv()
```

```csv
name,age,city
Alice,30,NYC
Bob,25,SF
Charlie,35,LA
```

## Print to TSV and SSV

[Section titled “Print to TSV and SSV”](#print-to-tsv-and-ssv)

For tab-separated and space-separated values, use [`print_tsv()`](/reference/functions/print_tsv) and [`print_ssv()`](/reference/functions/print_ssv):

```tql
from {
  record: {id: 1, name: "Alice Smith", status: "active"}
}
tsv = record.print_tsv()
ssv = record.print_ssv()
```

```tql
{
  record: {
    id: 1,
    name: "Alice Smith",
    status: "active",
  },
  tsv: "1\tAlice Smith\tactive",
  ssv: "1 \"Alice Smith\" active",
}
```

With [`write_tsv`](/reference/operators/write_tsv):

```tql
from {
  record: {id: 1, name: "Alice Smith", status: "active"}
}
write_tsv
```

```txt
record.id  record.name  record.status
1  Alice Smith  active
```

Note the additional double quotes with [`write_ssv`](/reference/operators/write_ssv) because space is overloaded as field separator.

```tql
from {
  record: {id: 1, name: "Alice Smith", status: "active"}
}
write_ssv
```

```txt
record.id record.name record.status
1 "Alice Smith" active
```

## Print to custom-separated values

[Section titled “Print to custom-separated values”](#print-to-custom-separated-values)

If none of the existing \*SV formats meet your needs, the [`print_xsv()`](/reference/functions/print_xsv) function to customize field separator, list separator, and what to render for absent values:

Use [`print_xsv()`](/reference/functions/print_xsv) for custom separators:

```tql
from {
  item: {sku: "A001", desc: "Widget", price: 9.99, scores: [42, 84], details: null}
}
pipe_separated = item.print_xsv(field_separator=" ⏸︎ ",
                                list_separator=" ⌘ ",
                                null_value="∅")
```

And for the entire event stream via [`write_xsv`](/reference/operators/write_xsv):

```tql
{
  item: {
    sku: "A001",
    desc: "Widget",
    price: 9.99,
    scores: [
      42,
      84,
    ],
    details: null,
  },
  pipe_separated: "A001 ⏸︎ Widget ⏸︎ 9.99 ⏸︎ 42 ⌘ 84 ⏸︎ ∅",
}
```

```txt
item.sku ⏸︎ item.desc ⏸︎ item.price ⏸︎ item.scores ⏸︎ item.details
A001 ⏸︎ Widget ⏸︎ 9.99 ⏸︎ 42 ⌘ 84 ⏸︎ ∅
```

## Print to YAML

[Section titled “Print to YAML”](#print-to-yaml)

Convert data to YAML format with [`print_yaml()`](/reference/functions/print_yaml):

```tql
from {
  config: {
    server: {
      host: "localhost",
      port: 8080,
      ssl: true
    },
    features: ["auth", "api", "websocket"]
  }
}
yaml = config.print_yaml()
```

```tql
{
  config: {...},
  yaml: "server:\n  host: localhost\n  port: 8080\n  ssl: true\nfeatures:\n  - auth\n  - api\n  - websocket"
}
```

Turn the entire event stream into a YAML document stream via [`write_yaml`](/reference/operators/write_yaml):

```tql
from {
  config: {
    server: {
      host: "localhost",
      port: 8080,
      ssl: true
    },
    features: ["auth", "api", "websocket"]
  }
}
write_yaml
```

```yaml
---
config:
  server:
    host: localhost
    port: 8080
    ssl: true
  features:
    - auth
    - api
    - websocket
...
```

## Print key-value pairs

[Section titled “Print key-value pairs”](#print-key-value-pairs)

Convert records to key-value format with [`print_kv()`](/reference/functions/print_kv):

```tql
from {
  event: {
    timestamp: "2024-01-15T10:30:00",
    level: "ERROR",
    message: "Connection failed",
    code: 500
  }
}
kv_default = event.print_kv()
kv_custom = event.print_kv(value_separator=": ", field_separator=" | ")
```

```tql
{
  event: {
    timestamp: "2024-01-15T10:30:00",
    level: "ERROR",
    message: "Connection failed",
    code: 500,
  },
  kv_default: "timestamp=2024-01-15T10:30:00 level=ERROR message=\"Connection failed\" code=500",
  kv_custom: "timestamp: 2024-01-15T10:30:00 | level: ERROR | message: Connection failed | code: 500",
}
```

## Print security formats

[Section titled “Print security formats”](#print-security-formats)

### CEF (Common Event Format)

[Section titled “CEF (Common Event Format)”](#cef-common-event-format)

Print security events in CEF format with [`print_cef()`](/reference/functions/print_cef):

```tql
from {
  extension: {
    src: "10.0.0.1",
    dst: "192.168.1.1",
    spt: 12345,
    dpt: 22
  }
}
cef = extension.print_cef(
  cef_version="0",
  device_vendor="Security Corp",
  device_product="Firewall",
  device_version="1.0",
  signature_id="100",
  name="Port Scan Detected",
  severity="7"
)
```

```tql
{
  extension: {src: "10.0.0.1", dst: "192.168.1.1", spt: 12345, dpt: 22},
  cef: "CEF:0|Security Corp|Firewall|1.0|100|Port Scan Detected|7|src=10.0.0.1 dst=192.168.1.1 spt=12345 dpt=22"
}
```

Turn print functions into write operators

There’s no `write_cef` sibling operator, but you use a combination of [`select`](/reference/operators/select) and [`write_lines`](/reference/operators/write_lines). For example:

```tql
from { ... }
cef = print_cef(...)
select cef
write_lines
```

You could now add [`save_file`](/reference/operators/save_file) or use [`write_lines`](/reference/operators/write_lines) as printing pipeline in [`to`](/reference/operators/to).

### LEEF (Log Event Extended Format)

[Section titled “LEEF (Log Event Extended Format)”](#leef-log-event-extended-format)

Print in IBM QRadar’s LEEF format with [`print_leef()`](/reference/functions/print_leef):

```tql
from {
  attributes: {
    srcIP: "10.0.0.5",
    dstIP: "192.168.1.10",
    action: "BLOCK"
  }
}
leef = attributes.print_leef(
  vendor="Security Corp",
  product_name="IDS",
  product_version="2.0",
  event_class_id="200"
)
```

```tql
{
  attributes: {srcIP: "10.0.0.5", dstIP: "192.168.1.10", action: "BLOCK"},
  leef: "LEEF:2.0|Security Corp|IDS|2.0|200|srcIP=10.0.0.5|dstIP=192.168.1.10|action=BLOCK"
}
```

As above, add `select leef | write_lines` to create line-based LEEF output.

## Convert between formats

[Section titled “Convert between formats”](#convert-between-formats)

Chain parsing and printing to convert between formats:

```tql
from {
  json_data: "{\"name\":\"Alice\",\"age\":30,\"city\":\"NYC\"}"
}
// JSON to CSV
set parsed = json_data.parse_json()
set as_csv = parsed.print_csv()


// JSON to YAML
set as_yaml = parsed.print_yaml()


// JSON to Key-Value
set as_kv = parsed.print_kv()
```

```tql
{
  json_data: "{\"name\":\"Alice\",\"age\":30,\"city\":\"NYC\"}",
  parsed: {
    name: "Alice",
    age: 30,
    city: "NYC",
  },
  as_csv: "Alice,30,NYC",
  as_yaml: "name: Alice\nage: 30\ncity: NYC",
  as_kv: "name=Alice age=30 city=NYC",
}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Choose appropriate formats**:

   * JSON for APIs and modern systems
   * CSV for spreadsheets and analysis
   * YAML for configuration files
   * Key-value for logging systems

2. **Handle special characters**: Be aware of how each format handles quotes, newlines, and separators

3. **Consider file size**: JSON is verbose, CSV is compact

4. **Preserve data types**: Some formats (CSV) lose type information

5. **Use proper escaping**: Let print functions handle escaping automatically

## Related guides

[Section titled “Related guides”](#related-guides)

* [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse different formats
* [Transform collections](/guides/data-shaping/transform-collections) - Prepare data for printing
* [Manipulate strings](/guides/data-shaping/manipulate-strings) - Work with printed output

# Deduplicate events

The [`deduplicate`](/reference/operators/deduplicate) operator provides a powerful mechanism to remove duplicate events in a pipeline.

There are numerous use cases for deduplication, such as reducing noise, optimizing costs and making threat detection and response more efficient.

## Basic deduplication

[Section titled “Basic deduplication”](#basic-deduplication)

Let’s start with a simple example to understand how deduplication works. Imagine you’re monitoring user logins and want to see only unique users, regardless of how many times they log in:

```tql
from {user: "alice", action: "login", time: 1},
     {user: "bob", action: "login", time: 2},
     {user: "alice", action: "login", time: 3},
     {user: "alice", action: "logout", time: 4}
deduplicate user
```

```tql
{user: "alice", action: "login", time: 1}
{user: "bob", action: "login", time: 2}
```

The operator keeps only the first occurrence of each unique value for the specified field(s). In this example:

* Alice’s first login (time: 1) is kept
* Bob’s login (time: 2) is kept
* Alice’s second login (time: 3) is dropped because we already saw `user: "alice"`
* Note that Alice’s logout (time: 4) would also be dropped with this simple deduplication

## Deduplicate by multiple fields

[Section titled “Deduplicate by multiple fields”](#deduplicate-by-multiple-fields)

Often you need more nuanced deduplication. For example, you might want to track unique user-action pairs to see each distinct activity per user:

```tql
from {user: "alice", action: "login", time: 1},
     {user: "bob", action: "login", time: 2},
     {user: "alice", action: "login", time: 3},
     {user: "alice", action: "logout", time: 4}
deduplicate {user: user, action: action}
```

```tql
{user: "alice", action: "login", time: 1}
{user: "bob", action: "login", time: 2}
{user: "alice", action: "logout", time: 4}
```

Now we keep unique combinations of user and action:

* Alice’s first login is kept (unique: alice+login)
* Bob’s login is kept (unique: bob+login)
* Alice’s second login is dropped (duplicate: alice+login already seen)
* Alice’s logout is kept (unique: alice+logout is a new combination)

This approach is useful for tracking distinct user activities rather than just unique users.

## Analyze unique host pairs

[Section titled “Analyze unique host pairs”](#analyze-unique-host-pairs)

When investigating network incidents, you often want to identify all unique communication patterns between hosts. This example shows network connections with nested ID fields containing origin and response hosts:

```tql
from {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 1024},
     {id: {orig_h: "10.0.0.2", resp_h: "192.168.1.1"}, bytes: 2048},
     {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 512},
     {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.2"}, bytes: 256}
deduplicate {orig_h: id.orig_h, resp_h: id.resp_h}
```

```tql
{id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 1024}
{id: {orig_h: "10.0.0.2", resp_h: "192.168.1.1"}, bytes: 2048}
{id: {orig_h: "10.0.0.1", resp_h: "192.168.1.2"}, bytes: 256}
```

The deduplication works on the extracted host pairs:

* First connection (10.0.0.1 → 192.168.1.1) is kept
* Second connection (10.0.0.2 → 192.168.1.1) is kept (different origin)
* Third connection (10.0.0.1 → 192.168.1.1) is dropped (duplicate of first)
* Fourth connection (10.0.0.1 → 192.168.1.2) is kept (different destination)

Note that flipped connections (A→B vs B→A) are considered different pairs. This helps identify bidirectional communication patterns.

## Remove duplicate alerts

[Section titled “Remove duplicate alerts”](#remove-duplicate-alerts)

Security monitoring often generates duplicate alerts that create noise and fatigue. Here’s how to suppress repeated alerts for the same threat pattern:

```tql
from {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 1},
     {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 2},
     {src_ip: "10.0.0.2", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 3},
     {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Port Scan", time: 4}
deduplicate {src: src_ip, dst: dest_ip, sig: signature}
```

```tql
{src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 1}
{src_ip: "10.0.0.2", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 3}
{src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Port Scan", time: 4}
```

The deduplication creates a composite key from source, destination, and signature:

* First “Suspicious DNS” from 10.0.0.1 is kept
* Second identical alert (time: 2) is suppressed as a duplicate
* “Suspicious DNS” from different source 10.0.0.2 is kept (different pattern)
* “Port Scan” from 10.0.0.1 is kept (different signature)

This approach reduces alert volume while preserving visibility into distinct threat patterns.

### Using timeout for time-based deduplication

[Section titled “Using timeout for time-based deduplication”](#using-timeout-for-time-based-deduplication)

In production environments, you often want to suppress duplicates only within a certain time window. This ensures you don’t miss recurring issues that happen over longer periods.

The `create_timeout` parameter resets the deduplication state after the specified duration:

```tql
deduplicate {src: src_ip, dst: dest_ip, sig: signature}, create_timeout=1h
```

This configuration:

* Suppresses duplicate alerts for the same source/destination/signature combination
* Resets after 1 hour, allowing the same alert pattern through again
* Helps balance noise reduction with visibility into persistent threats

For example, if a host is repeatedly targeted:

* 9:00 AM: First “Port Scan” alert is shown
* 9:15 AM: Duplicate suppressed
* 9:30 AM: Duplicate suppressed
* 10:05 AM: Same alert shown again (timeout expired)

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Choose fields carefully**: Deduplicate on fields that truly identify unique events for your use case. Too few fields may drop important events; too many may not deduplicate effectively.

2. **Consider order**: The [`deduplicate`](/reference/operators/deduplicate) operator keeps the *first* occurrence. If you need the latest, consider using [`reverse`](/reference/operators/reverse) first:

   ```tql
   reverse | deduplicate user | reverse
   ```

3. **Use timeout wisely**: For streaming data, `create_timeout` prevents memory from growing indefinitely while still reducing noise. Choose durations based on your threat detection windows.

4. **Combine with other operators**: Often you’ll want to filter ([`where`](/reference/operators/where)) or transform ([`set`](/reference/operators/set)) data before deduplication to normalize keys:

   ```tql
   normalized_ip = src_ip.string()
   deduplicate normalized_ip
   ```

# Extract structured data from text

Real-world data is messy. Log lines contain embedded JSON. CSV fields hide key-value pairs. Network packets wrap multiple protocols. This guide shows you how to extract structured data from text using TQL’s parsing functions, starting simple and building up to complex scenarios.

## Parse JSON embedded in strings

[Section titled “Parse JSON embedded in strings”](#parse-json-embedded-in-strings)

The most common parsing task is extracting JSON data from string fields. Let’s start with a simple example:

```tql
from {message: "{\"user\": \"alice\", \"action\": \"login\", \"timestamp\": 1234567890}"}
data = message.parse_json()
```

```tql
{
  message: "{\"user\": \"alice\", \"action\": \"login\", \"timestamp\": 1234567890}",
  data: {
    user: "alice",
    action: "login",
    timestamp: 1234567890,
  },
}
```

The [`parse_json()`](/reference/functions/parse_json) function converts the JSON string into a structured record. You can now access nested fields directly:

```tql
from {message: "{\"user\": \"alice\", \"action\": \"login\", \"timestamp\": 1234567890}"}
data = message.parse_json()
user = data.user
action = data.action
```

```tql
{
  message: "{\"user\": \"alice\", \"action\": \"login\", \"timestamp\": 1234567890}",
  data: {user: "alice", action: "login", timestamp: 1234567890},
  user: "alice",
  action: "login",
}
```

## Extract key-value pairs

[Section titled “Extract key-value pairs”](#extract-key-value-pairs)

Many logs use simple key-value formats. The [`parse_kv()`](/reference/functions/parse_kv) function handles these automatically:

```tql
from {log: "status=200 method=GET path=/api/users duration=45ms"}
fields = log.parse_kv()
```

```tql
{
  log: "status=200 method=GET path=/api/users duration=45ms",
  fields: {
    status: 200,
    method: "GET",
    path: "/api/users",
    duration: 45ms,
  },
}
```

Notice how `parse_kv()` automatically:

* Detects the `=` separator
* Converts numeric values (status becomes 200, not “200”)
* Parses duration values (duration becomes `45ms`, not “45ms”)

### Customize separators

[Section titled “Customize separators”](#customize-separators)

Not all key-value pairs use `=`. Specify custom separators:

```tql
from {log: "user:alice action:login time:2024-01-15"}
fields = log.parse_kv(field_split=" ", value_split=":")
```

```tql
{
  log: "user:alice action:login time:2024-01-15",
  fields: {
    user: "alice",
    action: "login",
    time: 2024-01-15T00:00:00Z,
  },
}
```

## Parse tabular data formats

[Section titled “Parse tabular data formats”](#parse-tabular-data-formats)

TQL provides several functions for parsing tabular data:

### CSV (Comma-Separated Values)

[Section titled “CSV (Comma-Separated Values)”](#csv-comma-separated-values)

Use [`parse_csv()`](/reference/functions/parse_csv) for standard CSV:

```tql
from {line: "alice,30,engineer,SF"}
fields = line.parse_csv(header=["name", "age", "role", "location"])
```

```tql
{
  line: "alice,30,engineer,SF",
  fields: {
    name: "alice",
    age: 30,
    role: "engineer",
    location: "SF",
  },
}
```

To get an array without field names, use `split()`:

```tql
from {line: "alice,30,engineer,SF"}
values = line.split(",")
```

```tql
{
  line: "alice,30,engineer,SF",
  values: [
    "alice",
    "30",
    "engineer",
    "SF",
  ],
}
```

### TSV (Tab-Separated Values)

[Section titled “TSV (Tab-Separated Values)”](#tsv-tab-separated-values)

For tab-separated data, use [`parse_tsv()`](/reference/functions/parse_tsv):

```tql
from {line: "alice\t30\tengineer"}
fields = line.parse_tsv(header=["name", "age", "role"])
```

```tql
{
  line: "alice\t30\tengineer",
  fields: {
    name: "alice",
    age: 30,
    role: "engineer",
  },
}
```

### SSV (Space-Separated Values)

[Section titled “SSV (Space-Separated Values)”](#ssv-space-separated-values)

For space-separated data, use [`parse_ssv()`](/reference/functions/parse_ssv):

```tql
from {line: "alice 30 engineer"}
fields = line.parse_ssv(header=["name", "age", "role"])
```

```tql
{
  line: "alice 30 engineer",
  fields: {
    name: "alice",
    age: 30,
    role: "engineer",
  },
}
```

### XSV (Custom-Separated Values)

[Section titled “XSV (Custom-Separated Values)”](#xsv-custom-separated-values)

For custom separators, use [`parse_xsv()`](/reference/functions/parse_xsv):

```tql
from {line: "alice|30|engineer|SF"}
fields = line.parse_xsv(field_separator="|",
                        list_separator=",",
                        null_value="",
                        header=["name", "age", "role", "location"])
```

```tql
{
  line: "alice|30|engineer|SF",
  fields: {
    name: "alice",
    age: 30,
    role: "engineer",
    location: "SF",
  },
}
```

## Parse YAML data

[Section titled “Parse YAML data”](#parse-yaml-data)

YAML is common in configuration files. Use [`parse_yaml()`](/reference/functions/parse_yaml):

```tql
from {config: "user: alice\nrole: admin\npermissions:\n  - read\n  - write"}
data = config.parse_yaml()
```

```tql
{
  config: "user: alice\nrole: admin\npermissions:\n  - read\n  - write",
  data: {
    user: "alice",
    role: "admin",
    permissions: [
      "read",
      "write",
    ],
  },
}
```

## Use Grok patterns for complex formats

[Section titled “Use Grok patterns for complex formats”](#use-grok-patterns-for-complex-formats)

When data doesn’t follow simple patterns, [`parse_grok()`](/reference/functions/parse_grok) provides powerful pattern matching:

```tql
from {log: "2024-01-15 10:30:45 ERROR [UserService] Failed to authenticate user alice"}
parsed = log.parse_grok("%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \\[%{DATA:service}\\] %{GREEDYDATA:message}")
```

```tql
{
  log: "2024-01-15 10:30:45 ERROR [UserService] Failed to authenticate user alice",
  parsed: {
    timestamp: 2024-01-15T10:30:45Z,
    level: "ERROR",
    service: "UserService",
    message: "Failed to authenticate user alice",
  },
}
```

Common Grok patterns include:

* `%{DATA:fieldname}` - Match any characters (non-greedy)
* `%{GREEDYDATA:fieldname}` - Match any characters (greedy)
* `%{NUMBER:fieldname}` - Match numbers
* `%{IP:fieldname}` - Match IP addresses
* `%{TIMESTAMP_ISO8601:fieldname}` - Match ISO timestamps
* `%{LOGLEVEL:fieldname}` - Match log levels (ERROR, WARN, INFO, etc.)
* `%{WORD:fieldname}` - Match a single word
* `%{QUOTEDSTRING:fieldname}` - Match quoted strings

## Parse standard log formats

[Section titled “Parse standard log formats”](#parse-standard-log-formats)

### Syslog messages

[Section titled “Syslog messages”](#syslog-messages)

Syslog is ubiquitous in system logging. Use [`parse_syslog()`](/reference/functions/parse_syslog):

```tql
from {line: "2024-01-15T10:30:45.123Z myhost myapp[1234]: User login failed"}
syslog = line.parse_syslog()
```

```tql
{
  line: "2024-01-15T10:30:45.123Z myhost myapp[1234]: User login failed",
  syslog: {
    facility: null,
    severity: null,
    timestamp: 2024-01-15T10:30:45.123Z,
    hostname: "myhost",
    app_name: "myapp",
    process_id: "1234",
    content: "User login failed",
  },
}
```

### CEF (Common Event Format)

[Section titled “CEF (Common Event Format)”](#cef-common-event-format)

For security tools using CEF, use [`parse_cef()`](/reference/functions/parse_cef):

```tql
from {log: "CEF:0|Security|Firewall|1.0|100|Connection Blocked|5|src=10.0.0.1 dst=192.168.1.1 spt=12345 dpt=443"}
event = log.parse_cef()
```

```tql
{
  log: "CEF:0|Security|Firewall|1.0|100|Connection Blocked|5|src=10.0.0.1 dst=192.168.1.1 spt=12345 dpt=443",
  event: {
    cef_version: 0,
    device_vendor: "Security",
    device_product: "Firewall",
    device_version: "1.0",
    signature_id: "100",
    name: "Connection Blocked",
    severity: "5",
    extension: {
      src: 10.0.0.1,
      dst: 192.168.1.1,
      spt: 12345,
      dpt: 443,
    },
  },
}
```

### LEEF (Log Event Extended Format)

[Section titled “LEEF (Log Event Extended Format)”](#leef-log-event-extended-format)

For IBM QRadar’s LEEF format, use [`parse_leef()`](/reference/functions/parse_leef):

```tql
from {log: "LEEF:1.0|Security|Firewall|1.0|100|src=10.0.0.1|dst=192.168.1.1|spt=12345|dpt=443"}
event = log.parse_leef()
```

```tql
{
  log: "LEEF:1.0|Security|Firewall|1.0|100|src=10.0.0.1|dst=192.168.1.1|spt=12345|dpt=443",
  event: {
    leef_version: "1.0",
    vendor: "Security",
    product_name: "Firewall",
    product_version: "1.0",
    event_class_id: "100",
    attributes: {
      src: "10.0.0.1|dst=192.168.1.1|spt=12345|dpt=443",
    },
  },
}
```

## Parse timestamps

[Section titled “Parse timestamps”](#parse-timestamps)

Convert time strings to proper timestamp values with [`parse_time()`](/reference/functions/parse_time):

```tql
from {
  log1: "Event at 2024-01-15",
  log2: "Event at 15/Jan/2024:10:30:45",
  log3: "Event at Mon Jan 15 10:30:45 2024"
}
time1 = log1.split(" at ")[1].parse_time("%Y-%m-%d")
time2 = log2.split(" at ")[1].parse_time("%d/%b/%Y:%H:%M:%S")
time3 = log3.split(" at ")[1].parse_time("%a %b %d %H:%M:%S %Y")
```

```tql
{
  log1: "Event at 2024-01-15",
  log2: "Event at 15/Jan/2024:10:30:45",
  log3: "Event at Mon Jan 15 10:30:45 2024",
  time1: 2024-01-15T00:00:00Z,
  time2: 2024-01-15T10:30:45Z,
  time3: 2024-01-15T10:30:45Z,
}
```

## Layer multiple parsers

[Section titled “Layer multiple parsers”](#layer-multiple-parsers)

Real-world logs often require multiple parsing steps. Let’s parse a web server log that contains syslog formatting with embedded JSON:

```tql
from {
  line: "2024-01-15T10:30:45Z web nginx[5678]: {\"method\":\"POST\",\"path\":\"/api/login\",\"status\":401,\"duration\":\"125ms\",\"client\":\"192.168.1.100\"}"
}
// First, parse the syslog wrapper
syslog = line.parse_syslog()
// Then parse the JSON content
request = syslog.content.parse_json()
// Extract specific fields we care about
method = request.method
path = request.path
status = request.status
client_ip = request.client.ip()
```

```tql
{
  line: "2024-01-15T10:30:45Z web nginx[5678]: {\"method\":\"POST\",\"path\":\"/api/login\",\"status\":401,\"duration\":\"125ms\",\"client\":\"192.168.1.100\"}",
  syslog: {
    facility: null,
    severity: null,
    timestamp: 2024-01-15T10:30:45Z,
    hostname: "web",
    app_name: "nginx",
    process_id: "5678",
    content: "{\"method\":\"POST\",\"path\":\"/api/login\",\"status\":401,\"duration\":\"125ms\",\"client\":\"192.168.1.100\"}",
  },
  request: {
    method: "POST",
    path: "/api/login",
    status: 401,
    duration: 125ms,
    client: 192.168.1.100,
  },
  method: "POST",
  path: "/api/login",
  status: 401,
  client_ip: 192.168.1.100,
}
```

## Parse and transform incrementally

[Section titled “Parse and transform incrementally”](#parse-and-transform-incrementally)

When dealing with complex nested data, work incrementally. Here’s a practical example with firewall logs:

```tql
from {
  log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN"
}
// Step 1: Extract the basic structure
parts = log.parse_grok("%{TIMESTAMP_ISO8601:time} %{DATA:device} %{DATA:action} %{GREEDYDATA:details}")
```

```tql
{
  log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN",
  parts: {
    time: 2024-01-15T10:30:45Z,
    device: "FW01",
    action: "BLOCK",
    details: "src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN",
  },
}
```

Now parse the details field:

```tql
from {
  log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN"
}
parts = log.parse_grok("%{TIMESTAMP_ISO8601:time} %{DATA:device} %{DATA:action} %{GREEDYDATA:details}")
// Step 2: Parse the key-value pairs
parts.details = parts.details.parse_kv()
```

```tql
{
  log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN",
  parts: {
    time: 2024-01-15T10:30:45Z,
    device: "FW01",
    action: "BLOCK",
    details: {
      src: "10.0.0.5:54321",
      dst: "93.184.216.34:443",
      proto: "TCP",
      flags: "SYN",
    },
  }
}
```

Finally, parse the IP:port combinations:

```tql
from {
  log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN"
}
parts = log.parse_grok("%{TIMESTAMP_ISO8601:time} %{DATA:device} %{DATA:action} %{GREEDYDATA:details}")
parts.details = parts.details.parse_kv()
// Step 3: Split IP:port combinations
src_parts = parts.details.src.split(":")
dst_parts = parts.details.dst.split(":")
// Step 4: Create clean output
this = {
  timestamp: parts.time,
  device: parts.device,
  action: parts.action,
  src_ip: src_parts[0].ip(),
  src_port: src_parts[1].int(),
  dst_ip: dst_parts[0].ip(),
  dst_port: dst_parts[1].int(),
  protocol: parts.details.proto,
  flags: parts.details.flags
}
```

```tql
{
  timestamp: 2024-01-15T10:30:45Z,
  device: "FW01",
  action: "BLOCK",
  src_ip: 10.0.0.5,
  src_port: 54321,
  dst_ip: 93.184.216.34,
  dst_port: 443,
  protocol: "TCP",
  flags: "SYN",
}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Work incrementally**: Parse complex data in stages, testing each step

2. **Check intermediate results**: Examine data after each parsing step

3. **Handle errors gracefully**: Parsing functions return null on failure

4. **Use appropriate parsers**:

   * JSON/YAML for structured data
   * Key-value for simple pairs
   * CSV/TSV/SSV for tabular data
   * Grok for complex patterns
   * Specific parsers (syslog, CEF, LEEF) for standard formats

5. **Transform types**: After parsing, convert strings to appropriate types (timestamps, IPs, numbers)

6. **Consider performance**: Simpler parsers (JSON, KV) are faster than complex ones (Grok)

## Related guides

[Section titled “Related guides”](#related-guides)

* [Filter and select data](/guides/data-shaping/filter-and-select-data) - Work with parsed fields
* [Transform basic values](/guides/data-shaping/transform-basic-values) - Convert parsed strings to proper types
* [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations

# Filter and select data

Filtering and selecting are fundamental operations when working with data streams. This guide shows you how to filter events based on conditions and select specific fields from your data.

## Understanding operators vs functions

[Section titled “Understanding operators vs functions”](#understanding-operators-vs-functions)

Before we dive in, it’s important to understand a key distinction in TQL:

* **Operators** like [`where`](/reference/operators/where), [`select`](/reference/operators/select), and [`drop`](/reference/operators/drop) work on entire event streams
* **Functions** like [`starts_with()`](/reference/functions/starts_with) or mathematical comparisons work on individual values within events

You’ll see both in action throughout this guide.

## Filter events with conditions

[Section titled “Filter events with conditions”](#filter-events-with-conditions)

Use the [`where`](/reference/operators/where) operator to keep only events that match specific conditions.

### Basic filtering

[Section titled “Basic filtering”](#basic-filtering)

Filter events based on a simple condition. This example keeps only successful HTTP requests (status code 200):

```tql
from {status: 200, path: "/api/users"},
     {status: 404, path: "/api/missing"},
     {status: 200, path: "/home"}
where status == 200
```

```tql
{status: 200, path: "/api/users"}
{status: 200, path: "/home"}
```

The event with `status: 404` is filtered out because it doesn’t match our condition.

### Combining conditions

[Section titled “Combining conditions”](#combining-conditions)

Use logical operators (`and`, `or`, `not`) to combine multiple conditions:

```tql
from {status: 200, path: "/api/users", size: 1024},
     {status: 404, path: "/api/missing", size: 512},
     {status: 200, path: "/home", size: 2048}
where status == 200 and size > 1000
```

```tql
{status: 200, path: "/api/users", size: 1024}
{status: 200, path: "/home", size: 2048}
```

You can also use `or` and `not` with functions like [`starts_with()`](/reference/functions/starts_with):

```tql
from {status: 200, path: "/api/users"},
     {status: 404, path: "/api/missing"},
     {status: 500, path: "/api/error"}
where status == 200 or not path.starts_with("/api/m")
```

```tql
{status: 200, path: "/api/users"}
{status: 500, path: "/api/error"}
```

### Using functions in filters

[Section titled “Using functions in filters”](#using-functions-in-filters)

Functions work on values to create more sophisticated filters. For example, [`ends_with()`](/reference/functions/ends_with) checks string suffixes:

```tql
from {user: "alice", email: "alice@example.com"},
     {user: "bob", email: "bob@gmail.com"},
     {user: "charlie", email: "charlie@example.com"}
where email.ends_with("example.com")
```

```tql
{user: "alice", email: "alice@example.com"}
{user: "charlie", email: "charlie@example.com"}
```

### Filtering with patterns

[Section titled “Filtering with patterns”](#filtering-with-patterns)

Match patterns using regular expressions with [`match_regex()`](/reference/functions/match_regex):

```tql
from {log: "Error: Connection timeout"},
     {log: "Info: Request processed"},
     {log: "Error: Invalid input"}
where log.match_regex("Error:")
```

```tql
{log: "Error: Connection timeout"}
{log: "Error: Invalid input"}
```

## Select specific fields

[Section titled “Select specific fields”](#select-specific-fields)

The [`select`](/reference/operators/select) operator lets you pick which fields to keep in your output.

### Basic field selection

[Section titled “Basic field selection”](#basic-field-selection)

Select only the fields you need:

```tql
from {name: "alice", age: 30, city: "NYC"},
     {name: "bob", age: 25, city: "SF"}
select name, age
```

```tql
{name: "alice", age: 30}
{name: "bob", age: 25}
```

### Renaming fields

[Section titled “Renaming fields”](#renaming-fields)

You can rename fields while selecting them. This is useful when standardizing field names from different data sources:

```tql
from {first_name: "alice", years: 30},
     {first_name: "bob", years: 25}
select name=first_name, age=years
```

```tql
{name: "alice", age: 30}
{name: "bob", age: 25}
```

Here `first_name` becomes `name` and `years` becomes `age` in the output.

### Computing new fields

[Section titled “Computing new fields”](#computing-new-fields)

Create new fields with expressions during selection:

```tql
from {price: 100, tax_rate: 0.08},
     {price: 50, tax_rate: 0.08}
select price, tax=price * tax_rate, total=price * (1 + tax_rate)
```

```tql
{price: 100, tax: 8.0, total: 108.0}
{price: 50, tax: 4.0, total: 54.0}
```

## Remove unwanted fields

[Section titled “Remove unwanted fields”](#remove-unwanted-fields)

The [`drop`](/reference/operators/drop) operator removes specified fields, keeping everything else.

### Basic field removal

[Section titled “Basic field removal”](#basic-field-removal)

Remove fields you don’t need:

```tql
from {user: "alice", password: "secret", email: "alice@example.com"},
     {user: "bob", password: "hidden", email: "bob@example.com"}
drop password
```

```tql
{user: "alice", email: "alice@example.com"}
{user: "bob", email: "bob@example.com"}
```

### Dropping multiple fields

[Section titled “Dropping multiple fields”](#dropping-multiple-fields)

Remove several fields at once:

```tql
from {id: 1, internal_id: "xyz", debug: true, name: "alice"},
     {id: 2, internal_id: "abc", debug: false, name: "bob"}
drop internal_id, debug
```

```tql
{id: 1, name: "alice"}
{id: 2, name: "bob"}
```

## Add computed fields

[Section titled “Add computed fields”](#add-computed-fields)

Use the [`set`](/reference/operators/set) operator to override existing fields and add new fields without removing existing ones.

Implied operator

The [`set`](/reference/operators/set) operator is *implied*, i.e., you can omit it to keep your pipeline definitions terse. In other words, it suffices to write an assignment in the form `x = y` instead of `set x = y`.

### Adding simple fields

[Section titled “Adding simple fields”](#adding-simple-fields)

Add a constant field to all events:

```tql
from {user: "alice"},
     {user: "bob"}
set source = "api"
```

```tql
{user: "alice", source: "api"}
{user: "bob", source: "api"}
```

### Computing field values

[Section titled “Computing field values”](#computing-field-values)

Add fields based on calculations:

```tql
from {bytes_sent: 1024, bytes_received: 2048},
     {bytes_sent: 512, bytes_received: 1024}
set total_bytes = bytes_sent + bytes_received
```

```tql
{bytes_sent: 1024, bytes_received: 2048, total_bytes: 3072}
{bytes_sent: 512, bytes_received: 1024, total_bytes: 1536}
```

### Using the `else` keyword

[Section titled “Using the else keyword”](#using-the-else-keyword)

Use the `else` word to provide default values for null fields:

```tql
from {name: "alice", score: 85},
     {name: "bob"},
     {name: "charlie", score: 95}
score = score? else 0
```

```tql
{name: "alice", score: 85}
{name: "bob", score: 0}
{name: "charlie", score: 95}
```

Accessing non-existent fields

Because the second event had no `score` field, the `else` keyword filled in a default value of `0`. However, accessing non-existent fields generates a warning. We use the trailing question mark (`?`) to suppress warnings when accessing fields that may not exist, i.e., `score? else 0` in the above example. This is distinctively different from an existing field with a null value, which does not elicit a warning.

## Combining operations

[Section titled “Combining operations”](#combining-operations)

Real-world pipelines often combine multiple operations:

```tql
from {method: "GET", path: "/api/users", status: 200, duration_ms: 45},
     {method: "POST", path: "/api/users", status: 201, duration_ms: 120},
     {method: "GET", path: "/api/users/123", status: 404, duration_ms: 15}
where status >= 200 and status < 300
select method, path, duration = duration_ms.milliseconds()
```

```tql
{method: "GET", path: "/api/users", duration: 45ms}
{method: "POST", path: "/api/users", duration: 120ms}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Filter early**: Apply [`where`](/reference/operators/where) conditions as early as possible to reduce the amount of data flowing through your pipeline.

2. **Select only what you need**: Use [`select`](/reference/operators/select) to keep only necessary fields, especially when dealing with large events.

3. **Choose the right operator**:

   * Use [`select`](/reference/operators/select) when you want to restrict the output to specific fields.
   * Use [`drop`](/reference/operators/drop) when you want to remove a few fields from many.
   * Use [`set`](/reference/operators/set) when you want to add/override fields without changing existing ones.

4. **Understand null handling**: The [`where`](/reference/operators/where) operator skips events where the condition evaluates to null or false. Use the question mark operator (`?`) to suppress warnings when accessing fields that may not exist.

## Related guides

[Section titled “Related guides”](#related-guides)

* [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations
* [Transform basic values](/guides/data-shaping/transform-basic-values) - Learn about value transformations
* [Slice and sample data](/guides/data-shaping/slice-and-sample-data) - Control the flow of events

# Manipulate strings

String manipulation is essential for cleaning, formatting, and transforming text data. This guide covers TQL’s comprehensive string functions, from simple case changes to complex pattern matching and encoding operations.

## Change text case

[Section titled “Change text case”](#change-text-case)

Transform strings to different cases for consistency and formatting:

```tql
from {name: "john smith", title: "data ENGINEER", code: "xyz-123"}
lower_name = name.to_lower()
upper_code = code.to_upper()
title_case = title.to_title()
cap_name = name.capitalize()
```

```tql
{
  name: "john smith",
  title: "data ENGINEER",
  code: "xyz-123",
  lower_name: "john smith",
  upper_code: "XYZ-123",
  title_case: "Data Engineer",
  cap_name: "John smith",
}
```

Functions explained:

* [`to_lower()`](/reference/functions/to_lower) - Converts all characters to lowercase
* [`to_upper()`](/reference/functions/to_upper) - Converts all characters to uppercase
* [`to_title()`](/reference/functions/to_title) - Capitalizes first letter of each word
* [`capitalize()`](/reference/functions/capitalize) - Capitalizes only the first letter

## Trim whitespace

[Section titled “Trim whitespace”](#trim-whitespace)

Clean up strings by removing unwanted whitespace:

```tql
from {
  raw: "  hello world  ",
  prefix: "\t\tdata",
  suffix: "value   \n"
}
trimmed = raw.trim()
no_prefix = prefix.trim_start()
no_suffix = suffix.trim_end()
```

```tql
{
  raw: "  hello world  ",
  prefix: "\t\tdata",
  suffix: "value   \n",
  trimmed: "hello world",
  no_prefix: "data",
  no_suffix: "value"
}
```

Functions:

* [`trim()`](/reference/functions/trim) - Removes whitespace from both ends
* [`trim_start()`](/reference/functions/trim_start) - Removes whitespace from beginning
* [`trim_end()`](/reference/functions/trim_end) - Removes whitespace from end

## Split and join strings

[Section titled “Split and join strings”](#split-and-join-strings)

Break strings apart and combine them back together:

```tql
from {
  path: "/home/user/documents/report.pdf",
  tags: "security,network,alert"
}
parts = path.split("/")
tag_list = tags.split(",")
rejoined = parts.join("-")
```

```tql
{
  path: "/home/user/documents/report.pdf",
  tags: "security,network,alert",
  parts: [
    "",
    "home",
    "user",
    "documents",
    "report.pdf",
  ],
  tag_list: [
    "security",
    "network",
    "alert",
  ],
  rejoined: "-home-user-documents-report.pdf",
}
```

### Split with regular expressions

[Section titled “Split with regular expressions”](#split-with-regular-expressions)

Use [`split_regex()`](/reference/functions/split_regex) for complex splitting:

```tql
from {text: "error:42|warning:7|info:125"}
entries = text.split_regex("[:|]")
```

```tql
{
  text: "error:42|warning:7|info:125",
  entries: [
    "error",
    "42",
    "warning",
    "7",
    "info",
    "125",
  ],
}
```

## Find and replace text

[Section titled “Find and replace text”](#find-and-replace-text)

Replace specific text or patterns within strings:

### Simple replacement

[Section titled “Simple replacement”](#simple-replacement)

```tql
from {
  log: "User 192.168.1.1 accessed /admin",
  template: "Hello {name}, welcome to {place}"
}
masked = log.replace("192.168.1.1", "xxx.xxx.xxx.xxx")
filled = template.replace("{name}", "Alice").replace("{place}", "Tenzir")
```

```tql
{
  log: "User 192.168.1.1 accessed /admin",
  template: "Hello {name}, welcome to {place}",
  masked: "User xxx.xxx.xxx.xxx accessed /admin",
  filled: "Hello Alice, welcome to Tenzir",
}
```

### Pattern-based replacement

[Section titled “Pattern-based replacement”](#pattern-based-replacement)

Use [`replace_regex()`](/reference/functions/replace_regex) for complex replacements:

```tql
from {
  text: "Contact us at 555-1234 or 555-5678",
  log: "Error at 2024-01-15 10:30:45: Connection failed"
}
redacted = text.replace_regex("\\d{3}-\\d{4}", "XXX-XXXX")
simple_log = log.replace_regex(
  "\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}",
  "TIMESTAMP"
)
```

```tql
{
  text: "Contact us at 555-1234 or 555-5678",
  log: "Error at 2024-01-15 10:30:45: Connection failed",
  redacted: "Contact us at XXX-XXXX or XXX-XXXX",
  simple_log: "Error at TIMESTAMP: Connection failed",
}
```

## Match patterns

[Section titled “Match patterns”](#match-patterns)

Check if strings match specific patterns:

```tql
from {
  email: "alice@example.com",
  url: "https://tenzir.com",
  file: "report_2024.pdf"
}
is_email = email.match_regex("^[^@]+@[^@]+\\.[^@]+$")
is_https = url.starts_with("https://")
is_pdf = file.ends_with(".pdf")
```

```tql
{
  email: "alice@example.com",
  url: "https://tenzir.com",
  file: "report_2024.pdf",
  is_email: true,
  is_https: true,
  is_pdf: true,
}
```

Pattern matching functions:

* [`match_regex()`](/reference/functions/match_regex) - Test against regular expression
* [`starts_with()`](/reference/functions/starts_with) - Check string prefix
* [`ends_with()`](/reference/functions/ends_with) - Check string suffix

## Validate string content

[Section titled “Validate string content”](#validate-string-content)

Check what type of characters a string contains:

```tql
from {
  id: "12345",
  name: "Alice",
  code: "abc123",
  mixed: "Hello World!",
  spaces: "hello world"
}
id_numeric = id.is_numeric()
name_alpha = name.is_alpha()
code_alnum = code.is_alnum()
mixed_alpha = mixed.is_alpha()
has_lower = spaces.is_lower()
has_upper = name.is_title()
```

```tql
{
  id: "12345",
  name: "Alice",
  code: "abc123",
  mixed: "Hello World!",
  spaces: "hello world",
  id_numeric: true,
  name_alpha: true,
  code_alnum: true,
  mixed_alpha: false,
  has_lower: true,
  has_upper: true,
}
```

Validation functions:

* [`is_numeric()`](/reference/functions/is_numeric) - Contains only digits
* [`is_alpha()`](/reference/functions/is_alpha) - Contains only letters
* [`is_alnum()`](/reference/functions/is_alnum) - Contains only letters and digits
* [`is_lower()`](/reference/functions/is_lower) - All cased characters are lowercase
* [`is_upper()`](/reference/functions/is_upper) - All cased characters are uppercase
* [`is_title()`](/reference/functions/is_title) - String is in title case
* [`is_printable()`](/reference/functions/is_printable) - Contains only printable characters

## Measure string properties

[Section titled “Measure string properties”](#measure-string-properties)

Get information about string characteristics:

```tql
from {
  text: "Hello 世界",
  emoji: "👋 Hello!",
  path: "/var/log/system.log"
}
char_count = text.length_chars()
byte_count = text.length_bytes()
reversed = emoji.reverse()
filename = path.file_name()
directory = path.parent_dir()
```

```tql
{
  text: "Hello 世界",
  emoji: "👋 Hello!",
  path: "/var/log/system.log",
  char_count: 8,
  byte_count: 12,
  reversed: "!olleH 👋",
  filename: "system.log",
  directory: "/var/log",
}
```

String property functions:

* [`length_chars()`](/reference/functions/length_chars) - Count Unicode characters
* [`length_bytes()`](/reference/functions/length_bytes) - Count bytes
* [`reverse()`](/reference/functions/reverse) - Reverse character order
* [`file_name()`](/reference/functions/file_name) - Extract filename from path
* [`parent_dir()`](/reference/functions/parent_dir) - Extract directory from path

## Extract substrings

[Section titled “Extract substrings”](#extract-substrings)

Use [`slice()`](/reference/functions/slice) to extract portions of strings:

```tql
from {
  text: "Hello, World!",
  id: "USER-12345-ACTIVE",
  timestamp: "2024-01-15T10:30:45"
}
greeting = text.slice(begin=0, end=5)
user_num = id.slice(begin=5, end=10)
date_part = timestamp.slice(begin=0, end=10)
status = id.slice(begin=11)
```

```tql
{
  text: "Hello, World!",
  id: "USER-12345-ACTIVE",
  timestamp: "2024-01-15T10:30:45",
  greeting: "Hello",
  user_num: "12345",
  date_part: "2024-01-15",
  status: "ACTIVE",
}
```

The `slice()` function parameters:

* `begin` - Starting position (0-based, negative counts from end)
* `end` - Ending position (exclusive, optional)
* `stride` - Step between characters (optional, can be negative)

## Encode and decode strings

[Section titled “Encode and decode strings”](#encode-and-decode-strings)

Transform strings between different encodings:

### Base64 encoding

[Section titled “Base64 encoding”](#base64-encoding)

```tql
from {secret: "my-api-key-12345"}
encoded = secret.encode_base64()
decoded = encoded.decode_base64()
```

```tql
{
  secret: "my-api-key-12345",
  encoded: "bXktYXBpLWtleS0xMjM0NQ==",
  decoded: b"my-api-key-12345",
}
```

### Hex encoding

[Section titled “Hex encoding”](#hex-encoding)

Use [`encode_hex()`](/reference/functions/encode_hex) and [`decode_hex()`](/reference/functions/decode_hex):

```tql
from {data: "Hello", hex_string: "48656c6c6f"}
hex = data.encode_hex()
decoded = hex.decode_hex()
decoded_blob = hex_string.decode_hex()
```

```tql
{
  data: "Hello",
  hex_string: "48656c6c6f",
  hex: "48656c6c6f",
  decoded: b"Hello",
  decoded_blob: b"Hello",
}
```

### URL encoding

[Section titled “URL encoding”](#url-encoding)

```tql
from {query: "search term with spaces & special=characters"}
encoded = query.encode_url()
decoded = encoded.decode_url()
```

```tql
{
  query: "search term with spaces & special=characters",
  encoded: "search%20term%20with%20spaces%20%26%20special%3Dcharacters",
  decoded: b"search term with spaces & special=characters",
}
```

Encoding functions:

* [`encode_base64()`](/reference/functions/encode_base64) / [`decode_base64()`](/reference/functions/decode_base64)
* [`encode_hex()`](/reference/functions/encode_hex) / [`decode_hex()`](/reference/functions/decode_hex)
* [`encode_url()`](/reference/functions/encode_url) / [`decode_url()`](/reference/functions/decode_url)

## Pad strings

[Section titled “Pad strings”](#pad-strings)

Add characters to reach a specific length:

```tql
from {
  id: "42",
  code: "ABC"
}
padded_id = id.pad_start(5, "0")
padded_code = code.pad_end(10, "-")
```

```tql
{
  id: "42",
  code: "ABC",
  padded_id: "00042",
  padded_code: "ABC-------"
}
```

Padding functions:

* [`pad_start()`](/reference/functions/pad_start) - Add characters to the beginning
* [`pad_end()`](/reference/functions/pad_end) - Add characters to the end

## Read file contents

[Section titled “Read file contents”](#read-file-contents)

Access text from files during processing:

```tql
from {}
hostname = file_contents("/etc/hostname")
```

```tql
{
  hostname: "my-server\n",
}
```

The [`file_contents()`](/reference/functions/file_contents) function reads the entire file as a string. The file path must be a constant expression. Use with caution on large files.

## Practical examples

[Section titled “Practical examples”](#practical-examples)

### Clean and normalize user input

[Section titled “Clean and normalize user input”](#clean-and-normalize-user-input)

```tql
from {
  user_input: "  JOHN.SMITH@EXAMPLE.COM  ",
  phone: "(555) 123-4567"
}
email = user_input.trim().to_lower()
clean_phone = phone.replace_regex("[^0-9]", "")
```

```tql
{
  user_input: "  JOHN.SMITH@EXAMPLE.COM  ",
  phone: "(555) 123-4567",
  email: "john.smith@example.com",
  clean_phone: "5551234567"
}
```

### Extract and validate identifiers

[Section titled “Extract and validate identifiers”](#extract-and-validate-identifiers)

```tql
from {
  log: "User ID: ABC-123-XYZ performed action",
  url: "https://api.example.com/v2/users/42"
}
user_id = log.split("User ID: ")[1].split(" ")[0]
valid_id = user_id.match_regex("^[A-Z]{3}-\\d{3}-[A-Z]{3}$")
api_version = url.split("/")[4]
user_num = url.split("/").last()
```

```tql
{
  log: "User ID: ABC-123-XYZ performed action",
  url: "https://api.example.com/v2/users/42",
  user_id: "ABC-123-XYZ",
  valid_id: true,
  api_version: "v2",
  user_num: "42"
}
```

### Build formatted output

[Section titled “Build formatted output”](#build-formatted-output)

```tql
from {
  first: "alice",
  last: "smith",
  dept: "engineering",
  id: 42
}
full_name = first.capitalize() + " " + last.to_upper()
email = first + "." + last + "@company.com"
badge = dept.to_upper().slice(begin=0, end=3) + "-" + id.string()
```

```tql
{
  first: "alice",
  last: "smith",
  dept: "engineering",
  id: 42,
  full_name: "Alice SMITH",
  email: "alice.smith@company.com",
  badge: "ENG-42",
}
```

## Generate hash values

[Section titled “Generate hash values”](#generate-hash-values)

Create checksums and identifiers using hash functions:

### Common hash algorithms

[Section titled “Common hash algorithms”](#common-hash-algorithms)

```tql
from {
  data: "Hello, World!",
  secret: "my-api-key-123"
}
md5 = data.hash_md5()
sha1 = data.hash_sha1()
sha224 = data.hash_sha224()
sha256 = data.hash_sha256()
sha384 = data.hash_sha384()
sha512 = data.hash_sha512()
xxh3 = data.hash_xxh3()
```

```tql
{
  data: "Hello, World!",
  secret: "my-api-key-123",
  md5: "65a8e27d8879283831b664bd8b7f0ad4",
  sha1: "0a0a9f2a6772942557ab5355d76af442f8f65e01",
  sha224: "72a23dfa411ba6fde01dbfabf3b00a709c93ebf273dc29e2d8b261ff",
  sha256: "dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f",
  sha384: "5485cc9b3365b4305dfb4e8337e0a598a574f8242bf17289e0dd6c20a3cd44a089de16ab4ab308f63e44b1170eb5f515",
  sha512: "374d794a95cdcfd8b35993185fef9ba368f160d8daf432d08ba9f1ed1e5abe6cc69291e0fa2fe0006a52570ef18c19def4e617c33ce52ef0a6e5fbe318cb0387",
  xxh3: "c7269dc5f8602ca5",
}
```

### Create unique identifiers

[Section titled “Create unique identifiers”](#create-unique-identifiers)

Use hashes to generate identifiers from multiple fields:

```tql
from {
  user_id: "alice123",
  timestamp: "2024-01-15T10:30:00",
  action: "login"
}
event_id = f"{user_id}-{timestamp}-{action}".hash_sha256().slice(begin=0, end=16)
short_hash = f"{user_id}{action}".hash_md5().slice(begin=0, end=8)
numeric_id = user_id.hash_xxh3()
```

```tql
{
  user_id: "alice123",
  timestamp: "2024-01-15T10:30:00",
  action: "login",
  event_id: "d5f456083b8fee43",
  short_hash: "1616f7f2",
  numeric_id: "ac6dfe13bd512d81",
}
```

### Verify data integrity

[Section titled “Verify data integrity”](#verify-data-integrity)

```tql
from {
  file_content: "Important document content here...",
  expected_checksum: "dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f"
}
actual_checksum = file_content.hash_sha256()
valid = actual_checksum == expected_checksum
```

```tql
{
  file_content: "Important document content here...",
  expected_checksum: "dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f",
  actual_checksum: "25f898ef7be64ead26e775e41778c6b5b5e5fe135d1b6658b6a27f9334c4f085",
  valid: false,
}
```

## Network security functions

[Section titled “Network security functions”](#network-security-functions)

Process network data with specialized security functions:

### Generate Community IDs

[Section titled “Generate Community IDs”](#generate-community-ids)

Use [`community_id()`](/reference/functions/community_id) to create standardized flow hashes:

```tql
from {
  src_ip: 192.168.1.100,
  dst_ip: 10.0.0.1,
  src_port: 54321,
  dst_port: 443,
  proto: "tcp"
}
flow_id = community_id(
  src_ip=src_ip,
  dst_ip=dst_ip,
  src_port=src_port,
  dst_port=dst_port,
  proto=proto
)
```

```tql
{
  src_ip: 192.168.1.100,
  dst_ip: 10.0.0.1,
  src_port: 54321,
  dst_port: 443,
  proto: "tcp",
  flow_id: "1:ZSU9hCO1tdr7pj3SCLkQ0XS3uvI=",
}
```

### Anonymize IP addresses

[Section titled “Anonymize IP addresses”](#anonymize-ip-addresses)

Use [`encrypt_cryptopan()`](/reference/functions/encrypt_cryptopan) for consistent IP anonymization:

```tql
from {
  client_ip: 192.168.1.100,
  server_ip: 8.8.8.8,
  internal_ip: 10.0.0.5
}
anon_client = client_ip.encrypt_cryptopan(seed="mysecretkey12345")
anon_server = server_ip.encrypt_cryptopan(seed="mysecretkey12345")
anon_internal = internal_ip.encrypt_cryptopan(seed="mysecretkey12345")
```

```tql
{
  client_ip: 192.168.1.100,
  server_ip: 8.8.8.8,
  internal_ip: 10.0.0.5,
  anon_client: 206.216.1.132,
  anon_server: 110.0.51.203,
  anon_internal: 109.255.195.194,
}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Chain operations efficiently**: Combine multiple string operations in one expression
2. **Validate before transforming**: Check string content before applying operations
3. **Handle edge cases**: Empty strings, null values, and special characters
4. **Use appropriate functions**: Choose `length_chars()` vs `length_bytes()` based on needs
5. **Be mindful of encoding**: Ensure correct encoding when dealing with international text

## Related guides

[Section titled “Related guides”](#related-guides)

* [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse complex text formats
* [Transform basic values](/guides/data-shaping/transform-basic-values) - Convert between data types
* [Filter and select data](/guides/data-shaping/filter-and-select-data) - Use string functions in filters

# Reshape complex data

Real-world data is rarely flat. It contains nested structures, arrays of objects, and deeply hierarchical information. This guide shows advanced techniques for reshaping complex data structures to meet your analysis needs.

## Flatten nested structures

[Section titled “Flatten nested structures”](#flatten-nested-structures)

Transform deeply nested data into flat structures for easier analysis.

### Basic flattening

[Section titled “Basic flattening”](#basic-flattening)

Start with simple nested objects:

```tql
from {
  user: {
    id: 123,
    profile: {
      name: "Alice",
      contact: {
        email: "alice@example.com",
        phone: "+1-555-0123"
      }
    }
  }
}
flat_user = {
  id: user.id,
  name: user.profile.name,
  email: user.profile.contact.email,
  phone: user.profile.contact.phone
}
```

```tql
{
  user: {...},
  flat_user: {
    id: 123,
    name: "Alice",
    email: "alice@example.com",
    phone: "+1-555-0123",
  },
}
```

### Flatten with prefixes

[Section titled “Flatten with prefixes”](#flatten-with-prefixes)

Preserve context when flattening:

```tql
from {
  event: {
    id: "evt-001",
    user: {name: "Alice", role: "admin"},
    system: {name: "web-server", version: "2.1"}
  }
}
flattened = {
  event_id: event.id,
  user_name: event.user.name,
  user_role: event.user.role,
  system_name: event.system.name,
  system_version: event.system.version
}
```

```tql
{
  event: {...},
  flattened: {
    event_id: "evt-001",
    user_name: "Alice",
    user_role: "admin",
    system_name: "web-server",
    system_version: "2.1",
  }
}
```

## Unflatten data

[Section titled “Unflatten data”](#unflatten-data)

Reconstruct hierarchical structures from flattened data using [`unflatten()`](/reference/functions/unflatten):

### Basic unflattening

[Section titled “Basic unflattening”](#basic-unflattening)

Convert dotted field names back to nested structures:

```tql
from {
  flattened: {
    "user.name": "Alice",
    "user.email": "alice@example.com",
    "user.address.city": "NYC",
    "user.address.zip": "10001",
    "status": "active"
  }
}
nested = flattened.unflatten()
```

```tql
{
  flattened: {
    "user.name": "Alice",
    "user.email": "alice@example.com",
    "user.address.city": "NYC",
    "user.address.zip": "10001",
    status: "active",
  },
  nested: {
    user: {
      name: "Alice",
      email: "alice@example.com",
      address: {
        city: "NYC",
        zip: "10001",
      },
    },
    status: "active",
  },
}
```

### Custom separator

[Section titled “Custom separator”](#custom-separator)

Use a different separator for field paths:

```tql
from {
  metrics: {
    cpu_usage_percent: 45,
    memory_used_gb: 8,
    memory_total_gb: 16,
    disk_root_used: 100,
    disk_root_total: 500
  }
}
structured = metrics.unflatten("_")
```

```tql
{
  metrics: {
    cpu_usage_percent: 45,
    memory_used_gb: 8,
    memory_total_gb: 16,
    disk_root_used: 100,
    disk_root_total: 500,
  },
  structured: {
    cpu: {
      usage: {
        percent: 45,
      },
    },
    memory: {
      used: {
        gb: 8,
      },
      total: {
        gb: 16,
      },
    },
    disk: {
      root: {
        used: 100,
        total: 500,
      },
    },
  },
}
```

## Normalize denormalized data

[Section titled “Normalize denormalized data”](#normalize-denormalized-data)

Convert wide data to long format for better analysis.

### Wide to long transformation

[Section titled “Wide to long transformation”](#wide-to-long-transformation)

```tql
from {
  metrics: {
    timestamp: "2024-01-15T10:00:00",
    cpu_usage: 45,
    memory_usage: 62,
    disk_usage: 78
  }
}
long_format = [
  {timestamp: metrics.timestamp, metric: "cpu", value: metrics.cpu_usage},
  {timestamp: metrics.timestamp, metric: "memory", value: metrics.memory_usage},
  {timestamp: metrics.timestamp, metric: "disk", value: metrics.disk_usage}
]
```

```tql
{
  metrics: {
    timestamp: "2024-01-15T10:00:00",
    cpu_usage: 45,
    memory_usage: 62,
    disk_usage: 78,
  },
  long_format: [
    {
      timestamp: "2024-01-15T10:00:00",
      metric: "cpu",
      value: 45,
    },
    {
      timestamp: "2024-01-15T10:00:00",
      metric: "memory",
      value: 62,
    },
    {
      timestamp: "2024-01-15T10:00:00",
      metric: "disk",
      value: 78,
    },
  ],
}
```

### Transform to event stream

[Section titled “Transform to event stream”](#transform-to-event-stream)

Convert arrays to event streams for processing:

```tql
from {
  readings: [
    {sensor: "temp", location: "room1", value: 72},
    {sensor: "humidity", location: "room1", value: 45},
    {sensor: "temp", location: "room2", value: 68},
    {sensor: "humidity", location: "room2", value: 50}
  ]
}
unroll readings
select sensor=readings.sensor,
       location=readings.location,
      value=readings.value
```

```tql
{sensor: "temp", location: "room1", value: 72}
{sensor: "humidity", location: "room1", value: 45}
{sensor: "temp", location: "room2", value: 68}
{sensor: "humidity", location: "room2", value: 50}
```

## Extract arrays from objects

[Section titled “Extract arrays from objects”](#extract-arrays-from-objects)

Transform objects with numbered keys into proper arrays.

```tql
from {
  response: {
    item_0: {name: "Widget", price: 9.99},
    item_1: {name: "Gadget", price: 19.99},
    item_2: {name: "Tool", price: 14.99},
    total_items: 3
  }
}
// Extract items manually when keys are known
items = [
  response.item_0,
  response.item_1,
  response.item_2
]
```

```tql
{
  response: {...},
  items: [
    {name: "Widget", price: 9.99},
    {name: "Gadget", price: 19.99},
    {name: "Tool", price: 14.99},
  ]
}
```

## Build hierarchical structures

[Section titled “Build hierarchical structures”](#build-hierarchical-structures)

Create nested structures from flat data.

### Group data with summarize

[Section titled “Group data with summarize”](#group-data-with-summarize)

Use the [`summarize`](/reference/operators/summarize) operator to group data:

```tql
from {
  records: [
    {dept: "Engineering", team: "Backend", member: "Alice"},
    {dept: "Engineering", team: "Backend", member: "Bob"},
    {dept: "Engineering", team: "Frontend", member: "Charlie"},
    {dept: "Sales", team: "Direct", member: "David"}
  ]
}
unroll records
summarize dept=records.dept, team=records.team, members=collect(records.member)
```

```tql
{
  dept: "Sales",
  team: "Direct",
  members: [
    "David",
  ],
}
{
  dept: "Engineering",
  team: "Frontend",
  members: [
    "Charlie",
  ],
}
{
  dept: "Engineering",
  team: "Backend",
  members: [
    "Alice",
    "Bob",
  ],
}
```

### Extract path components

[Section titled “Extract path components”](#extract-path-components)

```tql
from {
  paths: [
    "/home/user/docs/report.pdf",
    "/home/user/docs/summary.txt",
    "/home/user/images/photo.jpg",
    "/var/log/system.log"
  ]
}
path_info = paths.map(path => {
  full_path: path,
  directory: path.parent_dir(),
  filename: path.file_name(),
  parts: path.split("/").where(p => p.length_bytes() > 0)
})
```

```tql
{
  paths: [...],
  path_info: [
    {
      full_path: "/home/user/docs/report.pdf",
      directory: "/home/user/docs",
      filename: "report.pdf",
      parts: ["home", "user", "docs", "report.pdf"],
    },
    {
      full_path: "/home/user/docs/summary.txt",
      directory: "/home/user/docs",
      filename: "summary.txt",
      parts: ["home", "user", "docs", "summary.txt"],
    },
    {
      full_path: "/home/user/images/photo.jpg",
      directory: "/home/user/images",
      filename: "photo.jpg",
      parts: ["home", "user", "images", "photo.jpg"],
    },
    {
      full_path: "/var/log/system.log",
      directory: "/var/log",
      filename: "system.log",
      parts: ["var", "log", "system.log"],
    },
  ],
}
```

## Merge fragmented data

[Section titled “Merge fragmented data”](#merge-fragmented-data)

Combine data split across multiple records.

### Merge records with spread operator

[Section titled “Merge records with spread operator”](#merge-records-with-spread-operator)

```tql
from {
  user: {id: 1, name: "Alice"},
  profile: {email: "alice@example.com", dept: "Engineering"},
  metrics: {logins: 45, actions: 234}
}
merged = {
  ...user,
  ...profile,
  ...metrics
}
```

```tql
{
  user: {
    id: 1,
    name: "Alice",
  },
  profile: {
    email: "alice@example.com",
    dept: "Engineering",
  },
  metrics: {
    logins: 45,
    actions: 234,
  },
  merged: {
    id: 1,
    name: "Alice",
    email: "alice@example.com",
    dept: "Engineering",
    logins: 45,
    actions: 234,
  },
}
```

## Handle dynamic schemas

[Section titled “Handle dynamic schemas”](#handle-dynamic-schemas)

Work with data that has varying structures.

### Normalize inconsistent records

[Section titled “Normalize inconsistent records”](#normalize-inconsistent-records)

```tql
from {
  events: [
    {type: "user", data: {name: "Alice", email: "alice@example.com"}},
    {type: "system", data: {cpu: 45, memory: 1024}},
    {type: "error", message: "Connection failed", code: 500}
  ]
}
select normalized = events.map(e => {
  event_type: e.type,
  timestamp: now(),
  // Use conditional assignment for type-specific fields
  user_name: e.data.name if e.type == "user",
  user_email: e.data.email if e.type == "user",
  system_cpu: e.data.cpu if e.type == "system",
  system_memory: e.data.memory if e.type == "system",
  error_message: e.message if e.type == "error",
  error_code: e.code if e.type == "error"
})
```

```tql
{
  normalized: [
    {
      event_type: "user",
      timestamp: 2025-07-21T18:54:09.307412Z,
      user_name: "Alice",
      user_email: "alice@example.com",
      system_cpu: null,
      system_memory: null,
      error_message: null,
      error_code: null,
    },
    {
      event_type: "system",
      timestamp: 2025-07-21T18:54:09.307412Z,
      user_name: null,
      user_email: null,
      system_cpu: 45,
      system_memory: 1024,
      error_message: null,
      error_code: null,
    },
    {
      event_type: "error",
      timestamp: 2025-07-21T18:54:09.307412Z,
      user_name: null,
      user_email: null,
      system_cpu: null,
      system_memory: null,
      error_message: "Connection failed",
      error_code: 500,
    },
  ],
}
```

## Working with aggregated events

[Section titled “Working with aggregated events”](#working-with-aggregated-events)

### Unroll arrays to individual events

[Section titled “Unroll arrays to individual events”](#unroll-arrays-to-individual-events)

Some data sources aggregate multiple events into a single record. Use [`unroll`](/reference/operators/unroll) to expand these into individual events:

Expanding aggregated message types

```tql
// DHCP logs may contain multiple message types in one record
from {
  ts: 2024-01-15T10:00:00,
  uid: "C123abc",
  msg_types: ["DISCOVER", "OFFER", "REQUEST", "ACK"],
  client_addr: 192.168.1.100
}
unroll msg_types
// Now we have 4 separate events, one per message type
activity_name = msg_types.to_title()
```

```tql
{ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: "DISCOVER", client_addr: 192.168.1.100, activity_name: "Discover"}
{ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: "OFFER", client_addr: 192.168.1.100, activity_name: "Offer"}
{ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: "REQUEST", client_addr: 192.168.1.100, activity_name: "Request"}
{ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: "ACK", client_addr: 192.168.1.100, activity_name: "Ack"}
```

This pattern is essential when:

* Converting aggregated logs to event-per-row formats
* Normalizing data for OCSF or other schemas that expect individual events
* Processing batched API responses

## Safe arithmetic with optional fields

[Section titled “Safe arithmetic with optional fields”](#safe-arithmetic-with-optional-fields)

### Dynamic field computation with null safety

[Section titled “Dynamic field computation with null safety”](#dynamic-field-computation-with-null-safety)

When computing metrics from fields that might be null, use the `else` keyword for safe fallbacks:

Null-safe calculations

```tql
from {
  packets_sent: 1000,
  packets_received: 950,
  duration: 10s,
  bytes_sent: null,  // Missing data
  bytes_received: 5000
}


// Safe division with fallback
packets_per_second = packets_sent / duration.count_seconds() else 0


// Handle missing values in arithmetic
loss_rate = (packets_sent - packets_received) / packets_sent else 0


// Compute only when both values exist
throughput = (bytes_sent + bytes_received) / duration.count_seconds() if bytes_sent != null else null


// Complex calculation with multiple fallbacks
efficiency = (bytes_received / bytes_sent) * 100 if bytes_sent != null else
              100 if bytes_received > 0 else 0
```

```tql
{
  packets_sent: 1000,
  packets_received: 950,
  duration: 10s,
  bytes_sent: null,
  bytes_received: 5000,
  packets_per_second: 100.0,
  loss_rate: 0.05,
  throughput: null,
  efficiency: 100
}
```

## Conditional aggregation patterns

[Section titled “Conditional aggregation patterns”](#conditional-aggregation-patterns)

### Selective data collection

[Section titled “Selective data collection”](#selective-data-collection)

Collect values conditionally during aggregation:

Conditional collection in summarize

```tql
from {src_ip: 10.0.0.5, dst_port: 22, bytes: 1024},
     {src_ip: 192.168.1.10, dst_port: 80, bytes: 2048},
     {src_ip: 10.0.0.5, dst_port: 443, bytes: 4096},
     {src_ip: 192.168.1.10, dst_port: 22, bytes: 512}


let $critical_ports = [22, 3389, 5985]


summarize src_ip,
  total_bytes=sum(bytes),
  // Collect all unique ports
  all_ports=collect(dst_port),
  // Collect with conditional transformation
  port_types=collect("HIGH" if dst_port in $critical_ports else "LOW")
```

```tql
{src_ip: 192.168.1.10, total_bytes: 2560, all_ports: [80, 22], port_types: ["LOW", "HIGH"]}
{src_ip: 10.0.0.5, total_bytes: 5120, all_ports: [22, 443], port_types: ["HIGH", "LOW"]}
```

This pattern enables:

* Building risk profiles during aggregation
* Transforming values during collection based on conditions
* Creating categorical metrics from raw data

## Advanced transformations

[Section titled “Advanced transformations”](#advanced-transformations)

### Recursive flattening

[Section titled “Recursive flattening”](#recursive-flattening)

Flatten arbitrarily nested structures:

```tql
from {
  data: {
    level1: {
      level2: {
        level3: {
          value: "deep",
          items: [1, 2, 3]
        }
      },
      other: "value"
    }
  }
}
// Use flatten function for automatic recursive flattening
select flattened = data.flatten()
```

```tql
{
  flattened: {
    "level1.level2.level3.value": "deep",
    "level1.level2.level3.items": [
      1,
      2,
      3,
    ],
    "level1.other": "value",
  },
}
```

### Extract fields by prefix

[Section titled “Extract fields by prefix”](#extract-fields-by-prefix)

Use field access to extract specific configurations:

```tql
from {
  config: {
    env_DATABASE_HOST: "db.example.com",
    env_DATABASE_PORT: 5432,
    env_API_KEY: "secret123",
    app_name: "MyApp",
    app_version: "1.0"
  }
}
// Extract specific fields directly
select env_vars = {
    DATABASE_HOST: config.env_DATABASE_HOST,
    DATABASE_PORT: config.env_DATABASE_PORT,
    API_KEY: config.env_API_KEY
  },
  app_config = {
    name: config.app_name,
    version: config.app_version
  }
```

```tql
{
  env_vars: {
    DATABASE_HOST: "db.example.com",
    DATABASE_PORT: 5432,
    API_KEY: "secret123",
  },
  app_config: {
    name: "MyApp",
    version: "1.0",
  },
}
```

## Practical examples

[Section titled “Practical examples”](#practical-examples)

### Process nested API responses

[Section titled “Process nested API responses”](#process-nested-api-responses)

```tql
from {
  api_response: {
    status: "success",
    data: {
      user: {
        id: 123,
        profile: {
          personal: {name: "Alice", age: 30},
          professional: {title: "Engineer", company: "TechCorp"}
        }
      },
      metadata: {
        request_id: "req-001",
        timestamp: "2024-01-15T10:00:00"
      }
    }
  }
}
// Extract and reshape for database storage
select user_record = {
  user_id: api_response.data.user.id,
  name: api_response.data.user.profile.personal.name,
  age: api_response.data.user.profile.personal.age,
  job_title: api_response.data.user.profile.professional.title,
  company: api_response.data.user.profile.professional.company,
  last_updated: api_response.data.metadata.timestamp.parse_time("%Y-%m-%dT%H:%M:%S")
}
```

```tql
{
  user_record: {
    user_id: 123,
    name: "Alice",
    age: 30,
    job_title: "Engineer",
    company: "TechCorp",
    last_updated: 2024-01-15T10:00:00Z,
  },
}
```

### Transform log aggregations

[Section titled “Transform log aggregations”](#transform-log-aggregations)

```tql
from {
  log_stats: {
    "2024-01-15": {
      "/api/users": {GET: 150, POST: 20},
      "/api/orders": {GET: 200, POST: 50, DELETE: 5}
    },
    "2024-01-16": {
      "/api/users": {GET: 180, POST: 25},
      "/api/orders": {GET: 220, POST: 60}
    }
  }
}
// Flatten nested structure into individual events
select flattened = log_stats.flatten()
```

```tql
{
  flattened: {
    "2024-01-15./api/users.GET": 150,
    "2024-01-15./api/users.POST": 20,
    "2024-01-15./api/orders.GET": 200,
    "2024-01-15./api/orders.POST": 50,
    "2024-01-15./api/orders.DELETE": 5,
    "2024-01-16./api/users.GET": 180,
    "2024-01-16./api/users.POST": 25,
    "2024-01-16./api/orders.GET": 220,
    "2024-01-16./api/orders.POST": 60,
  },
}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Plan your structure**: Design the target schema before transforming
2. **Handle missing fields**: Use conditional logic or defaults for optional data
3. **Preserve information**: Don’t lose data during transformation unless intentional
4. **Test edge cases**: Verify transformations work with incomplete or unusual data
5. **Document complex logic**: Add comments explaining non-obvious transformations

## Related guides

[Section titled “Related guides”](#related-guides)

* [Transform collections](/guides/data-shaping/transform-collections) - Basic collection operations
* [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse before reshaping
* [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations

# Shape data

TQL comes with numerous transformation [operators](/reference/operators) that change the shape of events, as well as [functions](/reference/functions) that work on values within a single event.

This guide provides an overview of data shaping capabilities in TQL, showcasing the operators and functions that you need for day-to-day data transformation tasks.

## Understanding operators vs functions

[Section titled “Understanding operators vs functions”](#understanding-operators-vs-functions)

Before diving into specific operations, it’s important to understand a key distinction in TQL:

* **Operators** work on streams of events and can keep state between multiple events (e.g., [`where`](/reference/operators/where), [`select`](/reference/operators/select), [`summarize`](/reference/operators/summarize))
* **Functions** work on individual values within a single event (e.g., [`starts_with()`](/reference/functions/starts_with), [`round()`](/reference/functions/round), [`merge()`](/reference/functions/merge))

Here is a visual overview of transformations that you can perform over a stream of events:

![Shaping Overview](/_astro/shape-data.C9Ucev8W_19DKCs.svg)

## Common data shaping tasks

[Section titled “Common data shaping tasks”](#common-data-shaping-tasks)

Here are the most common data shaping operations organized by use case:

### Filter and select data

[Section titled “Filter and select data”](#filter-and-select-data)

The fundamental operations for controlling which events and fields flow through your pipeline. See the [Filter and select data](/guides/data-shaping/filter-and-select-data) guide for detailed examples of:

* Filtering events with [`where`](/reference/operators/where)
* Selecting fields with [`select`](/reference/operators/select)
* Removing fields with [`drop`](/reference/operators/drop)
* Adding fields with [`set`](/reference/operators/set)

### Control event flow

[Section titled “Control event flow”](#control-event-flow)

Manage how many events pass through your pipeline and in what order. See the [Slice and sample data](/guides/data-shaping/slice-and-sample-data) guide for:

* Getting first/last events with [`head`](/reference/operators/head) and [`tail`](/reference/operators/tail)
* Slicing ranges with [`slice`](/reference/operators/slice)
* Sampling by schema with [`taste`](/reference/operators/taste)
* Reversing order with [`reverse`](/reference/operators/reverse)

### Transform values

[Section titled “Transform values”](#transform-values)

Convert and manipulate individual values within events. See the [Transform basic values](/guides/data-shaping/transform-basic-values) guide for:

* Type conversions ([`int()`](/reference/functions/int), [`float()`](/reference/functions/float), [`string()`](/reference/functions/string), [`time()`](/reference/functions/time))
* String operations ([`to_upper()`](/reference/functions/to_upper), [`trim()`](/reference/functions/trim), [`capitalize()`](/reference/functions/capitalize))
* Mathematical operations ([`round()`](/reference/functions/round), [`abs()`](/reference/functions/abs), [`sqrt()`](/reference/functions/sqrt))
* Handling null values with the `else` keyword

### Work with complex structures

[Section titled “Work with complex structures”](#work-with-complex-structures)

#### Relocate fields with `move`

[Section titled “Relocate fields with move”](#relocate-fields-with-move)

Use the [`move`](/reference/operators/move) operator to rename and relocate fields in one operation:

```tql
from {old: 42}
move new = old
```

```tql
{new: 42}
```

Moving multiple fields:

```tql
from {foo: 1, bar: 2}
move foo=bar, qux=foo
```

```tql
{
  foo: 2,
  qux: 1,
}
```

#### Aggregate events with `summarize`

[Section titled “Aggregate events with summarize”](#aggregate-events-with-summarize)

Use [`summarize`](/reference/operators/summarize) to group and aggregate data. This example groups events by the `y` field and sums up the `x` values for each group:

```tql
from {x: 0, y: 0, z: 1},
     {x: 1, y: 1, z: 2},
     {x: 1, y: 1, z: 3}
summarize y, x=sum(x)
```

```tql
{y: 0, x: 0}
{y: 1, x: 2}
```

In this example:

* Events are grouped by the value of `y` (0 or 1)
* For `y=0`, there’s one event with `x=0`, so `sum(x)=0`
* For `y=1`, there are two events with `x=1` each, so `sum(x)=2`

See the [aggregation functions](/reference/functions#aggregation) reference for all available aggregation functions like `count()`, `mean()`, `max()`, etc.

#### Reorder events with `sort`

[Section titled “Reorder events with sort”](#reorder-events-with-sort)

Use [`sort`](/reference/operators/sort) to arrange events by field values:

```tql
from {x: 2, y: "bar"},
     {x: 3, y: "baz"},
     {x: 1, y: "foo"}
sort -x
```

```tql
{x: 3, y: "baz"}
{x: 2, y: "bar"}
{x: 1, y: "foo"}
```

Prepending the field with `-` reverses the sort order.

#### Break up lists with `unroll`

[Section titled “Break up lists with unroll”](#break-up-lists-with-unroll)

Use [`unroll`](/reference/operators/unroll) to expand lists into separate events:

```tql
from {
  xs: [{a: 1}, {a: 2}],
  y: "foo",
}
unroll xs
```

```tql
{
  xs: {
    a: 1,
  },
  y: "foo",
}
{
  xs: {
    a: 2,
  },
  y: "foo",
}
```

### Manipulate records and lists

[Section titled “Manipulate records and lists”](#manipulate-records-and-lists)

#### Combine records with `merge`

[Section titled “Combine records with merge”](#combine-records-with-merge)

Use the [`merge`](/reference/functions/merge) function to combine records. This is useful when you need to consolidate data from multiple sources:

```tql
from {
  foo: {
    bar: 1,
    baz: 2,
  },
  qux: {
    fred: 3,
    george: 4,
    bar: 5,
  }
}
set this = merge(foo, qux)
```

```tql
{
  bar: 5,
  baz: 2,
  fred: 3,
  george: 4
}
```

Note that the field `bar` appears in both records. The value from the second argument (`qux.bar = 5`) overwrites the value from the first (`foo.bar = 1`).

You can also use the spread expression as shorthand:

```tql
set this = {...foo, ...qux}
```

#### Combine lists with `concatenate`

[Section titled “Combine lists with concatenate”](#combine-lists-with-concatenate)

Use [`concatenate`](/reference/functions/concatenate) to join lists:

```tql
from {
  xs: [1,2,3],
  ys: [4,5,6],
}
select result = concatenate(xs, ys)
```

```tql
{
  result: [
    1,
    2,
    3,
    4,
    5,
    6,
  ],
}
```

Or use the spread expression:

```tql
select result = [...xs, ...ys]
```

#### Add values to lists

[Section titled “Add values to lists”](#add-values-to-lists)

Use [`append`](/reference/functions/append) and [`prepend`](/reference/functions/prepend):

```tql
from {
  xs: [2],
}
set xs = append(xs, 3)
set xs = prepend(xs, 1)
```

```tql
{
  xs: [
    1,
    2,
    3,
  ],
}
```

### Specialized operations

[Section titled “Specialized operations”](#specialized-operations)

#### Perform bitwise operations

[Section titled “Perform bitwise operations”](#perform-bitwise-operations)

TQL provides bitwise functions for low-level data manipulation using [`bit_and()`](/reference/functions/bit_and), [`bit_or()`](/reference/functions/bit_or), [`bit_xor()`](/reference/functions/bit_xor), [`bit_not()`](/reference/functions/bit_not), [`shift_left()`](/reference/functions/shift_left), and [`shift_right()`](/reference/functions/shift_right):

```tql
from {
  band: bit_and(5, 3),
  bor: bit_or(5, 3),
  bxor: bit_xor(5, 3),
  bnot: bit_not(5),
  shl: shift_left(5, 2),
  shr: shift_right(5, 1),
}
```

```tql
{
  band: 1,  // (0101 & 0011 = 0001)
  bor: 7,   // (0101 | 0011 = 0111)
  bxor: 6,  // (0101 ^ 0011 = 0110)
  bnot: -6, // (~0101 = 1010)
  shl: 20,  // (0101 << 2 = 10100)
  shr: 2,   // (0101 >> 1 = 0010)
}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Understand the operator/function distinction**: Use operators for stream-level operations and functions for value-level transformations.

2. **Filter early**: Apply [`where`](/reference/operators/where) conditions as early as possible to reduce data volume.

3. **Select only what you need**: Use [`select`](/reference/operators/select) to keep only necessary fields, especially with large events.

4. **Choose the right tool**:

   * Use [`select`](/reference/operators/select) to keep specific fields
   * Use [`drop`](/reference/operators/drop) to remove a few fields from many
   * Use [`set`](/reference/operators/set) to add fields without changing existing ones
   * Use [`move`](/reference/operators/move) to rename fields efficiently

5. **Be mindful of performance**: Some operators like [`tail`](/reference/operators/tail) and [`reverse`](/reference/operators/reverse) must buffer all input before producing output.

## Where to go from here

[Section titled “Where to go from here”](#where-to-go-from-here)

Explore these specialized guides for deeper coverage of specific topics:

* [Filter and select data](/guides/data-shaping/filter-and-select-data) - Master filtering and field selection
* [Transform basic values](/guides/data-shaping/transform-basic-values) - Type conversions and value manipulation
* [Manipulate strings](/guides/data-shaping/manipulate-strings) - Text processing and formatting
* [Work with time](/guides/data-shaping/work-with-time) - Parse, format, and calculate with timestamps
* [Transform collections](/guides/data-shaping/transform-collections) - Work with lists and records
* [Aggregate and summarize](/guides/data-shaping/aggregate-and-summarize) - Statistical operations and grouping
* [Slice and sample data](/guides/data-shaping/slice-and-sample-data) - Control event flow
* [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse complex text formats
* [Convert data formats](/guides/data-shaping/convert-data-formats) - Transform between JSON, CSV, YAML, and more
* [Reshape complex data](/guides/data-shaping/reshape-complex-data) - Advanced structural transformations
* [Deduplicate events](/guides/data-shaping/deduplicate-events) - Remove duplicate events

For a complete list of available operators and functions, see the [operators](/reference/operators) and [functions](/reference/functions) reference documentation.

# Slice and sample data

When working with data streams, you often need to control which events flow through your pipeline. This guide shows you how to slice event streams, sample data, and control event ordering using TQL operators.

## Understanding stream operators

[Section titled “Understanding stream operators”](#understanding-stream-operators)

The operators in this guide work on entire event streams:

* **[head](/reference/operators/head)** and **[tail](/reference/operators/tail)** - Get events from the beginning or end
* **[slice](/reference/operators/slice)** - Extract a specific range of events
* **[taste](/reference/operators/taste)** - Sample events by schema
* **[reverse](/reference/operators/reverse)** - Invert the order of events
* **[sample](/reference/operators/sample)** - Randomly sample events

These operators maintain state between events, unlike functions that work on individual values.

## Get events from the beginning

[Section titled “Get events from the beginning”](#get-events-from-the-beginning)

Use the [`head`](/reference/operators/head) operator to get the first N events from a stream.

### Basic usage

[Section titled “Basic usage”](#basic-usage)

Get the first 3 events:

```tql
from {id: 1, value: "a"},
     {id: 2, value: "b"},
     {id: 3, value: "c"},
     {id: 4, value: "d"},
     {id: 5, value: "e"}
head 3
```

```tql
{id: 1, value: "a"}
{id: 2, value: "b"}
{id: 3, value: "c"}
```

### Default behavior

[Section titled “Default behavior”](#default-behavior)

Without an argument, [`head`](/reference/operators/head) returns all events:

```tql
from {id: 1, value: "first"},
     {id: 2, value: "second"},
     {id: 3, value: "third"}
head
```

```tql
{id: 1, value: "first"}
{id: 2, value: "second"}
{id: 3, value: "third"}
```

## Get events from the end

[Section titled “Get events from the end”](#get-events-from-the-end)

Use the [`tail`](/reference/operators/tail) operator to get the last N events.

### Basic usage

[Section titled “Basic usage”](#basic-usage-1)

Get the last 2 events:

```tql
from {id: 1, value: "a"},
     {id: 2, value: "b"},
     {id: 3, value: "c"},
     {id: 4, value: "d"}
tail 2
```

```tql
{id: 3, value: "c"}
{id: 4, value: "d"}
```

Performance consideration

The [`tail`](/reference/operators/tail) operator must consume all input before producing output, making it memory-intensive for large streams. Use [`head`](/reference/operators/head) when possible for better performance.

## Slice event streams

[Section titled “Slice event streams”](#slice-event-streams)

The [`slice`](/reference/operators/slice) operator provides fine-grained control over which events to extract.

### Extract a range

[Section titled “Extract a range”](#extract-a-range)

Get events 2 through 4 (0-indexed):

```tql
from {n: 1}, {n: 2}, {n: 3}, {n: 4}, {n: 5}
slice begin=1, end=4
```

```tql
{n: 2}
{n: 3}
{n: 4}
```

### Skip events with stride

[Section titled “Skip events with stride”](#skip-events-with-stride)

Get every other event starting from the second:

```tql
from {n: 1}, {n: 2}, {n: 3}, {n: 4}, {n: 5}, {n: 6}
slice begin=1, stride=2
```

```tql
{n: 2}
{n: 4}
{n: 6}
```

### Combine parameters

[Section titled “Combine parameters”](#combine-parameters)

Skip 2, take every 3rd event, stop after 10 total:

```tql
from {n: 1}, {n: 2}, {n: 3}, {n: 4}, {n: 5},
     {n: 6}, {n: 7}, {n: 8}, {n: 9}, {n: 10}
slice begin=2, end=10, stride=3
```

```tql
{n: 3}
{n: 6}
{n: 9}
```

## Sample events by schema

[Section titled “Sample events by schema”](#sample-events-by-schema)

The [`taste`](/reference/operators/taste) operator samples events based on their structure, giving you examples of different data shapes in your stream.

### Get schema examples

[Section titled “Get schema examples”](#get-schema-examples)

See one example of each unique schema:

```tql
from {type: "user", name: "alice"},
     {type: "user", name: "bob"},
     {type: "event", id: 1},
     {type: "event", id: 2},
     {value: 42}
taste 1
```

```tql
{type: "user", name: "alice"}
{type: "event", id: 1}
{value: 42}
```

### Get multiple examples per schema

[Section titled “Get multiple examples per schema”](#get-multiple-examples-per-schema)

Get up to 2 examples of each schema:

```tql
from {x: 1, y: 1},
     {x: 2, y: 2},
     {x: 3},
     {x: 4},
     {z: "a"},
     {z: "b"}
taste 2
```

```tql
{x: 1, y: 1}
{x: 2, y: 2}
{x: 3}
{x: 4}
{z: "a"}
{z: "b"}
```

## Reverse event order

[Section titled “Reverse event order”](#reverse-event-order)

Use the [`reverse`](/reference/operators/reverse) operator to invert the order of events in a stream:

```tql
from {seq: 1, msg: "first"},
     {seq: 2, msg: "second"},
     {seq: 3, msg: "third"}
reverse
```

```tql
{seq: 3, msg: "third"}
{seq: 2, msg: "second"}
{seq: 1, msg: "first"}
```

Memory usage

Like [`tail`](/reference/operators/tail), the [`reverse`](/reference/operators/reverse) operator must buffer all input before producing output. Use with caution on large streams.

## Time-based sampling

[Section titled “Time-based sampling”](#time-based-sampling)

Use the [`sample`](/reference/operators/sample) operator to sample events based on time intervals:

### Sample by duration

[Section titled “Sample by duration”](#sample-by-duration)

Sample events at regular time intervals:

```tql
from {id: 1}, {id: 2}, {id: 3}, {id: 4}, {id: 5},
     {id: 6}, {id: 7}, {id: 8}, {id: 9}, {id: 10}
sample 1s
```

```tql
{id: 1}
{id: 2}
{id: 3}
{id: 4}
{id: 5}
{id: 6}
{id: 7}
{id: 8}
{id: 9}
{id: 10}
```

Note: The sample operator uses duration-based sampling, not random probability sampling.

## Combining operators

[Section titled “Combining operators”](#combining-operators)

Chain operators to create more complex sampling strategies:

```tql
from {user: "alice", action: "login", time: 1},
     {user: "bob", action: "view", time: 2},
     {user: "alice", action: "edit", time: 3},
     {user: "charlie", action: "login", time: 4},
     {user: "bob", action: "logout", time: 5}
where action == "login"
head 2
```

```tql
{user: "alice", action: "login", time: 1}
{user: "charlie", action: "login", time: 4}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Prefer [`head`](/reference/operators/head) over [`tail`](/reference/operators/tail)**: [`head`](/reference/operators/head) stops processing once it has enough events, while [`tail`](/reference/operators/tail) must process everything.

2. **Use [`taste`](/reference/operators/taste) for exploration**: When working with unfamiliar data, [`taste`](/reference/operators/taste) quickly shows you the different schemas present.

3. **Be mindful of memory**: Operators like [`tail`](/reference/operators/tail) and [`reverse`](/reference/operators/reverse) buffer all input, which can consume significant memory for large streams.

4. **Combine with filters**: Use [`where`](/reference/operators/where) before slicing operators to reduce the amount of data processed.

## Related guides

[Section titled “Related guides”](#related-guides)

* [Filter and select data](/guides/data-shaping/filter-and-select-data) - Learn about filtering events
* [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations
* [Deduplicate events](/guides/data-shaping/deduplicate-events) - Remove duplicate events from streams

# Transform basic values

Transforming values is a fundamental part of data processing. This guide shows you how to convert between different data types, perform basic calculations, and manipulate simple values within your events.

## Type conversions

[Section titled “Type conversions”](#type-conversions)

TQL provides functions to convert values between different types. This is essential when data arrives in the wrong format or when you need specific types for further processing.

### Convert to numbers

[Section titled “Convert to numbers”](#convert-to-numbers)

Use [`int()`](/reference/functions/int) and [`float()`](/reference/functions/float) to convert values to numeric types:

```tql
from {price: "42", quantity: "3.5"},
     {price: "99", quantity: "1.0"}
price = price.int()
quantity = quantity.float()
```

```tql
{price: 42, quantity: 3.5}
{price: 99, quantity: 1.0}
```

### Convert to strings

[Section titled “Convert to strings”](#convert-to-strings)

Use [`string()`](/reference/functions/string) to convert any value to its string representation:

```tql
from {status: 200, ratio: 0.95},
     {status: 404, ratio: 0.05}
message = status.string() + " - " + (ratio * 100).string() + "%"
```

```tql
{status: 200, ratio: 0.95, message: "200 - 95.0%"}
{status: 404, ratio: 0.05, message: "404 - 5.0%"}
```

### Parse times and durations

[Section titled “Parse times and durations”](#parse-times-and-durations)

Convert strings to time values with [`time()`](/reference/functions/time):

```tql
from {timestamp: "2024-01-15"},
     {timestamp: "2024-02-20"}
parsed_time = timestamp.time()
```

```tql
{timestamp: "2024-01-15", parsed_time: 2024-01-15T00:00:00Z}
{timestamp: "2024-02-20", parsed_time: 2024-02-20T00:00:00Z}
```

Convert strings to durations with [`duration()`](/reference/functions/duration):

```tql
from {interval: "5s"},
     {interval: "2.5min"}
parsed_duration = interval.duration()
```

```tql
{interval: "5s", parsed_duration: 5s}
{interval: "2.5min", parsed_duration: 2.5min}
```

### Convert to unsigned integers

[Section titled “Convert to unsigned integers”](#convert-to-unsigned-integers)

Use [`uint()`](/reference/functions/uint) for non-negative integers:

```tql
from {count: "42", ratio: 3.7},
     {count: "-5", ratio: 2.3}
count_uint = count.uint()
ratio_uint = ratio.uint()
```

```tql
{count: "42", ratio: 3.7, count_uint: 42, ratio_uint: 3}
{count: "-5", ratio: 2.3, count_uint: null, ratio_uint: 2}
```

This pipeline elicits the following warning:

```txt
warning: `uint` failed to convert some string
 --> /tmp/pipeline.tql:3:14
  |
3 | count_uint = count.uint()
  |              ~~~~~
  |
```

### Work with IP addresses and subnets

[Section titled “Work with IP addresses and subnets”](#work-with-ip-addresses-and-subnets)

TQL supports IP address and subnet literals directly. You can also parse them from strings using [`ip()`](/reference/functions/ip) and [`subnet()`](/reference/functions/subnet):

```tql
from {direct_ip: 192.168.1.1, direct_subnet: 10.0.0.0/24},
     {direct_ip: ::1, direct_subnet: 2001:db8::/32}
ipv6_check = direct_ip.is_v6()
```

```tql
{
  direct_ip: 192.168.1.1,
  direct_subnet: 10.0.0.0/24,
  ipv6_check: false,
}
{
  direct_ip: ::1,
  direct_subnet: 2001:db8::/32,
  ipv6_check: true,
}
```

Parse from strings when needed:

```tql
from {client: "192.168.1.1", network: "10.0.0.0/24"},
     {client: "10.0.0.5", network: "192.168.0.0/16"}
client_ip = client.ip()
network_subnet = network.subnet()
```

```tql
{
  client: "192.168.1.1",
  network: "10.0.0.0/24",
  client_ip: 192.168.1.1,
  network_subnet: 10.0.0.0/24,
}
{
  client: "10.0.0.5",
  network: "192.168.0.0/16",
  client_ip: 10.0.0.5,
  network_subnet: 192.168.0.0/16,
}
```

## IP address inspection

[Section titled “IP address inspection”](#ip-address-inspection)

Analyze and categorize IP addresses with inspection functions:

### Check IP address types

[Section titled “Check IP address types”](#check-ip-address-types)

Use IP inspection functions like [`is_v4()`](/reference/functions/is_v4), [`is_v6()`](/reference/functions/is_v6), [`is_private()`](/reference/functions/is_private), [`is_global()`](/reference/functions/is_global), [`is_loopback()`](/reference/functions/is_loopback), and [`is_multicast()`](/reference/functions/is_multicast) to analyze addresses:

```tql
from {ip1: 192.168.1.1, ip2: 8.8.8.8, ip3: ::1},
     {ip1: 10.0.0.1, ip2: 224.0.0.1, ip3: 2001:db8::1}
is_v4 = ip1.is_v4()
is_v6 = ip3.is_v6()
is_private = ip1.is_private()
is_global = ip2.is_global()
is_loopback = ip3.is_loopback()
is_multicast = ip2.is_multicast()
```

```tql
{
  ip1: 192.168.1.1,
  ip2: 8.8.8.8,
  ip3: ::1,
  is_v4: true,
  is_v6: true,
  is_private: true,
  is_global: true,
  is_loopback: true,
  is_multicast: false,
}
{
  ip1: 10.0.0.1,
  ip2: 224.0.0.1,
  ip3: 2001:db8::1,
  is_v4: true,
  is_v6: true,
  is_private: true,
  is_global: false,
  is_loopback: false,
  is_multicast: true,
}
```

### Categorize IP addresses

[Section titled “Categorize IP addresses”](#categorize-ip-addresses)

Get detailed IP address classification with [`ip_category()`](/reference/functions/ip_category):

```tql
from {client: "192.168.1.100", server: "8.8.8.8", local: "127.0.0.1"},
     {client: "10.0.0.5", server: "224.0.0.251", local: "::1"}
client_category = client.ip().ip_category()
server_category = server.ip().ip_category()
local_category = local.ip().ip_category()
```

```tql
{
  client: "192.168.1.100",
  server: "8.8.8.8",
  local: "127.0.0.1",
  client_category: "private",
  server_category: "global",
  local_category: "loopback",
}
{
  client: "10.0.0.5",
  server: "224.0.0.251",
  local: "::1",
  client_category: "private",
  server_category: "multicast",
  local_category: "loopback",
}
```

### Check link-local addresses

[Section titled “Check link-local addresses”](#check-link-local-addresses)

Identify link-local addresses with [`is_link_local()`](/reference/functions/is_link_local):

```tql
from {addr1: 169.254.1.1, addr2: fe80::1, addr3: 192.168.1.1},
     {addr1: 169.254.0.1, addr2: 2001:db8::1, addr3: 10.0.0.1}
link_local1 = addr1.is_link_local()
link_local2 = addr2.is_link_local()
link_local3 = addr3.is_link_local()
```

```tql
{
  addr1: 169.254.1.1,
  addr2: fe80::1,
  addr3: 192.168.1.1,
  link_local1: true,
  link_local2: true,
  link_local3: false,
}
{
  addr1: 169.254.0.1,
  addr2: 2001:db8::1,
  addr3: 10.0.0.1,
  link_local1: true,
  link_local2: false,
  link_local3: false,
}
```

## Basic string operations

[Section titled “Basic string operations”](#basic-string-operations)

Transform strings with simple operations to clean and standardize your data.

### Change case

[Section titled “Change case”](#change-case)

Convert strings to different cases:

```tql
from {name: "alice smith", code: "xyz"},
     {name: "BOB JONES", code: "ABC"}
name = name.to_title()
code = code.to_upper()
```

```tql
{name: "Alice Smith", code: "XYZ"}
{name: "Bob Jones", code: "ABC"}
```

### Trim whitespace

[Section titled “Trim whitespace”](#trim-whitespace)

Remove unwanted whitespace from strings:

```tql
from {input: "  hello  ", data: "world   "},
     {input: "   test", data: "  value  "}
input = input.trim()
data = data.trim()
```

```tql
{input: "hello", data: "world"}
{input: "test", data: "value"}
```

### Capitalize strings

[Section titled “Capitalize strings”](#capitalize-strings)

Capitalize the first letter of a string:

```tql
from {word: "hello", phrase: "good morning"},
     {word: "world", phrase: "how are you"}
word = word.capitalize()
```

```tql
{word: "Hello", phrase: "good morning"}
{word: "World", phrase: "how are you"}
```

## Mathematical operations

[Section titled “Mathematical operations”](#mathematical-operations)

Perform calculations on numeric values within your events.

### Basic arithmetic

[Section titled “Basic arithmetic”](#basic-arithmetic)

Use standard arithmetic operators:

```tql
from {a: 10, b: 3},
     {a: 20, b: 4}
sum = a + b
diff = a - b
product = a * b
quotient = (a / b).int()
```

```tql
{a: 10, b: 3, sum: 13, diff: 7, product: 30, quotient: 3}
{a: 20, b: 4, sum: 24, diff: 16, product: 80, quotient: 5}
```

### Rounding numbers

[Section titled “Rounding numbers”](#rounding-numbers)

Round numbers to specific precision:

```tql
from {value: 3.14159},
     {value: 2.71828}
rounded = value.round()
ceil_val = value.ceil()
floor_val = value.floor()
```

```tql
{value: 3.14159, rounded: 3, ceil_val: 4, floor_val: 3}
{value: 2.71828, rounded: 3, ceil_val: 3, floor_val: 2}
```

### Mathematical functions

[Section titled “Mathematical functions”](#mathematical-functions)

Use [`abs()`](/reference/functions/abs) for absolute values and [`sqrt()`](/reference/functions/sqrt) for square roots:

```tql
from {x: -5, y: 16},
     {x: -10, y: 25}
abs_x = x.abs()
sqrt_y = y.sqrt()
```

```tql
{
  x: -5,
  y: 16,
  abs_x: 5,
  sqrt_y: 4.0,
}
{
  x: -10,
  y: 25,
  abs_x: 10,
  sqrt_y: 5.0,
}
```

## Working with null values

[Section titled “Working with null values”](#working-with-null-values)

Handle missing or null values gracefully in your data.

### Provide default values

[Section titled “Provide default values”](#provide-default-values)

Use the `else` keyword to replace null values:

```tql
from {name: "alice", age: 30},
     {name: "bob"},
     {name: "charlie", age: 25}
age = age? else 0
status = status? else "unknown"
```

```tql
{name: "alice", age: 30, status: "unknown"}
{name: "bob", age: 0, status: "unknown"}
{name: "charlie", age: 25, status: "unknown"}
```

## Create new values

[Section titled “Create new values”](#create-new-values)

Generate new values using built-in functions:

### Generate unique identifiers

[Section titled “Generate unique identifiers”](#generate-unique-identifiers)

Use [`uuid()`](/reference/functions/uuid) to create unique identifiers:

```tql
from {user: "alice", action: "login"},
     {user: "bob", action: "create"}
event_id = uuid(version="v7")
session_id = uuid()
```

```tql
{
  user: "alice",
  action: "login",
  event_id: "0198147a-d167-7292-80fa-2665c1263279",
  session_id: "a09a7f44-b665-4f95-bc44-c52fbdb8f428",
}
{
  user: "bob",
  action: "create",
  event_id: "0198147a-d167-72ad-80b4-e052c2287add",
  session_id: "030349dc-2585-49ad-af58-d448ff718c05",
}
```

### Generate random numbers

[Section titled “Generate random numbers”](#generate-random-numbers)

Use [`random()`](/reference/functions/random) to generate random values:

```tql
from {
  random_float: random(),
  random_int: (random() * 100).int(),
  random_choice: "heads" if random() < 0.5 else "tails",
}
```

```tql
{
  random_float: 0.3215780368890365,
  random_int: 88,
  random_choice: "tails",
}
```

## Access external values

[Section titled “Access external values”](#access-external-values)

Retrieve values from external sources like the environment, configuration, or files:

### Read environment variables

[Section titled “Read environment variables”](#read-environment-variables)

Use [`env()`](/reference/functions/env) to access environment variables:

```tql
from {
  home_dir: env("HOME"),
  shell: env("SHELL"),
  custom_var: env("MY_APP_CONFIG") else "/default/config",
}
```

```tql
{
  home_dir: "/Users/alice",
  shell: "/opt/homebrew/bin/fish",
  custom_var: "/default/config",
}
```

### Access configuration

[Section titled “Access configuration”](#access-configuration)

Use [`config()`](/reference/functions/config) to read Tenzir’s configuration:

```tql
from {
  tenzir_config: config(),
}
```

### Read file contents

[Section titled “Read file contents”](#read-file-contents)

Use [`file_contents()`](/reference/functions/file_contents) to read files:

```tql
from {
  api_key: file_contents("/etc/secrets/api_key"),
}
```

### Access secrets

[Section titled “Access secrets”](#access-secrets)

Use [`secret()`](/reference/functions/secret) to retrieve [secrets](/explanations/secrets):

```tql
from {
  auth_token: secret("AUTH_TOKEN"),
}
```

## Type inspection

[Section titled “Type inspection”](#type-inspection)

Examine data types at runtime:

### Get type information

[Section titled “Get type information”](#get-type-information)

Use [`type_of()`](/reference/functions/type_of) to inspect value types. Note that this function returns detailed type information as objects:

```tql
from {
  str: "hello",
  num: 42,
  float: 3.14,
  bool: true,
  arr: [1, 2, 3],
  obj: {key: "value"}
}
str_type = str.type_of()
num_type = num.type_of()
float_type = float.type_of()
bool_type = bool.type_of()
arr_type = arr.type_of()
obj_type = obj.type_of()
```

```tql
{
  str: "hello",
  num: 42,
  float: 3.14,
  bool: true,
  arr: [1, 2, 3],
  obj: {key: "value"},
  str_type: {name: null, kind: "string", attributes: [], state: null},
  num_type: {name: null, kind: "int64", attributes: [], state: null},
  float_type: {name: null, kind: "double", attributes: [], state: null},
  bool_type: {name: null, kind: "bool", attributes: [], state: null},
  arr_type: {name: null, kind: "list", attributes: [], state: {type: {name: null, kind: "int64", attributes: [], state: null}}},
  obj_type: {name: null, kind: "record", attributes: [], state: {fields: [{name: "key", type: {name: null, kind: "string", attributes: [], state: null}}]}}
}
```

### Get type identifiers

[Section titled “Get type identifiers”](#get-type-identifiers)

Use [`type_id()`](/reference/functions/type_id) for type comparison:

```tql
from {value1: "text", value2: 123, value3: "456"}
type1 = value1.type_id()
type2 = value2.type_id()
type3 = value3.type_id()
same_type = value1.type_id() == value3.type_id()
```

```tql
{
  value1: "text",
  value2: 123,
  value3: "456",
  type1: "2476398993549b5",
  type2: "5b0d4f0b0b167404",
  type3: "2476398993549b5",
  same_type: true,
}
```

## Combining transformations

[Section titled “Combining transformations”](#combining-transformations)

Real-world data often requires multiple transformations:

```tql
from {temp_f: "72.5", location: "  new york  "},
     {temp_f: "89.1", location: "los angeles"}
temp_c = ((temp_f.float() - 32) * 5 / 9).round()
location = location.trim().to_title()
reading = f"{temp_c}°C in {location}"
```

```tql
{
  temp_f: "72.5",
  location: "New York",
  temp_c: 23,
  reading: "23°C in New York",
}
{
  temp_f: "89.1",
  location: "Los Angeles",
  temp_c: 32,
  reading: "32°C in Los Angeles",
}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Validate before converting**: Check that values can be converted to avoid errors.

2. **Use appropriate types**: Convert to the most specific type needed (e.g., `int` instead of `float` for whole numbers).

3. **Handle edge cases**: Always consider what happens with null values or invalid input.

4. **Chain operations efficiently**: Combine multiple transformations in a single `set` statement when possible.

## Related guides

[Section titled “Related guides”](#related-guides)

* [Filter and select data](/guides/data-shaping/filter-and-select-data) - Learn about filtering and field selection
* [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations
* [Manipulate strings](/guides/data-shaping/manipulate-strings) - Advanced string manipulation techniques

# Transform collections

Lists and records are fundamental data structures in TQL. This guide shows you how to work with these collections - accessing elements, transforming values, and combining data structures.

## Work with lists

[Section titled “Work with lists”](#work-with-lists)

Lists (arrays) contain ordered sequences of values. Let’s explore how to manipulate them.

### Access list elements

[Section titled “Access list elements”](#access-list-elements)

Get values from specific positions:

```tql
from {items: ["first", "second", "third", "fourth"]}
set first_item = items[0]
set last_item = items[-1]
set length = items.length()
```

```tql
{
  items: ["first", "second", "third", "fourth"],
  first_item: "first",
  last_item: "fourth",
  length: 4
}
```

Index notation:

* `[0]` - First element (0-based indexing)
* `[-1]` - Last element (negative indices count from end)
* `.length()` - Get the number of elements

### Add elements to lists

[Section titled “Add elements to lists”](#add-elements-to-lists)

Use [`append()`](/reference/functions/append) and [`prepend()`](/reference/functions/prepend):

```tql
from {colors: ["red", "green"]}
set with_blue = colors.append("blue")
set with_yellow = with_blue.prepend("yellow")
set multi_append = colors.append("blue").append("purple")
```

```tql
{
  colors: ["red", "green"],
  with_blue: ["red", "green", "blue"],
  with_yellow: ["yellow", "red", "green", "blue"],
  multi_append: ["red", "green", "blue", "purple"]
}
```

### Combine lists

[Section titled “Combine lists”](#combine-lists)

Join multiple lists with [`concatenate()`](/reference/functions/concatenate) or spread syntax:

```tql
from {
  list1: [1, 2, 3],
  list2: [4, 5, 6],
  list3: [7, 8, 9]
}
set combined = concatenate(concatenate(list1, list2), list3)
set spread = [...list1, ...list2, ...list3]
set with_value = [...list1, 10, ...list2]
```

```tql
{
  list1: [1, 2, 3],
  list2: [4, 5, 6],
  list3: [7, 8, 9],
  combined: [1, 2, 3, 4, 5, 6, 7, 8, 9],
  spread: [1, 2, 3, 4, 5, 6, 7, 8, 9],
  with_value: [1, 2, 3, 10, 4, 5, 6]
}
```

### Transform list elements

[Section titled “Transform list elements”](#transform-list-elements)

Apply functions to each element with [`map()`](/reference/functions/map):

```tql
from {
  prices: [10, 20, 30],
  names: ["alice", "bob", "charlie"]
}
set with_tax = prices.map(p => p * 1.1)
set uppercase = names.map(n => n.to_upper())
set squared = prices.map(x => x * x)
```

```tql
{
  prices: [10, 20, 30],
  names: ["alice", "bob", "charlie"],
  with_tax: [11.0, 22.0, 33.0],
  uppercase: ["ALICE", "BOB", "CHARLIE"],
  squared: [100, 400, 900]
}
```

### Filter list elements

[Section titled “Filter list elements”](#filter-list-elements)

Keep only elements that match a condition with [`where()`](/reference/functions/where):

```tql
from {
  numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
  users: ["alice", "bob", "anna", "alex"]
}
// Note: The modulo operator (%) is not currently supported in TQL
// Here's an alternative approach for filtering:
set big_nums = numbers.where(n => n > 5)
set small_nums = numbers.where(n => n <= 5)
set a_names = users.where(u => u.starts_with("a"))
```

```tql
{
  numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
  users: ["alice", "bob", "anna", "alex"],
  big_nums: [6, 7, 8, 9, 10],
  small_nums: [1, 2, 3, 4, 5],
  a_names: ["alice", "anna", "alex"]
}
```

### Sort lists

[Section titled “Sort lists”](#sort-lists)

Order elements with [`sort()`](/reference/functions/sort):

```tql
from {
  numbers: [3, 1, 4, 1, 5, 9],
  words: ["zebra", "apple", "banana"]
}
set sorted_nums = numbers.sort()
set sorted_words = words.sort()
// Note: reverse() is only for strings, not lists
// To reverse a list, you would need to use the reverse operator in a pipeline
```

```tql
{
  numbers: [3, 1, 4, 1, 5, 9],
  words: ["zebra", "apple", "banana"],
  sorted_nums: [1, 1, 3, 4, 5, 9],
  sorted_words: ["apple", "banana", "zebra"]
}
```

### Get unique values

[Section titled “Get unique values”](#get-unique-values)

Remove duplicates with [`distinct()`](/reference/functions/distinct):

```tql
from {
  items: ["a", "b", "a", "c", "b", "d"],
  numbers: [1, 2, 2, 3, 3, 3, 4]
}
set unique_items = distinct(items)
set unique_nums = distinct(numbers)
```

```tql
{
  items: ["a", "b", "a", "c", "b", "d"],
  numbers: [1, 2, 2, 3, 3, 3, 4],
  unique_items: ["a", "b", "c", "d"],
  unique_nums: [1, 2, 3, 4]
}
```

### Flatten nested lists

[Section titled “Flatten nested lists”](#flatten-nested-lists)

Note: Direct list flattening is not currently supported in TQL. The [`flatten()`](/reference/functions/flatten) function is designed for flattening records, not lists. To work with nested lists, you would need to process them element by element.

## Work with records

[Section titled “Work with records”](#work-with-records)

Records (objects) contain key-value pairs. Here’s how to manipulate them.

### Access record fields

[Section titled “Access record fields”](#access-record-fields)

Get values using dot notation or brackets:

```tql
from {
  user: {
    name: "Alice",
    age: 30,
    address: {
      city: "NYC",
      zip: "10001"
    }
  }
}
set name = user.name
set city = user.address.city
set zip = user["address"]["zip"]
set has_email = user.has("email")
```

```tql
{
  user: {
    name: "Alice",
    age: 30,
    address: {city: "NYC", zip: "10001"}
  },
  name: "Alice",
  city: "NYC",
  zip: "10001",
  has_email: false
}
```

### Get keys and values

[Section titled “Get keys and values”](#get-keys-and-values)

Extract field names and values:

```tql
from {
  config: {
    host: "localhost",
    port: 8080,
    ssl: true
  }
}
set field_names = config.keys()
// Note: values() function is not available
set num_fields = config.keys().length()
```

```tql
{
  config: {host: "localhost", port: 8080, ssl: true},
  field_names: ["host", "port", "ssl"],
  num_fields: 3
}
```

### Merge records

[Section titled “Merge records”](#merge-records)

Combine multiple records with [`merge()`](/reference/functions/merge) or spread syntax:

```tql
from {
  defaults: {host: "localhost", port: 80, ssl: false},
  custom: {port: 8080, ssl: true}
}
set merged = merge(defaults, custom)
set spread = {...defaults, ...custom}
set with_extra = {...defaults, ...custom, debug: true}
```

```tql
{
  defaults: {host: "localhost", port: 80, ssl: false},
  custom: {port: 8080, ssl: true},
  merged: {host: "localhost", port: 8080, ssl: true},
  spread: {host: "localhost", port: 8080, ssl: true},
  with_extra: {host: "localhost", port: 8080, ssl: true, debug: true}
}
```

### Transform record values

[Section titled “Transform record values”](#transform-record-values)

Note: The `map_values` function is not available in TQL. To transform record values, you would need to reconstruct the record with transformed values manually:

```tql
from {
  prices: {
    apple: 1.50,
    banana: 0.75,
    orange: 2.00
  }
}
// Manual transformation example:
set with_tax = {
  apple: prices.apple * 1.1,
  banana: prices.banana * 1.1,
  orange: prices.orange * 1.1
}
```

### Filter record fields

[Section titled “Filter record fields”](#filter-record-fields)

Keep only specific fields:

```tql
from {
  user: {
    id: 123,
    name: "Alice",
    email: "alice@example.com",
    password: "secret",
    api_key: "xyz123"
  }
}
// Note: filter_keys and select functions are not available
// Manual field selection:
set public_info = {
  id: user.id,
  name: user.name,
  email: user.email
}
set contact = {
  name: user.name,
  email: user.email
}
```

```tql
{
  user: {
    id: 123,
    name: "Alice",
    email: "alice@example.com",
    password: "secret",
    api_key: "xyz123"
  },
  public_info: {
    id: 123,
    name: "Alice",
    email: "alice@example.com"
  },
  contact: {
    name: "Alice",
    email: "alice@example.com"
  }
}
```

## Combine lists and records

[Section titled “Combine lists and records”](#combine-lists-and-records)

Work with collections of records:

```tql
from {
  users: [
    {name: "Alice", age: 30, city: "NYC"},
    {name: "Bob", age: 25, city: "SF"},
    {name: "Charlie", age: 35, city: "NYC"}
  ]
}
set names = users.map(u => u.name)
set nyc_users = users.where(u => u.city == "NYC")
set avg_age = users.map(u => u.age).sum() / users.length()
```

```tql
{
  users: [
    {name: "Alice", age: 30, city: "NYC"},
    {name: "Bob", age: 25, city: "SF"},
    {name: "Charlie", age: 35, city: "NYC"}
  ],
  names: ["Alice", "Bob", "Charlie"],
  nyc_users: [
    {name: "Alice", age: 30, city: "NYC"},
    {name: "Charlie", age: 35, city: "NYC"}
  ],
  avg_age: 30.0
}
```

## Advanced transformations

[Section titled “Advanced transformations”](#advanced-transformations)

### Zip lists together

[Section titled “Zip lists together”](#zip-lists-together)

Combine parallel lists with [`zip()`](/reference/functions/zip):

```tql
from {
  names: ["Alice", "Bob", "Charlie"],
  ages: [30, 25, 35],
  cities: ["NYC", "SF", "LA"]
}
// Note: zip only takes 2 arguments, returns records with left/right fields
set name_age = zip(names, ages)
set zipped = zip(name_age, cities)
set users = zipped.map(z => {
  name: z.left.left,
  age: z.left.right,
  city: z.right
})
```

```tql
{
  names: ["Alice", "Bob", "Charlie"],
  ages: [30, 25, 35],
  cities: ["NYC", "SF", "LA"],
  name_age: [
    {left: "Alice", right: 30},
    {left: "Bob", right: 25},
    {left: "Charlie", right: 35}
  ],
  zipped: [
    {left: {left: "Alice", right: 30}, right: "NYC"},
    {left: {left: "Bob", right: 25}, right: "SF"},
    {left: {left: "Charlie", right: 35}, right: "LA"}
  ],
  users: [
    {name: "Alice", age: 30, city: "NYC"},
    {name: "Bob", age: 25, city: "SF"},
    {name: "Charlie", age: 35, city: "LA"}
  ]
}
```

### Practical example: Pairing related data

[Section titled “Practical example: Pairing related data”](#practical-example-pairing-related-data)

Combine parallel arrays with transformations for data normalization:

```tql
from {
  // DNS query data with parallel arrays
  answers: ["192.168.1.1", "10.0.0.1", "172.16.0.1"],
  ttls: [300s, 600s, 900s],
  // Certificate SAN names
  domains: ["example.com", "www.example.com", "api.example.com"]
}


// Pair DNS answers with their TTLs and transform
dns_records = zip(answers, ttls).map(x => {
  rdata: x.left,
  ttl_seconds: x.right.count_seconds(),
  cached_until: now() + x.right
})


// Transform simple arrays to structured data
san_entries = domains.map(name => {
  name: name,
  type: "DNSName",
  verified: name.ends_with(".example.com")
})
```

```tql
{
  answers: ["192.168.1.1", "10.0.0.1", "172.16.0.1"],
  ttls: [300s, 600s, 900s],
  domains: ["example.com", "www.example.com", "api.example.com"],
  dns_records: [
    {
      rdata: "192.168.1.1",
      ttl_seconds: 300,
      cached_until: 2025-08-14T12:36:45.123456Z
    },
    {
      rdata: "10.0.0.1",
      ttl_seconds: 600,
      cached_until: 2025-08-14T12:41:45.123456Z
    },
    {
      rdata: "172.16.0.1",
      ttl_seconds: 900,
      cached_until: 2025-08-14T12:46:45.123456Z
    }
  ],
  san_entries: [
    {
      name: "example.com",
      type: "DNSName",
      verified: true
    },
    {
      name: "www.example.com",
      type: "DNSName",
      verified: true
    },
    {
      name: "api.example.com",
      type: "DNSName",
      verified: true
    }
  ]
}
```

This pattern is particularly useful when:

* Converting parallel arrays from APIs or logs into structured records
* Normalizing data for standard formats (like OCSF)
* Adding computed fields during the transformation

### Enumerate with indices

[Section titled “Enumerate with indices”](#enumerate-with-indices)

Add row numbers to your data using the [`enumerate`](/reference/operators/enumerate) operator:

```tql
from {item: "apple"}, {item: "banana"}, {item: "cherry"}
enumerate row
```

```tql
{row: 0, item: "apple"}
{row: 1, item: "banana"}
{row: 2, item: "cherry"}
```

This is useful for tracking position in sequences or creating unique identifiers for each event.

## Practical examples

[Section titled “Practical examples”](#practical-examples)

### Extract and transform nested data

[Section titled “Extract and transform nested data”](#extract-and-transform-nested-data)

```tql
from {
  response: {
    status: 200,
    data: {
      users: [
        {id: 1, name: "Alice", scores: [85, 92, 88]},
        {id: 2, name: "Bob", scores: [78, 81, 85]}
      ]
    }
  }
}
set users = response.data.users
set summaries = users.map(u, {
  name: u.name,
  avg_score: u.scores.sum() / u.scores.length(),
  max_score: u.scores.max()
})
```

```tql
{
  response: {...},
  users: [
    {id: 1, name: "Alice", scores: [85, 92, 88]},
    {id: 2, name: "Bob", scores: [78, 81, 85]}
  ],
  summaries: [
    {name: "Alice", avg_score: 88.33333333333333, max_score: 92},
    {name: "Bob", avg_score: 81.33333333333333, max_score: 85}
  ]
}
```

### Work with indexed data

[Section titled “Work with indexed data”](#work-with-indexed-data)

Create lookups by extracting specific fields:

```tql
from {
  items: [
    {id: "A001", name: "Widget", price: 10},
    {id: "B002", name: "Gadget", price: 20},
    {id: "C003", name: "Tool", price: 15}
  ]
}
set first_item = items.first()
set ids = items.map(item => item.id)
set names = items.map(item => item.name)
set expensive = items.where(item => item.price > 15)
```

```tql
{
  items: [...],
  first_item: {id: "A001", name: "Widget", price: 10},
  ids: ["A001", "B002", "C003"],
  names: ["Widget", "Gadget", "Tool"],
  expensive: [
    {id: "B002", name: "Gadget", price: 20}
  ]
}
```

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Choose the right structure**: Use lists for ordered data, records for named fields
2. **Avoid deep nesting**: Flatten structures when possible for easier access
3. **Use functional methods**: Prefer `map()`, `filter()`, etc. over manual loops
4. **Handle empty collections**: Check length before accessing elements
5. **Preserve immutability**: Collection functions return new values, not modify existing

## Related guides

[Section titled “Related guides”](#related-guides)

* [Filter and select data](/guides/data-shaping/filter-and-select-data) - Filter entire event streams
* [Transform basic values](/guides/data-shaping/transform-basic-values) - Work with simple types
* [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations

# Work with time

Time is fundamental in data analysis. Whether you’re analyzing logs, tracking events, or monitoring systems, you need to parse timestamps, calculate durations, and format dates. This guide shows you how to work with time values in TQL.

## Understand time types

[Section titled “Understand time types”](#understand-time-types)

TQL has two main time-related types:

* **`time`**: A specific point in time (timestamp)
* **`duration`**: A span of time (interval)

```tql
from {
  timestamp: 2024-01-15T10:30:45.123456,
  interval: 5min
}
later = timestamp + interval
earlier = timestamp - 2h
```

```tql
{
  timestamp: 2024-01-15T10:30:45.123456Z,
  interval: 5min,
  later: 2024-01-15T10:35:45.123456Z,
  earlier: 2024-01-15T08:30:45.123456Z
}
```

## Get the current time

[Section titled “Get the current time”](#get-the-current-time)

Use [`now()`](/reference/functions/now) to get the current timestamp:

```tql
from {
  current_time: now()
}
today = current_time.round(1d)
```

```tql
{
  current_time: 2025-07-21T19:06:55.047259Z,
  today: 2025-07-22T00:00:00Z,
}
```

## Parse time from strings

[Section titled “Parse time from strings”](#parse-time-from-strings)

Convert string representations to proper timestamps with [`parse_time()`](/reference/functions/parse_time):

```tql
from {
  iso: "2024-01-15T10:30:45",
  custom: "15/Jan/2024:10:30:45",
  unix: "1705316445"
}
iso_time = iso.parse_time("%Y-%m-%dT%H:%M:%S")
custom_time = custom.parse_time("%d/%b/%Y:%H:%M:%S")
unix_time = unix.int().seconds().from_epoch()
```

```tql
{
  iso: "2024-01-15T10:30:45",
  custom: "15/Jan/2024:10:30:45",
  unix: "1705316445",
  iso_time: 2024-01-15T10:30:45Z,
  custom_time: 2024-01-15T10:30:45Z,
  unix_time: 2024-01-15T11:00:45Z
}
```

Common format specifiers:

* `%Y` - 4-digit year
* `%m` - Month (01-12)
* `%d` - Day (01-31)
* `%H` - Hour (00-23)
* `%M` - Minute (00-59)
* `%S` - Second (00-59)
* `%b` - Month name (Jan, Feb, etc.)
* `%a` - Weekday name (Mon, Tue, etc.)

## Format time to strings

[Section titled “Format time to strings”](#format-time-to-strings)

Convert timestamps to custom string formats with [`format_time()`](/reference/functions/format_time):

```tql
from {event_time: 2024-01-15T10:30:45.123456}
iso = event_time.format_time("%Y-%m-%dT%H:%M:%S.%f")
date_only = event_time.format_time("%Y-%m-%d")
us_format = event_time.format_time("%m/%d/%Y %I:%M %p")
log_format = event_time.format_time("%d/%b/%Y:%H:%M:%S")
```

```tql
{
  event_time: 2024-01-15T10:30:45.123456Z,
  iso: "2024-01-15T10:30:45.123456000.%f",
  date_only: "2024-01-15",
  us_format: "01/15/2024 10:30 AM",
  log_format: "15/Jan/2024:10:30:45.123456000"
}
```

## Extract time components

[Section titled “Extract time components”](#extract-time-components)

Get individual parts of a timestamp using [`year()`](/reference/functions/year), [`month()`](/reference/functions/month), [`day()`](/reference/functions/day), [`hour()`](/reference/functions/hour), [`minute()`](/reference/functions/minute), and [`second()`](/reference/functions/second):

```tql
from {timestamp: 2024-01-15T10:30:45.123456}
year = timestamp.year()
month = timestamp.month()
day = timestamp.day()
hour = timestamp.hour()
minute = timestamp.minute()
second = timestamp.second()
```

```tql
{
  timestamp: 2024-01-15T10:30:45.123456Z,
  year: 2024,
  month: 1,
  day: 15,
  hour: 10,
  minute: 30,
  second: 45.123456
}
```

## Work with durations

[Section titled “Work with durations”](#work-with-durations)

Create and manipulate time intervals:

```tql
from {
  start: 2024-01-15T10:00:00,
  end: 2024-01-15T14:30:00
}
elapsed = end - start
hours = elapsed.count_hours()
minutes = elapsed.count_minutes()
seconds = elapsed.count_seconds()
```

```tql
{
  start: 2024-01-15T10:00:00Z,
  end: 2024-01-15T14:30:00Z,
  elapsed: 4.5h,
  hours: 4.5,
  minutes: 270.0,
  seconds: 16200.0
}
```

### Count duration components

[Section titled “Count duration components”](#count-duration-components)

Extract different time units from durations:

```tql
from {
  duration: 90d + 4h + 30min + 45s + 123ms + 456us + 789ns
}
years = duration.count_years()
months = duration.count_months()
weeks = duration.count_weeks()
days = duration.count_days()
hours = duration.count_hours()
minutes = duration.count_minutes()
seconds = duration.count_seconds()
milliseconds = duration.count_milliseconds()
microseconds = duration.count_microseconds()
nanoseconds = duration.count_nanoseconds()
```

```tql
{
  duration: 90.18802226223136d,
  years: 0.24692641809819874,
  months: 2.963117017178385,
  weeks: 12.884003180318764,
  days: 90.18802226223136,
  hours: 2164.5125342935526,
  minutes: 129870.75205761315,
  seconds: 7792245.123456789,
  milliseconds: 7792245123.456789,
  microseconds: 7792245123456.789,
  nanoseconds: 7792245123456789,
}
```

### Convert between time units

[Section titled “Convert between time units”](#convert-between-time-units)

Use [`months()`](/reference/functions/months) to create month-based durations:

```tql
from {
  quarterly_period: 3
}
quarter = quarterly_period.months()
days_in_quarter = quarter.count_days()
weeks_in_quarter = quarter.count_weeks()
```

```tql
{
  quarterly_period: 3,
  quarter: 91.310625d,
  days_in_quarter: 91.310625,
  weeks_in_quarter: 13.044375
}
```

### Create durations

[Section titled “Create durations”](#create-durations)

Use duration literals or functions:

```tql
from {
  five_minutes: 5min,
  one_hour: 1h,
  custom: 90.seconds(),
  from_parts: 2.hours() + 30.minutes(),
}
```

```tql
{
  five_minutes: 5min,
  one_hour: 1h,
  custom: 1.5min,
  from_parts: 2.5h,
}
```

Duration units:

* `ns` or `nanoseconds()` - Nanoseconds
* `us` or `microseconds()` - Microseconds
* `ms` or `milliseconds()` - Milliseconds
* `s` or `seconds()` - Seconds
* `min` or `minutes()` - Minutes
* `h` or `hours()` - Hours
* `d` or `days()` - Days (24 hours)
* `w` or `weeks()` - Weeks (7 days)
* `y` or `years()` - Years (365 days)

## Calculate time differences

[Section titled “Calculate time differences”](#calculate-time-differences)

Find elapsed time between events:

```tql
from {
  login: 2024-01-15T09:00:00,
  first_action: 2024-01-15T09:05:30,
  logout: 2024-01-15T17:30:00,
}
time_to_action = first_action - login
session_duration = logout - login
active_hours = session_duration.count_hours()
```

```tql
{
  login: 2024-01-15T09:00:00Z,
  first_action: 2024-01-15T09:05:30Z,
  logout: 2024-01-15T17:30:00Z,
  time_to_action: 5.5min,
  session_duration: 8.5h,
  active_hours: 8.5,
}
```

## Add and subtract time

[Section titled “Add and subtract time”](#add-and-subtract-time)

Perform time arithmetic:

```tql
from {
  event_time: 2024-01-15T10:30:00
}
one_hour_later = event_time + 1h
yesterday = event_time - 1d
next_week = event_time + 7d
thirty_mins_ago = event_time - 30min
```

```tql
{
  event_time: 2024-01-15T10:30:00Z,
  one_hour_later: 2024-01-15T11:30:00Z,
  yesterday: 2024-01-14T10:30:00Z,
  next_week: 2024-01-22T10:30:00Z,
  thirty_mins_ago: 2024-01-15T10:00:00Z
}
```

## Round timestamps

[Section titled “Round timestamps”](#round-timestamps)

Round timestamps to specific intervals:

```tql
from {
  precise_time: 2024-01-15T10:37:42.847621
}
to_minute = precise_time.round(1min)
to_hour = precise_time.round(1h)
to_day = precise_time.round(1d)
to_5min = precise_time.round(5min)
```

```tql
{
  precise_time: 2024-01-15T10:37:42.847621Z,
  to_minute: 2024-01-15T10:38:00Z,
  to_hour: 2024-01-15T11:00:00Z,
  to_day: 2024-01-15T00:00:00Z,
  to_5min: 2024-01-15T10:40:00Z
}
```

## Convert Unix timestamps

[Section titled “Convert Unix timestamps”](#convert-unix-timestamps)

Work with Unix epoch timestamps using [`from_epoch()`](/reference/functions/from_epoch) and [`since_epoch()`](/reference/functions/since_epoch):

```tql
from {
  unix_seconds: 1705316445,
  unix_millis: 1705316445123,
  unix_micros: 1705316445123456
}
from_seconds = unix_seconds.seconds().from_epoch()
from_millis = unix_millis.milliseconds().from_epoch()
from_micros = unix_micros.microseconds().from_epoch()
back_to_unix = from_seconds.since_epoch().count_seconds()
```

```tql
{
  unix_seconds: 1705316445,
  unix_millis: 1705316445123,
  unix_micros: 1705316445123456,
  from_seconds: 2024-01-15T11:00:45Z,
  from_millis: 2024-01-15T11:00:45.123Z,
  from_micros: 2024-01-15T11:00:45.123456Z,
  back_to_unix: 1705316445.0
}
```

## Practical examples

[Section titled “Practical examples”](#practical-examples)

### Calculate request duration

[Section titled “Calculate request duration”](#calculate-request-duration)

```tql
from {
  request_start: 2024-01-15T10:30:45.123,
  request_end: 2024-01-15T10:30:47.456,
}
duration = request_end - request_start
duration_ms = duration.count_milliseconds()
```

```tql
{
  request_start: 2024-01-15T10:30:45.123Z,
  request_end: 2024-01-15T10:30:47.456Z,
  duration: 2.333s,
  duration_ms: 2333.0,
}
```

### Group events by time window

[Section titled “Group events by time window”](#group-events-by-time-window)

```tql
from {
  event_time: 2024-01-15T10:37:42.847621,
  event_type: "login",
}
hour_bucket = event_time.round(1h)
day_bucket = event_time.round(1d)
five_min_bucket = event_time.round(5min)
```

```tql
{
  event_time: 2024-01-15T10:37:42.847621Z,
  event_type: "login",
  hour_bucket: 2024-01-15T11:00:00Z,
  day_bucket: 2024-01-15T00:00:00Z,
  five_min_bucket: 2024-01-15T10:40:00Z,
}
```

### Calculate age from timestamp

[Section titled “Calculate age from timestamp”](#calculate-age-from-timestamp)

```tql
from {
  created_at: 2024-01-01T00:00:00
}
age = now() - created_at
days_old = age.count_days()
hours_old = age.count_hours()
human_readable = f"{days_old.round()} days ago"
```

```tql
{
  created_at: 2024-01-01T00:00:00Z,
  age: 567.7989434994097d,
  days_old: 567.7989434994097,
  hours_old: 13627.174643985833,
  human_readable: "568 days ago",
}
```

### Parse various log timestamps

[Section titled “Parse various log timestamps”](#parse-various-log-timestamps)

```tql
from {
  apache: "15/Jan/2024:10:30:45 +0000",
  nginx: "2024/01/15 10:30:45",
  syslog: "Jan 15 10:30:45"
}
apache_time = apache.parse_time("%d/%b/%Y:%H:%M:%S %z")
nginx_time = nginx.parse_time("%Y/%m/%d %H:%M:%S")
// For syslog, we need to add the year
syslog_time = ("2024 " + syslog).parse_time("%Y %b %d %H:%M:%S")
```

```tql
{
  apache: "15/Jan/2024:10:30:45 +0000",
  nginx: "2024/01/15 10:30:45",
  syslog: "Jan 15 10:30:45",
  apache_time: 2024-01-15T10:30:45Z,
  nginx_time: 2024-01-15T10:30:45Z,
  syslog_time: 2024-01-15T10:30:45Z,
}
```

## Replay and adjust time series

[Section titled “Replay and adjust time series”](#replay-and-adjust-time-series)

When working with historical data, you often need to replay events with their original timing or adjust timestamps for analysis. TQL provides two operators for this: [`delay`](/reference/operators/delay) and [`timeshift`](/reference/operators/timeshift).

### Adjust timestamps with timeshift

[Section titled “Adjust timestamps with timeshift”](#adjust-timestamps-with-timeshift)

The `timeshift` operator adjusts timestamps to a new baseline while preserving relative time differences. This is essential when you need to merge datasets from different time periods into a coherent timeline for comparative analysis. For example, you might want to overlay security incidents from multiple years to identify recurring patterns, or align test data from different runs to compare performance metrics side-by-side.

```tql
from {event: "login", ts: 2020-06-15T09:00:00},
     {event: "action", ts: 2020-06-15T09:05:30},
     {event: "logout", ts: 2020-06-15T17:30:00}
timeshift ts, start=2024-01-01
```

```tql
{event: "login", ts: 2024-01-01T00:00:00Z}
{event: "action", ts: 2024-01-01T00:05:30Z}
{event: "logout", ts: 2024-01-01T08:30:00Z}
```

Notice how the 5.5-minute gap between login and action, and the 8.5-hour session duration are preserved, but all timestamps now start from January 1, 2024.

You can also scale the time intervals with the `speed` parameter:

```tql
from {event: "start", ts: 2020-01-01T00:00:00},
     {event: "middle", ts: 2020-01-01T00:30:00},
     {event: "end", ts: 2020-01-01T01:00:00}
// Make intervals 10x longer
timeshift ts, start=2024-01-01, speed=0.1
```

```tql
{event: "start", ts: 2024-01-01T00:00:00Z}
{event: "middle", ts: 2024-01-01T05:00:00Z}
{event: "end", ts: 2024-01-01T10:00:00Z}
```

The 30-minute intervals became 5-hour intervals (10x longer with speed=0.1).

### Replay events in real time with delay

[Section titled “Replay events in real time with delay”](#replay-events-in-real-time-with-delay)

The `delay` operator replays events according to their timestamps by introducing sleep periods between events:

```tql
from {ts: 2024-01-01T00:00:00, msg: "first"},
     {ts: 2024-01-01T00:00:02, msg: "second"},
     {ts: 2024-01-01T00:00:03, msg: "third"}
delay ts, start=now(), speed=0.1
```

With `speed=0.1`, the 2-second gap between first and second events becomes 20 seconds, and the 1-second gap between second and third becomes 10 seconds. This slower replay makes it easy to observe the delay in action.

For replaying historical data with original timing:

```tql
let $zeek_logs = "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst"
from_http $zeek_logs {
  decompress_zstd
  read_zeek_tsv
}
delay ts
```

This replays the logs with real-world inter-arrival times. If an event occurred at 10:00:00 and the next at 10:00:05, the operator waits 5 seconds between emitting them.

### Combine with periodic generation

[Section titled “Combine with periodic generation”](#combine-with-periodic-generation)

Use [`every`](/reference/operators/every) to generate events periodically, then replay them with modified timing:

```tql
every 1s {
  from {
    ts: now(),
    message: "Periodic event"
  }
}
head 5
delay ts, speed=0.5
```

This generates events every second but replays them at half speed (2 seconds between events).

### Practical example: Simulate real-time monitoring

[Section titled “Practical example: Simulate real-time monitoring”](#practical-example-simulate-real-time-monitoring)

Combine both operators to replay historical logs as if they’re happening now:

```tql
// Load historical logs
from_file "/path/to/historical-logs.json"
// Shift timestamps to current time
timeshift timestamp, start=now()
// Replay at 5x speed to quickly review a day's worth of logs
delay timestamp, speed=5.0
// Continue with normal processing
// ...
```

### Key differences

[Section titled “Key differences”](#key-differences)

* **`timeshift`**: Instantly adjusts all timestamps without delays
* **`delay`**: Introduces real-time delays between events based on their timestamps

Use `timeshift` when you need to analyze historical data with updated timestamps. Use `delay` when you want to replay events with realistic timing, such as for testing real-time processing systems or simulating live data streams.

## Best practices

[Section titled “Best practices”](#best-practices)

1. **Use proper types**: Convert strings to time values early in your pipeline
2. **Be consistent**: Standardize timestamp formats across your data
3. **Consider timezones**: Be aware that TQL timestamps are timezone-aware
4. **Round appropriately**: Use rounding to group events into time buckets
5. **Handle null values**: Check for missing timestamps before calculations

## Related guides

[Section titled “Related guides”](#related-guides)

* [Transform basic values](/guides/data-shaping/transform-basic-values) - Convert between data types
* [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse timestamps from logs
* [Filter and select data](/guides/data-shaping/filter-and-select-data) - Filter by time ranges

# Build from source

Tenzir uses [CMake](https://cmake.org) as build system. Aside from a modern C++23 compiler, you need to ensure availability of the dependencies in the table below.

Deterministic Builds via Nix

We provide a Nix flake to setup an environment in which all dependencies are available. Run `nix develop` inside the main source directory. You can also delegate the entire build process to Nix by invoking `nix build`, but be aware that this method does not support incremental builds.

## Dependencies

[Section titled “Dependencies”](#dependencies)

Every [release](https://github.com/tenzir/tenzir/releases) of Tenzir includes an [SBOM](https://en.wikipedia.org/wiki/Software_bill_of_materials) in [SPDX](https://spdx.dev) format that lists all dependencies and their versions.

[Latest SBOM](https://github.com/tenzir/tenzir/releases/latest/download/tenzir.spdx.json)

| Required |                           Dependency                           |     Version    | Description                                                                                                                                                      |
| :------: | :------------------------------------------------------------: | :------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|     ✓    |                          C++ Compiler                          | C++23 required | Tenzir is tested to compile with GCC >= 14.0 and Clang >= 19.0.                                                                                                  |
|     ✓    |                   [CMake](https://cmake.org)                   |     >= 3.30    | Cross-platform tool for building, testing and packaging software.                                                                                                |
|     ✓    |    [CAF](https://github.com/actor-framework/actor-framework)   |    >= 1.1.0    | Implementation of the actor model in C++. (Bundled as submodule.)                                                                                                |
|     ✓    |               [OpenSSL](https://www.openssl.org)               |                | Utilities for secure networking and cryptography.                                                                                                                |
|     ✓    |      [FlatBuffers](https://google.github.io/flatbuffers/)      |    >= 2.0.8    | Memory-efficient cross-platform serialization library.                                                                                                           |
|     ✓    |                 [Boost](https://www.boost.org)                 |    >= 1.83.0   | Required as a general utility library.                                                                                                                           |
|     ✓    |            [Apache Arrow](https://arrow.apache.org)            |    >= 18.0.0   | Required for in-memory data representation. Must be built with Compute, Filesystem, S3, Zstd and Parquet enabled. For the `gcs` plugin, GCS needs to be enabled. |
|     ✓    |              [re2](https://github.com/google/re2)              |                | Required for regular expressione evaluation.                                                                                                                     |
|     ✓    |         [yaml-cpp](https://github.com/jbeder/yaml-cpp)         |    >= 0.6.2    | Required for reading YAML configuration files.                                                                                                                   |
|     ✓    |        [simdjson](https://github.com/simdjson/simdjson)        |    >= 4.0.0    | Required for high-performance JSON parsing. (Bundled as submodule.)                                                                                              |
|     ✓    |           [spdlog](https://github.com/gabime/spdlog)           |     >= 1.5     | Required for logging.                                                                                                                                            |
|     ✓    |                     [fmt](https://fmt.dev)                     |    >= 10.0.0   | Required for formatted text output.                                                                                                                              |
|     ✓    |          [xxHash](https://github.com/Cyan4973/xxHash)          |    >= 0.8.0    | Required for computing fast hash digests.                                                                                                                        |
|     ✓    |        [robin-map](https://github.com/Tessil/robin-map)        |    >= 0.6.3    | Fast hash map and hash set using robin hood hashing. (Bundled as subtree.)                                                                                       |
|     ✓    |     [fast\_float](https://github.com/FastFloat/fast_float)     |    >= 3.2.0    | Required for parsing floating point numbers. (Bundled as submodule.)                                                                                             |
|     ✓    | [libbacktrace](https://github.com/ianlancetaylor/libbacktrace) |     >= 1.0     | Required for generating stack traces. (Only on Linux.)                                                                                                           |
|     ✓    |     [libmaxminddb](https://github.com/maxmind/libmaxminddb)    |    >= 1.8.0    | Required for the `geoip` context.                                                                                                                                |
|     ✓    |        [reproc++](https://github.com/DaanDeMeyer/reproc)       |   >= v14.2.5   | Required for subprocess control.                                                                                                                                 |
|          |               [libpcap](https://www.tcpdump.org)               |                | Required for building the `pcap` plugin.                                                                                                                         |
|          |    [librdkafka](https://github.com/confluentinc/librdkafka)    |                | Required for building the `kafka` plugin.                                                                                                                        |
|          |      [http-parser](https://github.com/nodejs/http-parser)      |                | Required for building the `web` plugin.                                                                                                                          |
|          |           [cppzmq](https://github.com/zeromq/cppzmq)           |                | Required for building the `zmq` plugin.                                                                                                                          |
|          | [clickhouse-cpp](https://github.com/clickhouse/clickhouse-cpp) |   >= fbd7945   | Required for building the `clickhouse` plugin. (Bundled as submodule.)                                                                                           |
|          |             [pfs](https://github.com/dtrugman/pfs)             |                | Required for the `processes` and `sockets` operators on Linux.                                                                                                   |
|          |            [Protocol Buffers](https://protobuf.dev)            |    >= 1.4.1    | Required for building the `velociraptor` plugin.                                                                                                                 |
|          |                    [gRPC](https://grpci.io)                    |     >= 1.51    | Required for building the `velociraptor` plugin.                                                                                                                 |
|          |       [rabbitmq-c](https://github.com/alanxz/rabbitmq-c)       |                | Required for building the `rabbitmq` plugin.                                                                                                                     |
|          |              [yara](https://yara.readthedocs.io/)              |    >= 4.4.0    | Required for building the `yara` plugin.                                                                                                                         |
|          |               [poetry](https://python-poetry.org)              |                | Required for building the Python bindings.                                                                                                                       |
|          |                [Doxygen](http://www.doxygen.org)               |                | Required to build documentation for libtenzir.                                                                                                                   |
|          |             [Pandoc](https://github.com/jgm/pandoc)            |                | Required to build the manpage for Tenzir.                                                                                                                        |
|          |           [bash](https://www.gnu.org/software/bash/)           |    >= 4.0.0    | Required to run the integration tests.                                                                                                                           |
|          |            [bats](https://bats-core.readthedocs.io)            |    >= 1.8.0    | Required to run the integration tests.                                                                                                                           |
|          |              [uv](https://github.com/astral-sh/uv)             |    >= 0.7.2    | Required to run the python operator.                                                                                                                             |

The minimum specified versions reflect those versions that we use in CI and manual testing. Older versions may still work in select cases.

macOS

On macOS, we recommend using Clang from the Homebrew `llvm@20` package with the following settings:

```sh
export LLVM_PREFIX="$(brew --prefix llvm@20)"
export PATH="${LLVM_PREFIX}/bin:${PATH}"
export CC="${LLVM_PREFIX}/bin/clang"
export CXX="${LLVM_PREFIX}/bin/clang++"
export LDFLAGS="-Wl,-rpath,${LLVM_PREFIX} ${LDFLAGS}"
export CPPFLAGS="-isystem ${LLVM_PREFIX}/include ${CPPFLAGS}"
export CXXFLAGS="-isystem ${LLVM_PREFIX}/include/c++/v1 ${CXXFLAGS}"
```

Installing via CMake on macOS configures a [launchd](https://www.launchd.info) agent to `~/Library/LaunchAgents/com.tenzir.tenzir.plist`. Use `launchctl` to spawn a node at login:

```sh
# To unload the agent, replace 'load' with 'unload'.
launchctl load -w ~/Library/LaunchAgents/com.tenzir.tenzir.plist
```

## Compile

[Section titled “Compile”](#compile)

Building Tenzir involves the following steps:

Clone the repository recursively:

```sh
git clone https://github.com/tenzir/tenzir
cd tenzir
git submodule update --init --recursive -- libtenzir plugins
```

Configure the build with CMake. For faster builds, we recommend passing `-G Ninja` to `cmake`.

```sh
cmake -B build
# CMake defaults to a "Debug" build. When performance matters, use "Release"
cmake -B build -DCMAKE_BUILD_TYPE=Release
```

Optionally, you can use the CMake TUI to visually configure the build:

```sh
ccmake build
```

The source tree also contains a set of CMake presets that combine various configuration options into curated build flavors. You can list them with:

```sh
cmake --list-presets
```

Build the executable:

```sh
cmake --build build --target all
```

## Test

[Section titled “Test”](#test)

After you have built the executable, run the unit and integration tests to verify that your build works as expected.

### Run unit tests

[Section titled “Run unit tests”](#run-unit-tests)

Run component-level unit tests via CTest:

```sh
ctest --test-dir build
```

### Run integration tests

[Section titled “Run integration tests”](#run-integration-tests)

Run end-to-end integration tests via [tenzir-test](https://github.com/tenzir/tenzir-test):

```sh
cd test
uvx tenzir-test
```

## Install

[Section titled “Install”](#install)

Install Tenzir system-wide:

```sh
cmake --install build
```

If you prefer to install into a custom install prefix, install with `--prefix /path/to/install/prefix`.

To remove debug symbols from the installed binaries and libraries, pass `--strip`.

To install only files relevant for running Tenzir and not for plugin development pass `--component Runtime`.

## Clean

[Section titled “Clean”](#clean)

In case you want to make changes to your build environment, we recommend deleting the build tree entirely:

```sh
rm -rf build
```

This avoids subtle configuration glitches of transitive dependencies. For example, CMake doesn’t disable assertions when switching from a `Debug` to a `Release` build, but would do so when starting with a fresh build of type `Release`.

# Build the Docker image

Our [Tenzir Node Dockerfile](https://github.com/tenzir/tenzir/blob/main/Dockerfile) has two starting points: a *development* and *production* layer.

Before building the image, make sure to fetch all submodules:

```sh
git clone --recursive https://github.com/tenzir/tenzir
cd tenzir
git submodule update --init --recursive -- libtenzir plugins tenzir
```

## Build the production image

[Section titled “Build the production image”](#build-the-production-image)

The production image is optimized for size and security. This is the official `tenzir/tenzir` image. From the repository root, build it as follows:

```sh
docker build -t tenzir/tenzir .
```

## Build the development image

[Section titled “Build the development image”](#build-the-development-image)

The development image `tenzir/tenzir-dev` contains all build-time dependencies of Tenzir. It runs with a `root` user to allow for building custom images that build additional Tenzir plugins. By default, Tenzir loads all installed plugins in our images.

Build the development image by specifying it as `--target`:

```sh
docker build -t tenzir/tenzir-dev --target development .
```

# Write a node plugin

Implementing a new plugin requires the following steps:

1. [Setup the scaffolding](#setup-the-scaffolding)
2. [Choose a plugin type](#choose-a-plugin-type)
3. [Implement the plugin interface](#implement-the-plugin-interface)
4. [Process configuration options](#process-configuration-options)
5. [Compile the source code](#compile-the-source-code)
6. [Add unit and integration tests](#add-unit-and-integration-tests)
7. [Package it](#package-it)

Next, we’ll discuss each step in more detail.

## Setup the scaffolding

[Section titled “Setup the scaffolding”](#setup-the-scaffolding)

The scaffolding of a plugin includes the CMake glue that makes it possible to use as static or dynamic plugin.

Pass `-DTENZIR_ENABLE_STATIC_PLUGINS:BOOL=ON` to `cmake` to build plugins alongside Tenzir as static plugins. This option is always on for static binary builds.

Tenzir ships with many plugins that showcase what a typical scaffold looks like. Have a look at the the [plugins](https://github.com/tenzir/tenzir/tree/main/plugins) directory, and an [example `CMakeLists.txt` file from the AMQP plugin](https://github.com/tenzir/tenzir/blob/main/plugins/amqp/CMakeLists.txt).

We highly urge calling the provided `TenzirRegisterPlugin` CMake in your plugin’s `CMakeLists.txt` file instead of handrolling your CMake build scaffolding code. This ensures that your plugin always uses the recommended defaults. Non-static installations of Tenzir contain the `TenzirRegisterPlugin.cmake` modules.

The typical structure of a plugin directory includes the following files/directories:

* `README.md`: An overview of the plugin and how to use it.

* `CHANGELOG.md`: A trail of user-facing changes.

* `schema/`: new schemas that ship with this plugin.

* `<plugin>.yaml.example`: the configuration knobs of the plugin. We comment out all options by default so that the file serves as reference. Users can uncomment specific settings they would like to adapt.

  The CMake build scaffolding installs all of the above files/directories, if present.

## Choose a plugin type

[Section titled “Choose a plugin type”](#choose-a-plugin-type)

Tenzir offers [a variety of customization points](/explanations/architecture), each of which defines its own API by inheriting from the plugin base class `tenzir::plugin`. When writing a new plugin, you can choose a subset of available types by inheriting from the respective plugin classes.

Dreaded Diamond

To avoid common issues with multiple inheritance, all intermediate plugin classes that inherit from `tenzir::plugin` use *virtual inheritance* to avoid issues with the [dreaded diamond](https://isocpp.org/wiki/faq/multiple-inheritance#mi-diamond).

## Implement the plugin interface

[Section titled “Implement the plugin interface”](#implement-the-plugin-interface)

After having the necessary CMake in place, you can now derive from one or more plugin base classes to define your own plugin. Based on the chosen plugin types, you must override one or more virtual functions with an implementation of your own.

The basic anatomy of a plugin class looks as follows:

```cpp
class example_plugin final : public virtual component_plugin,
                             public virtual command_plugin {
public:
  /// Loading logic.
  example_plugin();


  /// Teardown logic.
  ~example_plugin() override;


  /// Initializes a plugin with its respective entries from the YAML config
  /// file, i.e., `plugin.<NAME>`.
  /// @param plugin_config The relevant subsection of the configuration.
  /// @param global_config The entire Tenzir configuration for potential access
  /// to global options.
  caf::error initialize(const record& plugin_config,
                        const record& global_config) override;


  /// Returns the unique name of the plugin.
  std::string name() const override;


  // TODO: override pure virtual functions from the base classes.
  // ...
};
```

The plugin constructor should only perform minimal actions to instantiate a well-defined plugin instance. In particular, it should not throw or perform any operations that may potentially fail. For the actual plugin ramp up, please use the `initialize` function that processes the user configuration. The purpose of the destructor is to free any used resources owned by the plugin.

Each plugin must have a unique name. This returned string should consicely identify the plugin internally.

Please consult the documentation specific to each plugin type above to figure out what virtual function need overriding. In the above example, we have a `command_plugin` and a `component_plugin`. This requires implementing the following two interfaces:

```cpp
component_plugin_actor make_component(
  node_actor::stateful_pointer<node_state> node) const override;


std::pair<std::unique_ptr<command>, command::factory>
make_command() const override;
```

After completing the implementation, you must now register the plugin. For example, to register the `example` plugin, include the following line after the plugin class definition:

```cpp
// This line must not be in a namespace.
TENZIR_REGISTER_PLUGIN(tenzir::plugins::example_plugin)
```

Registering Type IDs

The example plugin also shows how to register additional type IDs with the actor system configuration, which is a requirement for sending custom types from the plugin between actors. For more information, please refer to the CAF documentation page [Configuring Actor Applications: Adding Custom Message Types](https://actor-framework.readthedocs.io/en/stable/ConfiguringActorApplications.html#adding-custom-message-types).

## Process configuration options

[Section titled “Process configuration options”](#process-configuration-options)

To configure a plugin at runtime, Tenzir first looks whether the YAML configuration contains a key with the plugin name under the top-level key `plugins`. Consider our example plugin with the name `example`:

```yaml
plugins:
  example:
    option: 42
```

Here, the plugin receives the record `{option: 42}` at load time. A plugin can process the configuration snippet by overriding the following function of `tenzir::plugin`:

```plaintext
caf::error initialize(const record& plugin_config,
                      const record& global_config) override;
```

Tenzir expects the plugin to be fully operational after calling `initialize`. Subsequent calls to the implemented customization points must have a well-defined behavior.

## Compile the source code

[Section titled “Compile the source code”](#compile-the-source-code)

### Building alongside Tenzir

[Section titled “Building alongside Tenzir”](#building-alongside-tenzir)

When configuring the Tenzir build, you need to tell CMake the path to the plugin source directory. The CMake variable `TENZIR_PLUGINS` holds a comma-separated list of paths to plugin directories.

To test that Tenzir loads the plugin properly, you can use `tenzir --plugins=example version` and look into the `plugins`. A key-value pair with your plugin name and version should exist in the output.

Refer to the [plugin loading](/explanations/configuration) section of the documentation to find out how to explicitly de-/activate plugins.

### Building against an installed Tenzir

[Section titled “Building against an installed Tenzir”](#building-against-an-installed-tenzir)

It is also possible to build plugins against an installed Tenzir. The `TenzirRegisterPlugin` CMake function contains the required scaffolding to set up `test` and `bats` targets that mimic Tenzir’s targets. Here’s how you can use it:

```sh
# Configure the build. Requires Tenzir to be installed in the CMake Module Path.
cmake -S path/to/plugin -B build
# Optionally you can manually specify a non-standard Tenzir install root:
#   TENZIR_DIR=/opt/tenzir cmake -S path/to/plugin -B build
cmake --build build
# Run plugin-specific unit tests.
ctest --test-dir build
# Install to where Tenzir is also installed.
cmake --install build
# Optionally you can manually specify a non-standard Tenzir install root:
#   cmake --install build --prefix /opt/tenzir
# Run plugin-specific integration tests against the installed Tenzir.
cmake --build build --target bats
```

## Add unit and integration tests

[Section titled “Add unit and integration tests”](#add-unit-and-integration-tests)

Tenzir comes with unit and integration tests. So does a robust plugin implementation. We now look at how you can hook into the testing frameworks.

### Unit tests

[Section titled “Unit tests”](#unit-tests)

Every plugin ideally comes with unit tests. The `TenzirRegisterPlugin` CMake function takes an optional `TEST_SOURCES` argument that creates a test binary `<plugin>-test` with `<plugin>` being the plugin name. The test binary links against the `tenzir::test` target. ou can find the test binary in `bin` within your build directory.

To execute registered unit tests, you can also simply run the test binary `<plugin>-test`, where `<plugin>` is the name of your plugin. The build target `test` sequentially runs tests for all plugins and Tenzir itself.

### Integration tests

[Section titled “Integration tests”](#integration-tests)

Every plugin ideally comes with integration tests as well. Our convention is that integration tests reside in an `integration` subdirectory. If you add a file called `bats/*.bats`, Tenzir runs them alongside the regular integration tests.

Note that plugins may affect the overall behavior of Tenzir. Therefore we recommend to to run all integrations regularly by running the build target `bats`.

To execute plugin-specific integration tests only, run the build target `bats-<plugin>`, where `<plugin>` is the name of your plugin.

## Package it

[Section titled “Package it”](#package-it)

If you plan to publish your plugin, you may want to create a GitHub repository. Please let us know if you do so, we can then link to community plugins from the documentation.

Contribute Upstream

If you think your plugin provides key functionality beneficial to all Tenzir users, feel free to [submit a pull request](https://github.com/tenzir/tenzir/pulls) to the main repository. But please consider swinging by our [community chat](/discord) or starting a [GitHub Discussion](https://github.com/tenzir/tenzir/discussions) to ensure that your contribution becomes a fruitful addition. 🙏

# Export from a node

Exporting (or *querying*) data can be done by [running a pipeline](/guides/basic-usage/run-pipelines) that begins with the [`export`](/reference/operators/export) input operator. When managing a pipeline through the app or the API, all pipeline operators run within the node. When using the CLI, at least the `export` operator runs within the node.

![Export](/_astro/export-from-a-node.BCWt8NLC_19DKCs.svg)

Let’s bring back a sample of historical data we [imported in the previous section](/guides/edge-storage/import-into-a-node):

```tql
export
head
```

Think of `export` being the entire data at a node. As this can grow quickly, you may query only subsets of it, e.g., by filtering using [`where`](/reference/operators/where):

```tql
export
where orig_bytes < 1 KiB
```

Logically, this query would *first* export the entire historical data, and *then* begin filtering the data. But since Tenzir does *predicate pushdown*, the pipeline executor will analyze the query and push the expression in `where` with the predicate `orig_bytes < 1 KiB` “down” to the `export` operator. Tenzir’s storage engine then asks its catalog to identify the relevant subset of partitions that the query should execute on. This dramatically improves the query performance for selective workloads, such as point queries for single values or specific time ranges.

To figure out the shape of the data to query, you can [show available schemas](/reference/operators/schemas).

# Import into a node

Importing (or *ingesting*) data can be done by [running a pipeline](/guides/basic-usage/run-pipelines) that ends with the [`import`](/reference/operators/import) output operator. When managing a pipeline through the app or the API, all pipeline operators run within the node. When using the CLI, at least the `import` operator runs within the node.

![Import](/_astro/import-into-a-node.B6oNZwnu_19DKCs.svg)

Consider this example that takes a Zeek conn.log from our M57 dataset:

```tql
load_file "Zeek/conn.log"
read_zeek_tsv
select id.orig_h, id.resp_h, orig_bytes, resp_bytes
where orig_bytes > 1 Mi
import
```

The [`import`](/reference/operators/import) operator requires a running node. To run the above pipeline successfully, you need to first [setup a node](/guides/node-setup/provision-a-node).

# Show available schemas

When you write a pipeline, you often reference field names. If you do not know the shape of your data, you can look up available schemas, i.e., the record types describing top-level events.

Many SQL databases have a `SHOW TABLES` command to show all available table names, and `SHOW COLUMNS` to display the individual fiels of a given table.

In Tenzir, the [`fields`](/reference/operators/fields) operator offers the ability for detailed schema introspection. Use it to display all schema fields; each event represents a single field.

```tql
fields
head
```

```json
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "ts", "path": ["ts"], "index": [0], "type": {"kind": "time", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "uid", "path": ["uid"], "index": [1], "type": {"kind": "string", "category": "atomic", "lists": 0, "name": "", "attributes": [{"key": "index", "value": "hash"}]}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "orig_h", "path": ["id", "orig_h"], "index": [2, 0], "type": {"kind": "ip", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "orig_p", "path": ["id", "orig_p"], "index": [2, 1], "type": {"kind": "uint64", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "resp_h", "path": ["id", "resp_h"], "index": [2, 2], "type": {"kind": "ip", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "resp_p", "path": ["id", "resp_p"], "index": [2, 3], "type": {"kind": "uint64", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "proto", "path": ["proto"], "index": [3], "type": {"kind": "string", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "trans_id", "path": ["trans_id"], "index": [4], "type": {"kind": "uint64", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "rtt", "path": ["rtt"], "index": [5], "type": {"kind": "duration", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
{"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "query", "path": ["query"], "index": [6], "type": {"kind": "string", "category": "atomic", "lists": 0, "name": "", "attributes": []}}
```

# Transform data at rest

Currently CLI only

This feature is currently only available on the command line using the `tenzir-ctl` binary. We’re working on bringing it back as an operator so that you can also use it from the app.

Tenzir provdides several features to transform historical data at a node.

## Delete old data when reaching storage quota

[Section titled “Delete old data when reaching storage quota”](#delete-old-data-when-reaching-storage-quota)

The disk-monitoring feature enables periodic deletion of events based on utilized disk storage. To limit the disk space used by a node, configure a disk quota:

```sh
tenzir-node --disk-quota-high=1TiB
```

Whenever a node detects that its database has exceeded the configured quota, it will erase the oldest data. You can specify a corridor for the disk space usage by additionally providing the option `--disk-quota-low`. This can be used to avoid running permanently at the upper limit and to instad batch the deletion operations together.

The full set of available options looks like this:

```yaml
tenzir:
  start:
    # Triggers removal of old data when the DB dir exceeds the disk budget.
    disk-budget-high: 0K
    # When the DB dir exceeds the budget, Tenzir erases data until the directory
    # size falls below this value.
    disk-budget-low: 0K
    # Seconds between successive disk space checks.
    disk-budget-check-interval: 90
```

Note

When using this method, we recommend placing the log file outside of the database directory. It counts towards the size calculations, but cannot be automatically deleted during a deletion cycle.

## Transform old data when reaching storage quota

[Section titled “Transform old data when reaching storage quota”](#transform-old-data-when-reaching-storage-quota)

Instead of deleting data periodically, a node can also trigger **spatial compaction** when exceeding a given disk budget. A spatial compaction cycle transforms data until disk usage falls below the budget, e.g., by removing columns or rows from certain events, or by deleting them entirely.

When the disk budget exceeds the configured threshold, the node decides what data to compact. The compaction *mode* defines how this happens. Currently, there exists only one mode: weighted age.

To compute the weighted age, the node divides the actual age of an event with the weight assigned to this event type. For example, applying a weight of 100 to an event that is 100 days old would yield a weighted age of 1 day. This causes it to be transformed after events that are 50 days old. Conversely, a weights less than one results in an older weighted age, resulting in earlier consideration in a compaction cycle.

The default weight is 1 for all event types. Here is an example configuration that adjusts the weights:

```yaml
tenzir:
  plugins: [compaction]
plugins:
  compaction:
    space:
      mode: weighted-age
      interval: 6 hours
      disk-budget-high: 10TiB
      disk-budget-low: 8TiB
      weights:
        - weight: 0.1
          types: [suricata.flow]
          #pipeline: …
        - weight: 100
          types: [suricata.alert]
          #pipeline: …
```

The `pipeline` key for each type is optional. If present, the corresponding pipeline processes all matching events. If absent, the nodes deletes matching events.

Two additional keys are useful to fine-tune the behavior of the compaction plugin:

1. `compaction.space.scan-binary`: an absolute path to a binary that should be executed to determine the current disk usage
2. `compaction.space.step-size`: adjust how many compaction candidates should be processed before re-checking the size of the database directory

## Transform data after exceeding a retention span

[Section titled “Transform data after exceeding a retention span”](#transform-data-after-exceeding-a-retention-span)

A node triggers **temporal compaction** according to a set of rules that define how to transform events after they reach a specfic age. This declarative specification makes it easy to express fine-grained data retention policies, which is often needed for regulatory requirements and compliance.

For each compaction cycle, the node processes all rules and identifies what subset of the data has become subject to transformation. To this end, each rule defines a *minimum* age, i.e., a lower bound that must be exceeded before the corresponding events undergo their configured pipeline.

To configure temporal compaction, provide a list of compaction rules under the key `plugins.compaction.time` in the configuration. A compaction rule defines the minimum age using key `after`, the pipeline to apply with the key `pipeline`, the scope in terms of schema using the key `types`, and a name to uniquely refer to the rule. Omitting the `types` key causes temporal compaction rules to be applied to all schemas.

By default, a compaction rule consumes its input, i.e., it erases the original events from the database and replaces them with the transformed events. The `preserve-input` option can be specified on a temporal compaction rule to override this behavior and to keep the input partitions available.

Note

A node applies each rule only once per partition and stores the applied rule name within the partition meta data. If you rename a rule in the configuration and reload a new compaction configuration, already compacted partitions will undergo another round of compaction.

The pipelines referenced in the compaction configuration must be defined in your configuration.

```yaml
plugins:
  compaction:
    time:
      # How often to check the `after` condition below.
      interval: 1 day
      rules:
        - after: 2 days
          name: uri_scrubbing
          pipeline: |
            replace net.url="xxx"
          types:
            - zeek.http
            - suricata.http
        - after: 7 days
          name: flow_reduction
          pipeline: |
            summarize
              pkts_toserver=sum(flow.pkts_toserver),
              pkts_toclient=sum(flow.pkts_toclient),
              bytes_toserver=sum(flow.bytes_toserver),
              bytes_toclient=sum(flow.bytes_toclient),
              start=min(flow.start),
              end=max(flow.end)
            by
              timestamp,
              src_ip,
              dest_ip
            resolution
              10 mins
          preserve-input: true
          types:
            - suricata.flow
```

## Trigger a compaction cycle manually

[Section titled “Trigger a compaction cycle manually”](#trigger-a-compaction-cycle-manually)

You can also interact with the compaction plugin on the command line, through the `compaction` subcommand. Use the `list` subcommand to show all configured compaction rules:

```sh
tenzir-ctl compaction list
```

You can then trigger a compaction manually via `run`:

```sh
tenzir-ctl compaction run <rule>
```

# Enrich with network inventory

Tenzir’s [enrichment framework](/explanations/enrichment) features *lookup tables* that you can use to enrich data in your pipelines. Lookup tables have a unique property that makes them attractive for tracking information associated with CIDR subnets: when you use `subnet` values as keys, you can probe the lookup table with `ip` values and will get a longest-prefix match.

To illustrate, consider this lookup table:

| Subnet      | Mapping  |
| ----------- | -------- |
| 10.0.0.0/22 | Machines |
| 10.0.0.0/24 | Servers  |
| 10.0.1.0/24 | Clients  |

When you have subnets as keys as above, you can query them with an IP address during enrichment. Say you want to enrich IP address `10.0.0.1`. Since the longest (bitwise) prefix match is `10.0.0.0/24`, you will get `Servers` as a result. The same goes for IP address `10.0.0.255`, but `10.0.1.1` will yield `Clients`. The IP address `10.0.2.1` yields Machines, since it is neither in `10.0.0.0/24` nor `10.0.1.0/24`, but `10.0.0.0/21`. The IP adress `10.0.4.1` won’t match at all, because it’s not any of the three subnets.

## Populate subnet mappings from a CSV file

[Section titled “Populate subnet mappings from a CSV file”](#populate-subnet-mappings-from-a-csv-file)

It’s common to have Excel sheets or exported CSV files of inventory data. Let’s consider this example:

inventory.csv

```csv
subnet,owner,function
10.0.0.0/22,John,machines
10.0.0.0/24,Derek,servers
10.0.1.0/24,Peter,clients
```

First, create the context:

```tql
context::create_lookup_table "subnets"
```

Then populate it:

```tql
from_file "inventory.csv" {
  read_csv
}
context::update "subnets", key=subnet
```

## Enrich IP addresses with the subnet table

[Section titled “Enrich IP addresses with the subnet table”](#enrich-ip-addresses-with-the-subnet-table)

Now that we have a lookup table with subnet keys, we can enrich any data containing IP addresses with it. For example, let’s consider this simplified Suricata flow record:

sample.json

```json
{
  "timestamp": "2021-11-17T13:32:43.237882",
  "src_ip": "10.0.0.1",
  "src_port": 54402,
  "dest_ip": "10.1.1.254",
  "dest_port": 53,
  "proto": "UDP",
  "event_type": "flow",
  "app_proto": "dns"
}
```

Let’s use the `enrich` operator to add the subnet context to the two IP address fields:

```tql
load_file "/tmp/sample.json"
read_json
context::enrich "subnets", key=src_ip, into=src_ip_context
context::enrich "subnets", key=dest_ip, into=dest_ip_context
```

```tql
{
  timestamp: 2021-11-17T13:32:43.237882,
  src_ip: 10.0.0.1,
  src_port: 54402,
  dest_ip: 10.1.1.254,
  dest_port: 53,
  proto: "UDP",
  event_type: "flow",
  app_proto: "dns",
  src_ip_context: {
    subnet: 10.0.0.0/24,
    owner: "Derek",
    function: "servers",
  },
  dest_ip_context: null
}
```

We have enriched all IP addresses in the flow event with the `subnets` context. Now go hunt down Derek!

# Enrich with threat intel

Tenzir has a powerful [enrichment framework](/explanations/enrichment) for real-time contextualization. The heart of the framework is a **context**—a stateful object that can be managed and used with pipelines.

## Setup a context

[Section titled “Setup a context”](#setup-a-context)

Prior to enriching, you need to populate a context with data. First, create a context called `threatfox` that uses a lookup table, i.e., a key-value mapping where a key is used to perform the lookup and the value can be any structured additional data.

```tql
context::create_lookup_table "threatfox"
```

After creating a context, we load data into the context. In our example, we load data from the [ThreatFox](https://threatfox.abuse.ch/) API:

```tql
from_http "https://threatfox-api.abuse.ch/api/v1/",
  body={query: "get_iocs", days: 1} {
  read_json
}
unroll data
where data.ioc_type == "domain"
context::update "threatfox", key=ioc, value=data
```

Example data for context updating

If we replace [`context::update`](/reference/operators/context/update) in the above pipeline with `head 5`, we get output similar to the following, depending on the current state of the API:

```tql
{
  id: "1213056",
  ioc: "deletefateoow.pw",
  threat_type: "botnet_cc",
  threat_type_desc: "Indicator that identifies a botnet command&control server (C&C)",
  ioc_type: "domain",
  ioc_type_desc: "Domain that is used for botnet Command&control (C&C)",
  malware: "win.lumma",
  malware_printable: "Lumma Stealer",
  malware_alias: "LummaC2 Stealer",
  malware_malpedia: "https://malpedia.caad.fkie.fraunhofer.de/details/win.lumma",
  confidence_level: 75,
  first_seen: "2023-12-15 15:31:00 UTC",
  last_seen: null,
  reference: "",
  reporter: "stoerchl",
  tags: ["LummaStealer"]
}
{
  id: "1213057",
  ioc: "perceivedomerusp.pw",
  threat_type: "botnet_cc",
  threat_type_desc: "Indicator that identifies a botnet command&control server (C&C)",
  ioc_type: "domain",
  ioc_type_desc: "Domain that is used for botnet Command&control (C&C)",
  malware: "win.lumma",
  malware_printable: "Lumma Stealer",
  malware_alias: "LummaC2 Stealer",
  malware_malpedia: "https://malpedia.caad.fkie.fraunhofer.de/details/win.lumma",
  confidence_level: 75,
  first_seen: "2023-12-15 15:31:00 UTC",
  last_seen: null,
  reference: "",
  reporter: "stoerchl",
  tags: ["LummaStealer"]
}
{
  id: "1213058",
  ioc: "showerreigerniop.pw",
  threat_type: "botnet_cc",
  threat_type_desc: "Indicator that identifies a botnet command&control server (C&C)",
  ioc_type: "domain",
  ioc_type_desc: "Domain that is used for botnet Command&control (C&C)",
  malware: "win.lumma",
  malware_printable: "Lumma Stealer",
  malware_alias: "LummaC2 Stealer",
  malware_malpedia: "https://malpedia.caad.fkie.fraunhofer.de/details/win.lumma",
  confidence_level: 75,
  first_seen: "2023-12-15 15:31:00 UTC",
  last_seen: null,
  reference: "",
  reporter: "stoerchl",
  tags: ["LummaStealer"]
}
{
  id: "1213059",
  ioc: "fortunedomerussea.pw",
  threat_type: "botnet_cc",
  threat_type_desc: "Indicator that identifies a botnet command&control server (C&C)",
  ioc_type: "domain",
  ioc_type_desc: "Domain that is used for botnet Command&control (C&C)",
  malware: "win.lumma",
  malware_printable: "Lumma Stealer",
  malware_alias: "LummaC2 Stealer",
  malware_malpedia: "https://malpedia.caad.fkie.fraunhofer.de/details/win.lumma",
  confidence_level: 75,
  first_seen: "2023-12-15 15:31:00 UTC",
  last_seen: null,
  reference: "",
  reporter: "stoerchl",
  tags: ["LummaStealer"]
}
{
  id: "1213060",
  ioc: "offerdelicateros.pw",
  threat_type: "botnet_cc",
  threat_type_desc: "Indicator that identifies a botnet command&control server (C&C)",
  ioc_type: "domain",
  ioc_type_desc: "Domain that is used for botnet Command&control (C&C)",
  malware: "win.lumma",
  malware_printable: "Lumma Stealer",
  malware_alias: "LummaC2 Stealer",
  malware_malpedia: "https://malpedia.caad.fkie.fraunhofer.de/details/win.lumma",
  confidence_level: 75,
  first_seen: "2023-12-15 15:31:00 UTC",
  last_seen: null,
  reference: "",
  reporter: "stoerchl",
  tags: ["LummaStealer"]
}
```

## Enrich with a context

[Section titled “Enrich with a context”](#enrich-with-a-context)

Now that we loaded IoCs into the context, we can enrich with it in other pipelines. Since we previously imported only domains, we look for fields in the data of that type.

The following pipeline subscribes to the import feed of all data arriving at the node via `export live=true` and applies the `threatfox` context to Suricata DNS requests in field `dns.rrname` via [`context::enrich`](/reference/operators/context/enrich):

```tql
export live=true
where @name == "suricata.dns"
context::enrich "threatfox", key=dns.rrname
```

Here is a sample of an event that the above pipeline yields:

```tql
{
  timestamp: 2021-11-17T16:57:42.389824,
  flow_id: 1542499730911936,
  pcap_cnt: 3167,
  vlan: null,
  in_iface: null,
  src_ip: 45.85.90.164,
  src_port: 56462,
  dest_ip: 198.71.247.91,
  dest_port: 53,
  proto: "UDP",
  event_type: "dns",
  community_id: null,
  dns: {
    version: null,
    type: "query",
    id: 1,
    flags: null,
    qr: null,
    rd: null,
    ra: null,
    aa: null,
    tc: null,
    rrname: "bza.fartit.com",
    rrtype: "RRSIG",
    rcode: null,
    ttl: null,
    tx_id: 0,
    grouped: null,
    answers: null
  },
  threatfox: {
    key: "bza.fartit.com",
    context: {
      id: "1209087",
      ioc: "bza.fartit.com",
      threat_type: "payload_delivery",
      threat_type_desc: "Indicator that identifies a malware distribution server (payload delivery)",
      ioc_type: "domain",
      ioc_type_desc: "Domain name that delivers a malware payload",
      malware: "apk.irata",
      malware_printable: "IRATA",
      malware_alias: null,
      malware_malpedia: "https://malpedia.caad.fkie.fraunhofer.de/details/apk.irata",
      confidence_level: 100,
      first_seen: "2023-12-03 14:05:20 UTC",
      last_seen: null,
      reference: "",
      reporter: "onecert_ir",
      tags: ["irata"]
    },
    timestamp: 2023-12-04T13:52:49.043157
  }
}
```

The sub-record `threatfox` holds the enrichment details. The field `key` contains the matching key. The field `context` is the row from the lookup table at key `bza.fartit.com`. The field `timestamp` is the time when the enrichment occurred.

# Execute Sigma rules

Tenzir supports executing [Sigma rules](https://github.com/SigmaHQ/sigma) using the [`sigma`](/reference/operators/sigma) operator. This allows you to run your Sigma rules in the pipeline. The operator transpiles the provided rules into an expression, and wraps matching events into a sighting record along with the matched rule.

Semantically, you can think of executing Sigma rules as applying the [`where`](/reference/operators/where) operator to the input. At a high level, the translation process looks as follows:

![Sigma Execution](/_astro/execute-sigma-rules.DU7cNKaT_19DKCs.svg)

pySigma Support

Unlike the legacy `sigmac` compiler that tailors a rule to specific backend, like Elastic or Splunk, the `sigma` operator only transpiles the structural YAML rules to produce an expression that is then used to filter a dataflow. In the future, we would like to write a native Tenzir backend for [pySigma](https://github.com/SigmaHQ/pySigma). Please reach out on our [Discord](/discord) if you would like to help us with that!

## Run a Sigma rule on an EVTX file

[Section titled “Run a Sigma rule on an EVTX file”](#run-a-sigma-rule-on-an-evtx-file)

You can run a Sigma rule on any pipeline input. For example, to apply a Sigma rule to an EVTX file, we can use the utility [`evtx_dump`](https://github.com/omerbenamram/evtx) to convert the binary EVTX format into JSON and then pipe it to `sigma` on the command line:

```bash
evtx_dump -o jsonl file.evtx | tenzir 'read_json | sigma "rule.yaml"'
```

# Work with lookup tables

A [lookup table](/explanations/enrichment#lookup-table) is a specific type of *context* in Tenzir’s [enrichment framework](/explanations/enrichment). It has “two ends” in that you can use pipelines to update it, as well as pipelines to perform lookups and attach the results to events. Lookup tables live in a node and multiple pipelines can safely use the same lookup table. All update operations propagate to disk, persisting the changes and making them resilient against node restarts.

Lookup tables are particularly powerful for:

* **Threat Intelligence**: Track indicators of compromise (IoCs) like malicious domains, IPs, or file hashes.
* **Asset Inventory**: Map IP addresses or subnets to organizational data.
* **Entity Tracking**: Build passive DNS tables or track user-to-host mappings.
* **Dynamic Enrichment**: Update context in real-time as your environment changes.

## Create a lookup table

[Section titled “Create a lookup table”](#create-a-lookup-table)

You can create a lookup table with the [`context::create_lookup_table`](/reference/operators/context/create_lookup_table) operator as a pipeline, or interactively in the platform.

### Create a lookup table from a pipeline

[Section titled “Create a lookup table from a pipeline”](#create-a-lookup-table-from-a-pipeline)

The [`context::create_lookup_table`](/reference/operators/context/create_lookup_table) operator creates a new, empty lookup table:

```tql
context::create_lookup_table "my_lookup_table"
```

### Create a lookup table as code

[Section titled “Create a lookup table as code”](#create-a-lookup-table-as-code)

You can also create a lookup table as code by adding it to `tenzir.contexts` in your `tenzir.yaml` configuration file:

\<prefix>/etc/tenzir/tenzir.yaml

```yaml
tenzir:
  contexts:
    my-lookup-table:
      type: lookup-table
```

This approach is useful for:

* **Infrastructure as Code**: Define lookup tables in version control
* **Automated Deployments**: Ensure lookup tables exist on node startup
* **Consistent Environments**: Replicate the same contexts across multiple nodes

### Create a lookup table in the platform

[Section titled “Create a lookup table in the platform”](#create-a-lookup-table-in-the-platform)

The following steps

1. In the **Contexts** tab in your node, click the `+` button:

   ![Create context](/_astro/context-add.CjA5yN5-_Z26lvns.webp)

2. Select type `lookup-table` and enter a name:

   ![Choose name and type](/_astro/context-name.BK7qez7U_ZY5Xxk.webp)

3. Click **Create** and observe the new lookup table context:

   ![View created context](/_astro/context-created.CLPG07O__1Umj0P.webp)

## Show all available lookup tables

[Section titled “Show all available lookup tables”](#show-all-available-lookup-tables)

Use the [`context::list`](/reference/operators/context/list) operator to retrieve all contexts in your node:

```tql
context::list
```

This shows all contexts including lookup tables, bloom filters, and GeoIP databases. Here’s an example output after creating different context types:

```tql
// First, create some contexts.
context::create_lookup_table "threat_intel"
context::create_bloom_filter "malware_hashes", capacity=1M, fp_probability=0.01
```

Now list them.

```tql
context::list
```

```tql
{
  num_entries: 0,
  name: "threat_intel",
  configured: false
}
{
  num_elements: 0,
  parameters: {m: 9585058, n: 1000000, p: 0.01, k: 7},
  name: "malware_hashes",
  configured: false
}
{
  name: "geo",
  type: "geoip"
}
```

Filter the results to find specific contexts:

```tql
context::list
where name.match_regex("[T|t]hreat")
```

## Delete a lookup table

[Section titled “Delete a lookup table”](#delete-a-lookup-table)

Use the [`context::remove`](/reference/operators/context/remove) operator to delete a lookup table:

```tql
context::remove "my_lookup_table"
```

This permanently deletes the specified lookup table and its persisted data.

## Perform lookups

[Section titled “Perform lookups”](#perform-lookups)

Use the [`context::enrich`](/reference/operators/context/enrich) operator to enrich events with data from a lookup table.

### Perform a point lookup

[Section titled “Perform a point lookup”](#perform-a-point-lookup)

First, create a simple lookup table:

```tql
context::create_lookup_table "user_roles"
```

Populate it with data:

```tql
from {user_id: 1001, role: "admin", department: "IT"},
     {user_id: 1002, role: "analyst", department: "Security"},
     {user_id: 1003, role: "engineer", department: "DevOps"}
context::update "user_roles", key=user_id
```

Now enrich events with this lookup table:

```tql
from {user_id: 1002, action: "login", timestamp: 2024-01-15T10:30:00}
context::enrich "user_roles", key=user_id
```

```tql
{
  user_id: 1002,
  action: "login",
  timestamp: 2024-01-15T10:30:00,
  user_roles: {
    user_id: 1002,
    role: "analyst",
    department: "Security"
  }
}
```

Specify where to place the enrichment:

```tql
from {user_id: 1002, action: "login"}
context::enrich "user_roles", key=user_id, into=user_info
```

```tql
{
  user_id: 1002,
  action: "login",
  user_info: {
    user_id: 1002,
    role: "analyst",
    department: "Security"
  }
}
```

Use OCSF format for standardized enrichment:

```tql
from {user_id: 1002, action: "login"}
context::enrich "user_roles", key=user_id, format="ocsf"
```

```tql
{
  user_id: 1002,
  action: "login",
  user_roles: {
    created_time: 2024-11-18T16:35:48.069981,
    name: "user_id",
    value: 1002,
    data: {
      user_id: 1002,
      role: "analyst",
      department: "Security"
    }
  }
}
```

### Perform a subnet lookup with an IP address key

[Section titled “Perform a subnet lookup with an IP address key”](#perform-a-subnet-lookup-with-an-ip-address-key)

When lookup table keys are of type `subnet`, you can probe the table with `ip` values. The lookup performs a longest-prefix match, perfect for network inventory and CMDB use cases:

```tql
context::create_lookup_table "network_inventory"
```

Populate with network infrastructure data:

```tql
from {subnet: 10.0.0.0/22, owner: "IT", location: "Datacenter-A"},
     {subnet: 10.0.0.0/24, owner: "IT-Web", location: "Datacenter-A"},
     {subnet: 10.0.1.0/24, owner: "IT-DB", location: "Datacenter-A"},
     {subnet: 10.0.2.0/24, owner: "Dev", location: "Office-B"},
     {subnet: 192.168.0.0/16, owner: "Guest", location: "All"}
context::update "network_inventory", key=subnet
```

Now enrich network traffic with infrastructure context:

```tql
from {
  timestamp: 2024-01-15T14:23:45,
  src_ip: 10.0.0.15,
  dst_ip: 10.0.1.20,
  bytes: 1048576,
  proto: "tcp"
}
context::enrich "network_inventory", key=src_ip, into=src_network
context::enrich "network_inventory", key=dst_ip, into=dst_network
```

```tql
{
  timestamp: 2024-01-15T14:23:45,
  src_ip: 10.0.0.15,
  dst_ip: 10.0.1.20,
  bytes: 1048576,
  proto: "tcp",
  src_network: {
    subnet: 10.0.0.0/24,
    owner: "IT-Web",
    location: "Datacenter-A"
  },
  dst_network: {
    subnet: 10.0.1.0/24,
    owner: "IT-DB",
    location: "Datacenter-A"
  }
}
```

The IP `10.0.0.15` matches `10.0.0.0/24` (Web frontends) rather than `10.0.0.0/22` because `/24` is a longer (more specific) prefix match.

### Perform a lookup with compound key

[Section titled “Perform a lookup with compound key”](#perform-a-lookup-with-compound-key)

Use record types as compound keys for complex matching scenarios.

```tql
context::create_lookup_table "threat_intel"
```

Populate a table with a compound key:

```tql
from {
  threat_id: "APT-2024-001",
  indicators: {
    domain: "malicious.example.com",
    port: 443
  },
  severity: "critical",
  campaign: "DarkStorm",
  first_seen: 2024-01-10T00:00:00
}
context::update "threat_intel", key=indicators
```

Pick a compound key for the table lookup:

```tql
from {
  timestamp: 2024-01-15T10:30:00,
  dest_domain: "malicious.example.com",
  dest_port: 443,
  src_ip: "10.0.1.50"
}
context::enrich "threat_intel",
  key={domain: dest_domain, port: dest_port},
  into=threat_info
```

Implement zone-based access control using compound keys:

```tql
context::create_lookup_table "access_rules"
```

Define rules for zone pairs:

```tql
from (
  {
    key: {source_zone: "internet", dest_zone: "dmz"},
    action: "allow",
    log: true
  },
  {
    key: {source_zone: "internet", dest_zone: "internal"},
    action: "deny",
    log: true,
    alert: true
  }
)
context::update "access_rules", key=key, value=this
```

Check access for firewall events:

```tql
from {
  timestamp: 2024-01-15T10:30:00,
  source_zone: "internet",
  dest_zone: "internal",
  src_ip: "203.0.113.10",
  dst_ip: "10.0.1.50"
}
context::enrich "access_rules",
  key={source_zone: source_zone, dest_zone: dest_zone},
  into=policy
```

Compound keys enable sophisticated matching based on multiple fields.

## Add/overwrite entries in lookup table

[Section titled “Add/overwrite entries in lookup table”](#addoverwrite-entries-in-lookup-table)

Use the [`context::update`](/reference/operators/context/update) operator to add or update entries. This is ideal for maintaining dynamic threat intelligence or asset inventory:

Update threat intelligence from an API:

```tql
from_http "https://threatfox-api.abuse.ch/api/v1/",
  body={query: "get_iocs", days: 1} {
  read_json
}
unroll data
where data.ioc_type == "domain"
context::update "threatfox", key=ioc, value=data
```

Track user login statistics:

```tql
from {
  user: "alice@company.com",
  login_time: 2024-01-15T09:00:00,
  source_ip: "10.0.50.100",
  success: true
}
context::update "user_logins", key=user, value={
  last_login: login_time,
  last_ip: source_ip,
  status: if success then "active" else "failed"
}
```

### Associate timeouts with entries

[Section titled “Associate timeouts with entries”](#associate-timeouts-with-entries)

Timeouts are essential for managing the lifecycle of threat intelligence and maintaining fresh context. You can set expiration timeouts on lookup table entries using the [`context::update`](/reference/operators/context/update) operator:

Most IoCs have a short half-life. Automatically expire stale entries:

```tql
from_file "threat_feed.json" {
  read_json
}
where confidence_score >= 70
context::update "active_threats",
  key=indicator,
  value={
    threat_type: threat_type,
    severity: severity,
    source: "ThreatFeed-Premium"
  },
  create_timeout=7d,      // Remove after 7 days regardless
  write_timeout=72h       // Remove if not updated for 3 days
```

Track sessions with activity-based expiration:

```tql
from {
  session_id: "sess_abc123",
  user: "alice@company.com",
  login_time: 2024-01-15T09:00:00,
  ip: "10.0.50.100"
}
context::update "active_sessions",
  key=session_id,
  write_timeout=30min,    // Session expires after 30 min of inactivity
  read_timeout=30min      // Also expire if not accessed for 30 min
```

Track DHCP leases with automatic expiration:

```tql
from {
  mac: "00:11:22:33:44:55",
  ip: "10.0.100.50",
  hostname: "laptop-alice",
  lease_time: 2024-01-15T10:00:00
}
context::update "dhcp_leases",
  key=mac,
  value={ip: ip, hostname: hostname, assigned: lease_time},
  create_timeout=4h       // DHCP lease duration
```

Implement API rate limiting with sliding windows:

```tql
from {api_key: "key_123", request_time: now()}
context::update "api_rate_limits",
  key=api_key,
  value={request_count: count()},
  create_timeout=1h,      // Reset counter every hour
  write_timeout=1h        // Also reset if no requests for 1 hour
```

The three timeout types work together:

* `create_timeout`: Hard expiration - useful for data with known shelf life
* `write_timeout`: Expire stale data - useful for removing inactive entries
* `read_timeout`: Expire unused data - useful for caching scenarios

## Remove entries from a lookup table

[Section titled “Remove entries from a lookup table”](#remove-entries-from-a-lookup-table)

Use the [`context::erase`](/reference/operators/context/erase) operator to remove specific entries, useful for allowlisting, removing false positives, or cleaning up outdated data:

Remove false positives from threat intelligence:

```tql
from {indicator: "legitimate-site.com", reason: "false_positive"}
context::erase "threat_indicators", key=indicator
```

Remove IPs from blocklist after remediation:

```tql
from_file "remediated_hosts.csv" {
  read_csv
}
where remediation_confirmed == true
context::erase "compromised_hosts", key=ip_address
```

Clean up old sessions on logout:

```tql
from {
  event_type: "logout",
  session_id: "sess_xyz789",
  user: "alice@company.com",
  timestamp: 2024-01-15T17:00:00
}
where event_type == "logout"
context::erase "active_sessions", key=session_id
```

Remove all entries older than 30 days from a context:

```tql
context::inspect "temp_indicators"
where first_seen > now() - 30d
context::erase "temp_indicators", key=indicator
```

## Show entries in a lookup table

[Section titled “Show entries in a lookup table”](#show-entries-in-a-lookup-table)

Use the [`context::inspect`](/reference/operators/context/inspect) operator to view and analyze the contents of a lookup table:

```tql
// View all entries
context::inspect "threat_indicators"
```

```tql
{
  key: "malicious.site.com",
  value: {
    threat_type: "phishing",
    first_seen: 2024-01-10T08:00:00,
    last_seen: 2024-01-15T14:30:00,
    severity: "high",
    source: "PhishTank"
  }
}
```

Analyze lookup table contents:

```tql
context::inspect "network_inventory"
top subnet
```

Find specific entries:

```tql
context::inspect "user_sessions"
where key.user == "alice@company.com"
```

Export data for reporting:

```tql
context::inspect "asset_inventory"
select asset_id=key, value.owner, value.department, value.last_seen
to "asset_report.csv"
```

Check table size and find old entries:

```tql
context::inspect "passive_dns"
set age = now() - value.last_seen
where age > 7d
summarize old_entries=count()
```

```tql
{
  old_entries: 42
}
```

## Update lookup tables from APIs

[Section titled “Update lookup tables from APIs”](#update-lookup-tables-from-apis)

Periodically poll APIs to maintain fresh reference data, threat intelligence, or asset information in lookup tables.

### Basic periodic updates

[Section titled “Basic periodic updates”](#basic-periodic-updates)

Use the [`every`](/reference/operators/every) operator to schedule regular API polls that update a lookup table:

```tql
every 1h {
  from_http "https://threatfeed.example.com/api/v1/indicators"
}
where confidence >= 80
context::update "threat_indicators", key=indicator, value=metadata
```

### Update with expiration

[Section titled “Update with expiration”](#update-with-expiration)

Combine periodic updates with timeouts to automatically remove stale entries:

```tql
every 30min {
  from_http "https://dns-blocklist.example.com/domains.json"
}
unroll domains
context::update "blocklist",
  key=domains.domain,
  value={
    category: domains.category,
    severity: domains.severity,
    last_updated: now()
  },
  create_timeout=24h,    // Remove after 24 hours
  write_timeout=2h       // Remove if not updated for 2 hours
```

## Export and import lookup table state

[Section titled “Export and import lookup table state”](#export-and-import-lookup-table-state)

### Export lookup table state

[Section titled “Export lookup table state”](#export-lookup-table-state)

Use the [`context::save`](/reference/operators/context/save) operator to create backups or migrate lookup tables between nodes:

Backup critical threat intelligence:

```tql
context::save "threat_indicators"
save_file "threat_intel_backup_2024_01_15.bin"
```

Export for migration to another node:

```tql
context::save "network_inventory"
save_file "network_inventory_prod.bin"
```

Automated daily backup:

```tql
every 1d {
  context::save "asset_tracking"
  save_file f"backups/assets_{now().format('%Y%m%d')}.bin"
}
```

### Import lookup table state

[Section titled “Import lookup table state”](#import-lookup-table-state)

Use the [`context::load`](/reference/operators/context/load) operator to restore lookup tables from backups or migrate data:

```tql
load_file "threat_intel_backup_2024_01_15.bin"
context::load "threat_indicators"
```

Caution

Loading replaces the entire lookup table state. Existing entries will be lost. Consider backing up with `context::save` before loading new data.

## Best Practices

[Section titled “Best Practices”](#best-practices)

1. **Design efficient keys**: Use the most selective field as the key to minimize table size
2. **Set appropriate timeouts**: Base timeouts on data freshness requirements to prevent unbounded growth
3. **Monitor table size**: Regularly inspect tables to prevent excessive growth
4. **Backup critical contexts**: Schedule regular exports of important tables
5. **Test enrichments**: Verify logic with `context::inspect` before production deployment

# Install MCP Server

To install the [Tenzir MCP Server](/reference/mcp-server) server, you can choose between two options:

1. **Docker**: `docker run -i tenzir/mcp`
2. **Native**: `uvx tenzir-mcp`

The Docker version runs a container that bundles the MCP server along with a Tenzir Node installation as a convenient one-stop solution.

The native version only runs the MCP server and you need to make sure that it can access a Tenzir Node installation yourself.

Always running latest versions

For the native installation, the `@latest` suffix ensures you always get the most recent MCP server package from PyPI when launching your MCP clients, at the cost of increased startup time. To reduce initial load time, omit `@latest` and manually refresh the cache with `uvx tenzir-mcp@latest` when you want updates.

For Docker, the `--pull=always` flag serves a similar purpose, ensuring you always pull the latest image before running the container. To avoid the pull overhead on every start, omit this flag and manually update with `docker pull tenzir/mcp` when needed.

## AI agent configuration

[Section titled “AI agent configuration”](#ai-agent-configuration)

All AI agents use the same JSON configuration structure for the Tenzir MCP server. The configuration always follows this pattern:

* Docker

  ```json
  {
    "mcpServers": {
      "tenzir": {
        "command": "docker",
        "args": ["run", "--pull=always", "-i", "tenzir/mcp"],
        "env": {}
      }
    }
  }
  ```

* Native

  ```json
  {
    "mcpServers": {
      "tenzir": {
        "command": "uvx",
        "args": ["tenzir-mcp@latest"],
        "env": {}
      }
    }
  }
  ```

The configuration file location varies by agent. See the specific sections below.

### Claude

[Section titled “Claude”](#claude)

Configure the Tenzir MCP server for [Claude Code](https://claude.ai/code) and [Claude Desktop](https://claude.ai/download).

#### Claude Code

[Section titled “Claude Code”](#claude-code)

For automatic configuration:

* Docker

  ```bash
  claude mcp add tenzir --scope user -- docker run --pull=always -i tenzir/mcp
  ```

* Native

  ```bash
  claude mcp add tenzir --scope user -- uvx tenzir-mcp@latest
  ```

For manual configuration, edit `~/.mcp.json` for user-wide settings or `.mcp.json` in your project directory for project-specific settings.

#### Claude Desktop

[Section titled “Claude Desktop”](#claude-desktop)

Edit the configuration file directly. The location depends on your operating system:

* **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
* **Windows**: `%APPDATA%\Claude\claude_desktop_config.json`
* **Linux**: `~/.config/Claude/claude_desktop_config.json`

After updating the configuration, restart Claude Desktop to load the MCP server.

### Gemini CLI

[Section titled “Gemini CLI”](#gemini-cli)

Google’s [Gemini CLI](https://github.com/google-gemini/gemini-cli) supports MCP servers natively. Configure the Tenzir MCP server by editing `~/.gemini/settings.json`.

Gemini Code Assist in VS Code shares the same MCP technology. The configuration automatically applies to both the CLI and VS Code integration.

### VS Code

[Section titled “VS Code”](#vs-code)

[VS Code](https://code.visualstudio.com/) supports MCP servers through [GitHub Copilot](https://github.com/features/copilot) starting with version 1.102.

For project-specific configuration, create `.vscode/mcp.json` in your project root.

For user-wide configuration:

1. Open Command Palette (`Ctrl+Shift+P` or `Cmd+Shift+P`)
2. Run `MCP: Open User Configuration`
3. Add the Tenzir server configuration

After configuration:

1. Open the GitHub Copilot chat panel
2. Look for the MCP icon in the chat input
3. Select the Tenzir server from available MCP servers
4. Start interacting with your security pipelines

# Overview

The [Tenzir Node](/explanations/architecture/node) is the vehicle to run [pipelines](/explanations/architecture/pipeline). It is light-weight server application that can be deployed on-premises or in the cloud.

Setting up a node involves the following basic steps:

1. [Provisioning](/guides/node-setup/provision-a-node): Create a new node in the [platform](/explanations/architecture/platform) and obtain a token.
2. [Sizing](/guides/node-setup/size-a-node): Determine the hardware requirements for the node based on the expected workload.
3. [Deploying](/guides/node-setup/deploy-a-node): Deploy the node to a server or cloud provider.
4. [Configuring](/guides/node-setup/configure-a-node): Tweak the default settings to optimize performance and security.

# Configure a node

The default node configuration is optimized for most common scenarios. But you can fine-tune the settings to match your specific requirements.

We recommend beginning with learning [how the node configuration process](/explanations/configuration) works, and then browse the [example configuration](/reference/node/configuration) for tuning knobs.

Here are few common configuration scenarios.

## Accept incoming connections

[Section titled “Accept incoming connections”](#accept-incoming-connections)

When your node starts it will listen for node-to-node connections on the TCP endpoint `127.0.0.1:5158`. Select a different endpoint via the `tenzir.endpoint` option. For example, to bind to an IPv6 address use `[::1]:42000`.

## Refuse incoming connections

[Section titled “Refuse incoming connections”](#refuse-incoming-connections)

Set `tenzir.endpoint` to `false` to disable the endpoint, making the node exclusively accessible through the Tenzir Platform. This effectively prevents connections from other `tenzir` or `tenzir-node` processes.

## Configure pipeline subprocesses

[Section titled “Configure pipeline subprocesses”](#configure-pipeline-subprocesses)

Pipelines that run in a node are now partially moved to a subprocess for improved error resilience and resource utilization. Operators that need to communicate with a component still run inside the main node process for architectural reasons. You can set `tenzir.disable-forked-pipelines: true` in `tenzir.yaml` or `TENZIR_DISABLE_FORKED_PIPELINES=true` on the command line to opt out. This feature is enabled by default on Linux.

Learn more about [pipeline subprocesses](/explanations/architecture/node#pipeline-subprocesses) and their trade-offs.

# Deploy a node

Deploying a node means spinning it up in one of the supported runtimes. The primary choice is between a containerized with Docker or a native deployment with our static binary that runs on amd64 and arm64 architectures.

## Docker

[Section titled “Docker”](#docker)

We recommend using Docker to deploy a Tenzir node, as it’s the easiest way to get started.

After [provisioning a node](/guides/node-setup/provision-a-node), proceed as follows:

1. Select the Docker tab and click the download button to obtain the `docker-compose.NODE.yaml` configuration file, where `NODE` is the name you entered for your node.

2. Run

   ```bash
   docker compose -f docker-compose.NODE.yaml up --detach
   ```

Edit the Docker Compose file and change [environment variables](/explanations/configuration) to adjust your configuration.

### Stop a node

[Section titled “Stop a node”](#stop-a-node)

Stop a node via the `down` command:

```bash
docker compose -f docker-compose.NODE.yaml down
```

Stop a node and delete its persistent state by adding `--volumes`:

```bash
docker compose -f docker-compose.NODE.yaml down --volumes
```

### Update a node

[Section titled “Update a node”](#update-a-node)

Run the following commands to update a Docker Compose deployment with a configuration file `docker-compose.NODE.yaml`:

```bash
docker compose -f docker-compose.NODE.yaml pull
docker compose -f docker-compose.NODE.yaml down
docker compose -f docker-compose.NODE.yaml up --detach
```

Note that we `pull` first so that the subsequent downtime between `down` and `up` is minimal.

## Linux

[Section titled “Linux”](#linux)

We offer a static binary package on various Linux distributions.

### Install a node

[Section titled “Install a node”](#install-a-node)

Second, choose the Linux tab and proceed as follows:

1. [Provision the node](/guides/node-setup/provision-a-node) and download its config.

2. Create a directory for the platform configuration.

   ```bash
   mkdir -p /opt/tenzir/etc/tenzir/plugin
   ```

3. Move the downloaded `platform.yaml` configuration file to the directory so that the node can find it during startup:

   ```bash
   mv platform.yaml /opt/tenzir/etc/tenzir/plugin
   ```

4. Run the installer and follow the instructions to download and start the node:

   ```bash
   curl https://get.tenzir.app | sh
   ```

The installer script asks for confirmation before performing the installation. If you prefer a manual installation you can also perform the installer steps yourself. See the [configuration files documentation](/explanations/configuration) for details on how the node loads config files at startup.

* Debian

  Download the latest [Debian package](https://github.com/tenzir/tenzir/releases/latest/download/tenzir-static-amd64-linux.deb) and install it via `dpkg`:

  ```bash
  dpkg -i tenzir-static-amd64-linux.deb
  ```

  You can uninstall the Tenzir package via `apt-get remove tenzir`. Use `purge` instead of `remove` if you also want to delete the state directory and leave no trace.

* RPM-based (RedHat, OpenSUSE, Fedora)

  Download the latest [RPM package](https://github.com/tenzir/tenzir/releases/latest/download/tenzir-static-amd64-linux.rpm) and install it via `rpm`:

  ```bash
  rpm -i tenzir-static-amd64-linux.rpm
  ```

* Nix

  Use our `flake.nix` to run an ad-hoc `tenzir` pipeline:

  ```bash
  nix run github:tenzir/tenzir/latest
  ```

  Or run a node without installing:

  ```bash
  nix shell github:tenzir/tenzir/latest -c tenzir-node
  ```

  Install a Tenzir Node by adding `github:tenzir/tenzir/latest` to your flake inputs, or use your preferred method to include third-party modules on classic NixOS.

  Pre-built only!

  The default attributes of the flake contain some closed source plugins and will fail to download locally. The flake is only for public consumption together with the [Tenzir cachix cache](https://tenzir.cachix.org).

* Any

  Download a tarball with our [static binary](https://github.com/tenzir/tenzir/releases/latest/download/tenzir-static-x86_64-linux.tar.gz) for all Linux distributions and unpack it into `/opt/tenzir`:

  ```bash
  tar xzf tenzir-static-x86_64-linux.tar.gz -C /
  ```

  We also offer prebuilt statically linked binaries for every Git commit to the `main` branch.

  ```bash
  curl -O https://storage.googleapis.com/tenzir-dist-public/packages/main/tarball/tenzir-static-main.gz
  ```

### Start a node manually

[Section titled “Start a node manually”](#start-a-node-manually)

The installer script uses the package manager of your Linux distribution to install the Tenzir package. This typically also creates a [systemd](https://systemd.io) unit and starts the node automatically.

For testing, development, our troubleshooting, run the `tenzir-node` executable to start a node manually:

```bash
tenzir-node
```

```plaintext
      _____ _____ _   _ ________ ____
     |_   _| ____| \ | |__  /_ _|  _ \
       | | |  _| |  \| | / / | || |_) |
       | | | |___| |\  |/ /_ | ||  _ <
       |_| |_____|_| \_/____|___|_| \_\


          v4.0.0-rc6-0-gf193b51f1f
Visit https://app.tenzir.com to get started.


[16:50:26.741] node listens for node-to-node connections on tcp://127.0.0.1:5158
[16:50:26.982] node connected to platform via wss://ws.tenzir.app:443/production
```

### Stop a node

[Section titled “Stop a node”](#stop-a-node-1)

There exist two ways stop a server:

1. Hit CTRL+C in the same TTY where you ran `tenzir-node`.
2. Send the process a SIGINT or SIGTERM signal, e.g., via `pkill -2 tenzir-node`.

Hitting CTRL+C is equivalent to manually sending a SIGTERM signal.

## AWS

[Section titled “AWS”](#aws)

The recommended way to run a Tenzir node in AWS is with [Elastic Container Service (ECS)](https://aws.amazon.com/ecs/). The diagram below shows the high-level architecture of an AWS deployment.

Prerequisites

Before you begin, you’ll need a valid `TENZIR_TOKEN` for your node. Obtain one by [provisioning a node](/guides/node-setup/provision-a-node), select the *other* tab, and click on the text box to copy the shown token.

![AWS Architecture](/_astro/deploy-node-on-aws.CJGSeWtf_19DKCs.svg)

You can deploy a Tenzir node using either [CloudFormation](https://aws.amazon.com/cloudformation/) for automated setup or manually through the AWS console. Both methods support deploying as many nodes as you need.

### Subscribe to Tenzir Node on AWS Marketplace

[Section titled “Subscribe to Tenzir Node on AWS Marketplace”](#subscribe-to-tenzir-node-on-aws-marketplace)

Before deploying with either method, you need to subscribe to the Tenzir Node product:

1. Go to [AWS Marketplace](https://console.aws.amazon.com/marketplace/) and subscribe to the free [Tenzir Node](https://console.aws.amazon.com/marketplace/search/listing/prodview-gsofc3z6f3vsu) product.

   ![AWS Marketplace Tenzir Node](/_astro/01-aws-marketplace.BV34EsMs_Z1SWAhS.webp)

2. Accept the terms to subscribe to the offering.

   ![AWS Marketplace Tenzir Node](/_astro/02-subscribe.BuWO6X8P_Z1LfgkR.webp)

### Choose your deployment method

[Section titled “Choose your deployment method”](#choose-your-deployment-method)

* CloudFormation

  Deploy a Tenzir node using our CloudFormation template for automated setup. Click the button below to launch the CloudFormation console with our template pre-loaded:

  [Launch CloudFormation Stack ](https://console.aws.amazon.com/cloudformation/home?#/stacks/create?templateURL=https://s3.amazonaws.com/tenzir-marketplace-resources/single-node-container.yaml)

  After clicking the button above, follow these steps in the AWS console:

  1. **Review the pre-filled template**: The CloudFormation console will open with our single-node container template already loaded:

     ![CloudFormation Create Stack](/_astro/01-create-stack.GmdJ5tXH_2okx2I.webp)

  2. **Configure your stack**:

     * **Stack name**: Choose a unique name for your stack (e.g., `tenzir-node-prod`)
     * **Parameters**: Enter your `TENZIR_TOKEN` in the provided field

     Double-check that the container image URL points to the Tenzir Node version you want to deploy.

     ![Stack Parameters](/_astro/02-stack-parameters.B4Oua0h9_Z2mdd0o.webp)

  3. **Accept the default stack options**: For most deployments, the defaults work perfectly:

     ![Stack Options](/_astro/03-stack-options.5n7bQip1_1voPe2.webp)

  4. **Review and acknowledge**: Confirm your configuration and check the acknowledgment box:

     ![Review Stack](/_astro/04-review-stack.DSPyQTC8_Z1t888J.webp)

  5. **Deploy your node**: Click *Submit* to start the deployment. Monitor progress in the *Events* tab.

  6. **Wait for connection**: Your node will automatically connect to your workspace once the stack creation completes (typically 2-3 minutes).

  Stack Management

  You can update your node by updating the CloudFormation stack with new parameters. To remove the node and all associated resources, simply delete the stack.

* Manual

  Follow these steps for manual deployment through the AWS console:

  1. Navigate to [Amazon Elastic Container Service (ECS)](https://console.aws.amazon.com/ecs).

     ![Amazon Elastic Container Service (ECS)](/_astro/01-ecs.D-9AB8KO_Z1P4kme.webp)

  2. Create a new cluster. Choose between **EC2** or **Fargate** based on your needs:

     * **EC2 clusters** give you full control over the underlying instances. They’re ideal for long-running workloads with consistent resource requirements.
     * **Fargate clusters** provide serverless container execution where you pay only for the resources you use. They’re cost-effective for workloads with variable demand.

     ![Create a Cluster](/_astro/02-create-cluster.DWOcGwyQ_Z1wYnvS.webp)

  3. Create a task definition to specify how the Tenzir node container should run.

     ![Create a Cluster](/_astro/03-create-task-definition.Ds8CzEXJ_1y9Jew.webp)

     In the *Containers* section, enter the repository URL from your AWS Marketplace subscription:

     ```plaintext
     709825985650.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-node:v<MAJOR>.<MINOR>.<PATCH>
     ```

     Each node version has its own image tag. Replace `v<MAJOR>.<MINOR>.<PATCH>` with the desired version. Check the [node changelog](/changelog/node/) for the latest version.

  4. Return to your cluster, navigate to the *Tasks* tab, and click *Run new task*. Select the task definition you just created.

     ![Create a Cluster](/_astro/04-run-task.BaW9wNbx_Z1TYNR4.webp)

     In the *Container overrides* section, add the `TENZIR_TOKEN` environment variable with the token value corresponding to your node.

     ![Container Overrides](/_astro/05-container-overrides.DgPIqnAZ_2rS1Oj.webp)

     Click *Create* to launch your node.

  5. Once the container starts successfully, your node will automatically connect to your workspace.

## Azure

[Section titled “Azure”](#azure)

To run a node in Azure, we recommend using [Azure Container Instances (ACI)](https://azure.microsoft.com/en-us/products/container-instances), which allows you to run Docker containers without having to setup VMs.

Prerequisites

Before you begin, you’ll need a valid `TENZIR_TOKEN` for your node. Obtain one by [provisioning a node](/guides/node-setup/provision-a-node), select the *other* tab, and click on the text box to copy the shown token.

### Azure Container Instances (ACI)

[Section titled “Azure Container Instances (ACI)”](#azure-container-instances-aci)

The following steps guide you through deploying a Tenzir Node using ACI.

#### Create a new container instance

[Section titled “Create a new container instance”](#create-a-new-container-instance)

1. Open [portal.azure.com](https://portal.azure.com/).
2. Navigate to the *Container instances*.
3. Click the *Create* button.

#### Basics

[Section titled “Basics”](#basics)

In the *Basics* tab, perform the following action:

1. Choose a container name.
2. For *Image source*, select *Other registry*.
3. For *Image*, enter `tenzir/tenzir-node`.

![Basics](/_astro/basics.bOCDyyP__Z18rgYp.webp)

#### Networking

[Section titled “Networking”](#networking)

In the *Networking* tab, configure the ports you plan to use for pipelines that receive incoming connections.

![Networking](/_astro/networking.DpOSLd0n_2quA37.webp)

#### Advanced

[Section titled “Advanced”](#advanced)

In the *Advanced* tab, enter the `TENZIR_TOKEN` environment variable from your Docker Compose file.

![Advanced](/_astro/advanced.afPiy2h8_Z76Ic4.webp)

#### Create

[Section titled “Create”](#create)

Once you’ve completed the configuration, click the *Create* button. Your node is now up and running.

## macOS

[Section titled “macOS”](#macos)

Looking for a native macOS package? We’re not quite there yet—but you can still run Tenzir smoothly on macOS using [Docker](/guides/node-setup/deploy-a-node#docker).

Want to see a native macOS build? Let us know! Drop your vote in our [Discord community](/discord)—we prioritize what our users need most.

Tenzir *does* run well on macOS under the hood. Docker just bridges the gap for now.

## Ansible

[Section titled “Ansible”](#ansible)

The Ansible role for Tenzir allows for easy integration of Tenzir into existing Ansible setups. The role uses either the Tenzir Debian package or the tarball installation method depending on which is appropriate for the target environment. The role definition is in the [`ansible/roles/tenzir`](https://github.com/tenzir/tenzir/tree/main/ansible/roles/tenzir) directory of the Tenzir repository. You need a local copy of this directory so you can use it in your playbook.

### Example

[Section titled “Example”](#example)

This example playbook shows how to run a Tenzir service on the machine `example_tenzir_server`:

```yaml
- name: Deploy Tenzir
  become: true
  hosts: example_tenzir_server
  remote_user: example_ansible_user
  roles:
    - role: tenzir
      vars:
        tenzir_config_dir: ./tenzir
        tenzir_read_write_paths: [/tmp]
        tenzir_archive: ./tenzir.tar.gz
        tenzir_debian_package: ./tenzir.deb
```

### Variables

[Section titled “Variables”](#variables)

#### `tenzir_config_dir` (required)

[Section titled “tenzir\_config\_dir (required)”](#tenzir_config_dir-required)

A path to directory containing a [`tenzir.yaml`](/reference/node/configuration) relative to the playbook.

#### `tenzir_read_write_paths`

[Section titled “tenzir\_read\_write\_paths”](#tenzir_read_write_paths)

A list of paths that Tenzir shall be granted access to in addition to its own state and log directories.

#### `tenzir_archive`

[Section titled “tenzir\_archive”](#tenzir_archive)

A tarball of Tenzir structured like those that can be downloaded from the [GitHub Releases Page](https://github.com/tenzir/tenzir/releases). This is used for target distributions that are not based on the `apt` package manager.

#### `tenzir_debian_package`

[Section titled “tenzir\_debian\_package”](#tenzir_debian_package)

A Debian package (`.deb`). This package is used for Debian and Debian-based Linux distributions.

# Provision a node

Provisioning a node means creating one in the [platform](/explanations/architecture/platform) in your workspace. After provisioning, you can download configuration file with an authentication token—ready to then deploy the node.

When you start out with a new Tenzir account, you have an empty workspace without any nodes:

![Landing page](/_astro/new-account.Qul5lKOD_1KFSe1.webp)

You have two options: either provision a self-hosted node or play with a cloud-hosted demo with a lifetime of 2 hours.

## Cloud-hosted Demo Node

[Section titled “Cloud-hosted Demo Node”](#cloud-hosted-demo-node)

Provision a cloud-hosted demo node by following these steps:

1. Click **Cloud-hosted demo-node**.
2. Click **Add node**.
3. Click **Get Started**.

🙌 You’re good to go. It takes up to 2 minutes for your node to be usable. Upon provisioning, the documentation pops in automatically so that you can familiarize yourself with key concepts in the meantime.

## Self-hosted Node

[Section titled “Self-hosted Node”](#self-hosted-node)

Provision a self-hosted node by following these steps:

1. Click **Self-hosted node**.
2. Click **Add node**.
3. Enter a name for your node.
4. Click **Add node**.

🚢 Your node is ready to be [deployed](/guides/node-setup/deploy-a-node). The easiest way to continue is by spinning up a node with [Docker](/guides/node-setup/deploy-a-node#docker).

Ephemeral Nodes

Instead of provisioning a node as a above, with a dedicated authentication token, you can also [configure a workspace-wide token](/guides/platform-management/use-ephemeral-nodes). Nodes that use this token are *ephemeral*, i.e., not explicitly managed by the platform and will vanish once they disconnect.

# Size a node

To better understand what resources you need to run a node, we provide guidance on sizing and a [calculator](#calculator) to derive concrete **CPU**, **RAM**, and **storage** requirements.

## Considerations

[Section titled “Considerations”](#considerations)

Several factors have an impact on sizing. Since you can run many types of workloads in pipelines, it is difficult to make a one-size-fits-all recommendation. The following considerations affect your resource requirements:

### Workloads

[Section titled “Workloads”](#workloads)

Depending on what you do with pipelines, you may generate a different resource profile.

#### Data Shaping

[Section titled “Data Shaping”](#data-shaping)

[Shaping](/guides/data-shaping/shape-data) operation changes the form of the data, e.g., filtering events, removing columns, or changing values. This workload predominantly incurs **CPU** load.

#### Aggregation

[Section titled “Aggregation”](#aggregation)

Performing in-stream or historical aggregations often requires extensive buffering of the aggregation groups, which adds to your **RAM** requirements. If you run intricate custom aggregation functions, you also may see an additional increase CPU usage.

#### Enrichment

[Section titled “Enrichment”](#enrichment)

[Enriching](/explanations/enrichment) dataflows with contexts requires holding in-memory state proportional to the context size. Therefore, enrichment affects your **RAM** requirements. Bloom filters are a fixed-size space-efficient structure for representing large sets, and lookup tables grow linearly with the number of entries.

### Data Diversity

[Section titled “Data Diversity”](#data-diversity)

The more data sources you have, the more pipelines you run. In the simplest scenario where you just import all data into a node, you deploy one pipeline per data source. The number of data sources is a thus a lower bound for the number of pipelines.

### Data Volume

[Section titled “Data Volume”](#data-volume)

The throughput of pipeline has an impact on performance. Pipelines with low data volume do not strain the system much, but high-volume pipelines substantially affect **CPU** and **RAM** requirements. Therefore, understanding your ingress volume, either as events per second or bytes per day, is helpful for sizing your node proportionally.

### Retention

[Section titled “Retention”](#retention)

When you leverage the node’s built-in storage engine by [importing](/guides/edge-storage/import-into-a-node) and [exporting](/guides/edge-storage/export-from-a-node) data, you need persistent **storage**. To assess your retention span, you need to understand your data volume and your capacity.

Tenzir storage engine builds sparse indexes to accelerate historical queries. Based on how aggressively configure indexing, your **RAM** requirements may vary.

## Calculator

[Section titled “Calculator”](#calculator)

# Start the API

The node offers a [REST API](/reference/node/api) for CRUD-style pipeline management. By default, the API is not accessible from the outside. Only the platform can access it internaly through the existing node-to-platform connection. To enable the API for direct access, you need to configure the built in web server that exposes the API.

## Expose the REST API

[Section titled “Expose the REST API”](#expose-the-rest-api)

To expose the REST API, start the web server component by adding the following to your configuration:

tenzir.yaml

```yaml
tenzir:
  start:
    commands:
      - web server \
        --certfile=/path/to/server.certificate \
        --keyfile=/path/to/private.key \
        --mode=MODE
```

Replace `MODE` with the TLS mode that best suits your deployment, as [explained below](#choose-a-tls-deployment-mode).

The YAML configuration is equivalent to the following command-line invocation:

```bash
tenzir-node --commands="web server [...]"
```

The server will only accept TLS requests by default. To allow clients to connect successfully, you need to pass a valid certificate and corresponding private key with the `--certfile` and `--keyfile` arguments.

## Generate an authentication token

[Section titled “Generate an authentication token”](#generate-an-authentication-token)

Clients must authenticate all requests with a valid token. The token is a short string that clients put in the `X-Tenzir-Token` request header. You can generate a valid token on the command line:

```bash
tenzir-ctl web generate-token
```

For local testing and development, generating suitable certificates and tokens can be a hassle. For this scenario, you can start the server in [developer mode](#developer-mode) where it accepts plain HTTP connections are does not perform token authentication.

## Choose a TLS deployment mode

[Section titled “Choose a TLS deployment mode”](#choose-a-tls-deployment-mode)

There exist four modes to start the REST API, each of which suits a slightly different use case.

### Developer mode

[Section titled “Developer mode”](#developer-mode)

The developer mode bypasses encryption and authentication token verification.

![Developer Mode](/_astro/rest-api-mode-developer.Bs3kilub_19DKCs.svg)

Pass `--mode=dev` to start the REST API in developer mode.

### Server mode

[Section titled “Server mode”](#server-mode)

The server mode reflects the “traditional” mode of operation where the server binds to a network interface. This mode only accepts HTTPS connections and requires a valid authentication token for every request. This is the default mode of operation.

![Server Mode](/_astro/rest-api-mode-server.BTJHvLsL_19DKCs.svg)

Pass `--mode=server` to start the REST API in server mode.

### Upstream TLS mode

[Section titled “Upstream TLS mode”](#upstream-tls-mode)

The upstream TLS mode is suitable when Tenzir sits upstream of a separate TLS terminator that is running on the same machine. This kind of setup is commonly encountered when running nginx as a reverse proxy.

![Upstream TLS Mode](/_astro/rest-api-mode-upstream.CLNud8Cc_19DKCs.svg)

Tenzir only listens on localhost addresses, accepts plain HTTP but still checks authentication tokens.

Pass `--mode=upstream` to start the REST API in server mode.

### Mutual TLS (mTLS) mode

[Section titled “Mutual TLS (mTLS) mode”](#mutual-tls-mtls-mode)

The mutual TLS mode is suitable when Tenzir sits upstream of a separate TLS terminator that may be running on a different machine. This setup is commonly encountered when running [nginx](https://nginx.org) as a load balancer. Tenzir would typically be configured to use a self-signed certificate in this setup.

Tenzir only accepts HTTPS requests, requires TLS client certificates for incoming connections, and requires valid authentication tokens for any authenticated endpoints.

![mTLS Mode](/_astro/rest-api-mode-mtls.DhDa_SVb_19DKCs.svg)

Pass `--mode=mtls` to start the REST API in server mode.

# Tune performance

This section describes tuning configuration knobs that have a notable effect on the performance of the node.

## Demand

[Section titled “Demand”](#demand)

Tenzir schedules operators asynchronously. Within a pipeline, every operator sends elements (events or bytes) downstream to the next operator, and demand upstream to the previous operator. If an operator has no downstream demand, Tenzir’s pipeline execution engine stops scheduling the operator.

The configuration section `tenzir.demand` controls how operators issue demand to their upstream operators. See the [example configuration](/reference/node/configuration) for all available options.

For example, to minimize memory usage of pipelines at the cost of performance, set the following option:

```yaml
tenzir:
  demand:
    max-batches: 1
```

## Batching

[Section titled “Batching”](#batching)

Tenzir processes events in batches. Because the structured data has the shape of a table, we call these batches *table slices*. The following options control their shape and behavior.

### Size

[Section titled “Size”](#size)

Most components in Tenzir operate on table slices, which makes the table slice size a fundamental tuning knob on the spectrum of throughput and latency. Small table slices allow for shorter processing times, resulting in more scheduler context switches and a more balanced workload. But the increased pressure on the scheduler comes at the cost of throughput. Conversely, a large table slice size creates more work for each actor invocation and makes them yield less frequently to the scheduler. As a result, other actors scheduled on the same thread may have to wait a little longer.

The option `tenzir.import.batch-size` sets an upper bound for the number of events per table slice. It defaults to 65,536.

The option controls the maximum number of events per table slice, but not necessarily the number of events until a component forwards a batch to the next stage in a stream. The CAF streaming framework uses a credit-based flow-control mechanism to determine buffering of tables slices.

Caution

Setting `tenzir.import.batch-size` to `0` causes the table slice size to be unbounded and leaves it to `tenzir.import.batch-timeout` to produce table slices. This can lead to very large table slices for sources with high data rates, and is not recommended.

### Import Timeout

[Section titled “Import Timeout”](#import-timeout)

The `tenzir.import.batch-timeout` option sets a timeout for forwarding buffered table slices to the remote Tenzir node. If the timeout fires before a table slice reaches `tenzir.import.batch-size`, then the table slice will contain fewer events and ship immediately.

The `tenzir.import.read-timeout` option determines how long a call to read data from the input will block. After the read timeout elapses, Tenzir tries again at a later. The default value is 10 seconds.

## Storage Engine

[Section titled “Storage Engine”](#storage-engine)

The central component of Tenzir’s storage engine is the *catalog*. It owns the partitions, keeps metadata about them, and maintains a set of sparse secondary indexes to identify relevant partitions for a given query.

![Catalog Indexes](/_astro/catalog-indexes.CTn-znil_19DKCs.svg)

The catalog’s secondary indexes are space-efficient sketch data structures (e.g., Bloom filters, min-max summaries) that have a low memory footprint but may yield false positives. Tenzir keeps all sketches in memory.

The amount of memory that the storage engine can consume is not explicitly configurable, but there exist various options that have a direct impact.

### Control the partition size

[Section titled “Control the partition size”](#control-the-partition-size)

Tenzir groups table slices with the same schema in a partition. There exist mutable *active partitions* that Tenzir writes to during ingestion, and immutable *passive partitions* that Tenzir reads from during query execution.

When constructing a partition, the parameter `tenzir.max-partition-size` (default: 4Mi / 2^22) sets an upper bound on the number of records in a partition, across all table slices. The parameter `tenzir.active-partition-timeout` (default: 10 seconds) provides a time-based upper bound: once reached, Tenzir considers the partition as complete, regardless of the number of records.

The two parameters are decoupled to allow for independent control of throughput and freshness. Tenzir also merges undersized partitions asynchronously in the background, which counter-acts the fragmentation effect from choosing a low partition timeout.

### Tune catalog fragmentation

[Section titled “Tune catalog fragmentation”](#tune-catalog-fragmentation)

The catalog keeps state that grows linear in the number of partitions. The configuration option `tenzir.max-partition-size` determines an upper bound of the number of records per partition, which is inversely linked to the number of partitions. For example, a large value yields fewer partitions whereas a small value creates more partitions.

In other words, increasing `tenzir.max-partition-size` is an effective method to reduce the memory footprint of the catalog, at the cost of creating larger partitions.

### Configure the catalog

[Section titled “Configure the catalog”](#configure-the-catalog)

You can configure catalog and partition indexes under the key `tenzir.index`. The configuration `tenzir.index.rules` is an array of indexing rules, each of which configures the indexing behavior of a set of extractors. A rule has the following keys:

* `targets`: a list of extractors to describe the set of fields whose values to add to the sketch.
* `fp-rate`: an optional value to control the false-positive rate of the sketch.

#### Tune catalog index parameters

[Section titled “Tune catalog index parameters”](#tune-catalog-index-parameters)

Catalog indexes may produce false positives that can have a noticeable impact on the query latency by materializing irrelevant partitions. Based on the cost of I/O, this penalty may be substantial. Conversely, reducing the false positive rate increases the memory consumption, leading to a higher resident set size and larger RAM requirements. You can control the false positive probability with the `fp-rate` key in an index rule.

By default, Tenzir creates one sketch per type, but not additional field-level sketches unless a dedicated rule with a matching target configuration exists. Here is an example configuration that adds extra field-level sketches:

```yaml
tenzir:
  index:
    # Set the default false-positive rate for type-level sketches
    default-fp-rate: 0.001
    rules:
      - targets:
          # field sketches require a fully qualified field name
          - suricata.http.http.url
        fp-rate: 0.005
      - targets:
          - :ip
        fp-rate: 0.1
```

This configuration includes two rules (= two catalog indexes) where the first rule includes a field extractor and the second a type extractor. The first rule applies to a single field, `suricata.http.http.url`, and has false-positive rate of 0.5%. The second rule creates one sketch for all fields of type `ip` that has a false-positive rate of 10%.

### Adjust the store compression

[Section titled “Adjust the store compression”](#adjust-the-store-compression)

Tenzir compresses partitions using Zstd for partitions at rest. To fine-tune the space-time trade-off, Tenzir offers a setting, `tenzir.zstd-compression-level` to allow fine-tuning the compression level:

```yaml
tenzir:
  zstd-compression-level: 1
```

Currently, the default value is taken from Apache Arrow itself.

### Rebuild partitions

[Section titled “Rebuild partitions”](#rebuild-partitions)

The `rebuild` command re-ingests events from existing partitions and replaces them with new partitions. This makes it possible to upgrade persistent state to a newer version, or recreate persistent state after changing configuration parameters, e.g., switching from the Feather to the Parquet store backend. The following diagram illustrates this “defragmentation” process:

![Rebuild](/_astro/rebuild.P1w_CnP2_19DKCs.svg)

Rebuilding partitions also recreates their sketches. The process takes place asynchronously in the background. Control this behavior in your `tenzir.yaml` configuration file, to disable or adjust the resources to spend on automatic rebuilding:

```yaml
tenzir:
  # Automatically rebuild undersized and outdated partitions in the background.
  # The given number controls how much resources to spend on it. Set to 0 to
  # disable. Defaults to 1.
  automatic-rebuild: 1
```

This is how you run it manually:

```bash
tenzir-ctl rebuild start [--all] [--undersized] [--parallel=<number>] [--max-partitions=<number>] [--detached] [<expression>]
```

A rebuild is not only useful when upgrading outdated partitions, but also when changing parameters of up-to-date partitions. (Internally, Tenzir versions the partition state via FlatBuffers. An outdated partition is one whose version number is not the newest.)

The `--undersized` flag causes Tenzir to rebuild partitions that are under the configured partition size limit `tenzir.max-partition-size`.

The `--all` flag causes Tenzir to rebuild all partitions.

The `--parallel` options is a performance tuning knob. The parallelism level controls how many sets of partitions to rebuild in parallel. This value defaults to 1 to limit the CPU and memory requirements of the rebuilding process, which grow linearly with the selected parallelism level.

The `--max-partitions` option allows for setting an upper bound to the number of partitions to rebuild.

An optional expression allows for restricting the set of partitions to rebuild. Tenzir performs a catalog lookup with the expression to identify the set of candidate partitions. This process may yield false positives, as with regular queries, which may cause unaffected partitions to undergo a rebuild. For example, to rebuild outdated partitions containing `suricata.flow` events older than 2 weeks, run the following command:

```bash
tenzir-ctl rebuild start '#schema == "suricata.flow" && #import_time < 2 weeks ago'
```

To stop an ongoing rebuild, use `tenzir-ctl rebuild stop`.

## Logging

[Section titled “Logging”](#logging)

The Tenzir Node writes log files into a file named `server.log` in the database directory by default. Set the option `tenzir.log-file` to change the location of the log file.

A `tenzir` client process does not write logs by default. Set the option `tenzir.client-log-file` to enable logging. Note that relative paths are interpreted relative to the current working directory of the client process.

Node log files rotate automatically after 10 MiB. The option `tenzir.disable-log-rotation` allows for disabling log rotation entirely, and the option `tenzir.log-rotation-threshold` sets the size limit when a log file should be rotated.

Tenzir processes log messages in a dedicated thread, which by default buffers up to 1M messages for servers, and 100 for clients. The option `tenzir.log-queue-size` controls this setting.

## Caching

[Section titled “Caching”](#caching)

Tenzir Nodes cache results for results for pipelines used in the Tenzir Platform’s Explorer. Internally, this utilizes the [`cache`](/reference/operators/cache) operator.

Caches have two primary tuning knobs:

1. The `tenzir.cache.capacity` option controls the total memory usage in bytes caches may use in the node. If this capacity is exceeded, the oldest caches will be removed. The option defaults to 1Gi, and must be set to at least 64Mi.
2. The `tenzir.cache.lifetime` option controls the maximum age of each cache. The option defaults to 10min.

If you expect that many users will be using the Explorer with a node at the same time, or if you want to explore large data sets with the Explorer, we recommend increasing the cache capacity option of the node. Otherwise, the Explorer may need to re-run pipelines more frequently because the caches get evicted earlier.

We recommend reducing the cache capacity to its minimum of 64Mi only when running in a memory-constrained environment.

Here’s how you can set the options:

/opt/tenzir/etc/tenzir/tenzir.yaml

```yaml
tenzir:
  cache:
    # Total cache capacity in bytes for the node. Must be at least 64Mi.
    # Defaults to 1Gi.
    capacity: 1Gi
    # Maximum lifetime of a cache. Defaults to 10min.
    lifetime: 10min
```

The options can also be specified as the `TENZIR_CACHE__CAPACITY` and `TENZIR_CACHE__LIFETIME` environment variables, respectively.

Restart the node after changing these options for them to take effect.

Additionally, in the Explorer, users can control how many events they want to cache per run of a pipeline. The available options are 1k, 10k, 100k, or 1M events, defaulting to 10k. Avoid using higher limits to be fair to other users of the Explorer on the same node, and only increase it when you must.

# Configure dashboards

You can pre-define dashboards for your [static workspaces](/guides/platform-management/configure-workspaces#define-static-workspaces). This practice provides users with ready-to-use visualizations when they access the workspace.

Use the `dashboards` key in your workspace configuration:

workspace.yaml

```yaml
workspaces:
  static0:
    name: Example Workspace
    # Other workspace configuration...
    dashboards:
      dashboard1: # Unique dashboard identifier
        name: Example Dashboard # Display name in the UI
        cells: # Dashboard cells/widgets
          - name: Partition Summary # Cell name
            definition: | # Pipeline to execute
              partitions
              where not internal
              summarize events=sum(events), schema
              sort -events
            type: table # Visualization type (table, bar, line, etc.)
            x: 0 # X-coordinate in the dashboard grid
            y: 0 # Y-coordinate in the dashboard grid
            w: 12 # Width in grid units
            h: 12 # Height in grid units
```

Dashboard Coordinates

The dashboard grid has a width of 24 units. When you position cells, ensure that `x + w ≤ 24`. This prevents cells from extending beyond the grid.

Users can modify these dashboards via the UI after their initial setup. However, the platform resets any changes to the configuration-defined state when it restarts.

# Configure workspaces

Workspaces in the [platform](/explanations/architecture/platform) logically group nodes, secrets, and dashboards.

You can configure workspaces in two ways:

1. **Dynamically**: Create, update, or delete workspaces using the command line interface. 👉 Suited for ad-hoc workspace management.
2. **Statically**: Pre-define workspaces declaratively in configuration files. 👉 Suited for GitOps-based infrastructure management.

## Manage workspaces dynamically

[Section titled “Manage workspaces dynamically”](#manage-workspaces-dynamically)

The Tenzir Platform CLI allows administrators to create, modify, and delete workspaces on demand.

On-premise setup required

This CLI functionality requires an on-premise platform deployment, available with the [Sovereign Edition](https://tenzir.com/pricing).

Only local platform administrators can manage workspaces dynamically. The [`TENZIR_PLATFORM_OIDC_ADMIN_RULES` variable](/guides/platform-setup/configure-identity-provider) defines who’s an administrator in your platform deployment.

### Creating and managing workspaces

[Section titled “Creating and managing workspaces”](#creating-and-managing-workspaces)

You can create workspaces for either individual users or organizations. When you create a user workspace, it’s automatically configured with access for that specific user. Organization workspaces start with no access rules, giving you full control over who can access them.

For a detailed overview, of the available commands, take a look at our [CLI reference documentation](/reference/platform/command-line-interface).

### Example: Setting up an organization workspace

[Section titled “Example: Setting up an organization workspace”](#example-setting-up-an-organization-workspace)

Let’s walk through creating a workspace for the fictional “Scrooge & Marley Counting House” organization:

1. **Create the organization workspace:**

   ```bash
   tenzir-platform admin create-workspace organization scrooge-marley --name "Scrooge & Marley Counting House"
   ```

   This creates a workspace with the organization ID `scrooge-marley` and the display name “Scrooge & Marley Counting House”.

2. **Configure access for employees with company email addresses:**

   ```bash
   tenzir-platform admin add-auth-rule email-domain <workspace_id> <connection> '@scroogemarley.com'
   ```

3. **Add a specific user:**

   The specific format of the user id depends on the OIDC provider you configured. The provided id must match the `sub` of the generated OIDC token in order to allow access to the workspace.

   ```bash
   tenzir-platform admin add-auth-rule user <workspace_id> 'sub|12345678901'
   ```

4. **Grant access to users with the “accountant” role:**

   ```bash
   tenzir-platform admin add-auth-rule organization-role <workspace_id> <connection> roles accountant organization scrooge-marley
   ```

If you later need to remove the workspace:

```bash
tenzir-platform admin delete-workspace <workspace_id>
```

### Understanding access control

[Section titled “Understanding access control”](#understanding-access-control)

Access rules determine who can enter a workspace. Users gain access to the workspace if they match any configured rule. Think of rules as multiple keys to the same door.

The platform supports various rule types, from allowing everyone to restricting access to specific users or organization members.

For a complete reference of all available authentication rules and their parameters, see the [CLI reference documentation](/reference/platform/command-line-interface#configure-access-rules).

## Define static workspaces

[Section titled “Define static workspaces”](#define-static-workspaces)

You pre-define **static workspaces** declaratively in configuration files. This “as-code” approach differs from the dynamic management approach, which you manage with the [command line interface](/reference/platform/command-line-interface).

Here’s a minimal example of a static workspace configuration:

workspaces.yaml

```yaml
workspaces:
  static0: # Unique workspace identifier
    name: Tenzir # Display name in the UI
    category: Statically Configured Workspaces # Grouping category in the UI
    icon-url: https://storage.googleapis.com/tenzir-public-data/icons/tenzir-logo-square.svg
    auth-rules:
      - { "auth_fn": "auth_allow_all" } # Authentication rule (this allows everyone)
```

The `auth-rules` section defines who can access the workspace. The example above uses `auth_allow_all`. This rule grants access to everyone.

Generating Auth Rules

Use the `print-auth-rule` [CLI](/reference/platform/command-line-interface) command to easily generate auth rules in the correct format:

```bash
tenzir-platform tools print-auth-rule allow-all
```

The `platform` service in a Tenzir Platform deployment uses the `WORKSPACE_CONFIG_FILE` environment variable to locate its static workspace configuration file:

docker-compose.yaml

```yaml
services:
  platform:
    environment:
      # Other environment variables...
      - WORKSPACE_CONFIG_FILE=/etc/tenzir/workspaces.yaml
    volumes:
      # Mount your config file.
      - ./workspaces.yaml:/etc/tenzir/workspaces.yaml
```

This example mounts a local `workspaces.yaml` file into the container. The platform service then accesses it at the location that `WORKSPACE_CONFIG_FILE` specifies.

# Use ephemeral nodes

An **ephemeral node** is ideal for temporary or auto-scaling deployments. It is a temporary node that you do not have to provision manually first, and it disappears from the workspace when the connection to the platform ends.

Using ephemeral nodes requires that you define a *workspace token*, a shared secret that you pass to the node so that it can self-register. You can define a workspace token in your workspace configuration:

workspaces.yaml

```yaml
workspaces:
  static0:
    name: Tenzir
    # Other configuration...
    token: wsk_e9ee76d4faf4b213745dd5c99a9be11f501d7009ded63f2d5NmDS38vXR
```

Caution

Workspace tokens have a specific format. Do not create them manually! Use the `tenzir-platform tools generate-workspace-token` command to create valid tokens, or read the *Workspace Token Format* section below for more details.

For improved security, store the token in a separate file:

workspaces.yaml

```yaml
workspaces:
  static0:
    name: Tenzir
    # Other configuration...
    token-file: /run/secrets/workspace_token
```

This approach works well when you use Docker or Kubernetes secrets.

### Deploy an ephemeral node

[Section titled “Deploy an ephemeral node”](#deploy-an-ephemeral-node)

Note

Workspace tokens require a Tenzir Node v5.1.6 or later.

To spawn an ephemeral node, create a configuration file with the workspace token:

config.yaml

```yaml
tenzir:
  token: wsk_e9ee76d4faf4b213745dd5c99a9be11f501d7009ded63f2d5NmDS38vXR
  platform-control-endpoint: http://tenzir-platform.example.org:3001
```

Then run the node with this configuration:

```bash
tenzir-node --config=config.yaml
```

### Workspace Token Format

[Section titled “Workspace Token Format”](#workspace-token-format)

A valid workspace token starts with the string `wsk_`, continues with 24 bytes of hex-encoded randomness, and ends with the base58-encoded workspace id.

More precisely, the Tenzir Platform CLI generates a workspace token according to the following logic:

```python
import os
import base58


def print_workspace_token(workspace_id: str) -> None:
    base58_workspace_id = base58.b58encode(workspace_id.encode()).decode()
    random_bytes = os.urandom(24).hex()
    print(f"wsk_{random_bytes}{base58_workspace_id}")
```

The `tenzir-platform tools generate-workspace-token` command generates a valid workspace key using exactly this logic. However, if you want to avoid external dependencies, you can use any other tool that prints a string in the format described above.

# Overview

The **Tenzir Platform** acts as a fleet management control plane for Tenzir Nodes. Use its web interface to explore data, create pipelines, and build dashboards.

Sovereign Edition required

This guide shows you how to set up the platform on your own premises with the [Sovereign Edition](https://tenzir.com/pricing). We also offer a hosted cloud version of the platform at [app.tenzir.com](https://app.tenzir.com).

## Services

[Section titled “Services”](#services)

The platform integrates three types of services, as the diagram below illustrates:

![Platform Services](/_astro/platform-services.-sBCic5A_19DKCs.svg)

1. **Internal**: Tenzir provides the (gray) internal services. They are core to the platform’s operation.
2. **External**: You provide the (blue) external services. We do not ship these, so you must bring your own.
3. **Configurable**: You can use our bundled (yellow) configurable services or provide your own.

## Deployment options

[Section titled “Deployment options”](#deployment-options)

There exist two ways to deploy the platform:

1. **Cloud**: Deploy the platform on a cloud provider.
2. **On-premises**: Deploy the platform on your own infrastructure.

### Get started in the cloud

[Section titled “Get started in the cloud”](#get-started-in-the-cloud)

For Amazon, read our [AWS deployment guide](/guides/platform-setup/deploy-on-aws) for an integrated approach to deploying the platform services in your own account using a **CloudFormation** template.

For Azure and GCP, we are still working on providing turnkey deployment setups.

### Get started on your own premises

[Section titled “Get started on your own premises”](#get-started-on-your-own-premises)

Follow these steps to set up the platform on your own premises:

1. [Choose a scenario](/guides/platform-setup/choose-a-scenario): we package several deployment options that match common deployments.

2. Configure the services: you must set up the external API endpoints so nodes and browsers can interact with the platform.

   * [Configure reverse proxy](/guides/platform-setup/configure-reverse-proxy)
   * [Configure internal services](/guides/platform-setup/configure-internal-services)
   * [Configure identity provider](/guides/platform-setup/configure-identity-provider)
   * [Configure database](/guides/platform-setup/configure-database)
   * [Configure blob storage](/guides/platform-setup/configure-blob-storage)

   You may skip some steps depending on your chosen scenario.

3. [Run the platform](/guides/platform-setup/run-the-platform): After you create a configuration, start the platform.

Usage guides

The above guides cover platform deployment. For platform management, see our user-centric guides:

1. [Configure workspaces](/guides/platform-management/configure-workspaces)
2. [Configure dashboards](/guides/platform-management/configure-dashboards)
3. [Use ephemeral nodes](/guides/platform-management/use-ephemeral-nodes)

# Choose a scenario

We provide several examples of possible platform deployment scenarios. Pick one that best suits your needs.

## Download the platform files

[Section titled “Download the platform files”](#download-the-platform-files)

Start by downloading the [latest Tenzir Platform release](https://github.com/tenzir/platform/releases/latest) and unpack the archive.

## Choose a scenario

[Section titled “Choose a scenario”](#choose-a-scenario)

The `examples` directory contains example scenarios in the form of Docker Compose configurations. Choose from the following options:

1. **localdev**: This scenario is designed for local development and testing purposes. It operates as a “Tenzir-in-a-box,” meaning all necessary components are bundled together for a quick and easy setup, making it ideal for single-user exploration and experimentation. Due to its simplified nature and focus on ease of use, it has a low barrier to entry. However, it is not optimized for performance, security, or scalability, making it ill-suited for production environments.
2. **keycloak**: The `keycloak` scenario provides a minimal yet complete setup for multi-user environments. It integrates Keycloak for robust authentication and authorization, enabling secure access for multiple users. This configuration includes all essential Tenzir services and is designed to be readily usable, especially when deployed behind a reverse proxy, which can handle TLS termination and provide an additional layer of security and load balancing.
3. **onprem**: Our `onprem` scenario represents an enterprise-grade deployment. It is structured with the assumption that critical services such as Kafka, S3, and the identity provider (like Keycloak) are already established and managed externally within your on-premises infrastructure. This allows Tenzir to integrate seamlessly into existing robust, production-ready environments, leveraging your organization’s established operational practices and infrastructure investments.

To customize a scenario to your needs, configure the services by populating a `.env` file with your settings as environment variables.

# Configure blob storage

The **blob storage** service exists for exchanging files between the platform and nodes. It facilitates not only downloading data from nodes, but also uploading files from your browser to the platform.

For example, when you click the *Download* button in the Explorer, you see various formats in which you can download the data, such as JSON, CSV, Parquet, and more. Once you initiate the download, a pipeline writes the requested data into the platform’s blob storage. Similarly, when you drag files into the Explorer, you upload them to the blob storage for use as pipeline inputs by nodes.

The following variables control the blob storage service:

```sh
TENZIR_PLATFORM_DOWNLOADS_ENDPOINT=
TENZIR_PLATFORM_INTERNAL_BUCKET_NAME=
TENZIR_PLATFORM_INTERNAL_ACCESS_KEY_ID=
TENZIR_PLATFORM_INTERNAL_SECRET_ACCESS_KEY=
```

When you use S3 or another external blob storage, you must create the bucket and provide a valid access key with read and write permissions on the bucket.

When you use the bundled seaweed instance, you can set these values to arbitrary strings. The bundled Seaweed container automatically provides a bucket with the specified name and access key.

# Configure database

A PostgreSQL database stores the internal state of the platform.

Provide the following environment variables so the platform can connect to the Postgres instance:

```sh
TENZIR_PLATFORM_POSTGRES_USER=
TENZIR_PLATFORM_POSTGRES_PASSWORD=
TENZIR_PLATFORM_POSTGRES_DB=
TENZIR_PLATFORM_POSTGRES_HOSTNAME=
```

# Configure identity provider

The identity provider (IdP) handles authentication for the Tenzir Platform. When you click the *Login* button in the Tenzir UI, the system redirects you to your chosen identity provider, which creates a signed token that certifies your identity.

You can either use an external identity provider that’s already set up inside your organization, or run an identity provider that’s bundled with our example `docker-compose.yaml`.

The Tenzir Platform supports the OpenID Connect (OIDC) protocol for external identity providers. Below, we describe the requirements for using a generic identity provider, as well as two hands-on guides for setting up Keycloak and Entra ID as identity providers for the Tenzir Platform.

## Generic Identity Provider

[Section titled “Generic Identity Provider”](#generic-identity-provider)

To use an external IdP, ensure that it supports the OIDC protocol, including the *OIDC Discovery* extension, and configure it to provide valid RS256 ID tokens.

Set up the external identity provider by creating two clients (also called applications in Auth0, or app registrations in Microsoft Entra) named `tenzir-app` and `tenzir-cli`.

The `tenzir-app` client handles logging into the Tenzir Platform in the web browser.

* You must enable the **Authorization Code** flow.
* The allowed redirect URLs must include `https://app.platform.example/login/oauth/callback`.
* You should note down the client secret so you can add it to the configuration of the Tenzir Platform in the next step.

The `tenzir-cli` client handles authentication with the `tenzir-platform` CLI.

* You must enable the **Device Code** flow.
* The identity provider must either return an `id_token` for the device code flow, or an `access_token` in JWT format.

You may want to run CLI commands in environments where no user is available to perform the device code authorization flow, for example when you run CLI commands as part of a CI job.

In this case, you can set up another client with the **Client Credentials** flow enabled. The `access_token` you obtain from this client must be in JWT format. The CLI automatically attempts to use the client credentials flow if the `TENZIR_PLATFORM_CLI_CLIENT_SECRET` environment variable is set. You can also force the use of the client credentials flow by using the `tenzir-platform auth login --non-interactive` option.

You must provide the following environment variables for the OIDC provider configuration used for logging into the platform:

```sh
TENZIR_PLATFORM_OIDC_PROVIDER_NAME=example-idp
TENZIR_PLATFORM_OIDC_PROVIDER_ISSUER_URL=https://my.idp.example


TENZIR_PLATFORM_OIDC_CLI_CLIENT_ID=tenzir-cli


TENZIR_PLATFORM_OIDC_APP_CLIENT_ID=tenzir-app
TENZIR_PLATFORM_OIDC_APP_CLIENT_SECRET=xxxxxxxxxxxxxxxxxxxxxxxx
```

You must provide the following environment variable containing a JSON object with the OIDC issuer and audiences that the platform should accept. You can also provide an array of objects to configure multiple trusted issuers.

```sh
# Single issuer configuration
TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='{"issuer": "http://platform.local:3004/realms/master","audiences": ["tenzir-app"]}'


# Multiple issuers configuration
TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='[
  {"issuer": "https://accounts.google.com", "audiences": ["audience1"]},
  {"issuer": "http://platform.local:3004/realms/master", "audiences": ["tenzir-app"]}
]'
```

## Keycloak

[Section titled “Keycloak”](#keycloak)

This section explains how to configure Keycloak as an external Identity Provider for use with the Tenzir Platform.

We assume that you already have a Keycloak instance up and running that you can reach with a browser. We provide an example stack of the Tenzir Platform with a bundled Keycloak instance [here](https://github.com/tenzir/platform/tree/main/examples/keycloak). The remainder of this section assumes you’re running this example stack, but the setup is the same for any other Keycloak instance.

### Setting up the Keycloak Instance

[Section titled “Setting up the Keycloak Instance”](#setting-up-the-keycloak-instance)

#### Admin Login

[Section titled “Admin Login”](#admin-login)

Navigate to the Keycloak instance and log in as a user with admin permissions. If you use the bundled Keycloak instance, the initial username is `admin` and password is `changeme`.

Remember to change the default password after logging in for the first time, and to set up 2-factor authentication for your admin user.

#### Create a new `tenzir` realm (optional)

[Section titled “Create a new tenzir realm (optional)”](#create-a-new-tenzir-realm-optional)

Keycloak defaults to the `master` realm and the bundled Docker Compose stack uses this realm by default.

If you want to use a different realm, or if you already have an existing one, update the `TENZIR_PLATFORM_OIDC_ISSUER_URL` variable to point to the new realm instead.

![Configuration 0](/_astro/keycloak-create-realm.Bya4pY8v_R2aSw.webp)

#### Create a client for the app

[Section titled “Create a client for the app”](#create-a-client-for-the-app)

Use the `Add Client` button in the `Clients` menu on the left. Configure the new client as follows:

Under *General settings*, set the client type to *OpenID Connect* and the client ID to `tenzir-app`. If you use a different client ID, remember to update the `TENZIR_PLATFORM_OIDC_PROVIDER_CLIENT_ID` variable in your Tenzir Platform configuration accordingly.

![Configuration 0](/_astro/keycloak-create-app-client-0.B0dL2lBu_Z1GlGsq.webp)

Under *Capability config*, enable client authentication and the *Standard flow* access method.

![Configuration 1](/_astro/keycloak-create-app-client-1.BwnUkGAP_ZBKCgB.webp)

Under *Login settings*, enter a redirect URL that points to `${TENZIR_PLATFORM_DOMAIN}/login/oauth/callback`, where `TENZIR_PLATFORM_DOMAIN` is the domain you configure in your Tenzir Platform configuration. For example, if the app runs under `https://tenzir-app.example.org` then this should be `https://tenzir-app.example.org/login/oauth/callback`.

![Configuration 2](/_astro/keycloak-create-app-client-2.CcGxuXQl_1UJfDb.webp)

Finally, in the client view, go to the Credentials tab and copy the value of the generated client secret. You must add this to the Tenzir Platform configuration under `TENZIR_PLATFORM_OIDC_PROVIDER_CLIENT_SECRET`.

![Configuration 3](/_astro/keycloak-create-app-client-3.nrdSE4hA_ZFjsuw.webp)

### Create a client for the CLI

[Section titled “Create a client for the CLI”](#create-a-client-for-the-cli)

To use the `tenzir-platform` CLI, you need to set up an additional client that supports device code authentication. (It’s possible but not recommended to use the same client for both the Tenzir UI and Tenzir Platform CLI)

To do this, proceed exactly as above, but use `tenzir-cli` as the client ID and under *Capability config* disable the *Client authentication* setting and enable the *OAuth 2.0 Device Authorization Grant* authentication flow.

![Create CLI client](/_astro/keycloak-create-cli-client-0.Bw33IN8V_Z1pKCTe.webp)

## Microsoft Entra Identity

[Section titled “Microsoft Entra Identity”](#microsoft-entra-identity)

To use Microsoft Entra Identity as an OIDC provider, you need to create two app registrations in Entra ID and configure them for use with the Tenzir UI and the Tenzir Platform CLI.

Follow these steps to create the required resources:

1. Navigate to `portal.azure.com` and open the page for “Microsoft Entra ID”.

![Navigate to Entra](/_astro/entra-oidc-navigate.UgOk1iDU_kfeLm.webp)

2. Open the “App registrations” sub page.

![Select App registrations](/_astro/entra-oidc-app-registrations.oSzkSuHl_28LLnA.webp)

3. Create a new registration named `Tenzir Platform CLI`.

![Register CLI](/_astro/entra-oidc-register-cli.Bg_oTrMa_Z1IgiIE.webp)

4. Enable the public client flows for this app.

![Enable public client flows](/_astro/entra-oidc-cli-public-client-flows.s5Ak9_BR_UEIqg.webp)

5. Create a second registration named `Tenzir UI`.

![Register UI](/_astro/entra-oidc-register-ui.jbBJuTxF_1ha3e4.webp)

6. For this registration, open “Certificates & Secrets”.

![Navigate Secrets](/_astro/entra-oidc-create-secret-1.DXZEOI8E_Z1EWunp.webp)

7. Create a new secret and give it a descriptive name.

![Create Secret](/_astro/entra-oidc-create-secret-2.DnIODWhC_ZN2d9t.webp)

8. Make a local copy of the secret value.

![Copy Secret](/_astro/entra-oidc-copy-secret.Ci3SrFI__28yGWS.webp)

9. Copy the client id to your local machine.

![Copy Client ID](/_astro/entra-oidc-client-id.DCKjpMwg_2928iB.webp)

10. Copy the issuer url to your local machine.

![Copy Issuer URL](/_astro/entra-oidc-issuer-url.C08Q45Ve_Z1kjxK6.webp)

Now you can supply the created resources and values to the stack by editing the `.env` file in your Compose folder:

```sh
TENZIR_PLATFORM_OIDC_PROVIDER_NAME="Entra ID"


# The client id of the registration created in step 3.
TENZIR_PLATFORM_OIDC_CLI_CLIENT_ID=082a9391-b645-4278-a16e-3cf54fb1bcf0


# The client id and secret created in steps 8 and 9.
TENZIR_PLATFORM_OIDC_APP_CLIENT_SECRET=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
TENZIR_PLATFORM_OIDC_APP_CLIENT_ID=d8ea5612-6745-47bc-b9fe-5024b1ca18fe


# The issuer url copied in step 10.
TENZIR_PLATFORM_OIDC_PROVIDER_ISSUER_URL=https://login.microsoftonline.com/40431729-d276-4582-abb4-01e21c8b58fe/v2.0
TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='{"issuer": "https://login.microsoftonline.com/40431729-d276-4582-abb4-01e21c8b58fe/v2.0","audiences": ["d8ea5612-6745-47bc-b9fe-5024b1ca18fe", "082a9391-b645-4278-a16e-3cf54fb1bcf0"]}'
```

Now you can restart your Tenzir Platform stack and the system will redirect you to a Microsoft login page when you next log in to the Tenzir UI.

### Additional Configuration for Client Credentials Flow (Optional)

[Section titled “Additional Configuration for Client Credentials Flow (Optional)”](#additional-configuration-for-client-credentials-flow-optional)

If you plan to use the client credentials flow for non-interactive CLI usage, you need to configure additional settings for the CLI registration created in step 3:

11. Navigate to “Expose an API” and add an Application ID URI (you can use the suggested default value).

![Expose API Configuration](/_astro/entra-oidc-expose-api.abcXYbMN_Z1aHdB.webp)

12. Navigate to “Manifest” and update the `requestedAccessTokenVersion` to `2` in the JSON manifest, then save the changes.

![Update Access Token Version](/_astro/entra-oidc-client-api-version.B78ur6Zl_Z2tAea4.webp)

13. When using the client credentials flow with this registration, you must override the scope by setting `TENZIR_PLATFORM_CLI_SCOPE` to `${APPLICATION_ID_URL}/.default`, where `${APPLICATION_ID_URL}` is the Application ID URI you configured in step 11.

## Single-user Mode

[Section titled “Single-user Mode”](#single-user-mode)

In some scenarios handling multiple users is not necessary for the Tenzir Platform. For example for purely local instances used for development or experimentation, or where access is already gated through some external mechanism.

For these situations, the Tenzir Platform offers the single-user mode. In this mode, the Tenzir Platform itself acts as the identity provider, and creates a static, global admin user with no password.

To stress, this means that everyone who can reach the Tenzir Platform will have full access to every Tenzir Node and every workspace, so this is not suitable for generic production deployments.

To use the single-user mode, we recommend starting from [this](https://github.com/tenzir/platform/tree/main/examples/localdev) example setup. This example is already fully configured and can run as-is, skipping the manual configuration described in the sections below.

### Tenzir Platform API

[Section titled “Tenzir Platform API”](#tenzir-platform-api)

In terms of configuration, the `platform` service needs to be put into single-user mode by setting the `TENANT_MANAGER_AUTH__SINGLE_USER_MODE` environment variable to true.

Additionally, the platform needs to have additional configuration that would usually be configured in the external identity provider, in particular the issuer url it should use for signing its own JWTs and the allowed redirect URLs and audiences of the UI and CLI:

```sh
TENANT_MANAGER_AUTH__SINGLE_USER_MODE=true


TENANT_MANAGER_AUTH__ISSUER_URL=http://platform:5000/oidc
TENANT_MANAGER_AUTH__PUBLIC_BASE_URL=${TENZIR_PLATFORM_API_ENDPOINT}/oidc
TENANT_MANAGER_AUTH__APP_AUDIENCE=tenzir-app
TENANT_MANAGER_AUTH__APP_REDIRECT_URLS=${TENZIR_PLATFORM_UI_ENDPOINT}/login/oauth/callback
TENANT_MANAGER_AUTH__CLI_AUDIENCE=tenzir-cli
```

### Tenzir UI

[Section titled “Tenzir UI”](#tenzir-ui)

The Tenzir UI needs to be pointed to the `platform` service as the new identity provider.

Since the single-user mode does not provide true user authentication, the corresponding client secret is also not truly secret but set to the static string “xxxx”.

```sh
PRIVATE_OIDC_PROVIDER_NAME=tenzir
PRIVATE_OIDC_PROVIDER_CLIENT_ID=tenzir-app
PRIVATE_OIDC_PROVIDER_CLIENT_SECRET=xxxx
PRIVATE_OIDC_PROVIDER_ISSUER_URL=http://platform:5000/oidc
```

### Tenzir Platform CLI

[Section titled “Tenzir Platform CLI”](#tenzir-platform-cli)

In single-user mode, the Tenzir Platform CLI can be set to non-interactive mode by adding the `TENZIR_PLATFORM_CLI_CLIENT_SECRET` environment variable.

Note that when running the Tenzir Platform inside a docker compose stack, in single-user mode the CLI must run the same docker compose stack as the platform. When running outside of docker compose, the requirement is that the CLI must be able to establish a network connection to the configured ISSUER\_URL.

```sh
TENZIR_PLATFORM_CLI_CLIENT_ID=tenzir-cli
TENZIR_PLATFORM_CLI_CLIENT_SECRET=xxxx
TENZIR_PLATFORM_CLI_ISSUER_URL=http://platform:5000/oidc
```

## Advanced Topics

[Section titled “Advanced Topics”](#advanced-topics)

### Custom Scopes

[Section titled “Custom Scopes”](#custom-scopes)

By default, the Tenzir Platform requests the `profile email openid offline_access` scopes when you log in. To adjust this, set the `PUBLIC_OIDC_SCOPES` environment variable to a space-separated list of scope names.

### Profile Pictures

[Section titled “Profile Pictures”](#profile-pictures)

To include custom profile pictures, include a `picture` claim in the returned ID token that contains a URL to the image file.

The Tenzir Platform reads that claim and uses it as the profile picture in the top right corner of the user interface, or falls back to a default image if the claim isn’t present.

### Refresh Tokens

[Section titled “Refresh Tokens”](#refresh-tokens)

The Tenzir Platform supports the use of refresh tokens and by default requests the `offline_access` scope to automatically refresh sessions after the initial ID token has expired. To this end, the `offline_access` scope is requested by default.

Unfortunately, the OIDC spec is ambiguous on the precise semantics of the `offline_access` scope, and Keycloak’s interpretation differs from most other OIDC providers: it always includes refresh tokens by default, and adds additional permissions to the token when the `offline_access` scope is requested.

Therefore, some organizations forbid the use of tokens with `offline_access` permissions for security reasons. In that case, add an environment variable `PUBLIC_OIDC_SCOPES=profile email oidc` to the `app` environment to explicitly remove the scope request. The bundled Docker Compose file in this directory does this by default.

### External JWT Authentication

[Section titled “External JWT Authentication”](#external-jwt-authentication)

The Tenzir Platform supports authentication using externally-supplied JWTs, which is useful for scenarios where the platform runs behind a proxy that handles authentication, such as Google Cloud IAP or other enterprise proxies.

To configure external JWT authentication, add the `PRIVATE_JWT_FROM_HEADER` environment variable with the name of the HTTP header that contains the JWT token to the environment of the Tenzir UI:

```sh
PRIVATE_JWT_FROM_HEADER=X-Goog-IAP-JWT-Assertion
```

You must also configure the trusted issuers and audiences using the `TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES` environment variable. This variable accepts a JSON object or array of objects containing the issuer and audiences that the platform should accept:

```sh
# Single issuer configuration
TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='{"issuer": "https://cloud.google.com/iap", "audiences": ["your-audience"]}'


# Multiple issuers configuration
TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='[
  {"issuer": "https://accounts.google.com", "audiences": ["audience1"]},
  {"issuer": "https://cloud.google.com/iap", "audiences": ["audience2"]}
]'
```

# Configure internal services

This guide explains how to configure the three internal Tenzir services: the Tenzir UI, the Tenzir Gateway, and the Tenzir Platform API. These core platform services require internal configuration.

The platform runs as a set of containers in a Docker Compose stack. The [example files](https://github.com/tenzir/platform/tree/main/examples) read configuration parameters from environment variables.

Create a file named `.env` in the same directory as your `docker-compose.yaml` file, and set the environment variables described below.

## Tenzir Platform API

[Section titled “Tenzir Platform API”](#tenzir-platform-api)

This section describes how to configure the Tenzir Platform API.

### Admin rules

[Section titled “Admin rules”](#admin-rules)

The platform distinguishes between *regular* and *admin* users. Admin users can create new workspaces and view, edit, and delete all existing workspaces.

Currently, only admin users can create shared workspaces by creating a new workspace and editing the access rules.

Configure admin users by providing a list of authentication rules. The platform considers any user who matches the provided rules an admin and allows them to use the `tenzir-platform admin` CLI commands.

Use the `tenzir-platform tools print-auth-rule` CLI command to generate valid rules for insertion here.

```sh
# Make everybody an admin.
# Use for deployments with few users who fully trust each other.
TENZIR_PLATFORM_ADMIN_RULES=[{"auth_fn":"auth_allow_all"}]


# Make the two users with IDs `google-oauth2:12345678901` and
# `google-oauth2:12345678902` admins of this platform instance.
TENZIR_PLATFORM_ADMIN_RULES=[{"auth_fn":"auth_user","user_id":"google-oauth2:12345678901"}, {"auth_fn":"auth_user","user_id":"google-oauth2:12345678902"}]
```

### Random seeds

[Section titled “Random seeds”](#random-seeds)

The platform requires random strings for encryption functions. Generate these strings with a secure random number generator and use unique values for each platform instance.

```sh
# Random string to encrypt frontend cookies.
# Generate with `openssl rand -hex 32`.
TENZIR_PLATFORM_INTERNAL_AUTH_SECRET=


# Random string to generate user keys.
# Generate with `openssl rand 32 | base64`.
TENZIR_PLATFORM_INTERNAL_TENANT_TOKEN_ENCRYPTION_KEY=


# Random string for the app to access restricted API endpoints.
# Generate with `openssl rand -hex 32`.
TENZIR_PLATFORM_INTERNAL_APP_API_KEY=
```

### Node blob storage configuration

[Section titled “Node blob storage configuration”](#node-blob-storage-configuration)

Advanced Topic

This is an advanced setting mostly relevant for enterprise use cases. You can most likely skip it when you first set up the Tenzir Platform.

When the Tenzir UI and Tenzir Nodes run in separate networks, the nodes may need to use a different URL to reach the blob storage service from the one the UI uses.

For example, the Tenzir UI may run on a browser of a remote employee, who accesses the configured blob storage via `https://downloads.example.com`. However, the nodes may run in a strictly regulated network that only allows access to the outside world through a reverse proxy. In this example, these nodes would have to talk to `https://downloads.corporate-proxy.example.com`. To allow both sides to use different URLs to talk to the same blob storage service, you can override the public endpoint.

Use the `BLOB_STORAGE__NODES_PUBLIC_ENDPOINT_URL` environment variable to override the URL that nodes use to access S3-compatible blob storage. Note that you need to add this variable to the `platform` container directly, since it currently doesn’t appear in any of the example setups:

```sh
# Override the blob storage URL for nodes
BLOB_STORAGE__NODES_PUBLIC_ENDPOINT_URL=https://s3.internal.example.com
```

This setting is particularly useful when:

* Nodes run in a separate network from the Tenzir UI
* Internal DNS resolution differs between the UI and node networks
* You need to route node traffic through specific network paths

## Tenzir UI

[Section titled “Tenzir UI”](#tenzir-ui)

This section describes how to configure the Tenzir Platform UI.

### Internal proxy

[Section titled “Internal proxy”](#internal-proxy)

Advanced Topic

This is an advanced setting mostly relevant for enterprise use cases. You can most likely skip it when you first set up the Tenzir Platform.

By default, the Tenzir UI sends requests directly from the browser to the Tenzir Gateway to query the status of connected nodes. This setup can cause problems in some network topologies, especially in zero-trust networks where accessing the Tenzir Gateway requires additional authentication headers.

Enable the internal proxy to route these requests through the Tenzir UI backend instead, which makes all requests from the Tenzir UI frontend go to the same domain. This approach adds some latency through an additional proxy hop.

```sh
TENZIR_PLATFORM_USE_INTERNAL_WS_PROXY=true
```

### External JWT authentication

[Section titled “External JWT authentication”](#external-jwt-authentication)

Advanced Topic

This is an advanced setting mostly relevant for enterprise use cases. You can most likely skip it when you first set up the Tenzir Platform.

The Tenzir Platform supports accepting externally-supplied JWTs from HTTP headers, which is useful when running behind authentication proxies like Google Cloud IAP or other enterprise authentication systems.

Configure the `app` service to accept JWTs from a specific HTTP header. Note that you need to add this variable to the `app` container directly, since it currently doesn’t appear in any of the example setups:

```sh
# Accept JWTs from the specified HTTP header
PRIVATE_JWT_FROM_HEADER=X-Goog-IAP-JWT-Assertion
```

When you set this variable, the platform bypasses the normal OIDC login flow and instead validates the JWT token provided in the specified header.

You must also configure the trusted issuers and audiences to match the values that the external JWT contains using the `TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES` environment variable for the Tenzir Platform API.

For more information about external JWT configuration, see the [external JWT authentication section](/guides/platform-setup/configure-identity-provider#external-jwt-authentication) in the identity provider configuration guide.

## Tenzir Gateway

[Section titled “Tenzir Gateway”](#tenzir-gateway)

The Tenzir Gateway doesn’t offer any user-controlled settings at the moment.

# Configure reverse proxy

The platform uses four distinct entry points with four different URLs:

1. **Tenzir UI**: The URL your browser connects to, e.g., `app.platform.example`. This serves the web frontend where you interact with the platform.
2. **Tenzir Gateway**: The URL nodes connect to, e.g., `nodes.platform.example`. Tenzir Nodes connect to this URL to establish long-running WebSocket connections.
3. **Blob Storage**: The URL where the platform’s S3-compatible blob storage is accessible, e.g., `downloads.platform.example`. When you click the *Download* button, the platform generates download links under this URL. When you drag files into the explorer window, the platform stores them here so that Tenzir Nodes can access them.
4. **Platform API**: The URL the Tenzir Platform CLI connects to, e.g., `api.platform.example`.

The typical setup for the Tenzir Platform uses a reverse proxy to terminate TLS connections and forward incoming traffic to the correct services. The choice of technology varies with your deployment scenario and ranges from an additional [Traefik](https://doc.traefik.io/traefik/getting-started/install-traefik/) container within the Docker Compose stack, over an nginx instance running on the same host, to global load balancing services offered by a commercial cloud provider.

In our example scenarios, set the following environment variables to configure these four endpoints:

```sh
# The domain under which the platform frontend is reachable.
# Must include the `http://` or `https://` scheme.
TENZIR_PLATFORM_UI_ENDPOINT=https://app.platform.example


# The endpoint to which Tenzir nodes should connect.
# Must include the `ws://` or `wss://` scheme.
TENZIR_PLATFORM_NODES_ENDPOINT=wss://nodes.platform.example


# The URL at which the platform's S3-compatible blob storage is accessible.
TENZIR_PLATFORM_DOWNLOADS_ENDPOINT=https://downloads.platform.example


# The URL at which the platform API is accessible.
TENZIR_PLATFORM_API_ENDPOINT=https://api.platform.example
```

## Native TLS

[Section titled “Native TLS”](#native-tls)

A reverse proxy typically runs on the same host as other platform containers and terminates TLS. When you can’t guarantee that the reverse proxy runs on the same host as other platform containers, or when you deploy the containers to different machines, enable native TLS support for individual platform containers.

### Obtaining Certificates

[Section titled “Obtaining Certificates”](#obtaining-certificates)

Purchase the domain name you want for the platform and use one of the globally trusted certificate authorities (CAs) to obtain a valid certificate. This approach provides the most straightforward and recommended certificate acquisition method.

If you run the platform in a private or air-gapped network, use methods like the DNS challenge offered by [Let’s Encrypt](https://letsencrypt.org/) and other providers to generate a certificate and transfer it to the target machine.

When you can’t use a globally trusted CA, use a corporate root CA instead. Naturally, certificates from this CA will only be trusted inside your organization or your Tenzir Platform setup.

If you don’t possess a corporate root CA, create a private CA for yourself. Signing and provisioning root certificates is a complex task, so we recommend using a tool like `trustme` for this purpose.

We provide a [sample script](https://github.com/tenzir/platform/tree/main/examples/native-tls) that shows how to create the necessary certificates for all components.

Certificate Validation Idiosyncrasies

Some libraries ignore the system-wide CA certificate store and use alternative, more strictly curated bundles. For example, Mozilla’s NSS root store is a popular choice. Additionally, the operating system’s default certificate bundles shipped within our Docker containers won’t trust private CAs by default. Therefore, when you use a private CA, perform the same configuration for corporate root CAs from a publicly trusted CA or your self-created private CA.

### Self-signed Certificates

[Section titled “Self-signed Certificates”](#self-signed-certificates)

Instead of creating a private CA, create a self-signed certificate that combines certificate and CA in a single file.

This approach simplifies setup and management compared to a private CA, but reduces security guarantees. For example, it nullifies several TLS security guarantees and provides only protection against passive eavesdropping.

Below we assume you store valid TLS certificates in files named `ssl/app-cert.pem`, `ssl/platform-cert.pem`, etc., where each file contains both the TLS certificate and private key. If you store the certificate and private key in separate files, mount both into the containers and adjust the environment variables to point towards the correct file.

When you use a private CA, store the public key of that CA in the file `ssl/ca.pem`.

#### Tenzir UI

[Section titled “Tenzir UI”](#tenzir-ui)

To have the Tenzir UI serve its traffic using TLS, add the following environment variables and volumes to your `docker-compose.yaml`:

```yaml
services:
  app:
    environment:
      - TLS_CERTFILE=/ssl/app-cert.pem
      - TLS_KEYFILE=/ssl/app-cert.pem


    volumes:
      - ./ssl/app-cert.pem:/ssl/app-cert.pem
```

When you use a private CA, add the following configuration:

```yaml
services:
  app:
    environment:
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt


    volumes:
      - ./ssl/ca.pem:/etc/ssl/certs/ca-certificates.crt
```

#### Tenzir Gateway

[Section titled “Tenzir Gateway”](#tenzir-gateway)

To enable TLS serving for the gateway, mount the certificate into the container and set the `TLS_CERTFILE` and `TLS_KEYFILE` environment variables:

```yaml
services:
  websocket-gateway:
    environment:
      - TLS_CERTFILE=/ssl/gateway-cert.pem
      - TLS_KEYFILE=/ssl/gateway-cert.pem
    volumes:
      - ./ssl/gateway-cert.pem:/ssl/gateway-cert.pem
```

When you use a private CA, add the following configuration:

```yaml
services:
  websocket-gateway:
    environment:
      - TLS_CAFILE=/ssl/ca.pem
    volumes:
      - ./ssl/ca.pem:/ssl/ca.pem
```

#### Platform API

[Section titled “Platform API”](#platform-api)

To enable TLS serving for the Platform API, mount the certificate into the container and set the `TLS_CERTFILE` and `TLS_KEYFILE` environment variables. This follows the same process as for the `websocket-gateway` container:

```yaml
services:
  platform:
    environment:
      - TLS_CERTFILE=/ssl/platform-cert.pem
      - TLS_KEYFILE=/ssl/platform-cert.pem
    volumes:
      - ./ssl/platform-cert.pem:/ssl/platform-cert.pem
```

When you use a private CA, add the following configuration:

```yaml
services:
  platform:
    environment:
      # 'requests' uses a baked-in CA bundle, so point it to our CA explicitly.
      - REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt
      - SSL_CERT_FILE=/ssl/ca.pem
    volumes:
      - ./ssl/ca.pem:/etc/ssl/certs/ca-certificates.crt
```

### Example

[Section titled “Example”](#example)

Refer to our [native TLS example](https://github.com/tenzir/platform/tree/main/examples/native-tls) for a complete configuration example, including the native TLS setup for the default bundled database, S3 storage and IdP services.

## Node TLS Settings

[Section titled “Node TLS Settings”](#node-tls-settings)

Nodes connect to the platform using the Tenzir Gateway via a TLS connection. When using a custom certificate for the gateway, provide it to the node to successfully establish a connection.

Set the following option to point to a CA certificate file in PEM format without password protection:

```sh
TENZIR_PLATFORM_CACERT=/path/to/ca-certificate.pem
```

When using a self-signed TLS certificate, additionally disable TLS certificate validation:

```sh
TENZIR_PLATFORM_SKIP_PEER_VERIFICATION=true
```

TLS: Platform vs. pipeline

These settings apply only to the connection that the node establishes with the platform, not to any TLS connections that pipelines may create within the node.

# Configure secret store

The Tenzir Platform provides a secret store for each workspace. All Tenzir Nodes connected to the workspace can access its secrets. You can manage secrets using the CLI or the web interface. Alternatively, you can use an external secret store.

Read more about how secrets work in our [explanations page](/explanations/secrets).

## Configuring the platform secret store

[Section titled “Configuring the platform secret store”](#configuring-the-platform-secret-store)

### Managing Secrets via the CLI

[Section titled “Managing Secrets via the CLI”](#managing-secrets-via-the-cli)

To add a new secret to the Platform’s secret store:

Add value to the Platform's secret store

```sh
tenzir-platform secret add geheim --value=1528F9F3-FAFA-45B4-BC3C-B755D0E0D9C2
```

Refer to the [CLI reference](/reference/platform/command-line-interface#manage-secrets) for more details on updating or deleting secrets.

### Managing Secrets via Web Interface

[Section titled “Managing Secrets via Web Interface”](#managing-secrets-via-web-interface)

To manage secrets from the web interface, go to the `Workspace Settings` screen by clicking the gear icon in the workspace selector.

![Screenshot](/_astro/secrets.Dd085Pjn_ZqXKpL.webp)

## Configuring External Secret Stores

[Section titled “Configuring External Secret Stores”](#configuring-external-secret-stores)

You can configure the Tenzir Platform to provide access to secrets stored in an external secret store instead of using it own store. This access is read-only.

### AWS Secrets Manager

[Section titled “AWS Secrets Manager”](#aws-secrets-manager)

To add AWS Secrets Manager as an external secret store, use the CLI:

Add AWS Secrets Manager as external secret store

```sh
tenzir-platform secret store add aws \
  --region='eu-west-1' \
  --assumed-role-arn='arn:aws:iam::1234567890:role/tenzir-platform-secrets-access' \
  --prefix=tenzir/
```

* The Tenzir Platform must have permissions to read secrets under the specified prefix from the external store.
* The platform must be able to assume the specified role in AWS.

See the [CLI reference](/reference/platform/command-line-interface#manage-external-secret-stores) for more details.

# Deploy on AWS

This guide shows you how to deploy the Tenzir Platform Sovereign Edition on AWS using a CloudFormation template. The deployment automates the setup of all required infrastructure components.

AWS Marketplace

You need a subscription to the [Tenzir Platform - Sovereign Edition](https://aws.amazon.com/marketplace/pp?sku=59c6z7kbdidospdl2al3f8i5e) in the AWS Marketplace.

## Architecture overview

[Section titled “Architecture overview”](#architecture-overview)

The deployment creates a complete platform infrastructure with the following components:

![Platform AWS Architecture](/_astro/aws-architecture.CTZmMWF-_19DKCs.svg)

The architecture consists of:

* **Frontend services**: Web UI (App Runner), Platform API (Lambda), and API Gateway for custom domains
* **Gateway service**: ECS Fargate tasks behind an Application Load Balancer for node connections
* **Data layer**: RDS PostgreSQL for platform state, S3 buckets for blob and sidepath data
* **Security**: Cognito for authentication, Secrets Manager for credentials, ACM for TLS certificates
* **Networking**: VPC with public/private subnets, NAT Gateway for outbound connectivity
* **DNS**: Route53 for domain management with automatic subdomain creation

Deployment architecture

This deployment mirrors our setup at [app.tenzir.com](https://app.tenzir.com) and includes publicly-accessible UI and nodes endpoints. OIDC login protects these endpoints. For a fully VPC-private deployment, contact us—we’ll help you set it up.

## Prerequisites

[Section titled “Prerequisites”](#prerequisites)

Before you begin, you need:

1. **AWS Account**: An AWS account with permissions to create CloudFormation stacks, VPCs, ECS services, Lambda functions, RDS databases, and other resources.
2. **Domain in Route53**: A registered domain name with a Route53 hosted zone in your AWS account. The deployment automatically discovers your hosted zone and creates DNS records for the platform services.
3. **Marketplace Subscription**: Subscribe to the Tenzir Platform - Sovereign Edition in the [AWS Marketplace](https://aws.amazon.com/marketplace).
4. **AWS CLI** (optional): Install and configure the [AWS CLI](https://aws.amazon.com/cli/) if you prefer CLI-based deployment.

## Step 1: Configure your domain

[Section titled “Step 1: Configure your domain”](#step-1-configure-your-domain)

The platform requires a custom domain. Create a Route53 hosted zone for your domain if you don’t have one.

* AWS Console

  1. Open the [Route53 console](https://console.aws.amazon.com/route53/)
  2. Click **Create hosted zone**
  3. Enter your domain name (e.g., `example.org`)
  4. Select **Public hosted zone**
  5. Click **Create hosted zone**
  6. Update your domain’s nameservers at your domain registrar with the Route53 nameservers that appear in the hosted zone

* AWS CLI

  Create a hosted zone for your domain:

  ```bash
  aws route53 create-hosted-zone \
    --name example.org \
    --caller-reference $(date +%s)
  ```

  Retrieve the nameservers for your hosted zone:

  ```bash
  aws route53 list-resource-record-sets \
    --hosted-zone-id $(aws route53 list-hosted-zones-by-name \
      --dns-name example.org \
      --query 'HostedZones[0].Id' \
      --output text) \
    --query "ResourceRecordSets[?Type=='NS'].ResourceRecords[*].Value" \
    --output text
  ```

  Update your domain’s nameservers at your domain registrar with the Route53 nameservers that this command returns.

The CloudFormation template automatically discovers the hosted zone based on the domain name you provide.

## Step 2: Prepare container images

[Section titled “Step 2: Prepare container images”](#step-2-prepare-container-images)

The platform requires three core service images (UI, API, and Gateway) in your AWS account. You must copy the container images from the AWS Marketplace to your ECR repositories before deploying the CloudFormation stack.

Why copy images?

AWS Marketplace vendors store container images in their own ECR repositories. Your AWS services (ECS, Lambda, App Runner) cannot directly pull from these cross-account repositories. Copying the images to your ECR gives your services the necessary access permissions to run the platform components.

Check your version

Replace `VERSION` with the version you subscribed to in AWS Marketplace. Using an incorrect version will cause the image pull to fail. Check your marketplace subscription or contact Tenzir support to confirm the latest version.

* Automated script

  Use this script to create the ECR repositories and copy the container images from the AWS Marketplace to your ECR repositories:

  ```bash
  #!/bin/bash
  set -e


  # IMPORTANT: Set this to your Marketplace subscription version
  VERSION="${VERSION:-REPLACE_WITH_YOUR_VERSION}"  # e.g., v1.0.0, v1.1.0, etc.
  AWS_REGION="${AWS_REGION:-$(aws configure get region)}"
  AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text)


  # Verify VERSION is set
  if [ "$VERSION" = "REPLACE_WITH_YOUR_VERSION" ]; then
    echo "ERROR: You must set VERSION to match your Marketplace subscription"
    echo "Example: VERSION=v1.0.0 ./copy-images.sh"
    exit 1
  fi


  MARKETPLACE_REGISTRY="709825985650.dkr.ecr.us-east-1.amazonaws.com"
  LOCAL_REGISTRY="$AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com"


  IMAGES=(
    "tenzir/tenzir-platform-ui"
    "tenzir/tenzir-platform-api"
    "tenzir/tenzir-platform-gateway"
  )


  echo "Creating ECR repositories..."
  for IMAGE in "${IMAGES[@]}"; do
    aws ecr create-repository --repository-name "$IMAGE" 2>/dev/null || echo "Repository $IMAGE already exists"
  done


  echo "Copying Tenzir Platform images version $VERSION to $LOCAL_REGISTRY"


  # Login to marketplace ECR
  aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $MARKETPLACE_REGISTRY


  # Login to local ECR
  aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $LOCAL_REGISTRY


  # Copy each image
  for IMAGE in "${IMAGES[@]}"; do
    echo "Copying $IMAGE:$VERSION..."
    SOURCE_IMAGE="$MARKETPLACE_REGISTRY/$IMAGE:$VERSION"
    TARGET_IMAGE="$LOCAL_REGISTRY/$IMAGE:$VERSION"
    LATEST_IMAGE="$LOCAL_REGISTRY/$IMAGE:latest"


    docker pull "$SOURCE_IMAGE"
    docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
    docker push "$TARGET_IMAGE"
    docker tag "$TARGET_IMAGE" "$LATEST_IMAGE"
    docker push "$LATEST_IMAGE"
  done


  echo "Successfully copied all images!"
  ```

  Save this script to a file (e.g., `copy-images.sh`), make it executable, and run it with your version:

  ```bash
  chmod +x copy-images.sh
  VERSION=v1.0.0 ./copy-images.sh  # Replace v1.0.0 with your subscription version
  ```

  The script copies the three core service container images to your ECR repositories. This process typically takes 10-15 minutes depending on your network speed.

* Manual steps

  Follow these steps to manually create the ECR repositories and copy the images.

  ### Create ECR repositories

  [Section titled “Create ECR repositories”](#create-ecr-repositories)

  Create three ECR repositories using the AWS Console:

  1. Open the [Amazon ECR console](https://console.aws.amazon.com/ecr/)

  2. Click **Create repository**

  3. Enter the repository name: `tenzir/tenzir-platform-ui`

  4. Leave other settings as default

  5. Click **Create repository**

  6. Repeat for the remaining repositories:

     * `tenzir/tenzir-platform-api`
     * `tenzir/tenzir-platform-gateway`

  ### Copy images from Marketplace

  [Section titled “Copy images from Marketplace”](#copy-images-from-marketplace)

  After creating the repositories, copy the images using Docker:

  ```bash
  # IMPORTANT: Replace with your Marketplace subscription version
  export VERSION=REPLACE_WITH_YOUR_VERSION  # e.g., v1.0.0
  export AWS_REGION=$(aws configure get region)
  export AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text)


  # Login to Marketplace ECR
  aws ecr get-login-password --region us-east-1 | \
    docker login --username AWS --password-stdin \
    709825985650.dkr.ecr.us-east-1.amazonaws.com


  # Login to your ECR
  aws ecr get-login-password --region $AWS_REGION | \
    docker login --username AWS --password-stdin \
    $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com


  # Copy each image
  for IMAGE in ui api gateway; do
    docker pull 709825985650.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION
    docker tag 709825985650.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION \
      $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION
    docker push $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION
    docker tag $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION \
      $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:latest
    docker push $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:latest
  done
  ```

  This process typically takes 10-15 minutes depending on your network speed.

## Step 3: Deploy the CloudFormation stack

[Section titled “Step 3: Deploy the CloudFormation stack”](#step-3-deploy-the-cloudformation-stack)

After copying the images, deploy the CloudFormation stack using our publicly hosted template.

* AWS Console

  1. Open the [CloudFormation console](https://console.aws.amazon.com/cloudformation/)

  2. Click **Create stack** → **With new resources**

  3. Select **Amazon S3 URL** and enter:

     ```plaintext
     https://tenzir-marketplace-resources.s3.eu-west-1.amazonaws.com/tenzir-platform.yml
     ```

  4. Click **Next**

  5. Enter a stack name (e.g., `tenzir-platform`)

  6. Configure the parameters:

     * **Domain Name**: Your domain (e.g., `example.org`)
     * **Use Random Subdomain**: Whether to add a random subdomain prefix (`false` for production)
     * **UI Container Image**: Full image URI with the `latest` tag you pushed in Step 2 (e.g., `123456789012.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-ui:latest`)
     * **API Container Image**: Full image URI with the `latest` tag you pushed in Step 2 (e.g., `123456789012.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-api:latest`)
     * **Gateway Container Image**: Full image URI with the `latest` tag you pushed in Step 2 (e.g., `123456789012.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-gateway:latest`)
     * **Demo Node Container Image**: (Optional) Full image URI. Leave as default to use the public demo node image.
     * **Use External OIDC Provider**: Whether to use an external identity provider instead of [AWS Cognito](https://aws.amazon.com/cognito/) (`false` to use Cognito)

  7. Click **Next** through the remaining screens

  8. On the final screen, check the box to acknowledge that CloudFormation will create IAM resources

  9. Click **Create stack**

  The `latest` tags resolve to the Marketplace version you copied in Step 2. Re-run the copy step whenever you update to a newer release.

  The stack creation takes approximately 15-20 minutes.

* AWS CLI

  Create the stack with your configuration:

  ```bash
  # Use the tag you pushed in Step 2 (default: latest)
  IMAGE_TAG=latest
  AWS_REGION=$(aws configure get region)
  AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text)


  aws cloudformation create-stack \
    --stack-name tenzir-platform \
    --template-url https://tenzir-marketplace-resources.s3.eu-west-1.amazonaws.com/tenzir-platform.yml \
    --parameters \
      ParameterKey=DomainName,ParameterValue=example.org \
      ParameterKey=RandomSubdomain,ParameterValue=false \
      ParameterKey=UseExternalOIDC,ParameterValue=false \
      ParameterKey=ContainerImageUI,ParameterValue=$AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-ui:$IMAGE_TAG \
      ParameterKey=ContainerImageAPI,ParameterValue=$AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-api:$IMAGE_TAG \
      ParameterKey=ContainerImageGateway,ParameterValue=$AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-gateway:$IMAGE_TAG \
    --capabilities CAPABILITY_NAMED_IAM
  ```

  Update these values:

  * Set `example.org` to your domain name.
  * Set `IMAGE_TAG` to a specific version tag if you prefer to pin the deployment.

  The stack creation takes approximately 15-20 minutes.

### Parameter details

[Section titled “Parameter details”](#parameter-details)

The template accepts these key parameters:

* **Domain Configuration**:

  * `DomainName`: Your base domain name (required)
  * `RandomSubdomain`: Add a random subdomain prefix. For example, with `example.org`, this creates `ui.abc123.example.org` instead of `ui.example.org` (default: `false`)

* **Container Images**:

  * `ContainerImageUI`: Full URI for the UI container image (required)
  * `ContainerImageAPI`: Full URI for the API container image (required)
  * `ContainerImageGateway`: Full URI for the Gateway container image (required)
  * `ContainerImageNode`: Full URI for the Demo Node container image (optional, defaults to public ghcr.io image)

* **Authentication Configuration**:

  * `UseExternalOIDC`: Use external OIDC instead of Cognito (default: `false`)

  * External OIDC requires these additional parameters:

    * `ExternalOIDCIssuerURL`: OIDC issuer URL
    * `ExternalOIDCClientID`: OIDC client ID
    * `ExternalOIDCClientSecret`: OIDC client secret

### What gets deployed

[Section titled “What gets deployed”](#what-gets-deployed)

The CloudFormation template creates a complete platform infrastructure:

#### Networking

[Section titled “Networking”](#networking)

* VPC with public and private subnets across multiple availability zones
* Internet Gateway and NAT Gateway for outbound connectivity
* Security groups for each service
* VPC endpoints for AWS services (Secrets Manager, ECR, S3, STS)

#### Compute

[Section titled “Compute”](#compute)

* **ECS Cluster**: Runs the gateway service as a Fargate task
* **Lambda Function**: Hosts the platform API
* **App Runner Service**: Hosts the web UI
* **Application Load Balancer**: Routes traffic to the gateway service

#### Storage

[Section titled “Storage”](#storage)

* **RDS PostgreSQL Database**: Stores platform state and metadata
* **S3 Buckets**: Store blob data and sidepath data

#### Security & Authentication

[Section titled “Security & Authentication”](#security--authentication)

* **AWS Cognito User Pool** (optional): Provides authentication with a default admin user
* **Secrets Manager**: Stores database credentials and encryption keys
* **IAM Roles**: Provide least-privilege access for each service
* **ACM Certificates**: Automatically provision SSL/TLS certificates for your domain

#### DNS & Routing

[Section titled “DNS & Routing”](#dns--routing)

* **Route53 DNS Records**: Create `api.`, `ui.`, and `nodes.` subdomains
* **API Gateway**: Provides a custom domain for the Lambda-based API

## Step 4: Access the platform

[Section titled “Step 4: Access the platform”](#step-4-access-the-platform)

Once the stack creation completes, retrieve the platform URLs and credentials from the CloudFormation outputs:

```bash
aws cloudformation describe-stacks \
  --stack-name tenzir-platform \
  --query 'Stacks[0].Outputs'
```

The outputs include:

* **UIDomain**: The web UI URL (e.g., `https://ui.example.org`)
* **APIDomain**: The API URL (e.g., `https://api.example.org`)
* **AdminUsername**: Default admin username (when using Cognito)
* **AdminInitialPassword**: Initial admin password (when using Cognito, stored in Secrets Manager)
* **OIDCProviderType**: The authentication provider type (`cognito` or `external`)

### Access the web UI

[Section titled “Access the web UI”](#access-the-web-ui)

1. Navigate to the UI URL in your browser
2. Log in with the admin credentials (if using Cognito)
3. If using Cognito, the system prompts you to change your password on first login

### Retrieve the admin password

[Section titled “Retrieve the admin password”](#retrieve-the-admin-password)

If using Cognito, retrieve the admin password from the CloudFormation outputs:

```bash
aws cloudformation describe-stacks \
  --stack-name tenzir-platform \
  --query 'Stacks[0].Outputs[?OutputKey==`AdminInitialPassword`].OutputValue' \
  --output text
```

## Next steps

[Section titled “Next steps”](#next-steps)

After deploying the platform:

1. **Change the admin password**: If using Cognito, change the default admin password on first login
2. **Configure users**: Add additional users through the Cognito User Pool or your external OIDC provider
3. **Connect nodes**: Deploy Tenzir Nodes and connect them to the platform using the gateway endpoint
4. **Configure workspaces**: Organize your nodes and pipelines into workspaces

## Troubleshooting

[Section titled “Troubleshooting”](#troubleshooting)

### Certificate validation

[Section titled “Certificate validation”](#certificate-validation)

ACM certificates require DNS validation. The template automatically creates the necessary DNS records in Route53, but validation can take 5-30 minutes. Monitor the certificate status in the [ACM console](https://console.aws.amazon.com/acm/).

### Service health

[Section titled “Service health”](#service-health)

Monitor service health in the AWS Console:

* **ECS**: Check the gateway service status
* **Lambda**: Check the API function logs in CloudWatch
* **App Runner**: Check the UI service status and logs
* **RDS**: Verify database connectivity

### Stack deletion

[Section titled “Stack deletion”](#stack-deletion)

To delete the stack and all resources:

```bash
aws cloudformation delete-stack --stack-name tenzir-platform
```

Caution

Empty the S3 buckets before deleting the stack. CloudFormation cannot delete buckets that contain data. Manually empty the blobs and sidepath buckets first.

## Cost considerations

[Section titled “Cost considerations”](#cost-considerations)

The deployed infrastructure incurs AWS costs. Key cost factors include:

* **RDS Database**: db.t3.micro instance (adjustable in the template)
* **NAT Gateway**: Data processing charges apply
* **App Runner**: Pay-per-use based on compute and memory
* **Lambda**: Pay-per-invocation
* **ECS Fargate**: Pay-per-task
* **Data Transfer**: Outbound data transfer charges

For production deployments, consider reserved instances or savings plans to reduce costs.

# Run the platform

## Set up Docker registry access

[Section titled “Set up Docker registry access”](#set-up-docker-registry-access)

As part of your Sovereign Edition distribution, we provided you with an authentication token (`YOUR_DOCKER_TOKEN` below) to fetch the Docker images.

Log in with the token as follows:

```bash
echo YOUR_DOCKER_TOKEN | docker login ghcr.io -u tenzir-distribution --password-stdin
```

You are now ready to pull the images.

## Start the platform

[Section titled “Start the platform”](#start-the-platform)

Once you have configured all platform services and created a `docker-compose.yaml`, start the platform in the foreground with

```sh
docker compose up
```

or in the background with `docker compose up --detach`.

❯ docker compose up

```text
[+] Running 5/5
 ✔ Container compose-app-1                Running
 ✔ Container compose-websocket-gateway-1  Running
 ✔ Container compose-seaweed-1            Running
 ✔ Container compose-platform-1           Running
Attaching to app-1, platform-1, postgres-1, seaweed-1, websocket-gateway-1
platform-1           | {"event": "connecting to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.205616Z"}
platform-1           | {"event": "connected to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.210667Z"}
platform-1           | {"event": "created table", "level": "info", "ts": "2024-04-10T10:13:20.210883Z"}
platform-1           | {"event": "connecting to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.217700Z"}
platform-1           | {"event": "connected to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.221194Z"}
platform-1           | {"event": "creating a table", "level": "info", "ts": "2024-04-10T10:13:20.221248Z"}
platform-1           | {"event": "connecting to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.221464Z"}
platform-1           | {"event": "connected to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.224226Z"}
app-1                | Listening on 0.0.0.0:3000
websocket-gateway-1  | {"event": "connecting to postgres", "level": "debug", "ts": "2024-04-10T10:15:37.033632Z"}
websocket-gateway-1  | {"event": "connected to postgres", "gtlevel": "debug", "ts": "2024-04-10T10:15:37.038510Z"}
websocket-gateway-1  | {"event": "created table", "level": "info", "ts": "2024-04-10T10:15:37.042555Z"}
websocket-gateway-1  | {"host": "0.0.0.0", "port": 5000, "common_env": {"base_path": "", "tenzir_proxy_timeout": 60.0}, "local_env": {"store": {"postgres_uri": "postgresql://postgres:postgres@postgres:5432/platform"}, "tenant_manager_app_api_key": "d3d185cc4d9a1bde0e07e24c2eb0bfe9d2726acb3a386f8882113727ac6e90cf", "tenant_manager_tenant_token_encryption_key": "CBOXE4x37RKRLHyUNKeAsfg8Tbejm2N251aKnBXakpU="}, "event": "HTTP server running", "level": "info", "ts": "2024-04-10T10:15:37.045244Z"}
...
```

It may take up to a minute for all services to be fully available.

## Update the platform

[Section titled “Update the platform”](#update-the-platform)

To update to the latest platform version, pull the latest images:

```sh
docker compose pull
```

# Quickstart

Drowning in logs, alerts, and rigid tools? Meet **Tenzir**—your engine for taming security data. In just a few minutes, you'll be ingesting, transforming, and enriching data on your terms, with full control. Here's what you'll accomplish:

1. Use Tenzir instantly
2. Deploy your first pipeline
3. See results in action

## Prerequisites

[Section titled “Prerequisites”](#prerequisites)

You need zero infrastructure to get started—just a browser and access to [app.tenzir.com](https://app.tenzir.com).

It helps if you have basic familiarity with logs or security telemetry, but it's not required.

## Setup & deploy a demo node

[Section titled “Setup \&amp; deploy a demo node”](#setup--deploy-a-demo-node)

Visit [app.tenzir.com](https://app.tenzir.com), sign in to [create a free account](/guides/account-creation), and you'll see this page:

![Landing page](/_astro/new-account.Qul5lKOD_VK4oR.webp)

Begin with deploying a node:

1. Select the **Cloud-hosted demo-node** tab.
2. Click **Add node**.
3. Click **Get Started**.

Spinning up a demo node can take up to 2 minutes. Sorry for the wait, we'll cut down this time soon! Grab a coffee or learn more about the [key Tenzir concepts](/explanations/architecture) while we get everything set up. ☕

## Get started with demo data

[Section titled “Get started with demo data”](#get-started-with-demo-data)

Our demo nodes have the Demo Node [package](/explanations/packages) pre-installed, giving you sample pipelines that fetch data from a public cloud bucket and [store it into the node's edge storage](/guides/edge-storage/import-into-a-node). Once you have the sample data in the node, it will be simpler to work with it.

The **Packages** tab shows that the demo node package is installed:

![Demo node package installed](/_astro/demo-node-packages.B7a3szfC_Z2eXOXM.webp)

When you go back to the **Pipelines** tab, you see the pipelines churning away:

![Nodes page after demo node package](/_astro/demo-node-pipelines.B3TjVIac_1MXejo.webp)

Note the two new pipelines that import data into our demo node. If you click on them, a context pane opens on the right and you'll see details about their activity as well as the their definition.

Tenzir Query Language (TQL)

One major difference to other pipeline tools is that Tenzir comes with a full-fledged streaming execution engine and domain-specific language to process data. TQL is a powerful dataflow language similar to Splunk's SPL or Microsoft's KQL, just with a lot more power when it comes to transforming data.

Also check out our [FAQ entry](/explanations/faqs/#why-did-you-create-a-new-query-language-why-not-sql) for why we did not opt for SQL.

## Explore the demo data

[Section titled “Explore the demo data”](#explore-the-demo-data)

The first step in understanding new data sources is getting a sense of their structural complexlity, or simply put, how messy or clean the data is. Let's take a taste of the demo data. Click the **Explorer** tab and [run](/guides/basic-usage/run-pipelines) this pipeline:

```tql
export
taste
```

This pipelines does the following: [`export`](/reference/operators/export) references all data in the node's edge storage, and [`taste`](/reference/operators/taste) samples 10 events of every unique schema. You'll now see Explorer filling up with events.

![Getting a taste](/_astro/demo-node-export-taste.DKQLkuB8_2mLjT6.webp)

Auto-completion of pipelines

If you look carefully, you'll note that the above pipeline doesn't end with an *output operator*. Just an input (`export`) and a transformation (`taste`). But for a [pipeline](/explanations/architecture/pipeline) to be well-formed, it must have at least one input and one output. Otherwise data would leak somewhere! So what's happening? 🤔

If you don't write an output operator in the Explorer, the platform auto-completes the [`serve`](/reference/operators/serve) operator, turning your pipeline into a tiny REST API, so that platform can extract events from it and show them to you in the browser.

Also note the **Schemas** pane. It gives you an overview of how heterogeneous the data is. Click on a schema to zoom into all events having the same shape. Later, you'll learn to normalize the data to make it more homogeneous by reducing the number of unique schemas.

Now click on a row in the results table. The **Inspector** pops up for a vertical view of the event, which can be helpful for seeing the full structure of an event, especially for wide schemas with many fields.

One more detail: you can **uncollapse nested records** in the results table by clicking on the column name. This switches from the record-style display to more of a data-frame-style view, allowing you to see more data at once.

## Reshape data at ease

[Section titled “Reshape data at ease”](#reshape-data-at-ease)

Now that you have a rough understanding of our cockpit, let's wrangle that data. This is what we've designed TQL for, so it should be fun—or at least more fun compared to other tools.

Begin with selecting a subset of available schemas:

```tql
export
where @name.starts_with("zeek")
```

Here we filter on event metadata, starting with `@`. The special `@name` field is a string that contains the name of the event. Actually, let's hone in on the connection logs only, once for [Zeek](https://zeek.org) and once for [Suricata](https://suricata.io):

* Zeek

  ```tql
  export
  where @name == "zeek.conn"
  set src_endpoint = {
    ip: id.orig_h,
    port: id.orig_p,
  }
  set dst_endpoint = {
    ip: id.resp_h,
    port: id.resp_p,
  }
  select src_endpoint, dst_endpoint, protocol=proto
  set @name = "flow"
  ```

  ```tql
  {
    src_endpoint: {
      ip: 89.248.165.145,
      port: 43831
    },
    dst_endpoint: {
      ip: 198.71.247.91,
      port: 52806
    },
    protocol: "tcp"
  }
  ```

* Suricata

  ```tql
  export
  where @name == "suricata.flow"
  set src_endpoint = {
    ip: src_ip,
    port: src_port,
  }
  set dst_endpoint = {
    ip: dest_ip,
    port: dest_port,
  }
  select src_endpoint, dst_endpoint, protocol=proto.to_lower()
  set @name = "flow"
  ```

  ```tql
  {
    src_endpoint: {
      ip: 10.0.0.167,
      port: 51666
    },
    dst_endpoint: {
      ip: 97.74.135.10,
      port: 993
    },
    protocol: "tcp"
  }
  ```

A few notes:

* The [`set`](/reference/operators/set) operator performs an assignment and creates new fields.
* Because `set` is *the* most frequently used operator, it is "implied" and you just write `x = y` instead of `set x = y`. We generally recommend doing so and write it out only out for didactic reasons.
* You can use `set` to assign schema names, e.g., `@name = "new-schema-name"`.
* [`select`](/reference/operators/select) selects the fields to keep, but also supports an assignment to rename the new field in one shot.
* As you can see in the `select` operator (Suricata tab) above, TQL expressions have [functions](/reference/functions) like [`to_lower`](/reference/functions/to_lower), which makes working with values a breeze.

![Reshaping Suricata flow logs](/_astro/demo-node-reshape-flow.DGbYIpBh_ZSMfAP.webp)

Now what do you do with this normalized data from these two data sources? It just has a new shape, so what? Read on, we'll show you next.

## Composing pipelines via publish/subscribe

[Section titled “Composing pipelines via publish/subscribe”](#composing-pipelines-via-publishsubscribe)

The above example starts with a specific input operator (`export`) and no output operator (we used the Explorer). This is useful for explorative data analysis, but in practice you'd want these sorts of transformations to run continuously. In fact, what you really want is a streaming pipeline that accepts data, potentially from multiple sources, and exposes its results in a way so that you can route it to multiple destinations.

To this end, nodes have a publish/subscribe feature, allowing you to efficiently connect pipelines using static topics (and very soon dynamic routes). The [`publish`](/reference/operators/publish) and [`subscribe`](/reference/operators/subscribe) operators are all you need for this. The typical pipeline pattern for composable pipelines looks like this:

```tql
subscribe "in"
// transformations go here
publish "out"
```

Let's adapt our transformation pipelines from above:

* Zeek

  ```tql
  subscribe "zeek"
  where @name == "zeek.conn"
  set src_endpoint = {
    ip: id.orig_h,
    port: id.orig_p,
  }
  set dst_endpoint = {
    ip: id.resp_h,
    port: id.resp_p,
  }
  select src_endpoint, dst_endpoint, protocol=proto
  publish "flow"
  ```

* Suricata

  ```tql
  subscribe "suricata"
  where @name == "suricata.flow"
  set src_endpoint = {
    ip: src_ip,
    port: src_port,
  }
  set dst_endpoint = {
    ip: dest_ip,
    port: dest_port,
  }
  select src_endpoint, dst_endpoint, protocol=proto.to_lower()
  publish "flow"
  ```

When clicking the **Run** button for these pipeline, the events will *not* show up in the Explorer because we now use `publish` as output operator. Instead, you'll see this deployment modal:

![Deployment modal](/_astro/demo-node-deploy-modal.C8YJw_nJ_Z1xvfub.webp)

After you give the pipeline a name (or leave it blank for a dummy name), click **Confirm** to deploy the pipeline. You'll see it popping up on the **Pipelines** tab:

![New pipeline](/_astro/demo-node-pipelines-new.BSYbUO4v_22Vvze.webp)

Now that you've deployed one pipeline with two topics as its "interface," you can direct data to it from other pipelines. For example, you can create a pipeline that accepts logs via [Syslog](/integrations/syslog) and forwards them to the transformation pipeline. Then you can write two more pipelines that each take a subset to implement split-routing scenario.

Data fabric as network of pipelines

You've probably heard the buzzy term *data fabric* before. With Tenzir, you implement a data fabric as a network of pipelines, potentially distributed over multiple nodes. With our growing library of [packages](/explanations/packages) to 1-click-deploy pre-packaged pipelines, and of course AI to generate pipelines, you can roll out sophisticated use cases without the need for dedicated data engineers.

## What's Next?

[Section titled “What\&#39;s Next?”](#whats-next)

You've just scratched the surface. Here's where to go next:

1. Explore the [Library](https://app.tenzir.com/library) and browse through packages of pre-built pipelines.
2. [Visualize pipeline insights and build dashboards](/tutorials/plot-data-with-charts)
3. [Map your logs to OCSF](/tutorials/map-data-to-ocsf)
4. Send events to your data lake, such as [Amazon Security Lake](/integrations/amazon/security-lake)

Curious how it all fits together? Brush up on the [Tenzir architecture](/explanations/architecture) to learn more about all moving parts. We're here to help. Join us at our friendly [Tenzir Discord](/discord) if you have any questions.

# Add custom runners

Runners tell `tenzir-test` how to execute a discovered file. This guide shows you how to register the XXD runner from the example project so you can compare binary artifacts by dumping their hexadecimal representation with `xxd`.

## Prerequisites

[Section titled “Prerequisites”](#prerequisites)

* Complete the setup in [write tests](/guides/testing/write-tests) so you have a project root with `runners/` and at least one passing test.
* Install the [`xxd`](https://man.archlinux.org/man/xxd.1.en) utility. It ships with most platforms.

## Step 1: Prepare the runners package

[Section titled “Step 1: Prepare the runners package”](#step-1-prepare-the-runners-package)

Create `runners/__init__.py` if it does not exist yet. The harness imports this module automatically on start-up.

```sh
touch runners/__init__.py
```

## Step 2: Implement the XXD runner

[Section titled “Step 2: Implement the XXD runner”](#step-2-implement-the-xxd-runner)

Copy the runner from the example project and keep it under version control so teammates can use the same convention. The latest reference implementation lives in [example-project/runners/xxd.py](https://github.com/tenzir/test/blob/main/example-project/runners/xxd.py).

If you prefer to start from a scaffold, drop the following template into `runners/xxd.py` and fill in the `TODO` notes where your implementation needs to differ (for example when you call a different tool or emit additional artifacts).

runners/xxd.py

```python
from __future__ import annotations


from pathlib import Path


from tenzir_test import runners
from tenzir_test.runners._utils import get_run_module




class XxdRunner(runners.ExtRunner):
    """Hexdump runner that turns *.xxd files into reference artifacts."""


    def __init__(self) -> None:
        super().__init__(name="xxd", ext="xxd")


    def run(self, test: Path, update: bool, coverage: bool = False) -> bool:
        del coverage  # this runner does not integrate with LLVM coverage


        run_mod = get_run_module()
        passthrough = run_mod.is_passthrough_enabled()


        # 1. Prepare the command. Adjust flags or the executable for your tool.
        cmd = ["xxd", "-g1", str(test)]
        # TODO: Replace "xxd" or tweak arguments when you wrap a different command.


        try:
            completed = run_mod.run_subprocess(
                cmd,
                capture_output=not passthrough,
            )
        except FileNotFoundError:
            run_mod.report_failure(test, "└─▶ xxd is not available on PATH")
            return False


        if completed.returncode != 0:
            # 2. Surface a readable error message and bail out early.
            run_mod.report_failure(test, f"└─▶ xxd exited with {completed.returncode}")
            return False


        if passthrough:
            # 3. Passthrough runs stop after executing the command.
            run_mod.success(test)
            return True


        output = completed.stdout or b""


        # 4. Update reference artifacts when requested.
        ref_path = test.with_suffix(".txt")
        if update:
            ref_path.write_bytes(output)
            run_mod.success(test)
            return True


        if not ref_path.exists():
            run_mod.report_failure(test, f"└─▶ Missing reference file {ref_path}")
            return False


        # 5. Compare against the baseline and print a diff on mismatch.
        expected = ref_path.read_bytes()
        if expected != output:
            run_mod.report_failure(test, "")
            run_mod.print_diff(expected, output, ref_path)
            return False


        run_mod.success(test)
        return True




runners.register(XxdRunner())
```

Finally, expose the runner from `runners/__init__.py` so the harness picks it up on start-up:

runners/\_\_init\_\_.py

```python
"""Project runners."""


# Import bundled runners so they register on package import.
from . import xxd  # noqa: F401


__all__ = ["xxd"]
```

## Step 3: Add a hexdump test

[Section titled “Step 3: Add a hexdump test”](#step-3-add-a-hexdump-test)

Create a directory for the new tests and add a sample input string.

```sh
mkdir -p tests/hex
cat <<EOD > tests/hex/hello.xxd
Hello Tenzir!
EOD
```

## Step 4: Capture the reference output

[Section titled “Step 4: Capture the reference output”](#step-4-capture-the-reference-output)

Run the harness in update mode so it generates the expected hexdump next to the `.xxd` file.

```sh
uvx tenzir-test --update
```

The command produces `tests/hex/hello.txt` similar to the following snippet:

```text
00000000: 48 65 6c 6c 6f 20 54 65 6e 7a 69 72 21 0a        Hello Tenzir!.
```

Subsequent runs without `--update` rerun `xxd` and compare the fresh dump with the stored baseline.

Pass `--debug` when you want inline runner and fixture details together with the comparison activity. Use `--summary` if you prefer the tabular breakdown and failure tree at the end, or set `TENZIR_TEST_DEBUG=1` in CI to enable the same diagnostics without passing the flag explicitly.

## Step 5: Reuse the runner across projects

[Section titled “Step 5: Reuse the runner across projects”](#step-5-reuse-the-runner-across-projects)

Keep the runner in your template repository or internal tooling so other projects can copy it verbatim. Use `runners.register_alias("xxd-hexdump", "xxd")` when you prefer a more descriptive name in frontmatter.

When you invoke the harness with multiple projects in the same command, pass `--all-projects` so the root project executes alongside the satellites. The positional paths you list after the flags form the **selection**; in this case it usually only names satellite roots. `--all-projects` opts the root back in, its runners load first, and satellites reuse them automatically. That makes it simple to keep shared runners (like `xxd`) in a central project while satellite projects focus on their own tests.

## Next steps

[Section titled “Next steps”](#next-steps)

* Pair the runner with fixtures that download or generate binary artifacts before each test.
* Use directory-level `test.yaml` files or per-test frontmatter to set `inputs:` when the runner should read data from a different directory than the project default.
* Extend the runner to emit `*.diff` artifacts when the hexdumps diverge.
* Branch on `run.get_harness_mode()` or `run.is_passthrough_enabled()` when you need bespoke behaviour for passthrough runs, but prefer to rely on `run.run_subprocess()` for most cases so output handling stays consistent.
* Review the [test framework reference](/reference/test-framework/) to explore additional runner hooks and helpers.

# Create fixtures

Fixtures let `tenzir-test` prepare external services before a scenario runs and clean everything up afterwards. In this guide you build an HTTP echo fixture from scratch, wire it into the harness, and exercise it with a TQL test that posts a JSON payload via [`http`](/reference/operators/http).

## Prerequisites

[Section titled “Prerequisites”](#prerequisites)

* Follow [write tests](/guides/testing/write-tests) to scaffold a project and install `tenzir-test`.
* Make sure your project root already contains `fixtures/`, `inputs/`, and `tests/` directories (they can be empty).

## Step 1: Expose a fixtures package

[Section titled “Step 1: Expose a fixtures package”](#step-1-expose-a-fixtures-package)

`tenzir-test` imports `fixtures/__init__.py` automatically. Import the modules that host your fixtures so their decorators run as soon as the package loads.

fixtures/\_\_init\_\_.py

```python
"""Project fixtures."""


from . import http  # noqa: F401  (side effect: register fixture)


__all__ = ["http"]
```

## Step 2: Implement the HTTP echo fixture

[Section titled “Step 2: Implement the HTTP echo fixture”](#step-2-implement-the-http-echo-fixture)

Create `fixtures/http.py` with a tiny HTTP server that echoes POST bodies. The fixture yields the server URL with `@fixture()` and tears the server down inside its `finally` block.

fixtures/http.py

```python
from __future__ import annotations


import threading
from http import HTTPStatus
from http.server import BaseHTTPRequestHandler, ThreadingHTTPServer
from typing import Iterator


from tenzir_test import fixture




class EchoHandler(BaseHTTPRequestHandler):
    def do_POST(self) -> None:  # noqa: N802
        stated_length = self.headers.get("Content-Length", "0")
        try:
            length = int(stated_length)
        except ValueError:
            length = 0
        body = self.rfile.read(length) if length else b"{}"
        self._reply(body or b"{}")


    def log_message(self, *_: object) -> None:  # noqa: D401
        return  # Keep the console quiet.


    def _reply(self, payload: bytes) -> None:
        self.send_response(HTTPStatus.OK)
        self.send_header("Content-Type", "application/json")
        self.send_header("Content-Length", str(len(payload)))
        self.end_headers()
        self.wfile.write(payload)




@fixture()
def http() -> Iterator[dict[str, str]]:
    server = ThreadingHTTPServer(("127.0.0.1", 0), EchoHandler)
    worker = threading.Thread(target=server.serve_forever, daemon=True)
    worker.start()


    try:
        port = server.server_address[1]
        url = f"http://127.0.0.1:{port}/"
        yield {"HTTP_FIXTURE_URL": url}
    finally:
        server.shutdown()
        worker.join()
```

## Step 3: Author a test that uses the fixture

[Section titled “Step 3: Author a test that uses the fixture”](#step-3-author-a-test-that-uses-the-fixture)

Create `tests/http/echo-read.tql` and request the `http` fixture in the frontmatter (or rely on `tests/http/test.yaml`). The pipeline creates sample data and then crafts a HTTP request body using the [`http`](/reference/operators/http) operator. The fixture echoes the payload back into the result stream.

```tql
---
fixtures: [http]
---


from {x: 42, y: "foo"}
http env("HTTP_FIXTURE_URL"), body=this
```

Fixtures receive the same per-test scratch directory as the scenario itself via `TENZIR_TMP_DIR`. Use it to stage temporary files or logs that should disappear after the run. Launch the harness with `--keep` when you want to retain those artifacts for debugging.

## Step 4: Capture the reference output

[Section titled “Step 4: Capture the reference output”](#step-4-capture-the-reference-output)

Run the harness in update mode so it records the HTTP response next to the test.

```sh
uvx tenzir-test --update
```

You should now have `tests/http/echo-read.txt` with the echoed payload:

```tql
{
  x: 42,
  y: "foo",
}
```

Subsequent runs without `--update` bring the fixture online, hit the server, and compare the live response against the baseline.

Need to inspect the fixture while it runs? Append `--debug` to emit lifecycle logs together with comparison targets. In CI-only scenarios set `TENZIR_TEST_DEBUG=1` to enable the same diagnostics without passing the flag at runtime, and add `--summary` if you also need the tabular breakdown and failure tree afterwards.

### Stabilise flaky fixture scenarios

[Section titled “Stabilise flaky fixture scenarios”](#stabilise-flaky-fixture-scenarios)

Occasionally a fixture-backed test may need a couple of tries—for example when a service takes slightly longer to initialise. Add `retry` to the frontmatter to let the harness rerun the test before giving up:

```yaml
---
fixtures: [http]
retry: 4
---
```

The number is the total attempt budget (four in the example above). Intermediate attempts stay quiet and the final log line reports how many tries were required. Treat this as a temporary safety net and investigate persistent flakes; long retry chains mask underlying race conditions.

## Step 5: Iterate on the fixture

[Section titled “Step 5: Iterate on the fixture”](#step-5-iterate-on-the-fixture)

With the echo workflow in place you can:

* Serve canned responses from files under `inputs/` for more realistic scenarios, or redirect `TENZIR_INPUTS` with an `inputs:` entry in `test.yaml` when the data lives elsewhere.
* Add environment keys (for example `HTTP_FIXTURE_TOKEN`) so tests can assert on authentication behavior.
* Harden the cleanup in your `finally` block (or factor it into a helper) when fixtures manage external processes, containers, or cloud resources.
* Reuse the fixture from satellite projects. When you call `tenzir-test --root main-project --all-projects satellite-a`, the selection only names the satellite root, so `--all-projects` keeps the main project in the run. The satellite automatically imports the root project’s fixtures before loading its own. If a satellite needs specialized behavior, it can register an additional fixture with a new name while still leaning on the shared implementations.

Check the [test framework reference](/reference/test-framework/) for more fixture APIs, including helpers that tell you which fixtures the current test requested. In Python-mode tests you can call `fixtures()` to inspect the selection (for example `fixtures().http` returns `True` when the fixture is active).

## Step 6: Share the fixture across multiple tests

[Section titled “Step 6: Share the fixture across multiple tests”](#step-6-share-the-fixture-across-multiple-tests)

Suites keep a fixture alive for several tests. Declare one in a directory-level `test.yaml`; every descendant test joins automatically.

tests/http/test.yaml

```yaml
suite: smoke-http
fixtures: [http]
timeout: 45
```

With that configuration the harness:

* Starts the `http` fixture once, runs all members in lexicographic order, and tears the fixture down afterwards.
* Pins the suite to a single worker while still running different suites (and standalone tests) in parallel when `--jobs` permits it.
* Keeps policies such as `fixtures`, `timeout`, and `retry` in the directory defaults. Tests inside the suite cannot set `suite`, `fixtures`, or `retry` in their own frontmatter; everything else (for example `inputs:` or additional metadata) remains overridable per file. Outside of suites you can still use frontmatter for any of those keys.

Run the directory that owns the suite when you want to focus on it:

```sh
uvx tenzir-test tests/http
```

Selecting a single file inside that suite now fails fast with a descriptive error. This keeps the lifecycle predictable and avoids partially exercised fixtures.

## Manual controllers

[Section titled “Manual controllers”](#manual-controllers)

Python-mode tests often need fine-grained control over fixture lifecycle to simulate crashes, restarts, or configuration changes. The manual controller API builds on the same registration flow shown above and lets you call directly into the fixture from a test. The harness preloads common helpers (including `fixtures()` for quick introspection and `acquire_fixture()` for manual control) so you can use them without imports.

```python
# runner: python
# timeout: 30


import fixtures


with acquire_fixture("http") as http:
    env = http.env              # provided by the fixture's start()
    client = Executor.from_env(env)


    # Exercise the system under test while the fixture runs
    client.run("status")


# Restart explicitly when you need another lifecycle
http = acquire_fixture("http")
http.start()
http.stop()
```

Key ideas:

* `acquire_fixture(name)` returns a controller that wraps the registered fixture factory. Calling `start()` (or using `with acquire_fixture(...) as controller:`) enters the context manager lazily and stores the returned environment mapping on `controller.env` so you can reuse it between calls.
* `with acquire_fixture(...)` is shorthand for `start()`/`stop()`; call those methods manually when you need to restart or interleave multiple lifecycle steps.
* The Python runner serialises the active test context into `TENZIR_PYTHON_FIXTURE_CONTEXT`; the helpers in `tenzir_test.fixtures` consume it automatically so manual controllers see the same configuration as the declarative flow.
* Fixtures advertise optional operations by returning a `FixtureHandle` with extra callables (for example `kill`, `restart`, or `configure`). Treat these hooks as part of the fixture contract—assert their presence (or rely on type hints) when your test depends on them so failures are immediate and obvious.
* `Executor.from_env(env)` and similar helpers make it straightforward to reuse the active fixture environment in client calls while the fixture is running; in Python-mode tests these helpers are imported for you automatically.
* For stricter typing you can import lightweight `Protocol` definitions (e.g. `SupportsKill`) and cast controllers once a fixture documents the hooks it exposes.

The declarative workflow (`fixtures: [http]` combined with automatic activation) remains the default. Manual controllers complement it when tests need imperative control over setup and teardown—skip the frontmatter entry when you prefer to start and stop the fixture yourself from Python code.

### Fixture contracts

[Section titled “Fixture contracts”](#fixture-contracts)

The controller API works best when fixture authors and consumers agree on the available hooks.

**Fixture author**

```python
import signal


@fixture()
def node():
    process = _start_node()


    def _kill(signal_no: int = signal.SIGTERM) -> None:
        process.send_signal(signal_no)


    return FixtureHandle(
        env=_make_env(process),
        teardown=lambda: _stop_node(process),
        hooks={"kill": _kill},
    )
```

**Test author**

```python
import signal


node = acquire_fixture("node")
node.start()


assert hasattr(node, "kill"), "node fixture must expose kill() for this test"
node.kill(signal.SIGKILL)
node.stop()
```

If the fixture stops exporting `kill`, the assertion fails immediately so the test never produces misleading results.

# Write tests

This guide walks you through creating a standalone repository for integration tests, wiring it up to [`tenzir-test`](/reference/test-framework), and running your first scenarios end to end. You will create a minimal project structure, add a pipeline test, record reference output, and rerun the harness to make sure everything passes.

## Prerequisites

[Section titled “Prerequisites”](#prerequisites)

* A working installation of Tenzir. Place the `tenzir` and `tenzir-node` binaries on your `PATH`, or be ready to pass explicit paths to the harness.
* Python 3.12 or later. The `tenzir-test` package distributes as a standard Python project.
* [`uv`](https://docs.astral.sh/uv/) or `pip` to install Python dependencies.

## Step 1: Scaffold a project

[Section titled “Step 1: Scaffold a project”](#step-1-scaffold-a-project)

Create a clean directory that holds nothing but integration tests and their shared assets. The harness treats this directory as the **project root**.

```sh
mkdir demo
cd demo
```

## Step 2: Check the harness

[Section titled “Step 2: Check the harness”](#step-2-check-the-harness)

Run the harness through `uvx` to make sure the tooling works without setting up a virtual environment. `uvx` downloads and caches the latest release when needed.

```sh
uvx tenzir-test --help
```

If the command succeeds, you’re ready to add tests.

## Step 3: Add shared data

[Section titled “Step 3: Add shared data”](#step-3-add-shared-data)

Populate `inputs/` with artifacts that tests will read. The example below stores a short NDJSON dataset that models a few alerts.

```json
{"id": 1, "severity": 5, "message": "Disk usage above 90%"}
{"id": 2, "severity": 2, "message": "Routine backup completed"}
{"id": 3, "severity": 7, "message": "Authentication failure on admin"}
```

Save the snippet as `inputs/alerts.ndjson`.

## Step 4: Author a pipeline test

[Section titled “Step 4: Author a pipeline test”](#step-4-author-a-pipeline-test)

Create your first scenario under `tests/`. The harness discovers tests recursively, so you can organize them by feature or risk level. Here, you create `tests/high-severity.tql`.

tests/high-severity.tql

```tql
from_file f"{env("TENZIR_INPUTS")}/alerts.ndjson"
where severity >= 5
project id, message
sort id
```

The harness also injects a unique scratch directory into `TENZIR_TMP_DIR` while each test executes. Use it for transient files you do not want under version control; pass `--keep` when you run `tenzir-test` if you need to inspect the generated artifacts afterwards.

### Stream raw output while iterating

[Section titled “Stream raw output while iterating”](#stream-raw-output-while-iterating)

During early iterations you may want to inspect command output before you record reference artifacts. Enable *passthrough mode* via `--passthrough` (`-p`) to pipe the `tenzir` process output directly to your terminal while the harness still provisions fixtures and environment variables:

```sh
uvx tenzir-test --passthrough tests/high-severity.tql
```

The harness enforces the exit code but skips comparisons, letting you decide when to capture the baseline with `--update`.

## Step 5: Capture the reference output

[Section titled “Step 5: Capture the reference output”](#step-5-capture-the-reference-output)

Run the harness once in update mode to execute the pipeline and write the expected output next to the test.

```sh
uvx tenzir-test --update
```

The command produces `tests/high-severity.txt` with the captured stdout.

```json
{"id":1,"message":"Disk usage above 90%"}
{"id":3,"message":"Authentication failure on admin"}
```

Review the reference file, adjust the pipeline if needed, and rerun `--update` until you are satisfied with the results. Commit the `.tql` test and `.txt` baseline together so future runs can compare against known-good output.

## Step 6: Rerun the tests

[Section titled “Step 6: Rerun the tests”](#step-6-rerun-the-tests)

After you check in the reference output, execute the tests *without* `--update`. The harness verifies that the actual output matches the baseline.

```sh
uvx tenzir-test
```

When the output diverges, the harness prints a diff and returns a non-zero exit code. Use `--debug` to see comparison targets alongside the usual harness diagnostics. For CI-only visibility you can set `TENZIR_TEST_DEBUG=1`. Add `--summary` when you also want the tabular breakdown and failure tree at the end.

### Retry flaky tests (sparingly)

[Section titled “Retry flaky tests (sparingly)”](#retry-flaky-tests-sparingly)

If a scenario fails intermittently, add a `retry` entry to its frontmatter so the harness reruns it before flagging a failure. The value is the **total** attempt budget:

```yaml
---
retry: 3
---
```

With `retry: 3`, the test runs up to three times. Intermediate attempts stay quiet; the final result line includes `attempts=3/3` (or the actual number on a success). Use this as a guardrail while you investigate the underlying flake and keep the budget small to avoid masking issues.

### Run multiple projects together

[Section titled “Run multiple projects together”](#run-multiple-projects-together)

Large organisations often split tests across several repositories but still want an aggregated run. List additional project directories after `--root` and add `--all-projects` to execute the root alongside its satellites under a single invocation. Those positional paths form the selection; here it only names the satellite project:

```sh
uvx tenzir-test --root example-project --all-projects ../example-satellite
```

The root project (`example-project` above) supplies the shared fixtures and runners. Satellites inherit those definitions, can register their own helpers, and run their tests in isolation. Because the selection only listed the satellite, `--all-projects` keeps the root in scope. The CLI prints a compact summary showing how many tests each project contributes and which runners are involved. Add `--summary` when you prefer the tabular breakdown and detailed failure listing after each project.

## Step 7: Introduce a fixture

[Section titled “Step 7: Introduce a fixture”](#step-7-introduce-a-fixture)

Fixtures let you bootstrap external resources and expose their configuration through environment variables. Add a simple `node`-driven test to exercise a running Tenzir node.

Create `tests/node/ping.tql` with the following contents:

```tql
---
fixtures: [node]
timeout: 10
---


// Get the version from the running node.
remote {
  version
}
```

Because the test needs a node to run, include the built-in `node` fixture and give it a reasonable timeout. The fixture starts `tenzir-node`, injects connection details into the environment, and tears the process down after the run. Capture the baseline via `--update` just like before.

The fixture launches `tenzir-node` from the directory that owns the test file, so `tenzir-node.yaml` placed next to the scenario can refer to files with relative paths (for example `../inputs/alerts.ndjson`).

### Reuse fixtures with suites

[Section titled “Reuse fixtures with suites”](#reuse-fixtures-with-suites)

When several tests should share the same fixture lifecycle, promote their directory to a **suite**. Add `suite:` to the directory’s `test.yaml` and keep the fixture selection alongside the other defaults:

tests/http/test.yaml

```yaml
suite: smoke-http
fixtures: [http]
timeout: 45
retry: 2
```

Key behaviour:

* Suites are directory-scoped. Once a `test.yaml` declares `suite`, every test in that directory *and its subdirectories* joins automatically. Move the scenarios that should remain independent into a sibling directory.
* Suites run sequentially on a single worker. The harness activates the shared fixtures once, executes members in lexicographic order of their relative paths, and tears the fixtures down afterwards. Other suites (and standalone tests) still run in parallel when `--jobs` allows it.
* Per-test frontmatter cannot introduce `suite`, and suite members may not define their own `fixtures` or `retry`. Keep those policies in the directory defaults so every member agrees on the shared lifecycle. Outside a suite, frontmatter can still set `fixtures`, `retry`, or `timeout` as before.
* Tests can override other keys (for example `inputs:` or additional metadata) on a per-file basis when necessary.

Run the `http` directory that defines the suite when you iterate on it:

```sh
uvx tenzir-test tests/http
```

Selecting a single file inside that suite fails fast with a descriptive error, which keeps the fixture lifecycle predictable and prevents partial runs from leaving shared state behind.

### Drive fixtures manually

[Section titled “Drive fixtures manually”](#drive-fixtures-manually)

When you switch to the Python runner you can drive fixtures manually. The controller API makes it easy to start, stop, or even crash the same `node` fixture inside a single test:

```python
# runner: python
# fixtures: [node]


import signal


# Context-manager style: `with` automatically calls `start()` and `stop()` on
# the fixture.
with acquire_fixture("node") as node:
    tenzir = Executor.from_env(node.env)
    tenzir.run("remove { version }")  # talk to the running node


# Without the context manager, you need to call `start()` and `stop()` manually.
node.start()
Executor.from_env(node.env).run("version")
node.stop()
```

This imperative style complements the declarative `fixtures: [node]` flow and is especially useful for fault-injection scenarios. The harness preloads helpers like `acquire_fixture`, `Executor`, and `fixtures()`, so Python-mode tests can call them directly.

When you restart the same controller, the node keeps using the state and cache directories it created during the first `start()`. Those paths (exported via `TENZIR_NODE_STATE_DIRECTORY` and `TENZIR_NODE_CACHE_DIRECTORY`) live inside the test’s scratch directory by default and are cleaned up automatically when the controller goes out of scope. Acquire a fresh controller when you need a brand new workspace.

## Step 8: Organize defaults with `test.yaml`

[Section titled “Step 8: Organize defaults with test.yaml”](#step-8-organize-defaults-with-testyaml)

As suites grow, you can extract shared configuration into directory-level defaults. Place a `tests/node/test.yaml` file with convenient settings:

```yaml
fixtures: [node]
timeout: 120
# Optional: reuse datasets that live in tests/data/ instead of the project root.
inputs: ../data
```

The harness merges this mapping into every test under `tests/node/`. Relative paths resolve against the directory that owns the YAML file, so `inputs: ../data` points at `tests/data/`. Individual files still override keys in their frontmatter when necessary.

## Step 9: Automate runs

[Section titled “Step 9: Automate runs”](#step-9-automate-runs)

Once the suite passes locally, integrate it into your CI pipeline. Configure the job to install Python 3.12, install `tenzir-test`, provision or download the required Tenzir binaries, and execute `uvx tenzir-test --root .`. For reproducible results, keep your datasets small and deterministic, and prefer fixtures that wipe state between runs.

## Next steps

[Section titled “Next steps”](#next-steps)

You now have a project that owns its inputs, tests, fixtures, and baselines. From here you can:

* Add custom runners under `runners/` when you need specialized logic around `tenzir` invocations.
* Build Python fixtures that publish or verify data through the helper APIs in `tenzir_test.fixtures`.
* Explore coverage collection by passing `--coverage` to the harness.

Refer back to the [test framework reference](/reference/test-framework/) whenever you need deeper details about runners, fixtures, or configuration knobs.

# Overview

A Tenzir **integration** is a specific way to integrate with a third-party tool or technology.

All integrations rely on [pipelines](/explanations/architecture/pipeline) in some form. For some applications, there exist *dedicated* operators, e.g., the [Splunk](/integrations/splunk) integration using the [`to_splunk`](/reference/operators/to_splunk) output operator. Integrating with other applications means merely using an existing *generic* operator, such as the [`http`](/reference/operators/http) operator to [fetch data from APIs](/guides/data-loading/fetch-data-from-apis).

Often, integrations with tools end up as a parameterizable [package](/explanations/packages) in our freely available [library on GitHub](https://github.com/tenzir/library). In particular, packages can also provide [enrichment contexts](/explanations/enrichment/) and accompanying pipelines that periodically load external data into the context. Several packages that provide threat intelligence feeds in our library follow this pattern.

![Integrations](/_astro/integrations.COlNcjp-_19DKCs.svg)

# Overview

Tenzir integrates with the services from [Amazon Web Services (AWS)](https://aws.amazon.com) listed below.

## Configuration

[Section titled “Configuration”](#configuration)

To interact with AWS services, you need to provide appropriate credentials. This defaults to using AWS’s [default credentials provider chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html).

Make sure to configure AWS credentials for the same user account that runs `tenzir` and `tenzir-node`. The AWS CLI creates configuration files for the current user under `~/.aws`, which can only be read by the same user account.

The `tenzir-node` systemd unit by default creates a `tenzir` user and runs as that user, meaning that the AWS credentials must also be configured for that user. The directory `~/.aws` must be readable for the `tenzir` user.

If a config file `<prefix>/etc/tenzir/plugin/$PLUGIN.yaml` or `~/.config/tenzir/plugin/$PLUGIN.yaml` exists, it is always preferred over the default AWS credentials. Here, `$PLUGIN` is the Tenzir plugin name, such as `s3` or `sqs`. The configuration file must have the following format:

```yaml
access-key: your-access-key
secret-key: your-secret-key
session-token: your-session-token # optional
```

# MSK

[Amazon Managed Streaming for Apache Kafka (Amazon MSK)](https://aws.amazon.com/msk/) is a streaming data service that manages Apache Kafka infrastructure and operations, making it easier for developers and DevOps managers to run Apache Kafka applications and Apache Kafka Connect connectors on AWS without becoming experts in operating Apache Kafka.

![Amazon MSK](/_astro/msk.BWTbNq0e_19DKCs.svg)

## Sending and Receiving

[Section titled “Sending and Receiving”](#sending-and-receiving)

Tenzir’s Kafka connectors [`load_kafka`](/reference/operators/load_kafka) and [`save_kafka`](/reference/operators/save_kafka) can send and receive events from Amazon MSK Clusters.

## Authentication

[Section titled “Authentication”](#authentication)

Provisioned MSK Clusters support different authentication mechanisms such as mTLS, SASL/SCRAM, IAM etc. However Serverless MSK instances currently only support IAM Authentication.

The [`load_kafka`](/reference/operators/load_kafka) and [`save_kafka`](/reference/operators/save_kafka) operators can authenticate with MSK using AWS IAM by simply specifying the `aws_iam` option with a record of configuration values such as:

```tql
load_kafka "kafkaesque-data", aws_iam={region: "eu-west-1"}
```

The above pipeline will try to fetch credentials from [various different locations](/reference/operators/load_kafka#aws_iam--record-optional) including the Instance Metadata Services. This means you can attach a role with the necessary permissions directly to an EC2 instance and Tenzir will automatically pick it up.

Other Authentication methods

Tenzir relies on [librdkafka](https://github.com/confluentinc/librdkafka) for the Kafka client protocol and all other mechanisms supported by librdkafka can be used by specifying respective configuration values, e.g., [mTLS](https://github.com/confluentinc/librdkafka/wiki/Using-SSL-with-librdkafka).

### Assuming roles

[Section titled “Assuming roles”](#assuming-roles)

Roles can also be assumed by giving the `assume_role` parameter to the `aws_iam` option.

```tql
save_kafka "topic", aws_iam={
    region: "eu-west-1",
    assume_role: "arn:aws:iam::1234567890:role/my-msk-role"
  }
```

The above pipeline attempts to fetch temporary credentials from Amazon STS for the given ARN.

### Example

[Section titled “Example”](#example)

#### Collecting High Severity OCSF events from MSK

[Section titled “Collecting High Severity OCSF events from MSK”](#collecting-high-severity-ocsf-events-from-msk)

The following pipeline reads OCSF events from MSK, assuming the role referenced by the provided ARN. The incoming data is then filtered for severity and sent to Splunk clusters in a load balanced fashion.

```tql
let $endpoints = ["indexer-1-url", "indexer-2-url"]


load_kafka "ocsf-events", aws_iam={region: "us-east-2", assume_role: "arn"}
read_json
where severity_id >= 4 // High and above
load_balance $endpoints {
    to_splunk $endpoints, hec_token=secret("SPLUNK_TOKEN")
}
```

# S3

[Amazon Simple Storage Service (S3)](https://aws.amazon.com/s3/) is an object storage service. Tenzir can treat it like a local filesystem to read and write files.

![S3](/_astro/s3.iRFUIJJn_19DKCs.svg)

URL Support

The URL scheme `s3://` dispatches to [`load_s3`](/reference/operators/load_s3) and [`save_s3`](/reference/operators/save_s3) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Configuration

[Section titled “Configuration”](#configuration)

Follow the [standard configuration instructions](/integrations/amazon) to authenticate with your AWS credentials.

## Examples

[Section titled “Examples”](#examples)

### Write to an S3 bucket

[Section titled “Write to an S3 bucket”](#write-to-an-s3-bucket)

```tql
from {foo: 42}
to "s3://my-bucket/path/to/file.json.gz"
```

### Read from an S3 bucket

[Section titled “Read from an S3 bucket”](#read-from-an-s3-bucket)

```tql
from "s3://my-bucket/path/to/file.json.gz"
```

```tql
{foo: 42}
```

# Security Lake

[Amazon Security Lake](https://aws.amazon.com/security-lake/) is a centralized security data lake service that collects and stores security data in the Open Cybersecurity Schema Framework (OCSF) format.

![Amazon security Lake](/_astro/basic.CxkCu6X6_19DKCs.svg)

Tenzir sends events to Amazon Security Lake using the [`to_amazon_security_lake`](/reference/operators/to_amazon_security_lake) operator.

## Configuration

[Section titled “Configuration”](#configuration)

![Amazon security Lake](/_astro/detailed.CEMIgE0m_19DKCs.svg)

The current architectural pattern for Amazon Security Lake requires creating one custom source per OCSF event class. This design ensures clean data organization, with each custom source receiving its own dedicated directory under `/ext` in the S3 bucket. Since each Parquet file must contain records of only one OCSF event class, this one-to-one mapping between custom sources and event classes is the most practical approach. The partition path follows this structure:

```plaintext
/ext/{custom-source-name}/region={region}/accountId={accountID}/eventDay={YYYYMMDD}/
```

This architecture naturally leads to deploying one Tenzir pipeline per custom source. The [`to_amazon_security_lake`](/reference/operators/to_amazon_security_lake) operator handles the partitioning according to this structure automatically.

### Custom Source Setup

[Section titled “Custom Source Setup”](#custom-source-setup)

Limited Custom Sources

Amazon Security Lake supports a maximum of 50 custom sources, but there are over 70 [supported event classes](https://docs.aws.amazon.com/security-lake/latest/userguide/adding-custom-sources.html). You must choose which event classes to prioritize. Future versions may increase this limit.

To set up a custom source:

1. Provide a globally unique 20-character source name using the pattern `tnz-ocsf-${class_uid}` (for example, `tnz-ocsf-1001` for File System Activity)

2. Select an OCSF event class (for example, “Network Activity” or “DNS Activity”)

3. Configure your AWS account ID and external ID

4. Set up a service access role for the AWS Glue crawler

For detailed instructions, refer to the [AWS documentation on adding custom sources](https://docs.aws.amazon.com/security-lake/latest/userguide/adding-custom-sources.html).

#### Automated Custom Source Creation

[Section titled “Automated Custom Source Creation”](#automated-custom-source-creation)

For streamlined custom source creation, you can use Tenzir’s [security-lake-tools](https://github.com/tenzir/security-lake-tools) to automate the setup process. The easiest way to use this tool is with `uvx` (from the `uv` Python package manager):

```bash
# Create a custom source for Network Activity (class_uid 4001)
uvx security-lake-tools create-source \
  --region eu-west-1 \
  --external-id tenzir \
  --account-id 123456789012 \
  --profile my-aws-profile \
  4001


# List all available OCSF event classes
uvx security-lake-tools create-source --list
```

The tool automates:

* Custom source creation with proper naming (`tnz-ocsf-${class_uid}` pattern)
* IAM role and policy configuration
* S3 bucket permissions
* AWS Glue crawler setup
* OCSF event class validation

This approach is particularly useful when setting up multiple custom sources across different OCSF event classes.

### Tenzir Setup

[Section titled “Tenzir Setup”](#tenzir-setup)

To run Tenzir pipelines that send data to Security Lake, you’ll need a Tenzir node running on AWS. See our guide on [how to deploy a node on AWS](/guides/node-setup/deploy-a-node/#aws) for detailed instructions.

Follow the [standard configuration instructions](/integrations/amazon) to authenticate with your AWS credentials. Tenzir supports multiple authentication methods including IAM roles, access keys, and credential profiles.

After deployment, create pipelines using this pattern:

```tql
let $in = ...
let $url = ...
let $region = ...
let $account_id = ...


subscribe $in
where @name == "ocsf.http_activity"
ocsf::apply
to_amazon_security_lake $url, region=$region, account_id=$account_id, timeout=10m
```

Available package

Use the [`amazon-security-lake`](https://github.com/tenzir/library/tree/main/amazon-security-lake) package from the Tenzir Library, which includes pre-built pipelines for common event classes.

## Examples

[Section titled “Examples”](#examples)

### Send OCSF events from a Kafka topic to Security Lake

[Section titled “Send OCSF events from a Kafka topic to Security Lake”](#send-ocsf-events-from-a-kafka-topic-to-security-lake)

This example assumes:

* An Amazon Security Lake instance in the `eu-west-2` region
* A custom source named `tnz-ocsf-4001` (Network Activity class UID)
* An AWS account ID of `123456789012`

```tql
let $s3_uri = "s3://aws-security-data-lake-eu-west-2-lake-abcdefghijklmnopqrstuvwxyz1234/ext/tnz-ocsf-4001/"


load_kafka "ocsf_events"
read_ndjson
where class_uid == ocsf::class_uid("Network Activity")
to_amazon_security_lake $s3_uri,
  region="eu-west-2",
  accountId="123456789012"
```

# SQS

[Amazon Simple Queuing Service (SQS)](https://aws.amazon.com/sqs/) is a managed message queue for microservices, distributed systems, and serverless applications.

Tenzir can interact with SQS by sending messages to and reading messages from SQS queues.

![SQS](/_astro/sqs.CmbjyY_r_19DKCs.svg)

When reading from SQS queues, you cannot specify individual messages. Instead, you determine the maximum number of messages you wish to retrieve, up to a limit of 10.

If the parameter `poll_interval` is non-zero, the pipeline automatically performs [long polling](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html). This means that you may receive messages in bursts, according to your specified poll interval.

Tenzir pipelines that read from an SQS queue automatically send a deletion request after having received the messages.

URL Support

The URL scheme `sqs://` dispatches to [`load_sqs`](/reference/operators/load_sqs) and [`save_sqs`](/reference/operators/save_sqs) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Configuration

[Section titled “Configuration”](#configuration)

Follow the [standard configuration instructions](/integrations/amazon) to authenticate with your AWS credentials.

## Examples

[Section titled “Examples”](#examples)

### Send a message to an SQS queue

[Section titled “Send a message to an SQS queue”](#send-a-message-to-an-sqs-queue)

```tql
from {foo: 42}
to "sqs://my-queue" {
  write_json
}
```

### Receive messages from an SQS queue

[Section titled “Receive messages from an SQS queue”](#receive-messages-from-an-sqs-queue)

```tql
from "sqs://my-queue", poll_interval=5s {
  read_json
}
```

```tql
{foo: 42}
```

# AMQP

The [Advanced Message Queuing Protocol (AMQP)](https://www.amqp.org/) is an open standard application layer protocol for message-oriented middleware.

The diagram below shows the key abstractions and how they relate to a pipeline:

![AMQP Diagram](/_astro/amqp.B-TDw5B5_19DKCs.svg)

Tenzir supports sending and receiving messages via AMQP version 0-9-1.

URL Support

The URL scheme `amqp://` dispatches to [`load_amqp`](/reference/operators/load_amqp) and [`save_amqp`](/reference/operators/save_amqp) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Examples

[Section titled “Examples”](#examples)

### Send events to an AMQP exchange

[Section titled “Send events to an AMQP exchange”](#send-events-to-an-amqp-exchange)

```tql
from {
  x: 42,
  y: "foo",
}
to "amqp://admin:pass@0.0.0.1:5672/vhost"
```

### Receive events from an AMQP queue

[Section titled “Receive events from an AMQP queue”](#receive-events-from-an-amqp-queue)

```tql
from "amqp://admin:pass@0.0.0.1:5672/vhost"
```

# ClickHouse

[ClickHouse](https://clickhouse.com/clickhouse) is an open-source analytical database that enables its users to generate powerful analytics, using SQL queries, in real-time.

![ClickHouse integration](/_astro/clickhouse.DHWXsxpE_19DKCs.svg)

## How Tenzir Connects to ClickHouse

[Section titled “How Tenzir Connects to ClickHouse”](#how-tenzir-connects-to-clickhouse)

Tenzir connects to ClickHouse over the network using the native ClickHouse TCP protocol using the official [clickhouse-cpp](https://github.com/ClickHouse/clickhouse-cpp) library. Tenzir communicates with ClickHouse via the host and port you specify in the [`to_clickhouse`](/reference/operators/to_clickhouse) operator. This means:

* **Network**: Tenzir and ClickHouse can run on the same machine (using `localhost`) or on different machines in the same network. You just need to make sure that Tenzir can reach the ClickHouse server.
* **IPC**: There is no direct inter-process communication (IPC) mechanism; all communication uses ClickHouse’s network protocol.
* **Co-deployment**: For best performance and security, deploy Tenzir and ClickHouse in the same trusted network or use secure tunnels if needed.

## Setting Up ClickHouse

[Section titled “Setting Up ClickHouse”](#setting-up-clickhouse)

To get started with ClickHouse, follow the [official quick start guide](https://clickhouse.com/docs/getting-started/quick-start/oss):

### Native Binary

[Section titled “Native Binary”](#native-binary)

1. Download the binary:

   ```sh
   curl https://clickhouse.com/ | sh
   ```

2. Start the server:

   ```sh
   ./clickhouse server
   ```

   This downloads the ClickHouse binary and starts the server. You can then connect to ClickHouse at `localhost:9000` (native protocol) or `localhost:8123` (HTTP interface).

3. (Optionally) Start CLI client:

   ```sh
   ./clickhouse client
   ```

   With this client, you can now run SQL queries on your ClickHouse server.

### Docker

[Section titled “Docker”](#docker)

1. Run Docker:

   ```sh
   docker run -d --name clickhouse-server --ulimit nofile=262144:262144 \
     -p 9000:9000 -p 8123:8123 clickhouse/clickhouse-server
   ```

***

You can now connect to ClickHouse at `localhost:9000` (native protocol) or `localhost:8123` (HTTP interface).

## Usage Examples

[Section titled “Usage Examples”](#usage-examples)

These examples assume that the ClickHouse server is running on the same host as Tenzir and that it allows non-TLS connections (hence using `tls=false` in the pipelines).

You can find out more about how to configure TLS on the [`to_clickhouse`](/reference/operators/to_clickhouse) documentation and the [Clickhouse SSL-TLS configuration guide](https://clickhouse.com/docs/guides/sre/configuring-ssl)

### 1. Easy Mode: Automatic table creation

[Section titled “1. Easy Mode: Automatic table creation”](#1-easy-mode-automatic-table-creation)

Tenzir can automatically create tables in ClickHouse based on the incoming data schema. For example, to ingest OCSF network activity data:

```tql
from "ocsf_network_activity.json"
ocsf::apply
to_clickhouse table="ocsf.network_activity", primary=timestamp, tls=false
```

When creating a table, the [`to_clickhouse`](/reference/operators/to_clickhouse) operator uses the first event to determine the schema. You must take care that there are no untyped nulls in this event, as the operator cannot transmit those.

In this example, we use the [`ocsf::apply`](/reference/operators/ocsf/apply) operator, which will automatically align events with the correct OCSF schema, giving all fields the correct types and adding all fields that should be in `ocsf.network_activity`. This ensures that we create a complete table without missing or incorrectly typed columns.

You can now query the data in ClickHouse, e.g.:

```sql
SELECT median(traffic.bytes_in), median(traffic.bytes_out)
FROM ocsf.network_activity
GROUP BY *
```

### 2. Advanced: Explicit Table Creation

[Section titled “2. Advanced: Explicit Table Creation”](#2-advanced-explicit-table-creation)

For more control, you can create the table in ClickHouse first. Use this approach when you know the full schema of your table, but not all events contain all fields and as such the operator would not create the correct table.

1. Create the table in ClickHouse:

   ```sql
   CREATE TABLE my_table (
     id Int64,
     name String,
     mice_caught Nullable(Int64)
   ) ENGINE = MergeTree() ORDER BY id;
   ```

2. Ingest data from Tenzir:

   my\_file.csv

   ```csv
   id,name,mice_caught
   0,Jerry,
   1,Tom,0
   ```

   ```tql
   from "my_file.csv"
   to_clickhouse table="my_table", mode="append", tls=false
   ```

   We use the explicit `mode="append"` to ensure that the table already exists.

   In this example *Jerry*, being a mouse, has no value for `mice_caught`. Since we created a table with the expected type, this is not an issue.

# Elasticsearch

[Elasticsearch](https://www.elastic.co/elasticsearch) is a search and observability suite for unstructured data. Tenzir can send events to Elasticsearch and emulate and Elasticsearch Bulk API endpoint.

![How to send and receive data](/_astro/elasticsearch.G89ivtEj_19DKCs.svg)

When sending data to Elasticsearch, Tenzir uses the [Bulk API](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html) and attempts to maximally batch events for throughput, accumulating multiple events before shipping them within a single API call. You can control batching behavior with the `max_content_length` and `send_timeout` options.

For more details, see the documentation for the [`to_opensearch`](/reference/operators/to_opensearch) operator.

Tenzir can also present an Elasticsearch-compatible REST API via the [`from_opensearch`](/reference/operators/from_opensearch) operator.

## Examples

### Send events to an Elasticsearch index

Send an example event to the `main` index:

```tql
from {event: "example"}
to "elasticsearch://1.2.3.4:9200", action="create", index="main"
```

Instead of treating the entire event as document to be indexed by Elasticsearch, you can designate a nested record as document:

```tql
from {category: "qux", doc_id: "XXX", event: {foo: "bar"}}
to "elasticsearch://localhost:9200", id=doc_id, doc=event, action="update", index=category
```

The above example updates the document with ID `XXX` with the contents from the nested field `event`.

### Accept data by emulating Elasticsearch

Tenzir can act as drop-in replacement for Elasticsearch by accepting data via a Bulk API endpoint. This allows you to point your [Logstash](https://www.elastic.co/logstash) or Beats instances to Tenzir instead.

```tql
from "elasticsearch://localhost:9200", keep_actions=true
publish "elasticsearch"
```

This pipeline accepts data on port 9200 and publishes all received events on the

`elasticsearch` topic for further processing by other pipelines.

Setting `keep_actions=true` causes command events to remain in the stream, e.g., like this:

```tql
{create:{_index:"filebeat-8.17.3"}} // 👈 command event
{"@timestamp":2025-03-31T13:42:28.068Z,log:{offset:1,file:{path:"/mounted/logfile"}},message:"hello",input:{type:"log"},host:{name:"eb21"},agent:{id:"682cfcf4-f251-4576-abcb-6c8bcadfda08",name:"eb21",type:"filebeat",version:"8.17.3",ephemeral_id:"17f74f6e-36f0-4045-93e6-c549874716df"},ecs:{version:"8.0.0"}}
{create:{_index:"filebeat-8.17.3"}} // 👈 command event
{"@timestamp":2025-03-31T13:42:28.068Z,log:{offset:7,file:{path:"/mounted/logfile"}},message:"this",input:{type:"log"},host:{name:"eb21"},agent:{id:"682cfcf4-f251-4576-abcb-6c8bcadfda08",name:"eb21",type:"filebeat",version:"8.17.3",ephemeral_id:"17f74f6e-36f0-4045-93e6-c549874716df"},ecs:{version:"8.0.0"}}
```

#### Ship data via Filebeat

Configure [Filebeat](https://www.elastic.co/beats/filebeat) as follows to ship data to Tenzir:

filebeat.yml

```yaml
output:
  elasticsearch:
  hosts: ["localhost:9200"]
```

Set `hosts` to the endpoint of the Tenzir pipeline accepting data.

#### Ship data via Logstash

Configure [Logstash](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html) with the

`elasticsearch` output plugin to ship data to Tenzir:

pipeline.conf

```javascript
output {
  elasticsearch {
    hosts => "https://localhost:9200"
  }
}
```

Set `hosts` to the endpoint of the Tenzir pipeline accepting data.

# Email

Tenzir supports sending events as email using the [`save_email`](/reference/operators/save_email) operator. To this end, the operator establishes a connection with an SMTP server that sends the message on behalf of Tenzir.

![Pipeline to email](/_astro/email.DwOnOCKn_19DKCs.svg)

## Examples

[Section titled “Examples”](#examples)

### Email the Tenzir version as CSV message

[Section titled “Email the Tenzir version as CSV message”](#email-the-tenzir-version-as-csv-message)

```tql
version
write_csv
save_email "Example User <user@example.org>"
```

### Send the email body as MIME part

[Section titled “Send the email body as MIME part”](#send-the-email-body-as-mime-part)

```tql
version
write_json
save_email "user@example.org, mime=true
```

This results in an email body of this shape:

```plaintext
--------------------------s89ecto6c12ILX7893YOEf
Content-Type: application/json
Content-Transfer-Encoding: quoted-printable


{
  "version": "4.10.4+ge0a060567b-dirty",
  "build": "ge0a060567b-dirty",
  "major": 4,
  "minor": 10,
  "patch": 4
}


--------------------------s89ecto6c12ILX7893YOEf--
```

# File

Tenzir supports reading from and writing to files, including non-regular files, such as [Unix domain sockets](https://en.wikipedia.org/wiki/Unix_domain_socket), standard input, standard output, and standard error.

![File](/_astro/file.BdqTXxJ3_19DKCs.svg)

When `~` is the first character in the file path, the operator substitutes it with the `$HOME` environment variable.

URL Support

The URL scheme `file://` dispatches to [`load_file`](/reference/operators/load_file) and [`save_file`](/reference/operators/save_file) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Examples

[Section titled “Examples”](#examples)

### Read a file

[Section titled “Read a file”](#read-a-file)

Read from a file and parse it in the format applied by the file extension:

```tql
from "/tmp/file.json"
```

The [`from`](/reference/operators/from) operator automatically decompresses the file, if the suffix list contains a [supported compression algorithm](/reference/operators/from#compression):

```tql
from "/tmp/file.json.gz"
```

Some operators perform better when the entire file arrives as a single block of bytes, such as the [`yara`](/reference/operators/yara) operator. In this case, passing `mmap=true` runs more efficiently:

```tql
from "/sandbox/malware.gz", mmap=true {
  decompress "gzip"
  yara "rule.yaml"
}
```

### Follow a file

[Section titled “Follow a file”](#follow-a-file)

A pipeline typically completes once it reads the end of a file. Pass `follow=true` to disable this behavior and instead wait for new data written to it. This is similar to running `tail -f` on a file.

```plaintext
from "/tmp/never-ending-stream.ndjson", follow=true
```

### Write a file

[Section titled “Write a file”](#write-a-file)

Write to a file in the format implied by the file extension:

```tql
version
to "/tmp/tenzir-version.json"
```

The [`to`](/reference/operators/to) operator automatically compresses the file, if the suffix list contains a [supported compression algorithm](/reference/operators/to#compression):

```tql
version
to "/tmp/tenzir-version.json.bz2"
```

### Append to a file

[Section titled “Append to a file”](#append-to-a-file)

In case the file exists and you do not want to overwrite it, pass `append=true` as option:

```tql
from {x: 42}
to "/tmp/event.csv", append=true
```

### Read/write a Unix domain socket

[Section titled “Read/write a Unix domain socket”](#readwrite-a-unix-domain-socket)

Pass `uds=true` to signal that the file is a Unix domain socket:

```tql
to "/tmp/socket", uds=true {
  write_ndjson
}
```

When reading from a Unix domain socket, Tenzir automatically figures out whether the file is regular or a socket:

```tql
from "/tmp/socket" {
  read_ndjson
}
```

# Fluent Bit

[Fluent Bit](https://fluentbit.io) is a an open source observability pipeline. Tenzir embeds Fluent Bit, exposing all its [inputs](https://docs.fluentbit.io/manual/pipeline/inputs) via [`from_fluent_bit`](/reference/operators/from_fluent_bit) and [outputs](https://docs.fluentbit.io/manual/pipeline/outputs) via [`to_fluent_bit`](/reference/operators/to_fluent_bit)

This makes Tenzir effectively a superset of Fluent Bit.

![Fluent Bit Inputs & Outputs](/_astro/fluent-bit.BMlvI90p_19DKCs.svg)

Fluent Bit [parsers](https://docs.fluentbit.io/manual/pipeline/parsers) map to Tenzir operators that accept bytes as input and produce events as output. Fluent Bit [filters](https://docs.fluentbit.io/manual/pipeline/filters) correspond to Tenzir operators that perform event-to-event transformations. Tenzir does not expose Fluent Bit parsers and filters, only inputs and output.

Internally, Fluent Bit uses [MsgPack](https://msgpack.org/) to encode events whereas Tenzir uses [Arrow](https://arrow.apache.org) record batches. The `fluentbit` source operator transposes MsgPack to Arrow, and the `fluentbit` sink performs the reverse operation.

## Usage

[Section titled “Usage”](#usage)

An invocation of the `fluent-bit` commandline utility

```bash
fluent-bit -o input_plugin -p key1=value1 -p key2=value2 -p…
```

translates to Tenzir’s [`from_fluent_bit`](/reference/operators/from_fluent_bit) operator as follows:

```tql
from_fluent_bit "input_plugin", options={key1: value1, key2: value2, …}
```

with the [`to_fluent_bit`](/reference/operators/to_fluent_bit) operator working exactly analogous.

## Examples

[Section titled “Examples”](#examples)

### Ingest OpenTelemetry logs, metrics, and traces

[Section titled “Ingest OpenTelemetry logs, metrics, and traces”](#ingest-opentelemetry-logs-metrics-and-traces)

```tql
from_fluent_bit "opentelemetry"
```

You can then send JSON-encoded log data to a freshly created API endpoint:

```bash
curl \
  --header "Content-Type: application/json" \
  --request POST \
  --data '{"resourceLogs":[{"resource":{},"scopeLogs":[{"scope":{},"logRecords":[{"timeUnixNano":"1660296023390371588","body":{"stringValue":"{\"message\":\"dummy\"}"},"traceId":"","spanId":""}]}]}]}' \
  http://0.0.0.0:4318/v1/logs
```

### Imitate a Splunk HEC endpoint

[Section titled “Imitate a Splunk HEC endpoint”](#imitate-a-splunk-hec-endpoint)

```tql
from_fluent_bit "splunk", options = {port: 8088}
```

Tip

Use the dedicated [`to_splunk` operator](/reference/operators/to_splunk) to send events to a Splunk HEC.

### Imitate an ElasticSearch & OpenSearch Bulk API endpoint

[Section titled “Imitate an ElasticSearch & OpenSearch Bulk API endpoint”](#imitate-an-elasticsearch--opensearch-bulk-api-endpoint)

This allows you to ingest from beats (e.g., Filebeat, Metricbeat, Winlogbeat).

```tql
from_fluent_bit "elasticsearch", options = {port: 9200}
```

### Send to Datadog

[Section titled “Send to Datadog”](#send-to-datadog)

```tql
to_fluent_bit "datadog", options = {apikey: "XXX"}
```

### Send to ElasticSearch

[Section titled “Send to ElasticSearch”](#send-to-elasticsearch)

```tql
to_fluent_bit "es", options = {host: 192.168.2.3, port: 9200, index: "my_index", type: "my_type"}
```

# FTP

Tenzir supports the [File Transfer Protocol (FTP)](https://en.wikipedia.org/wiki/File_Transfer_Protocol), both downloading and uploading files.

![FTP](/_astro/ftp.zkl0U-rL_19DKCs.svg)

FTP consists of two separate TCP connections, one control and one data connection. This can be tricky for some firewalls and may require special attention.

URL Support

The URL schemes `ftp://` and `ftps://` dispatch to [`load_ftp`](/reference/operators/load_ftp) and [`save_ftp`](/reference/operators/save_ftp) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Examples

[Section titled “Examples”](#examples)

### Download a file from an FTP server

[Section titled “Download a file from an FTP server”](#download-a-file-from-an-ftp-server)

```tql
from "ftp://user:pass@ftp.example.org/path/to/file.json"
```

### Upload events to an FTP server

[Section titled “Upload events to an FTP server”](#upload-events-to-an-ftp-server)

```tql
from {
  x: 42,
  y: "foo",
}
to "ftp://user:pass@ftp.example.org/a/b/c/events.json.gz"
```

# Cloud Logging

[Google Cloud Logging](https://cloud.google.com/logging) is Google’s log management solution. Tenzir can send events to Google Cloud Logging.

![Google Cloud Logging](/_astro/cloud-logging.CbMzNYg2_19DKCs.svg)

## Examples

[Section titled “Examples”](#examples)

### Send an event to Google Cloud Logging

[Section titled “Send an event to Google Cloud Logging”](#send-an-event-to-google-cloud-logging)

The easiest way to send data to Cloud Logging is via Google [Applciation Default Credentials (ADC)](https://cloud.google.com/docs/authentication/application-default-credentials). Assuming you have configured your node so that it finds the credentials, you can pipe any data to the [`to_google_cloud_logging`](/reference/operators/to_google_cloud_logging) operator:

```tql
from {
  content: "log message",
  timestamp: now(),
}
to_google_cloud_logging name="projects/PROJECT_ID/logs/LOG_ID",
  resource_type="global"
```

# Cloud Pub/Sub

[Google Cloud Pub/Sub](https://cloud.google.com/pubsub) ingests events for streaming into BigQuery, data lakes, or operational databases. Tenzir can act as a publisher that sends messages to a topic, and as a subscriber that receives messages from a subscription.

![Google Cloud Pub/Sub](/_astro/cloud-pubsub.CbURM6vM_19DKCs.svg)

URL Support

The URL scheme `gcps://` dispatches to [`load_google_cloud_pubsub`](/reference/operators/load_google_cloud_pubsub) and [`save_google_cloud_pubsub`](/reference/operators/save_google_cloud_pubsub) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Examples

[Section titled “Examples”](#examples)

### Publish a message to a topic

[Section titled “Publish a message to a topic”](#publish-a-message-to-a-topic)

```tql
from {foo: 42}
to "gcps://my-project/my-topic" {
  write_json
}
```

### Receive messages from a subscription

[Section titled “Receive messages from a subscription”](#receive-messages-from-a-subscription)

```tql
from "gcps://my-project/my-topic" {
  read_json
}
```

# Cloud Storage

[Cloud Storage](https://cloud.google.com/storage) is Google’s object storage service. Tenzir can treat it like a local filesystem to read and write files.

![Google Cloud Storage](/_astro/cloud-storage.IUao_ukw_19DKCs.svg)

URL Support

The URL scheme `gs://` dispatches to [`load_gcs`](/reference/operators/load_gcs) and [`save_gcs`](/reference/operators/save_gcs) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Configuration

[Section titled “Configuration”](#configuration)

You need to configure appropriate credentials using Google’s [Application Default Credentials](https://google.aip.dev/auth/4110).

## Examples

[Section titled “Examples”](#examples)

### Write an event to a file in a bucket

[Section titled “Write an event to a file in a bucket”](#write-an-event-to-a-file-in-a-bucket)

```tql
from {foo: 42}
to "gs://bucket/path/to/file.json"
```

### Read events from a file in a bucket

[Section titled “Read events from a file in a bucket”](#read-events-from-a-file-in-a-bucket)

```tql
from "gs://bucket/path/to/file.json"
```

```tql
{foo: 42}
```

# SecOps

[Google Security Operations (SecOps)](https://cloud.google.com/security/products/security-operations) is Google’s security operations platform that enables detection, investigation and response to incidents. Tenzir can send events to Google SecOps using the [unstructured logs ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api#unstructuredlogentries).

![Google Security Operations](/_astro/secops.DocXpfno_19DKCs.svg)

## Examples

[Section titled “Examples”](#examples)

### Send an event to Google SecOps

[Section titled “Send an event to Google SecOps”](#send-an-event-to-google-secops)

```tql
from {log: "31-Mar-2025 01:35:02.187 client 0.0.0.0#4238: query: tenzir.com IN A + (255.255.255.255)"}
to_google_secops \
  customer_id="00000000-0000-0000-00000000000000000",
  private_key=secret("my_secops_key"),
  client_email="somebody@example.com",
  log_text=log,
  log_type="BIND_DNS",
  region="europe"
```

# Graylog

[Graylog](https://graylog.org/) is a log management solution based on top of OpenSearch. Tenzir can send data to and receive data from Graylog.[1](#user-content-fn-1)

![Graylog](/_astro/graylog.DlsP54zt_19DKCs.svg)

## Receive data from Graylog

[Section titled “Receive data from Graylog”](#receive-data-from-graylog)

To receive data from Graylog with a Tenzir pipeline, you need to configure a new output and setup a stream that sends data to that output. The example below assumes that Graylog sends data in GELF to a TCP endpoint that listens on IP address 1.2.3.4 at port 5678.

### Configure a GELF TCP output

[Section titled “Configure a GELF TCP output”](#configure-a-gelf-tcp-output)

1. Navigate to *System/Outputs* in Graylog’s web interface.

2. Click *Manage Outputs*.

3. Select `GELF TCP` as the output type.

4. Configure the output settings:

   * Specify the target server’s address in the `host` field (e.g., `1.2.3.4`).
   * Enter the port number for the TCP connection (e.g., `5678`).
   * Optionally adjust other settings like reconnect delay, queue size, and send buffer size.

5. Save the configuration.

Now Graylog will forward messages in GELF format to the specified TCP endpoint.

### Create a Graylog stream

[Section titled “Create a Graylog stream”](#create-a-graylog-stream)

The newly created output still needs to be connected to a stream to produce data. For example, to route all incoming traffic in Graylog to an output:

1. Go to *Streams* in the Graylog web interface.
2. Create a new stream or edit an existing one.
3. In the stream’s settings, configure it to match all incoming messages. You can do this by setting up a rule that matches all messages or by leaving the rules empty.
4. Once the stream is configured, go to the *Outputs* tab in the stream’s settings.
5. Add the previously configured GELF TCP output to this stream.

This setup will direct all messages that arrive in Graylog to the specified output. Adapt your filters for more fine-grained forwarding.

### Test the connection with a Tenzir pipeline

[Section titled “Test the connection with a Tenzir pipeline”](#test-the-connection-with-a-tenzir-pipeline)

Now that Graylog is configured, you can test that data is flowing using the following Tenzir pipeline:

```tql
from "tcp://1.2.3.4:5678" {
  read_gelf
}
```

This pipelines opens a listening socket at IP address 1.2.3.4 at port 5678 via [`from`](/reference/operators/from) and then spawns a nested pipeline per accepted connection, each of which reads a stream of GELF messages using [`read_gelf`](/reference/operators/read_gelf). Graylog will connect to this socket, based on the reconnect interval that you configured in the output (by default 500ms).

Now that data is flowing, you can decide what to do with the Graylog data, e.g., make available the data on an topic using [`publish`](/reference/operators/publish):

```tql
from "tcp://1.2.3.4:5678" {
  read_gelf
}
publish "graylog"
```

## Footnotes

[Section titled “Footnotes”](#footnote-label)

1. This guide focuses currently focuses only on receiving data to Graylog, although it’s already possible to send data to Graylog. [↩](#user-content-fnref-1)

# HTTP

Tenzir supports HTTP and HTTPS, both as sender and receiver.

When retrieving data from an API or website, you prepare your HTTP request and get back the HTTP response body as your pipeline data:

![HTTP from](/_astro/http-from.CK3DDVvm_19DKCs.svg)

When sending data from a pipeline to an API or website, the events in the pipeline make up the HTTP request body. If the HTTP status code is not 2\*\*, you will get a warning.

![HTTP from](/_astro/http-to.DgFU5GZf_19DKCs.svg)

In both cases, you can only provide static header data.

URL Support

The URL schemes `http://` and `https://` dispatch to [`load_http`](/reference/operators/load_http) and [`save_http`](/reference/operators/save_http) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

Since the majority of HTTP activity uses JSON-encoded request/response bodies, you do not need to provide a separate pipeline argument with [`read_json`](/reference/operators/read_json) and [`write_ndjson`](/reference/operators/write_ndjson).

## Examples

[Section titled “Examples”](#examples)

### Perform a GET request with URL parameters

[Section titled “Perform a GET request with URL parameters”](#perform-a-get-request-with-url-parameters)

```tql
from "http://example.com:8888/api", method="GET", params={query: "tenzir"}
```

### Perform a POST request with JSON body

[Section titled “Perform a POST request with JSON body”](#perform-a-post-request-with-json-body)

```tql
from "http://example.com:8888/api", method="POST", data={query: "tenzir"}
```

### Call a webhook API with pipeline data

[Section titled “Call a webhook API with pipeline data”](#call-a-webhook-api-with-pipeline-data)

```tql
from {
  x: 42,
  y: "foo",
}
to "http://example.com:8888/api"
```

# Kafka

[Apache Kafka](https://kafka.apache.org) is a distributed open-source message broker. The Tenzir integration can publish (send messages to a topic) or subscribe (receive) messages from a topic.

![Kafka Diagram](/_astro/kafka.C4MFfO6p_19DKCs.svg)

Internally, we use Confluent’s official [librdkafka](https://github.com/confluentinc/librdkafka) library, which gives us full control in passing options.

URL Support

The URL scheme `kafka://` dispatches to [`load_kafka`](/reference/operators/load_kafka) and [`save_kafka`](/reference/operators/save_kafka) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Examples

[Section titled “Examples”](#examples)

### Send events to a Kafka topic

[Section titled “Send events to a Kafka topic”](#send-events-to-a-kafka-topic)

```tql
from {
  x: 42,
  y: "foo",
}
to "kafka://topic" {
  write_ndjson
}
```

### Subscribe to a topic

[Section titled “Subscribe to a topic”](#subscribe-to-a-topic)

The `offset` option controls where to start reading:

```tql
from "kafka://topic", offset="beginning" {
  read_ndjson
}
```

Other values are `"end"` to read at the last offset, `"stored"` to read at the stored offset, a positive integer representing an absolute offset, or a negative integer representing a relative offset counting from the end.

# Azure Blob Storage

[Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs) is Azure’s object storage service. Tenzir can treat it like a local filesystem to read and write files.

![Azure Blob Storage](/_astro/azure-blob-storage.Sc3L-GkA_19DKCs.svg)

URL Support

The URL scheme `abfs[s]://` dispatches to [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage) and [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Examples

[Section titled “Examples”](#examples)

### Write an event to a file in a container

[Section titled “Write an event to a file in a container”](#write-an-event-to-a-file-in-a-container)

```tql
from {foo: 42}
to "abfss://user@container/path/to/file.json"
```

### Read events from a file in a container

[Section titled “Read events from a file in a container”](#read-events-from-a-file-in-a-container)

```tql
from "abfss://user@container/path/to/file.json"
```

```tql
{foo: 42}
```

# Azure Log Analytics

Azure Monitor is Microsoft’s cloud solution for collecting and analyzing logs and system events. Azure Log Analytics is a part of Monitor and comes with an [Logs Ingestion API](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview) for sending data to tables within a [Log Analytics workspace](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview) that which is a unique environment for log data, such as from [Microsoft Sentinel](https://learn.microsoft.com/en-us/azure/sentinel/overview?tabs=azure-portal) and [Microsoft Defender for Cloud](https://learn.microsoft.com/en-us/azure/defender-for-cloud/defender-for-cloud-introduction). Log Anlaytics tables are either pre-defined [standard tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview#supported-tables) that follow a given schema, or user-defined [custom tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/create-custom-table#create-a-custom-table).

The diagram below illustrates the key components involved when sending data to a Log Analytics table:

![Log Ingestion Workflow](/_astro/azure-log-analytics.HULrdTJf_19DKCs.svg)

The [Data Collection Endpoint (DCE)](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-collection-endpoint-overview) is an authenticated HTTPS API endpoint that accepts HTTP POST requests with events encoded as JSON arrays in the request body. The [Data Collection Rule (DCR)](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-collection-rule-overview) offers optional transformation of arriving data and routes the data to a Log Analytics table.

Azure Monitor Setup Tutorial

The following use cases assume that you have already set up the Azure Monitor side, for example, by following the [official tutorial](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/tutorial-logs-ingestion-portal) that walks through for setting up a sample Entra application to authenticate against the API, to create a DCE to receive data, to create a custom table in a Log Analytics workspace and DCR to forward data to that table, and to give the applciation the proper permissions to access the created DCE and DCR.

## Examples

[Section titled “Examples”](#examples)

### Send logs to custom table

[Section titled “Send logs to custom table”](#send-logs-to-custom-table)

Let’s assume that you have the following CSV file that you want to send to a custom table:

users.csv

```csv
user,age
Alice,42
Bob,43
Charlie,44
```

Assuming you have already [created a custom table](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/create-custom-table#create-a-custom-table) called `Custom-Users`, you can send this file to the table using the [`to_azure_log_analytics`](/reference/operators/to_azure_log_analytics) operator:

```tql
load_file "users.csv"
read_csv
to_azure_log_analytics \
  tenant_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  client_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  client_secret="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  dce="https://my-dce.westeurope-1.ingest.monitor.azure.com",
  dcr="dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  table="Custom-Users"
```

# Defender

[Microsoft Defender](https://learn.microsoft.com/en-us/defender-xdr/microsoft-365-defender-portal) offers protection, detection, investigation, and response to threats. Defender comes in multiple editions, [Defender for Office 365](https://learn.microsoft.com/en-us/defender-office-365/mdo-about), [Defender for Endpoint](https://learn.microsoft.com/en-us/defender-endpoint/), [Defender for IoT](https://learn.microsoft.com/en-us/defender-for-iot/microsoft-defender-iot), [Defender for Identity](https://learn.microsoft.com/en-us/defender-for-identity/what-is), and [Defender for Cloud](https://learn.microsoft.com/en-us/defender-xdr/microsoft-365-security-center-defender-cloud). All Defender products can stream events in real time to Tenzir using [Azure Event Hubs](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about).

![Microsoft Defender](/_astro/defender.0xY2bvqA_19DKCs.svg)

Microsoft Defender Setup

The following example assumes that you have already set up Microsoft Defender and Microsoft Defender XDR, for example, by following the [official documentation](https://learn.microsoft.com/en-us/azure/defender-for-cloud/connect-azure-subscription).

## Requirements and Setup

[Section titled “Requirements and Setup”](#requirements-and-setup)

### Azure Event Hub & Kafka

[Section titled “Azure Event Hub & Kafka”](#azure-event-hub--kafka)

To stream security events from Defender in realtime, you can use Azure Event Hub, which provides a Kafka endpoint starting at the Standard tier. Make sure to enable *Kafka Surface* after the Event Hub setup.

### Microsoft Security Center

[Section titled “Microsoft Security Center”](#microsoft-security-center)

In Microsoft Security Center, configure Streaming under `System -> Settings -> Microsoft Defender XDR -> General -> Streaming API`. Add a new Streaming API for the target Event Hub and enable all event types that you want to collect.

## Examples

[Section titled “Examples”](#examples)

### Process Defender events with a pipeline

[Section titled “Process Defender events with a pipeline”](#process-defender-events-with-a-pipeline)

Tenzir’s [Kafka integration](/integrations/kafka) allows for seamless consumption of Defender events. In the following pipeline, replace all strings starting with `YOUR_`with the configuration values in Azure under `Event Hub Namespace -> Settings -> Shared access policies -> (Your policy)`.

```tql
from "kafka://YOUR_EVENT_HUB_NAME", options = {
  "bootstrap.servers": "YOUR_EVENT_HUB_NAME.servicebus.windows.net:9093",
  "security.protocol": "SASL_SSL",
  "sasl.mechanism": "PLAIN",
  "sasl.username": "$ConnectionString",
  "sasl.password": "YOUR_CONNECTION_STRING" // Connection string-primary key
} {
  read_json
}
```

After replacing the configuration values, your pipeline may look like this:

```tql
from "kafka://tenzir-defender-event-hub", options = {
  "bootstrap.servers": "tenzir-defender-event-hub.servicebus.windows.net:9093",
  "security.protocol": "SASL_SSL",
  "sasl.mechanism": "PLAIN",
  "sasl.username": "$ConnectionString",
  "sasl.password": "Endpoint=sb://tenzir-defender-event-hub.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=SECRET123456"
} {
  read_json
}
```

```tql
{
  records: [
    {
      time: "2024-12-04T13:38:20.360851",
      tenantId: "40431729-d276-4582-abb4-01e21c8b58fe",
      operationName: "Publish",
      category: "AdvancedHunting-IdentityLogonEvents",
      _TimeReceivedBySvc: "2024-12-04T13:36:26.632556",
      properties: {
        ActionType: "LogonFailed",
        LogonType: "Failed logon",
        Protocol: "Ntlm",
        AccountDisplayName: null,
        AccountUpn: null,
        AccountName: "elias",
        AccountDomain: "tenzir.com",
        AccountSid: null,
        AccountObjectId: null,
        IPAddress: null,
        Location: null,
        DeviceName: "WIN-P3MCS4024KP",
        OSPlatform: null,
        DeviceType: null,
        ISP: null,
        DestinationDeviceName: "ad-test.tenzir.com",
        TargetDeviceName: null,
        FailureReason: "UnknownUser",
        Port: null,
        DestinationPort: null,
        DestinationIPAddress: null,
        TargetAccountDisplayName: null,
        AdditionalFields: {
          Count: "1",
          Category: "Initial Access",
          AttackTechniques: "Valid Accounts (T1078), Domain Accounts (T1078.002)",
          SourceAccountName: "tenzir.com\\elias",
          SourceComputerOperatingSystemType: "unknown",
          DestinationComputerObjectGuid: "793e9b90-9eef-4620-aaa2-442a22f81321",
          DestinationComputerOperatingSystem: "windows server 2022 datacenter",
          DestinationComputerOperatingSystemVersion: "10.0 (20348)",
          DestinationComputerOperatingSystemType: "windows",
          SourceComputerId: "computer win-p3mcs4024kp",
          FROM.DEVICE: "WIN-P3MCS4024KP",
          TO.DEVICE: "ad-test",
          ACTOR.DEVICE: "",
        },
        ReportId: "3d359b95-f8d5-4dbd-a64b-7327c92d32f1",
        Timestamp: "2024-12-04T13:33:19.801823",
        Application: "Active Directory",
      },
      Tenant: "DefaultTenant",
    },
  ]
}
```

# Windows Event Logs

Windows Event Logs are records generated by the Windows operating system and applications that detail system, security, and application-related events for monitoring and troubleshooting purposes.

Once Windows Event Logs are flowing in a Tenzir pipeline, you can use any operator to process them. The below examples simply [import all data into a node](/guides/edge-storage/import-into-a-node).

## Collect logs with an agent

[Section titled “Collect logs with an agent”](#collect-logs-with-an-agent)

Installing a third-party agent to ship logs away from a Windows machine is common way to send events to a remote location.

![Windows Events with Agent](/_astro/windows-events-with-agent.9XvCYTpA_19DKCs.svg)

Regardless of the concrete agent you are using for shipping, the high-level setup is always the same: the agent sends events in a push-based to a Tenzir pipeline.

### Winlogbeat

[Section titled “Winlogbeat”](#winlogbeat)

[Winlogbeat](https://www.elastic.co/beats/winlogbeat) is Elastic’s log shipper to get Windows Event Logs out of Windows machines into the Elastic stack.

#### Configure Winlogbeat

[Section titled “Configure Winlogbeat”](#configure-winlogbeat)

After [installing Winlogbeat](https://www.elastic.co/guide/en/beats/winlogbeat/current/winlogbeat-installation-configuration.html), create a configuration:

winlogbeat.yml

```yaml
# Choose your channels.
winlogbeat.event_logs:
  - name: Application
  - name: System
  - name: Security
  - name: ForwardedEvents
  - name: Windows PowerShell
  - name: Microsoft-Windows-Sysmon/Operational
  - name: Microsoft-Windows-PowerShell/Operational
  - name: Microsoft-Windows-Windows Defender/Operational
  - name: Microsoft-Windows-TaskScheduler/Operational
  - name: Microsoft-Windows-TerminalServices-LocalSessionManager/Operational
  - name: Microsoft-Windows-TerminalServices-RDPClient/Operational


# Send data to a Tenzir pipeline with an ElasticSearch source.
output.elasticsearch:
  hosts: ["https://10.0.0.1:9200"]
  username: "$USER"
  password: "$PASSWORD"
  ssl:
    enabled: true
    certificate_authorities: [C:\Program Files\Winlogbeat\ca.crt]
    # PEM format
    certificate: C:\Program Files\Winlogbeat\tenzir.crt
    key: C:\Program Files\Winlogbeat\beat-win10\tenzir.key
```

#### Start Winlogbeat as a service

[Section titled “Start Winlogbeat as a service”](#start-winlogbeat-as-a-service)

After completing your configuration, start the Winlogbeat service:

```plaintext
C:\Program Files\Winlogbeat> Start-Service winlogbeat
```

#### Run a Tenzir pipeline

[Section titled “Run a Tenzir pipeline”](#run-a-tenzir-pipeline)

Now consume the data via a Tenzir pipeline using the [Elasticsearch](/integrations/elasticsearch)/[OpenSearch](/integrations/opensearch) integration that mimics a bulk ingest endpoint:

```tql
from "elasticsearch://10.0.0.1:92000" \
  tls=true,
  certfile="server.crt",
  keyfile="private.key"
import
```

### Fluent Bit

[Section titled “Fluent Bit”](#fluent-bit)

Since Tenzir has native Fluent Bit support, collecting logs via the Fluent Bit agent is a simple approach.

#### Configure Fluent Bit

[Section titled “Configure Fluent Bit”](#configure-fluent-bit)

First, install Fluent Bit on Windows according to the [official instructions](https://docs.fluentbit.io/manual/installation/windows).

Then create a [YAML configuration](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/yaml/configuration-file) to send Windows Event Logs out via the [Forward output](https://docs.fluentbit.io/manual/pipeline/outputs/forward), which encodes the events using Fluent Bit’s MsgPack-based wire format:

```yaml
input:
  - name: winevtlog
    channels: Setup,Windows PowerShell
    interval_sec: 1
    db: winevtlog.sqlite
output:
  - name: forward
    match: "*"
    host: 10.0.0.1
```

Adapt `input.channels` according to the Event Log channels you would like Fluent Bit to monitor.

#### Run a Tenzir pipeline

[Section titled “Run a Tenzir pipeline”](#run-a-tenzir-pipeline-1)

Use the [`from_fluent_bit`](/reference/operators/from_fluent_bit) source operator with the [Forward input](https://docs.fluentbit.io/manual/pipeline/inputs/forward):

```tql
from_fluent_bit "forward", options={
  listen: 10.0.0.1,
}
import
```

Ensure that the `listen` parameter matches the `host` value in your Fluent Bit configuration.

#### Test the configuration

[Section titled “Test the configuration”](#test-the-configuration)

Test the setup by running the Fluent Bit command line utility:

```plaintext
C:\Program Files\fluent-bit\bin\fluent-bit.exe -c \fluent-bit\conf\fluent-bit.yaml
```

#### Deploy Fluent Bit as a service

[Section titled “Deploy Fluent Bit as a service”](#deploy-fluent-bit-as-a-service)

To make the setup permanent, [run Fluent Bit as a service](https://docs.fluentbit.io/manual/installation/windows#windows-service-support).

Create the service:

```plaintext
sc.exe create fluent-bit binpath= "\fluent-bit\bin\fluent-bit.exe -c \fluent-bit\conf\fluent-bit.yaml"
```

Start and check the service:

```plaintext
sc.exe start fluent-bit
sc.exe query fluent-bit
```

Start the service at boot:

```plaintext
sc.exe config fluent-bit start= auto
```

### NXLog

[Section titled “NXLog”](#nxlog)

The NXLog agent collects Windows Event Logs and offers numerous [output modules](https://docs.nxlog.co/refman/current/om/index.html). You have several options with Tenzir. We use the JSON extension to format the data in all examples.

#### Ship logs via TCP

[Section titled “Ship logs via TCP”](#ship-logs-via-tcp)

To send logs straight to a TCP socket, use the [TCP output module](https://docs.nxlog.co/refman/current/om/tcp.html) with the following configuration:

```plaintext
<Extension json>
  Module    xm_json
</Extension>


<Output tcp>
  Module    om_tcp
  Host      10.0.0.1:1514
  Exec      to_json();
</Output>
```

Import the logs via TCP:

```tql
from "tcp://10.0.0.1:1514" {
  read_json
}
import
```

#### Ship logs via TLS/SSL

[Section titled “Ship logs via TLS/SSL”](#ship-logs-via-tlsssl)

For an encrypted connection, use the [SSL output module](https://docs.nxlog.co/refman/current/om/ssl.html) with the following configuration:

```plaintext
<Extension json>
  Module          xm_json
</Extension>


<Output ssl>
  Module          om_ssl
  Host            example.com:23456
  CAFile          %CERTDIR%/ca.pem
  CertFile        %CERTDIR%/client-cert.pem
  CertKeyFile     %CERTDIR%/client-key.pem
  KeyPass         secret
  AllowUntrusted  TRUE
  OutputType      Binary
  Exec            to_json();
</Output>
```

Import the logs via TCP:

```tql
load_tcp "127.0.0.1:4000",
  tls=true,
  certfile="key_and_cert.pem",
  keyfile="key_and_cert.pem" {
    read_json
  }
import
```

#### Ship logs via Kafka

[Section titled “Ship logs via Kafka”](#ship-logs-via-kafka)

The [Kafka output module](https://docs.nxlog.co/refman/current/om/kafka.html) publishes to a Kafka topic that Tenzir can read from. Use the following output configuration to publish to the `nxlog` topic:

```plaintext
<Output out>
  Module          om_kafka
  BrokerList      localhost:9092
  Topic           nxlog
  LogqueueSize    100000
  Partition       0
  Protocol        ssl
  CAFile          %CERTDIR%/ca.pem
  CertFile        %CERTDIR%/client-cert.pem
  CertKeyFile     %CERTDIR%/client-key.pem
  KeyPass         thisisasecret
</Output>
```

Then use our [Kafka integration](/integrations/kafka) to read from the topic:

```tql
from "kafka://nxlog" {
  read_json
}
import
```

## Collect logs via WEF & WEC

[Section titled “Collect logs via WEF & WEC”](#collect-logs-via-wef--wec)

Instead of deploying an agent on a Windows endpoint, you can also use native facilities to collect logs centrally.

To this end, Windows comes with a **Windows Event Forwarding (WEF)** mechanism on the endpoints that uses standard *Windows Remote Management (WinRM)* protocols to transmit events. The **Windows Event Collector (WEC)** is the service running on a server that receives the events sent by clients through WEF. The WEC aggregates these logs and makes them available for review and analysis. Administrators can create subscriptions on the WEC that define which events to collect from which clients, using criteria such as event IDs, keywords, or log levels.

The diagram below illustrates a typical setup:

![WEF & WEC](/_astro/windows-events-wef-wec.xYR9mcD5_19DKCs.svg)

On the WEC, you typically ship the collected logs away using a third-party agent, as described above. Read below on using [OpenWEC](#collect-logs-via-openwec) as an agent-free alternative.

The following configuration steps are heavily inspired by [SEKOIA](https://docs.sekoia.io/xdr/features/collect/integrations/endpoint/windows/#windows-event-forwarder-to-windows-event-collector-to-a-concentrator)’s instructions.

### Setup the Window Event Collector (WEC)

[Section titled “Setup the Window Event Collector (WEC)”](#setup-the-window-event-collector-wec)

We begin with setting up the WEC prior to connecting the client to it.

#### Setup Windows Remote Management (WinRM)

[Section titled “Setup Windows Remote Management (WinRM)”](#setup-windows-remote-management-winrm)

Configure WinRM as follows:

```cmd
winrm qc -q
```

The argument `qc` stands for “quick configuration” to perform a basic configuration of WinRM with default settings. This configuration includes starting the WinRM service, setting it to start automatically with the system, creating a listener on HTTP to accept WS-Management protocol requests, and configuring the Windows Firewall to allow WinRM traffic.

The `-q` flag means “quiet mode” to avoid prompting you for any input or confirmation, i.e., it makes the process non-interactive.

:::caution Unencrypted HTTP This command sets up WinRM to listen on HTTP, which is not encrypted. For a secure production environment, it’s advisable to configure WinRM to use HTTPS, which requires additional steps, including setting up an appropriate server certificate for encryption. :::

#### Enable the Event Collector service

[Section titled “Enable the Event Collector service”](#enable-the-event-collector-service)

Use the `wecutil` command to perform the necessary steps to set up the Windows Event Collector service:

```cmd
wecutil qc /q
```

As above, `qc` stands for “quick configuration” and `/q` for “quiet”. The setup includes actions such as configuring the service to start automatically and ensuring that the service is in a state ready to create and manage event subscriptions.

#### Create a subscription file

[Section titled “Create a subscription file”](#create-a-subscription-file)

Create a new file with the following contents:

DC\_SUBSCRIPTION.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Subscription xmlns="http://schemas.microsoft.com/2006/03/windows/events/subscription">
    <!-- Name of subscription -->
    <SubscriptionId>DC_SUBSCRIPTION</SubscriptionId>
    <!-- Push mode (DC to WEC) -->
    <SubscriptionType>SourceInitiated</SubscriptionType>
    <Description>Source Initiated Subscription from DC_SUBSCRIPTION</Description>
    <!-- Subscription is active -->
    <Enabled>true</Enabled>
    <Uri>http://schemas.microsoft.com/wbem/wsman/1/windows/EventLog</Uri>
    <!-- This mode ensures that events are delivered with minimal delay -->
    <!-- It is an appropriate choice if you are collecting alerts or critical events -->
    <!-- It uses push delivery mode and sets a batch timeout of 30 seconds -->
    <ConfigurationMode>MinLatency</ConfigurationMode>
    <!-- Event log to retrieved -->
    <Query>
        <![CDATA[
            <QueryList>
                <Query Id="0">
                    <Select Path="Application">*</Select>
                    <Select Path="Security">*</Select>
                    <Select Path="System">*</Select>
                </Query>
            </QueryList>
        ]]>
    </Query>
    <!-- Collect events generated since the subscription (not oldest) -->
    <ReadExistingEvents>false</ReadExistingEvents>
    <!-- Protocol and port used (DC to WEC) -->
    <TransportName>http</TransportName>
    <!-- Mandatory value (https://www-01.ibm.com/support/docview.wss?crawler=1&uid=swg1IV71375) -->
    <ContentFormat>RenderedText</ContentFormat>
    <Locale Language="en-US"/>
    <!-- Target Event log on WEC -->
    <LogFile>ForwardedEvents</LogFile>
    <!-- Define which domain computers are allowed or not to initiate subscriptions -->
    <!-- This exemple grants members of the Domain Computers domain group, as well as the local Network Service group (for local forwarder) -->
    <AllowedSourceDomainComputers>O:NSG:NSD:(A;;GA;;;DC)(A;;GA;;;NS)</AllowedSourceDomainComputers>
</Subscription>
```

Key elements are:

* `SubscriptionId`: Give a unique name to your subscription.
* `SubscriptionType`: Choose between `SourceInitiated` (push) or `CollectorInitiated` (pull).
* `Description`: Provide a meaningful description.
* `Query`: Modify the event log query to specify which events to collect.
* `LogFile`: Define the destination log file on the collector.
* `AllowedSourceDomainComputers`: Adjust the [SDDL](https://learn.microsoft.com/en-gb/windows/win32/secauthz/security-descriptor-definition-language?redirectedfrom=MSDN) string to specify which computers can forward events.

There are [several](https://github.com/palantir/windows-event-forwarding/tree/master/wef-subscriptions) [GitHub](https://github.com/nsacyber/Event-Forwarding-Guidance/tree/master/Subscriptions/samples) [repositories](https://github.com/mdecrevoisier/Windows-WEC-server_auto-deploy/tree/master/windows-subscriptions) out there with ideas for additional subscriptions.

#### Activate the subscription

[Section titled “Activate the subscription”](#activate-the-subscription)

Now that the collector is running, activate the subscription:

```cmd
wecutil cs "<FILE_PATH>\DC_SUBSCRIPTION.xml"
```

The `cs` subcommand stands for “create subscription” and creates a subscription according to the file passed as the next argument.

#### Verify the subscription

[Section titled “Verify the subscription”](#verify-the-subscription)

Finally, verify that the subscription is active:

```cmd
wecutil gr DC_SUBSCRIPTION
```

The `gr` subcommand stands for “get runtime status” and displays the subscription status for the ID `DC_SUBSCRIPTION`, which corresponds to the `<SubscriptionId>` XML tag in the configuration file.

### Setup Windows Event Forwarding (WEF)

[Section titled “Setup Windows Event Forwarding (WEF)”](#setup-windows-event-forwarding-wef)

After you completed the server-side configuration, now configure the machines that should log to the WEC.

#### Setup Windows Remote Management (WinRM)

[Section titled “Setup Windows Remote Management (WinRM)”](#setup-windows-remote-management-winrm-1)

Active WinRM as follows:

```cmd
winrm qc -q \
```

This step is identical to the WinRM configuration on the WEC.

#### Change local group policy settings

[Section titled “Change local group policy settings”](#change-local-group-policy-settings)

Use the Local Group Policy Editor (`gpedit.msc`) to navigate to the `Computer Configuration\Administrative Templates\Windows Components\Event Forwarding` path. Here, you’ll need to open the policy named “Configure the server address, refresh interval, and issuer certificate authority of a target Subscription Manager.”

In the policy settings, enable the policy and click the “Show…” button for SubscriptionManagers. Enter the server details for your WEC:

```plaintext
Server=http://WEC_FQDN:5985/wsman/SubscriptionManager/WEC,Refresh=60
```

Replace `WEC_FQDN` with the actual FQDN of your WEC.

#### Apply the local group policy

[Section titled “Apply the local group policy”](#apply-the-local-group-policy)

Refresh the Local Group Policy settings and apply the changes by running:

```plaintext
gpupdate /force
```

On the WEC, now [verify that the machine forwards events](#verify-the-subscription).

## Collect logs via OpenWEC

[Section titled “Collect logs via OpenWEC”](#collect-logs-via-openwec)

Instead of natively running a WEC on a Windows machine, you can also run the third-party implementation [OpenWEC](https://github.com/cea-sec/openwec).

![OpenWEC](/_astro/windows-events-openwec.o1DLW_jW_19DKCs.svg)

From a functional perspective, this setup is identical to running a native WEC, but it does not require an additional agent at the WEC. In addition, OpenWEC can be scaled redundantly for high availability setups.

### Setup OpenWEC

[Section titled “Setup OpenWEC”](#setup-openwec)

Refer to the [OpenWEC getting started guide](https://github.com/cea-sec/openwec/blob/main/doc/getting_started.md) for setup instructions.

For running OpenWEC on non-Windows machines that are likely not joined to a Windows domain, it is most useful to configure TLS client authentication. The OpenWEC documentation (see above) recommends to use [a script collection from NXLog](https://gitlab.com/nxlog-public/contrib/-/tree/master/windows-event-forwarding) that creates keys/certificates that can immediately be used to configure both Windows clients and the OpenWEC collector. Make sure to pay attention to specifying the correct hostnames for the sending and receiving machines.

```bash
git clone https://gitlab.com/nxlog-public/contrib
cd contrib/windows-event-forwarding
./genca.sh myca
./gencert-client.sh myclient.domain.com
./gencert-server.sh openwec.domain.com
```

Use the following for the Subscription Manager string:

```plaintext
Server=HTTPS://openwec.domain.com:5985/wsman/,Refresh=14400,IssuerCA=6605742C5400141B76A747E19EA585E29B09F017
```

The string in the last line can be used to configure the Windows Event Forwarding subscription manager on the sending side as described in the section above (“Change local group policy settings”). While you’re at it, also import the `client.pfx` and `ca-cert.pem` into the corresponding stores (see [the documentation](https://github.com/cea-sec/openwec/blob/main/doc/tls.md#client-configuration)).

Then, configure the OpenWEC server as follows (assuming the output files are in `/etc`, the directory `/var/db/openwec` exists and it is writable by the current user):

openwec.conf.toml

```toml
[server]


[logging]
verbosity = "info"


[database]
type = "SQLite"
path = "/var/db/openwec/openwec.sqlite"


[[collectors]]
listen_address = "0.0.0.0"
hostname = "openwec.domain.com"


[collectors.authentication]
type = "Tls"
ca_certificate = "/etc/ca-cert.pem"
server_certificate = "/etc/server-cert.pem"
server_private_key = "/etc/server-key.pem"
```

Create the database (only needs to be done once):

```bash
openwec -c openwec.conf.toml db init
```

Start the server:

```bash
openwecd -c openwec.conf.toml
```

```plaintext
2024-01-30T13:59:26.295792509+01:00 INFO server - Server settings: Server { db_sync_interval: None, flush_heartbeats_interval: None, heartbeats_queue_size: None, node_name: None, keytab: None, tcp_keepalive_time: None, tcp_keepalive_intvl: None, tcp_keepalive_probes: None }
2024-01-30T13:59:26.295947557+01:00 INFO server::subscription - reload_subscriptions task started
2024-01-30T13:59:26.296046314+01:00 INFO server::heartbeat - Heartbeat task started
2024-01-30T13:59:26.297503212+01:00 WARN server::subscription - There are no active subscriptions!
2024-01-30T13:59:26.306151854+01:00 INFO server::tls - Loaded TLS configuration with server certificate /etc/server-cert.pem
2024-01-30T13:59:26.309876793+01:00 INFO server - Server listenning on 0.0.0.0:5985
```

It might make sense to ensure that the server is started and kept up via some automated means, like systemd.

Then, while the server is running, create a subscription in OpenWEC for the desired channels. For example, to match the subscription from the example above, create an XML file like this:

subscription.xml

```xml
<QueryList>
    <Query Id="0">
        <Select Path="Application">*</Select>
        <Select Path="Security">*</Select>
        <Select Path="System">*</Select>
    </Query>
</QueryList>
```

Pass this file to `openwec` to create a subscription, e.g., with name `DC_SUBSCRIPTION`:

```bash
openwec subscriptions new DC_SUBSCRIPTION subscription.xml
```

For the new subscription, configure JSON over TCP as [output](https://github.com/cea-sec/openwec/blob/main/doc/outputs.md):

```bash
openwec subscriptions edit DC_SUBSCRIPTION outputs add --format json tcp 10.0.0.1 1514
```

Finally, enable the subscription:

```bash
openwec subscriptions enable DC_SUBSCRIPTION
```

That’s it! You should now be able read the Windows event logs in JSON format by spinning up a server that listens at `tcp://10.0.0.1:1514`.

### Run a Tenzir pipeline

[Section titled “Run a Tenzir pipeline”](#run-a-tenzir-pipeline-2)

Accept the logs sent with the configuration above into Tenzir via [TCP](/integrations/tcp):

```tql
from "tcp://10.0.0.1:1514" {
  read_json
}
publish "wec"
```

## Increase visibility with Sysmon

[Section titled “Increase visibility with Sysmon”](#increase-visibility-with-sysmon)

[Sysmon](https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon) (System Monitor) is a Windows system service and device driver that, once installed on a system, remains resident across system reboots to monitor and log system activity to the Windows event log. Key features include:

* **Process creation tracking**: Logs details of new processes.
* **Network connection monitoring**: Records incoming and outgoing network connections.
* **File creation time changes**: Tracks changes to file creation times.
* **Driver and image load monitoring**: Logs loading of drivers and DLL files.
* **Registry tracking**: Monitors changes to the Windows registry.

### Download and extract Sysmon via Powershell

[Section titled “Download and extract Sysmon via Powershell”](#download-and-extract-sysmon-via-powershell)

Download Sysmon:

```ps
Invoke-WebRequest -Uri "https://download.sysinternals.com/files/Sysmon.zip" -OutFile "Sysmon.zip"
```

Extract the archive:

```plaintext
Expand-Archive -Path Sysmon.zip -DestinationPath Sysmon
```

### Choose a Symon configuration

[Section titled “Choose a Symon configuration”](#choose-a-symon-configuration)

Choose a suitable Sysmon configuration, e.g., from [Florian Roth](https://github.com/Neo23x0/sysmon-config/) or [SwiftOnSecurity](https://github.com/SwiftOnSecurity/sysmon-config):

```ps
Invoke-WebRequest -Uri "https://raw.githubusercontent.com/Neo23x0/sysmon-config/master/sysmonconfig-export.xml" -OutFile "sysmonconfig-export.xml"
```

### Install Symon with a configuration

[Section titled “Install Symon with a configuration”](#install-symon-with-a-configuration)

```ps
.\Sysmon64.exe -accepteula -i sysmonconfig-export.xml
```

Now use any of the above techniques to collect event logs through the channel `Microsoft-Windows-Sysmon/Operational`.

# Network Interface

Tenzir supports reading packets from a network interface card (NIC).

The [`load_nic`](/reference/operators/load_nic) produces a stream of bytes in PCAP file format:

![Packet pipeline](/_astro/nic.RqhazQst_19DKCs.svg)

We designed `load_nic` such that it produces a byte stream in the form of a PCAP file. That is, when the pipeline starts, it first produces a file header, followed by chunks of packets. This creates a byte stream that is wire-compatible with the PCAP format, allowing you to exchange `load_nic` with [`load_file`](/reference/operators/load_file) and It Just Works™.

## Examples

[Section titled “Examples”](#examples)

### List active network interfaces

[Section titled “List active network interfaces”](#list-active-network-interfaces)

If you don’t know what interface to read from, use the [`nics`](/reference/operators/nics) operator to identify suitable candidates:

```tql
nics
select name, addresses, up
where up
```

```tql
{
  name: "eth0",
  addresses: [
    "169.254.172.2",
    "fe80::6471:53ff:fe5f:a8cc",
  ],
  up: true,
}
{
  name: "eth1",
  addresses: [
    "10.0.101.13",
    "fe80::f7:75ff:fe66:94e5",
  ],
  up: true,
}
{
  name: "lo",
  addresses: [
    "127.0.0.1",
    "::1",
  ],
  up: true,
}
```

### Read packets from a network interface

[Section titled “Read packets from a network interface”](#read-packets-from-a-network-interface)

Load packets from `eth0` and parse them as PCAP:

```tql
load_nic "eth0"
read_pcap
head 3
```

```tql
{
  linktype: 1,
  timestamp: "2021-11-17T13:32:43.237882",
  captured_packet_length: 74,
  original_packet_length: 74,
  data: "ABY88f1tZJ7zvttmCABFAAA8inQAADQGN+yADoaqxkf3W+B8AFDc3z7hAAAAAKACchATrQAAAgQFtAQCCApMw7SVAAAAAAEDAwc=",
}
{
  linktype: 1,
  timestamp: "2021-11-17T13:32:43.237939",
  captured_packet_length: 74,
  original_packet_length: 74,
  data: "ZJ7zvttmABY88f1tCABFAAA8AABAAEAGdmDGR/dbgA6GqgBQ4HzXXzhD3N8+4qAS/ohsJAAAAgQFtAQCCAqjGGhDTMO0lQEDAwc=",
}
{
  linktype: 1,
  timestamp: "2021-11-17T13:32:43.249425",
  captured_packet_length: 66,
  original_packet_length: 66,
  data: "ABY88f1tZJ7zvttmCABFAAA0inUAADQGN/OADoaqxkf3W+B8AFDc3z7i1184RIAQAOWYkQAAAQEICkzDtJijGGhD",
}
```

### Decapsulate packets

[Section titled “Decapsulate packets”](#decapsulate-packets)

After you have structured data in the form of PCAP events, you can use the [`decapsulate`](/reference/functions/decapsulate) function to decode the binary `data`:

```tql
load_nic "eth0"
read_pcap
select packet = decapsulate(this)
head 1
```

```tql
{
  packet: {
    ether: {
      src: "64-9E-F3-BE-DB-66",
      dst: "00-16-3C-F1-FD-6D",
      type: 2048,
    },
    ip: {
      src: "128.14.134.170",
      dst: "198.71.247.91",
      type: 6,
    },
    tcp: {
      src_port: 57468,
      dst_port: 80,
    },
    community_id: "1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=",
  }
```

Decapsulation automatically computes a [Community ID](https://github.com/corelight/community-id-spec) for correlation in the `community_id` field. You could also use the [`community_id`](/reference/functions/community_id) function to compute this value manually for different events.

# OpenSearch

[OpenSearch](https://opensearch.org) is a search and observability suite for unstructured data. Tenzir can send events to OpenSearch and emulate and OpenSearch Bulk API endpoint.

![How to send and receive data](/_astro/opensearch.dpUdbHQj_19DKCs.svg)

When sending data to OpenSearch, Tenzir uses the [Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/) and attempts to maximally batch events for throughput, accumulating multiple events before shipping them within a single API call. You can control batching behavior with the `max_content_length` and `send_timeout` options.

For more details, see the documentation for the [`to_opensearch`](/reference/operators/to_opensearch) operator.

Tenzir can also present an OpenSearch-compatible REST API via the [`from_opensearch`](/reference/operators/from_opensearch) operator.

## Examples

### Send events to an OpenSearch index

Send an example event to the `main` index:

```tql
from {event: "example"}
to "opensearch://1.2.3.4:9200", action="create", index="main"
```

Instead of treating the entire event as document to be indexed by OpenSearch, you can designate a nested record as document:

```tql
from {category: "qux", doc_id: "XXX", event: {foo: "bar"}}
to "opensearch://localhost:9200", id=doc_id, doc=event, action="update", index=category
```

The above example updates the document with ID `XXX` with the contents from the nested field `event`.

### Accept data by emulating OpenSearch

Tenzir can act as drop-in replacement for OpenSearch by accepting data via a Bulk API endpoint. This allows you to point your [Logstash](https://opensearch.org/docs/latest/tools/logstash/index/) or Beats instances to Tenzir instead.

```tql
from "opensearch://localhost:9200", keep_actions=true
publish "opensearch"
```

This pipeline accepts data on port 9200 and publishes all received events on the

`opensearch` topic for further processing by other pipelines.

Setting `keep_actions=true` causes command events to remain in the stream, e.g., like this:

```tql
{create:{_index:"filebeat-8.17.3"}} // 👈 command event
{"@timestamp":2025-03-31T13:42:28.068Z,log:{offset:1,file:{path:"/mounted/logfile"}},message:"hello",input:{type:"log"},host:{name:"eb21"},agent:{id:"682cfcf4-f251-4576-abcb-6c8bcadfda08",name:"eb21",type:"filebeat",version:"8.17.3",ephemeral_id:"17f74f6e-36f0-4045-93e6-c549874716df"},ecs:{version:"8.0.0"}}
{create:{_index:"filebeat-8.17.3"}} // 👈 command event
{"@timestamp":2025-03-31T13:42:28.068Z,log:{offset:7,file:{path:"/mounted/logfile"}},message:"this",input:{type:"log"},host:{name:"eb21"},agent:{id:"682cfcf4-f251-4576-abcb-6c8bcadfda08",name:"eb21",type:"filebeat",version:"8.17.3",ephemeral_id:"17f74f6e-36f0-4045-93e6-c549874716df"},ecs:{version:"8.0.0"}}
```

#### Ship data via Filebeat

Configure [Filebeat](https://www.elastic.co/beats/filebeat) as follows to ship data to Tenzir:

filebeat.yml

```yaml
output:
  elasticsearch:
  hosts: ["localhost:9200"]
```

Set `hosts` to the endpoint of the Tenzir pipeline accepting data.

#### Ship data via Logstash

Configure [Logstash](https://opensearch.org/docs/latest/tools/logstash/ship-to-opensearch/) with the

`opensearch` output plugin to ship data to Tenzir:

pipeline.conf

```javascript
output {
  opensearch {
    hosts => "https://localhost:9200"
  }
}
```

Set `hosts` to the endpoint of the Tenzir pipeline accepting data.

# SentinelOne Data Lake

[SentinelOne](https://www.sentinelone.com) is a cybersecurity platform that provides endpoint protection and threat detection. The SentinelOne [Singularity Data Lake](https://www.sentinelone.com/products/singularity-data-lake/) allows you to store and analyze security events at scale. Tenzir can send structured security events to the SentinelOne Data Lake via its REST API.

![SentinelOne Data Lake](/_astro/sentinelone-data-lake.UmtBpY1y_19DKCs.svg)

The operator provides special handling for OCSF events. If it detects that the input event in OCSF, it will automatically map timestamp and severity fields to the corresponding SentinelOne Data Lake fields.

## Examples

[Section titled “Examples”](#examples)

### Send events to SentinelOne Data Lake

[Section titled “Send events to SentinelOne Data Lake”](#send-events-to-sentinelone-data-lake)

To send events from a pipeline to SentinelOne Data Lake, use the [`to_sentinelone_data_lake`](/reference/operators/to_sentinelone_data_lake) operator:

```tql
subscribe "suricata"
where @name == "suricata.alert"
to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net",
  token=secret("SENTINELONE_TOKEN")
```

Replace `https://ingest.eu1.sentinelone.net` with your conigured SentinelOne Data Lake ingest URL and configure the `SENTINELONE_TOKEN` secret with your bearer token.

### Send events with additional session information

[Section titled “Send events with additional session information”](#send-events-with-additional-session-information)

You can include additional session information that identifies the source of the events:

```tql
subscribe "network-events"
to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net",
  token=secret("SENTINELONE_TOKEN"),
  session_info={
    serverHost: "Tenzir Node 01",
    serverType: "Tenzir Node",
    region: "US East"
  }
```

### Send OCSF events

[Section titled “Send OCSF events”](#send-ocsf-events)

If the datastream input is valid OCSF, the operator will automatically extract timestamp and severity fields and map them to the corresponding SentinelOne Data Lake fields `ts` and `sev`:

```tql
subscribe "ocsf"
where severity_id >= 4  // High and Critical events only
to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net",
  token=secret("SENTINELONE_TOKEN"),
  session_info={serverHost: "Security Gateway"}
```

### Send unstructured data

[Section titled “Send unstructured data”](#send-unstructured-data)

You can also use the operator to send unstructured data and let SentinelOne parse it. Simply give the operator a `message` field as input and specify a `parser` in the `session_info` argument:

```tql
select message = this.print_ndjson();         // Format the entire event as JSON
to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net",
  token=secret("sentinelone-token"),
  session_info={
    serverHost: "Node 42",
    parser: "json",                           // Have SentinelOne parse the data
  }
```

In this example, we are formatting the entire event as JSON and sending it as the message field. The SentinelOne `json` parser will then parse the event again.

Ingest Costs

SentinelOne charges per ingested byte in any value, including the unstructured `message`. This means SentinelOne charges for keys, structural elements and whitespace in `message`.

If you already have structured data in Tenzir, prefer sending structured data. SentinelOne will only charge for the values, one byte per key and nothing for the requests structure.

# Snowflake

[Snowflake](https://snowflake.com) is a multi-cloud data warehouse. Tenzir can send events from a pipeline to [Snowflake databases](https://docs.snowflake.com/en/sql-reference/ddl-database).

![Snowflake](/_astro/snowflake.B5IsBZ0A_19DKCs.svg)

Use the [`to_snowflake`](/reference/operators/to_snowflake) output operator at the end of a pipeline to send events to a specific table.

ADBC

Tenzir uses [Apache Arrow](https://arrow.apache.org) under the hood to encode batches of events into a columnar representation. The [Arrow Database Connectivity (ADBC)](https://arrow.apache.org/docs/format/ADBC.html) API makes makes it possible to efficiently transfer large datasets between Tenzir and a database. Think of ADBC as the columnar equivalent of JDBC/ODBC. ADBC has a [Snowflake driver](https://arrow.apache.org/adbc/current/driver/snowflake.html) that Tenzir uses to send events to Snowflake with the bulk ingestion API. For further details on ADBC, read the [introductory blog post](https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/) from the Arrow project.

## Examples

[Section titled “Examples”](#examples)

### Send data to a Snowflake database

[Section titled “Send data to a Snowflake database”](#send-data-to-a-snowflake-database)

```tql
from {foo: 42, bar: true}
to_snowflake \
  account_identifier="asldyuf-xgb47555",
  user_name="tenzir_user",
  password="password1234",
  database="MY_DB",
  schema="MY_SCHEMA",
  table="TENZIR"
```

# Splunk

[Splunk](https://splunk.com) is a SIEM solution for storing and processing logs. Tenzir can send data to Splunk via HEC.

![Splunk](/_astro/splunk.CB0WFkHp_19DKCs.svg)

## Examples

[Section titled “Examples”](#examples)

### Send data to an existing HEC endpoint

[Section titled “Send data to an existing HEC endpoint”](#send-data-to-an-existing-hec-endpoint)

To send data from a pipeline to a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector) endpoint, use the [`to_splunk`](/reference/operators/to_splunk) operator.

For example, deploy the following pipeline to forward [Suricata](/integrations/suricata) alerts to Splunk:

```tql
subscribe "suricata"
where @name == "suricata.alert"
to_splunk "https://1.2.3.4:8088", hec_token="TOKEN", tls_no_verify=true
```

Replace `1.2.3.4` with the IP address of your Splunk host and `TOKEN` with your HEC token.

For more details, see the documentation for the [`to_splunk`](/reference/operators/to_splunk) operator.

### Spawn a HEC endpoint as pipeline source

[Section titled “Spawn a HEC endpoint as pipeline source”](#spawn-a-hec-endpoint-as-pipeline-source)

To send data to a Tenzir pipeline instead of Splunk, you can open a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector) endpoint using the [`from_fluent_bit`](/reference/operators/from_fluent_bit) source operator.

For example, to onboard all data into a Tenzir node instead of Splunk, point your data source to the IP address of the Tenzir node at port 9880 by deploying this pipeline:

```tql
from_fluent_bit "splunk", options={
  splunk_token: "TOKEN",
}
publish "splunk"
```

Replace `TOKEN` with the Splunk token configured at your data source.

To listen on a different IP address, e.g., 1.2.3.4 add `listen: 1.2.3.4` to the `options` argument.

For more details, read the official [Fluent Bit documentation of the Splunk input](https://docs.fluentbit.io/manual/pipeline/inputs/splunk).

## Example Co-Deployment

[Section titled “Example Co-Deployment”](#example-co-deployment)

To test Splunk and Tenzir together, use the following [Docker Compose](https://docs.docker.com/compose/) setup.

### Setup the containers

[Section titled “Setup the containers”](#setup-the-containers)

docker-compose.yaml

```yaml
version: "3.9"


services:
  splunk:
    image: ${SPLUNK_IMAGE:-splunk/splunk:latest}
    platform: linux/amd64
    container_name: splunk
    environment:
      - SPLUNK_START_ARGS=--accept-license
      - SPLUNK_HEC_TOKEN=abcd1234
      - SPLUNK_PASSWORD=tenzir123
    ports:
      - 8000:8000
      - 8088:8088


  tenzir-node:
    container_name: "Demo"
    image: tenzir/tenzir:latest
    pull_policy: always
    environment:
      - TENZIR_PLUGINS__PLATFORM__CONTROL_ENDPOINT=wss://ws.tenzir.app/production
      - TENZIR_PLUGINS__PLATFORM__API_KEY=<PLATFORM_API_KEY>
      - TENZIR_PLUGINS__PLATFORM__TENANT_ID=<PLATFORM_TENANT_ID>
      - TENZIR_ENDPOINT=tenzir-node:5158
    entrypoint:
      - tenzir-node
    volumes:
      - tenzir-node:/var/lib/tenzir/
      - tenzir-node:/var/log/tenzir/


  tenzir:
    image: tenzir/tenzir:latest
    pull_policy: never
    profiles:
      - donotstart
    depends_on:
      - tenzir-node
    environment:
      - TENZIR_ENDPOINT=tenzir-node:5158


volumes:
  tenzir-node:
    driver: local
```

### Configure Splunk

[Section titled “Configure Splunk”](#configure-splunk)

After you spun up the containers, configure Splunk as follows:

1. Go to `http://localhost:8000` and login with `admin`:`tenzir123`

2. Navigate to *Add data* → *Monitor* → *HTTP Event Collector*

3. Configure the event collector:

   * Name: Tenzir
   * Click *Next*
   * Copy the token
   * Keep *Start searching*

# Suricata

[Suricata](https://suricata.io/) is network monitor with a rule matching engine to detect threats. Use Tenzir to acquire, process, and store Suricata logs.

![Suricata](/_astro/suricata.DtM9yz2q_19DKCs.svg)

## Examples

[Section titled “Examples”](#examples)

### Ingest EVE JSON logs into a node

[Section titled “Ingest EVE JSON logs into a node”](#ingest-eve-json-logs-into-a-node)

[EVE JSON](https://docs.suricata.io/en/latest/output/eve/eve-json-output.html) is the log format in which Suricata generates events.

A typical Suricata configuration looks like this:

suricata.yaml

```yaml
outputs:
  # Extensible Event Format (nicknamed EVE) event log in JSON format
  - eve-log:
      enabled: yes
      filetype: regular #regular|syslog|unix_dgram|unix_stream|redis
      filename: eve.json
```

The `filetype` setting determines how you’d process the log file and defaults to `regular`.

Onboard Suricata EVE JSON logs via the [`read_suricata`](/reference/operators/read_suricata) operator as follows:

```tql
load_file "/path/to/eve.json"
read_suricata
publish "suricata"
```

If your set `filetype` to `unix_stream`, you need to create a Unix domain socket first, e.g., like this:

```bash
nc -U -l /tmp/eve.socket
```

Then use the same pipeline as above; Tenzir automatically detects the file type.

# Syslog

Tenzir supports parsing and emitting Syslog messages across multiple transport protocols, including both UDP and TCP. This enables seamless integration with Syslog-based systems for ingesting or exporting logs.

![Syslog](/_astro/syslog.B1nQyzBj_19DKCs.svg)

Syslog support in Tenzir is powered by two components:

* [`read_syslog`](/reference/operators/read_syslog): a parser that turns unstructured Syslog messages into structured events.
* [`write_syslog`](/reference/operators/write_syslog): a printer that transforms structured events into compliant Syslog messages.

Together, these building blocks enable round-trip Syslog processing.

## Examples

[Section titled “Examples”](#examples)

### Create a Syslog Server

[Section titled “Create a Syslog Server”](#create-a-syslog-server)

To receive Syslog messages on a UDP socket, use `from` with [`read_syslog`](/reference/operators/read_syslog):

```tql
from "udp://0.0.0.0:514", insert_newlines=true {
  read_syslog
}
publish "syslog"
```

To use TCP instead of UDP, change the scheme and omit the `insert_newlines` option:

```tql
from "tcp://0.0.0.0:514" {
  read_syslog
}
publish "syslog"
```

One pipeline per accepted connection

For TCP, the pipeline inside `from` executes *for each accepted connection*.

### Parsing CEF, LEEF, or JSON Payloads

[Section titled “Parsing CEF, LEEF, or JSON Payloads”](#parsing-cef-leef-or-json-payloads)

If your Syslog messages embed structured formats like CEF, LEEF, or JSON, you can follow up with an additional parser. For example, assume you have a Syslog message that includes CEF:

```txt
Nov 13 16:00:02 host123 FOO: CEF:0|FORCEPOINT|Firewall|6.6.1|78002|TLS connection state|0|deviceExternalId=Master FW node 1 dvc=10.1.1.40 dvchost=10.1.1.40 msg=TLS: Couldn't establish TLS connection (11, N/A) deviceFacility=Management rt=Jan 17 2020 08:52:09
```

Why you throw [`read_syslog`](/reference/operators/read_syslog) at this line, you’ll get this output:

sample.syslog

```tql
{
  facility: null,
  severity: null,
  timestamp: "Nov 13 16:00:02",
  hostname: "host123",
  app_name: "FOO",
  process_id: null,
  content: "CEF:0|FORCEPOINT|Firewall|6.6.1|78002|TLS connection state|0|deviceExternalId=Master FW node 1 dvc=10.1.1.40 dvchost=10.1.1.40 msg=TLS: Couldn't establish TLS connection (11, N/A) deviceFacility=Management rt=Jan 17 2020 08:52:09",
}
```

Note that the `content` field is just a big string. Parse it with [`parse_cef`](/reference/functions/parse_cef):

```tql
load_file "/tmp/sample.syslog"
read_syslog
content = content.parse_cef()
```

This yields the following structured output:

```tql
{
  facility: null,
  severity: null,
  timestamp: "Nov 13 16:00:02",
  hostname: "host123",
  app_name: "FOO",
  process_id: null,
  content: {
    cef_version: 0,
    device_vendor: "FORCEPOINT",
    device_product: "Firewall",
    device_version: "6.6.1",
    signature_id: "78002",
    name: "TLS connection state",
    severity: "0",
    extension: {
      deviceExternalId: "Master FW node 1",
      dvc: 10.1.1.40,
      dvchost: 10.1.1.40,
      msg: "TLS: Couldn't establish TLS connection (11, N/A)",
      deviceFacility: "Management",
      rt: "Jan 17 2020 08:52:09",
    },
  },
}
```

### Handling Multi-line Syslog Messages

[Section titled “Handling Multi-line Syslog Messages”](#handling-multi-line-syslog-messages)

Tenzir’s Syslog parser supports multi-line messages using a heuristic:

1. Split the input at newlines.
2. Try parsing the next line as a new Syslog message.
3. If successful, treat it as a new message.
4. If parsing fails, append the line to the current message and repeat.

This allows ingesting logs with stack traces or other verbose content correctly.

## Emit Events as Syslog

[Section titled “Emit Events as Syslog”](#emit-events-as-syslog)

Tenzir also supports **creating** Syslog messages from structured events via [`write_syslog`](/reference/operators/write_syslog).

Here’s a basic example that emits a single Syslog line over UDP:

```tql
from {
  facility: 3,
  severity: 6,
  timestamp: 2020-03-02T18:44:46,
  hostname: "parallels-Parallels-Virtual-Platform",
  app_name: "packagekitd",
  process_id: "1370",
  message_id: "",
  structured_data: {},
  message: " PARENT process running...",
}
write_syslog
save_udp "1.2.3.4:514"
```

This pipeline sends the following RFC 5424-formatted message to `1.2.3.4:514/udp`:

```txt
<30>1 2020-03-02T18:44:46.000000Z parallels-Parallels-Virtual-Platform packagekitd 1370 - -  PARENT process running...
```

### Example with Structured Data

[Section titled “Example with Structured Data”](#example-with-structured-data)

Here is a richer event with structured Syslog fields. Let’s create a Syslog event from it:

```tql
from {
  facility: 20,
  severity: 5,
  version: 8,
  timestamp: 2003-10-11T22:14:15,
  hostname: "mymachineexamplecom",
  app_name: "evntslog",
  process_id: "",
  message_id: "ID47",
  structured_data: {
    "exampleSDID@32473": {
      iut: 5,
      eventSource: "Applic\\ation",
      eventID: 1011,
    },
    "examplePriority@32473": {
      class: "high",
    },
  },
  message: null,
}
write_syslog
```

Output:

```txt
<165>1 2003-10-11T22:14:15.000000Z mymachineexamplecom evntslog - ID47 [exampleSDID@32473 iut="5" eventSource="Applic\\ation" eventID="1011"][examplePriority@32473 class="high"]
```

The [`write_syslog`](/reference/operators/write_syslog) operator converts the `structured_data` field into a valid [RFC 5424](https://datatracker.ietf.org/doc/html/rfc5424) structured block.

# TCP

The [Transmission Control Protocol (TCP)](https://en.wikipedia.org/wiki/Transmission_Control_Protocol) offers a bi-directional byte stream between applications that communicate via IP. Tenzir supports writing to and reading from TCP sockets, both in server (listening) and client (connect) mode.

![TCP](/_astro/tcp.C0uV4vEi_19DKCs.svg)

Use the IP address `0.0.0.0` to listen on all available network interfaces.

URL Support

The URL schemes `tcp://` and `tcps://` dispatch to [`load_tcp`](/reference/operators/load_tcp) and [`save_tcp`](/reference/operators/save_tcp) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## SSL/TLS

[Section titled “SSL/TLS”](#ssltls)

To enable TLS, use `tls=true`. You can optionally pass a PEM-encoded certificate and private key via the `certfile` and `keyfile` options.

For testing purposes, you can quickly generate a self-signed certificate as follows:

```bash
openssl req -x509 -newkey rsa:2048 -keyout key_and_cert.pem -out key_and_cert.pem -days 365 -nodes
```

An easy way to test a TLS connection is to try connecting via OpenSSL:

```bash
openssl s_client 127.0.0.1:443
```

## Examples

[Section titled “Examples”](#examples)

### Read data by connecting to a remote TCP server

[Section titled “Read data by connecting to a remote TCP server”](#read-data-by-connecting-to-a-remote-tcp-server)

```tql
from "tcp://127.0.0.1:443", connect=true {
  read_json
}
```

### Read data by listen on localhost with TLS enabled

[Section titled “Read data by listen on localhost with TLS enabled”](#read-data-by-listen-on-localhost-with-tls-enabled)

```tql
from "tcp://127.0.0.1:443", tls=true, certfile="cert.pem", keyfile="key.pem" {
  read_json
}
```

# UDP

The [User Datagram Protocol (UDP)](https://en.wikipedia.org/wiki/User_Datagram_Protocol) is a connection-less protocol to send messages on an IP network. Tenzir supports writing to and reading from UDP sockets, both in server (listening) and client (connect) mode.

![UDP](/_astro/udp.BzerWlJj_19DKCs.svg)

Use the IP address `0.0.0.0` to listen on all available network interfaces.

URL Support

The URL scheme `udp://` dispatches to [`load_udp`](/reference/operators/load_udp) and [`save_udp`](/reference/operators/save_udp) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Examples

[Section titled “Examples”](#examples)

### Accept Syslog messages over UDP

[Section titled “Accept Syslog messages over UDP”](#accept-syslog-messages-over-udp)

```tql
from "udp://127.0.0.1:541", insert_newlines=true {
  read_syslog
}
```

### Send events to a UDP socket

[Section titled “Send events to a UDP socket”](#send-events-to-a-udp-socket)

```tql
from {message: "Tenzir"}
to "udp://1.2.3.4:8080" {
  write_ndjson
}
```

# Velociraptor

[Velociraptor](https://docs.velociraptor.app) is a digital forensics and incident response (DFIR) tool for interrogating endpoints.

Use Tenzir to conveniently speak with a Velociraptor server over the [gRPC API](https://docs.velociraptor.app/docs/server_automation/server_api/).

![Velociraptor](/_astro/velociraptor.3H76nfIW_19DKCs.svg)

## Create a TLS certificate to communicate with Velociraptor

[Section titled “Create a TLS certificate to communicate with Velociraptor”](#create-a-tls-certificate-to-communicate-with-velociraptor)

The `velociraptor` acts as client and establishes a connection to a Velociraptor server via gRPC. All Velociraptor client-to-server communication is mutually authenticated and encrypted via TLS certificates. This means you must provide a client-side certificate, which you can generate as follows. (Velociraptor ships as a static binary that we refer to as `velociraptor-binary` here.)

1. Create a server configuration `server.yaml`:

   ```bash
   velociraptor-binary config generate > server.yaml
   ```

2. Create an API client:

   ```bash
   velociraptor-binary -c server.yaml config api_client --name tenzir client.yaml
   ```

   Copy the generated `client.yaml` to your Tenzir [plugin configuration](/reference/node/configuration) directory as `velociraptor.yaml` so that the operator can find it:

   ```bash
   cp client.yaml /etc/tenzir/plugin/velociraptor.yaml
   ```

3. Create a user (e.g., an admin named `tenzir`):

   ```bash
   velociraptor-binary -v -c server.yaml user add --role administrator tenzir
   ```

4. Run the frontend with the server configuration:

   ```bash
   velociraptor-binary -c server.yaml frontend
   ```

## Examples

[Section titled “Examples”](#examples)

### Run raw VQL

[Section titled “Run raw VQL”](#run-raw-vql)

After you have created a TLS certificate, you can use the [`from_velociraptor`](/reference/operators/from_velociraptor) operator to execute a [Velociraptor Query Language (VQL)](https://docs.velociraptor.app/docs/vql/) query:

```tql
from_velociraptor query="select * from pslist()"
select Name, Pid, PPid, CommandLine
where Name == "remotemanagement"
```

### Subscribe to forensic artifacts

[Section titled “Subscribe to forensic artifacts”](#subscribe-to-forensic-artifacts)

You can also hunt for forensic artifacts, such as dropped files or specific entries in the Windows registry, on assets connected to your Velociraptor server. Every time a client reports back on an artifact that matches a given Regex, e.g., `Windows` or `Windows.Sys.StartupItems`, the Velociraptor server sends the result into the pipeline.

For example, run this pipeline to subscribe to an artifact collection of Windows startup items and import them into a node:

```tql
from_velociraptor subscribe="Windows.Sys.StartupItems"
import
```

# Zeek

The [Zeek](https://zeek.org) network monitor translates raw packets into structured logs. Tenzir supports various Zeek use cases, such as continuous ingestion, ad-hoc log file processing, and even generating Zeek logs.

![Zeek](/_astro/zeek.dLhB74jT_19DKCs.svg)

Zeek logs come in three forms in practice, all of which Tenzir can parse natively:

1. Tab-Separated Values (TSV) with a custom header.
2. One NDJSON file for all log types combined (aka. *JSON Streaming*)
3. One NDJSON file per log type.

## Examples

[Section titled “Examples”](#examples)

### Ingest logs into a node

[Section titled “Ingest logs into a node”](#ingest-logs-into-a-node)

To ingest Zeek logs into a Tenzir node, you have multiple options.

#### Easy-button import with the official Zeek package

[Section titled “Easy-button import with the official Zeek package”](#easy-button-import-with-the-official-zeek-package)

Our official [Zeek package](https://github.com/tenzir/zeek-tenzir) makes it easy to ship your Zeek logs to a Tenzir node. Install the package first:

```bash
zkg install zeek-tenzir
```

Then add this to your `$PREFIX/share/zeek/site/local.zeek` to send all logs that Zeek produces to a Tenzir node:

```plaintext
@load tenzir/import


# Uncomment to keep the original Zeek logs.
# redef Tenzir::delete_after_postprocesing=F;
```

For ad-hoc command line processing you can also pass `tenzir/import` to a Zeek invocation:

```bash
# Ship logs to it and delete the original files.
zeek -r trace.pcap tenzir/import


# Ship logs to it and keep the original files.
zeek -r trace.pcap tenzir/import Tenzir::delete_after_postprocesing=F
```

#### Run an import pipeline when rotating logs

[Section titled “Run an import pipeline when rotating logs”](#run-an-import-pipeline-when-rotating-logs)

If you cannot use our Zeek package, it is still possible to let Zeek trigger an import pipeline upon rotation. Zeek’s [logging framework](https://docs.zeek.org/en/master/frameworks/logging.html) can execute a shell script whenever it rotates a log file.

This requires setting `Log::default_rotation_interval` to a non-zero value. The default of `0 secs` means that log rotation is disabled. Add this to `$PREFIX/share/zeek/site/local.zeek`, which is the place for local configuration changes:

```plaintext
redef Log::default_rotation_interval = 1 day;
```

Then redefine [`Log::default_rotation_postprocessor_cmd`](https://docs.zeek.org/en/master/scripts/base/frameworks/logging/main.zeek.html#id-Log::default_rotation_postprocessor_cmd) to point to your shell script, e.g., `/usr/local/bin/ingest`:

```plaintext
redef Log::default_rotation_postprocessor_cmd=/usr/local/bin/ingest;
```

Here is an example `ingest` script that imports all files into a Tenzir node:

ingest

```bash
#!/bin/sh


file_name="$1"
base_name="$2"
from="$3"
to="$4"
terminating="$5"
writer="$6"


if [ "$writer" = "ascii" ]; then
  read="read_zeek_tsv"
elif [ "$writer" = "json" ]; then
  read="read_zeek_json"
else
  echo "unsupported Zeek writer: $writer"
  exit 1
fi


pipeline="load_file \"$file_name\" | $read | import"


tenzir "$pipeline"
```

Our blog post [Native Zeek Log Rotation & Shipping](https://tenzir.com/blog/native-zeek-log-rotation-and-shipping) provides further details on this method.

### Run Zeek on a packet pipeline

[Section titled “Run Zeek on a packet pipeline”](#run-zeek-on-a-packet-pipeline)

You can run Zeek on a pipeline of PCAP packets and continue processing the logs in the same pipeline. A stock Tenzir installation comes with a user-defined `zeek` operator that looks as follows:

tenzir.yaml

```yaml
tenzir:
  operators:
    zeek: |
      shell "eval \"$(zkg env)\" &&
             zeek -r - LogAscii::output_to_stdout=T
             JSONStreaming::disable_default_logs=T
             JSONStreaming::enable_log_rotation=F
             json-streaming-logs"
      read_zeek_json
```

This allows you run Zeek on a packet trace as follows:

```bash
tenzir 'load_file "/path/to/trace.pcap" | zeek'
```

You can also perform more elaborate packet filtering after light-weight [decapsulation](/reference/functions/decapsulate):

```bash
tenzir 'load_file "/path/to/trace.pcap"
        read_pcap
        this = decapsulate(this)
        where ip.src in 10.0.0.0/8 || community == "1:YXWfTYEyYLKVv5Ge4WqijUnKTrM="
        write_pcap
        zeek'
```

### Process logs with a pipeline on the command line

[Section titled “Process logs with a pipeline on the command line”](#process-logs-with-a-pipeline-on-the-command-line)

Zeek ships with a helper utility `zeek-cut` that operators on Zeek’s tab-separated logs. For example, to extract the host pairs from a conn log:

```bash
zeek-cut id.orig_h id.resp_h < conn.log
```

The list of arguments to `zeek-cut` are the column names of the log. The [`select`](/reference/operators/select) operator performs the equivalent in Tenzir after we parse the logs as Zeek TSV:

```bash
tenzir 'read_zeek_tsv | select id.orig_h id.resp_h' < conn.log
```

Since pipelines are *multi-schema* and the Zeek TSV parser is aware of log boundaries, you can also concatenate logs of various types:

```bash
cat *.log | tenzir 'read_zeek_tsv | select id.orig_h id.resp_h'
```

### Generate Zeek TSV from arbitrary data

[Section titled “Generate Zeek TSV from arbitrary data”](#generate-zeek-tsv-from-arbitrary-data)

You can render any data as Zeek TSV log using [`write_zeek_tsv`](/reference/operators/write_zeek_tsv):

For example, this is how you create a filtered version of a Zeek conn.log:

```tql
subscribe "zeek"
where @name == "zeek.conn"
where duration > 2s and id.orig_p != 80
write_zeek_tsv
save_file "filtered_conn.log"
```

# ZeroMQ

[ZeroMQ](https://zeromq.org/) (0mq) is a light-weight messaging framework with various socket types. Tenzir supports writing to [PUB sockets](https://zeromq.org/socket-api/#pub-socket) and reading from [SUB sockets](https://zeromq.org/socket-api/#sub-socket), both in server (listening) and client (connect) mode.

![ZeroMQ](/_astro/zeromq.CtgcNXCM_19DKCs.svg)

Use the IP address `0.0.0.0` to listen on all available network interfaces.

Because ZeroMQ is entirely asynchronous, publishers send messages even when no subscriber is present. This can lead to lost messages when the publisher begins operating before the subscriber. To avoid data loss due to such races, pass `monitor=true` to activate message buffering until at least one remote peer has connected.

URL Support

The URL scheme `zmq://` dispatches to [`load_zmq`](/reference/operators/load_zmq) and [`save_zmq`](/reference/operators/save_zmq) for seamless URL-style use via [`from`](/reference/operators/from) and [`to`](/reference/operators/to).

## Examples

[Section titled “Examples”](#examples)

### Accept Syslog messages over ZeroMQ

[Section titled “Accept Syslog messages over ZeroMQ”](#accept-syslog-messages-over-zeromq)

```tql
from "zmq://127.0.0.1:541" {
  read_syslog
}
```

### Send events to a ZeroMQ socket

[Section titled “Send events to a ZeroMQ socket”](#send-events-to-a-zeromq-socket)

```tql
from {message: "Tenzir"}
to "zmq://1.2.3.4:8080" {
  write_ndjson
}
```

# Zscaler

Zscaler’s [Nanolog Streaming Service (NSS)](https://help.zscaler.com/zia/understanding-nanolog-streaming-service) is a family of products that enable Zscaler cloud communication with third-party security solution devices for exchanging event logs. You can either use Zscaler’s Cloud NSS or deploy an on-prem NSS server to obtain the logs. Tenzir can receive Zscaler logs in either case.

![Zscaler NSS](/_astro/zscaler.CbIXkL3W_19DKCs.svg)

## Use a Cloud NSS Feed to send events to Tenzir

[Section titled “Use a Cloud NSS Feed to send events to Tenzir”](#use-a-cloud-nss-feed-to-send-events-to-tenzir)

### Configure Tenzir

[Section titled “Configure Tenzir”](#configure-tenzir)

First, spin up a Tenzir pipeline that mimics a [Splunk HEC endpoint](/integrations/splunk):

```tql
from_fluent_bit "splunk", options={
  listen: 0.0.0.0,
  port: 8088,
  splunk_token: YOUR_TOKEN,
}
publish "zscaler"
```

In the above example, the pipeline uses `0.0.0.0` to listen on all IP addresses available.

### Create a Cloud NSS Feed

[Section titled “Create a Cloud NSS Feed”](#create-a-cloud-nss-feed)

Perform the following steps to create a Cloud NSS feed:

1. Log in to your ZIA Admin Portal and go to *Administration → Nanolog Streaming Service*.
2. Select the *Cloud NSS Feeds* tab.
3. Click *Add Cloud NSS Feed*.

In the new dialog, configure the following options:

* **Feed Name**: Enter a name, e.g., `Tenzir ZIA Logs`

* **NSS Type**: NSS for Web

* **Status**: Enabled

* **SIEM Rate**: Unlimited

* **SIEM Type**: Other

* **OAuth 2.0 Authentication**: disabled

* **Max Batch Size**: 16 KB

* **API URL**: Enter the URL that identifies the Tenzir pipeline where the Splunk HEC endpoint is listening, e.g., <https://1.2.3.4:8080/services/collector>.

* **HTTP Headers**: Add your token from the Tenzir pipeline enable compression with the following two headers.

  * `Authorization`: `YOUR_TOKEN`
  * `Content-Encoding`: `gzip`

* **Feed Output Type**: JSON

* **JSON Array Notation**: disabled

* **Feed Escape Character**: `,\"`

After a web, firewall, or DNS feed has been configured, activate the changes as needed and test the feed. To test the connection:

1. Go to *Administration* → *Nanolog Streaming Service* → *Cloud NSS Feeds*.
2. Click the *Cloud icon*. This sends a test message to the Tenzir receiver.

## Use an NSS Feed to send events to Tenzir

[Section titled “Use an NSS Feed to send events to Tenzir”](#use-an-nss-feed-to-send-events-to-tenzir)

The on-prem NSS server VM pulls logs from the Zscaler cloud and pushes them to Tenzir via Syslog over TCP. The NSS server also buffers logs if the TCP connection goes down.

Unencrypted Communication

Note that the forwarding TCP connection is *unencrypted*, as the assumption is that the NSS server is on premises next to the receiver.

### Configure Tenzir

[Section titled “Configure Tenzir”](#configure-tenzir-1)

Spin up a Tenzir pipeline that accepts [TCP](/integrations/tcp) connection and parses [Syslog](/integrations/syslog):

```tql
from "tcp://0.0.0.0:514" {
  read_syslog
}
publish "zscaler"
```

In the above example, the pipeline uses `0.0.0.0` to listen on all IP addresses available.

Depending on how your NSS Feed is configured, you can futher dissect the opaque Syslog message into a structured record.

### Create an NSS Feed

[Section titled “Create an NSS Feed”](#create-an-nss-feed)

First, [deploy an NSS server](https://help.zscaler.com/zia/adding-nss-servers). Then log into ZIA using your administrator account:

1. Go to *Administration* → *Cloud Configuration* → *Nanolog Streaming Service*.
2. Verify that the NSS State is Healthy. Deploy a new NSS server by following the steps at [NSS Deployment Guides](https://help.zscaler.com/zia/nanolog-streaming-service).
3. Click the *NSS Feeds* tab, and then click *Add NSS Feed*.

Follow the [official documentation to add an NSS feed](https://help.zscaler.com/zia/adding-nss-feeds). You can add up to 16 NSS feeds per NSS server.

In the new dialog, configure the following options:

* **Feed Name**: Enter a name, e.g., `Tenzir ZIA Logs`
* **NSS Server**: Choose your server
* **Status**: Enabled
* **SIEM Destination Type**: FQDN
* **SIEM IP Address**: Enter the address of your Tenzir Node
* **SIEM TCP Port**: Enter the port of your pipeline
* **SIEM Rate**: Unlimited
* **Feed Output Type**: JSON
* **Feed Escape Character**: `,\"`

# Reference

The **reference** has nitty-gritty technical descriptions of how Tenzir works. Its is most useful when you need detailed information about Tenzir's building blocks.

<!-- The SVG image -->

![Documentation structure](/reference.svg)

<!-- Clickable overlay areas -->

<!-- Tutorials (top-left) -->

[](/tutorials/)

<!-- Guides (top-right) -->

[](/guides/)

<!-- Explanations (bottom-left) -->

[](/explanations/)

<!-- Reference (bottom-right) -->

# Functions

Functions appear in [expressions](/explanations/language/expressions) and take positional and/or named arguments, producing a value as a result of their computation.

Function signatures have the following notation:

```tql
f(arg1:<type>, arg2=<type>, [arg3=type]) -> <type>
```

* `arg:<type>`: positional argument
* `arg=<type>`: named argument
* `[arg=type]`: optional (named) argument
* `-> <type>`: function return type

## Aggregation

[Section titled “Aggregation”](#aggregation)

### [all](/reference/functions/all)

[→](/reference/functions/all)

Computes the conjunction (AND) of all grouped boolean values.

```tql
all([true,true,false])
```

### [any](/reference/functions/any)

[→](/reference/functions/any)

Computes the disjunction (OR) of all grouped boolean values.

```tql
any([true,false,true])
```

### [collect](/reference/functions/collect)

[→](/reference/functions/collect)

Creates a list of all non-null grouped values, preserving duplicates.

```tql
collect([1,2,2,3])
```

### [count](/reference/functions/count)

[→](/reference/functions/count)

Counts the events or non-null grouped values.

```tql
count([1,2,null])
```

### [count\_distinct](/reference/functions/count_distinct)

[→](/reference/functions/count_distinct)

Counts all distinct non-null grouped values.

```tql
count_distinct([1,2,2,3])
```

### [count\_if](/reference/functions/count_if)

[→](/reference/functions/count_if)

Counts the events or non-null grouped values matching a given predicate.

```tql
count_if([1,2,null], x => x > 1)
```

### [distinct](/reference/functions/distinct)

[→](/reference/functions/distinct)

Creates a sorted list without duplicates of non-null grouped values.

```tql
distinct([1,2,2,3])
```

### [entropy](/reference/functions/entropy)

[→](/reference/functions/entropy)

Computes the Shannon entropy of all grouped values.

```tql
entropy([1,1,2,3])
```

### [first](/reference/functions/first)

[→](/reference/functions/first)

Takes the first non-null grouped value.

```tql
first([null,2,3])
```

### [last](/reference/functions/last)

[→](/reference/functions/last)

Takes the last non-null grouped value.

```tql
last([1,2,null])
```

### [max](/reference/functions/max)

[→](/reference/functions/max)

Computes the maximum of all grouped values.

```tql
max([1,2,3])
```

### [mean](/reference/functions/mean)

[→](/reference/functions/mean)

Computes the mean of all grouped values.

```tql
mean([1,2,3])
```

### [median](/reference/functions/median)

[→](/reference/functions/median)

Computes the approximate median of all grouped values using a t-digest algorithm.

```tql
median([1,2,3,4])
```

### [min](/reference/functions/min)

[→](/reference/functions/min)

Computes the minimum of all grouped values.

```tql
min([1,2,3])
```

### [mode](/reference/functions/mode)

[→](/reference/functions/mode)

Takes the most common non-null grouped value.

```tql
mode([1,1,2,3])
```

### [otherwise](/reference/functions/otherwise)

[→](/reference/functions/otherwise)

Returns a `fallback` value if `primary` is `null`.

```tql
x.otherwise(0)
```

### [quantile](/reference/functions/quantile)

[→](/reference/functions/quantile)

Computes the specified quantile of all grouped values.

```tql
quantile([1,2,3,4], q=0.5)
```

### [stddev](/reference/functions/stddev)

[→](/reference/functions/stddev)

Computes the standard deviation of all grouped values.

```tql
stddev([1,2,3])
```

### [sum](/reference/functions/sum)

[→](/reference/functions/sum)

Computes the sum of all values.

```tql
sum([1,2,3])
```

### [value\_counts](/reference/functions/value_counts)

[→](/reference/functions/value_counts)

Returns a list of all grouped values alongside their frequency.

```tql
value_counts([1,2,2,3])
```

### [variance](/reference/functions/variance)

[→](/reference/functions/variance)

Computes the variance of all grouped values.

```tql
variance([1,2,3])
```

## Bit Operations

[Section titled “Bit Operations”](#bit-operations)

### [bit\_and](/reference/functions/bit_and)

[→](/reference/functions/bit_and)

Computes the bit-wise AND of its arguments.

```tql
bit_and(lhs, rhs)
```

### [bit\_not](/reference/functions/bit_not)

[→](/reference/functions/bit_not)

Computes the bit-wise NOT of its argument.

```tql
bit_not(x)
```

### [bit\_or](/reference/functions/bit_or)

[→](/reference/functions/bit_or)

Computes the bit-wise OR of its arguments.

```tql
bit_or(lhs, rhs)
```

### [bit\_xor](/reference/functions/bit_xor)

[→](/reference/functions/bit_xor)

Computes the bit-wise XOR of its arguments.

```tql
bit_xor(lhs, rhs)
```

### [shift\_left](/reference/functions/shift_left)

[→](/reference/functions/shift_left)

Performs a bit-wise left shift.

```tql
shift_left(lhs, rhs)
```

### [shift\_right](/reference/functions/shift_right)

[→](/reference/functions/shift_right)

Performs a bit-wise right shift.

```tql
shift_right(lhs, rhs)
```

## Decoding

[Section titled “Decoding”](#decoding)

### [decode\_base64](/reference/functions/decode_base64)

[→](/reference/functions/decode_base64)

Decodes bytes as Base64.

```tql
decode_base64("VGVuemly")
```

### [decode\_hex](/reference/functions/decode_hex)

[→](/reference/functions/decode_hex)

Decodes bytes from their hexadecimal representation.

```tql
decode_hex("4e6f6E6365")
```

### [decode\_url](/reference/functions/decode_url)

[→](/reference/functions/decode_url)

Decodes URL encoded strings.

```tql
decode_url("Hello%20World")
```

## Encoding

[Section titled “Encoding”](#encoding)

### [encode\_base64](/reference/functions/encode_base64)

[→](/reference/functions/encode_base64)

Encodes bytes as Base64.

```tql
encode_base64("Tenzir")
```

### [encode\_hex](/reference/functions/encode_hex)

[→](/reference/functions/encode_hex)

Encodes bytes into their hexadecimal representation.

```tql
encode_hex("Tenzir")
```

### [encode\_url](/reference/functions/encode_url)

[→](/reference/functions/encode_url)

Encodes strings using URL encoding.

```tql
encode_url("Hello World")
```

## Hashing

[Section titled “Hashing”](#hashing)

### [hash\_md5](/reference/functions/hash_md5)

[→](/reference/functions/hash_md5)

Computes an MD5 hash digest.

```tql
hash_md5("foo")
```

### [hash\_sha1](/reference/functions/hash_sha1)

[→](/reference/functions/hash_sha1)

Computes a SHA-1 hash digest.

```tql
hash_sha1("foo")
```

### [hash\_sha224](/reference/functions/hash_sha224)

[→](/reference/functions/hash_sha224)

Computes a SHA-224 hash digest.

```tql
hash_sha224("foo")
```

### [hash\_sha256](/reference/functions/hash_sha256)

[→](/reference/functions/hash_sha256)

Computes a SHA-256 hash digest.

```tql
hash_sha256("foo")
```

### [hash\_sha384](/reference/functions/hash_sha384)

[→](/reference/functions/hash_sha384)

Computes a SHA-384 hash digest.

```tql
hash_sha384("foo")
```

### [hash\_sha512](/reference/functions/hash_sha512)

[→](/reference/functions/hash_sha512)

Computes a SHA-512 hash digest.

```tql
hash_sha512("foo")
```

### [hash\_xxh3](/reference/functions/hash_xxh3)

[→](/reference/functions/hash_xxh3)

Computes an XXH3 hash digest.

```tql
hash_xxh3("foo")
```

## IP

[Section titled “IP”](#ip)

### [ip\_category](/reference/functions/ip_category)

[→](/reference/functions/ip_category)

Returns the type classification of an IP address.

```tql
ip_category(8.8.8.8)
```

### [is\_global](/reference/functions/is_global)

[→](/reference/functions/is_global)

Checks whether an IP address is a global address.

```tql
is_global(8.8.8.8)
```

### [is\_link\_local](/reference/functions/is_link_local)

[→](/reference/functions/is_link_local)

Checks whether an IP address is a link-local address.

```tql
is_link_local(169.254.1.1)
```

### [is\_loopback](/reference/functions/is_loopback)

[→](/reference/functions/is_loopback)

Checks whether an IP address is a loopback address.

```tql
is_loopback(127.0.0.1)
```

### [is\_multicast](/reference/functions/is_multicast)

[→](/reference/functions/is_multicast)

Checks whether an IP address is a multicast address.

```tql
is_multicast(224.0.0.1)
```

### [is\_private](/reference/functions/is_private)

[→](/reference/functions/is_private)

Checks whether an IP address is a private address.

```tql
is_private(192.168.1.1)
```

### [is\_v4](/reference/functions/is_v4)

[→](/reference/functions/is_v4)

Checks whether an IP address has version number 4.

```tql
is_v4(1.2.3.4)
```

### [is\_v6](/reference/functions/is_v6)

[→](/reference/functions/is_v6)

Checks whether an IP address has version number 6.

```tql
is_v6(::1)
```

### [network](/reference/functions/network)

[→](/reference/functions/network)

Retrieves the network address of a subnet.

```tql
10.0.0.0/8.network()
```

## List

[Section titled “List”](#list)

### [add](/reference/functions/add)

[→](/reference/functions/add)

Adds an element into a list if it doesn't already exist (set-insertion).

```tql
xs.add(y)
```

### [append](/reference/functions/append)

[→](/reference/functions/append)

Inserts an element at the back of a list.

```tql
xs.append(y)
```

### [concatenate](/reference/functions/concatenate)

[→](/reference/functions/concatenate)

Merges two lists.

```tql
concatenate(xs, ys)
```

### [get](/reference/functions/get)

[→](/reference/functions/get)

Gets a field from a record or an element from a list

```tql
xs.get(index, fallback)
```

### [length](/reference/functions/length)

[→](/reference/functions/length)

Retrieves the length of a list.

```tql
[1,2,3].length()
```

### [map](/reference/functions/map)

[→](/reference/functions/map)

Maps each list element to an expression.

```tql
xs.map(x => x + 3)
```

### [prepend](/reference/functions/prepend)

[→](/reference/functions/prepend)

Inserts an element at the start of a list.

```tql
xs.prepend(y)
```

### [remove](/reference/functions/remove)

[→](/reference/functions/remove)

Removes all occurrences of an element from a list.

```tql
xs.remove(y)
```

### [sort](/reference/functions/sort)

[→](/reference/functions/sort)

Sorts lists and record fields.

```tql
xs.sort()
```

### [where](/reference/functions/where)

[→](/reference/functions/where)

Filters list elements based on a predicate.

```tql
xs.where(x => x > 5)
```

### [zip](/reference/functions/zip)

[→](/reference/functions/zip)

Combines two lists into a list of pairs.

```tql
zip(xs, ys)
```

## Math

[Section titled “Math”](#math)

### [abs](/reference/functions/abs)

[→](/reference/functions/abs)

Returns the absolute value.

```tql
abs(-42)
```

### [ceil](/reference/functions/ceil)

[→](/reference/functions/ceil)

Computes the ceiling of a number or a time/duration with a specified unit.

```tql
ceil(4.2)
```

### [floor](/reference/functions/floor)

[→](/reference/functions/floor)

Computes the floor of a number or a time/duration with a specified unit.

```tql
floor(4.8)
```

### [round](/reference/functions/round)

[→](/reference/functions/round)

Rounds a number or a time/duration with a specified unit.

```tql
round(4.6)
```

### [sqrt](/reference/functions/sqrt)

[→](/reference/functions/sqrt)

Computes the square root of a number.

```tql
sqrt(49)
```

## Networking

[Section titled “Networking”](#networking)

### [community\_id](/reference/functions/community_id)

[→](/reference/functions/community_id)

Computes the Community ID for a network connection/flow.

```tql
community_id(src_ip=1.2.3.4, dst_ip=4.5.6.7, proto="tcp")
```

### [decapsulate](/reference/functions/decapsulate)

[→](/reference/functions/decapsulate)

Decapsulates packet data at link, network, and transport layer.

```tql
decapsulate(this)
```

### [encrypt\_cryptopan](/reference/functions/encrypt_cryptopan)

[→](/reference/functions/encrypt_cryptopan)

Encrypts an IP address via Crypto-PAn.

```tql
encrypt_cryptopan(1.2.3.4)
```

## OCSF

[Section titled “OCSF”](#ocsf)

### [ocsf::category\_name](/reference/functions/ocsf/category_name)

[→](/reference/functions/ocsf/category_name)

Returns the `category_name` for a given `category_uid`.

```tql
ocsf::category_name(2)
```

### [ocsf::category\_uid](/reference/functions/ocsf/category_uid)

[→](/reference/functions/ocsf/category_uid)

Returns the `category_uid` for a given `category_name`.

```tql
ocsf::category_uid("Findings")
```

### [ocsf::class\_name](/reference/functions/ocsf/class_name)

[→](/reference/functions/ocsf/class_name)

Returns the `class_name` for a given `class_uid`.

```tql
ocsf::class_name(4003)
```

### [ocsf::class\_uid](/reference/functions/ocsf/class_uid)

[→](/reference/functions/ocsf/class_uid)

Returns the `class_uid` for a given `class_name`.

```tql
ocsf::class_uid("DNS Activity")
```

### [ocsf::type\_name](/reference/functions/ocsf/type_name)

[→](/reference/functions/ocsf/type_name)

Returns the `type_name` for a given `type_uid`.

```tql
ocsf::type_name(400704)
```

### [ocsf::type\_uid](/reference/functions/ocsf/type_uid)

[→](/reference/functions/ocsf/type_uid)

Returns the `type_uid` for a given `type_name`.

```tql
ocsf::type_uid("SSH Activity: Fail")
```

## Parsing

[Section titled “Parsing”](#parsing)

### [parse\_cef](/reference/functions/parse_cef)

[→](/reference/functions/parse_cef)

Parses a string as a CEF message

```tql
string.parse_cef()
```

### [parse\_csv](/reference/functions/parse_csv)

[→](/reference/functions/parse_csv)

Parses a string as CSV (Comma-Separated Values).

```tql
string.parse_csv(header=["a","b"])
```

### [parse\_grok](/reference/functions/parse_grok)

[→](/reference/functions/parse_grok)

Parses a string according to a grok pattern.

```tql
string.parse_grok("%{IP:client} …")
```

### [parse\_json](/reference/functions/parse_json)

[→](/reference/functions/parse_json)

Parses a string as a JSON value.

```tql
string.parse_json()
```

### [parse\_kv](/reference/functions/parse_kv)

[→](/reference/functions/parse_kv)

Parses a string as key-value pairs.

```tql
string.parse_kv()
```

### [parse\_leef](/reference/functions/parse_leef)

[→](/reference/functions/parse_leef)

Parses a string as a LEEF message

```tql
string.parse_leef()
```

### [parse\_ssv](/reference/functions/parse_ssv)

[→](/reference/functions/parse_ssv)

Parses a string as space separated values.

```tql
string.parse_ssv(header=["a","b"])
```

### [parse\_syslog](/reference/functions/parse_syslog)

[→](/reference/functions/parse_syslog)

Parses a string as a Syslog message.

```tql
string.parse_syslog()
```

### [parse\_tsv](/reference/functions/parse_tsv)

[→](/reference/functions/parse_tsv)

Parses a string as tab separated values.

```tql
string.parse_tsv(header=["a","b"])
```

### [parse\_xsv](/reference/functions/parse_xsv)

[→](/reference/functions/parse_xsv)

Parses a string as delimiter separated values.

```tql
string.parse_xsv(",", ";", "", header=["a","b"])
```

### [parse\_yaml](/reference/functions/parse_yaml)

[→](/reference/functions/parse_yaml)

Parses a string as a YAML value.

```tql
string.parse_yaml()
```

## Printing

[Section titled “Printing”](#printing)

### [print\_cef](/reference/functions/print_cef)

[→](/reference/functions/print_cef)

Prints records as Common Event Format (CEF) messages

```tql
extension.print_cef(cef_version="0", device_vendor="Tenzir", device_product="Tenzir Node", device_version="5.5.0", signature_id=id, name="description", severity="7")
```

### [print\_csv](/reference/functions/print_csv)

[→](/reference/functions/print_csv)

Prints a record as a comma-separated string of values.

```tql
record.print_csv()
```

### [print\_json](/reference/functions/print_json)

[→](/reference/functions/print_json)

Transforms a value into a JSON string.

```tql
record.print_json()
```

### [print\_kv](/reference/functions/print_kv)

[→](/reference/functions/print_kv)

Prints records in a key-value format.

```tql
record.print_kv()
```

### [print\_leef](/reference/functions/print_leef)

[→](/reference/functions/print_leef)

Prints records as LEEF messages

```tql
attributes.print_leef(vendor="Tenzir",product_name="Tenzir Node", product_name="5.5.0",event_class_id=id)
```

### [print\_ndjson](/reference/functions/print_ndjson)

[→](/reference/functions/print_ndjson)

Transforms a value into a single-line JSON string.

```tql
record.print_ndjson()
```

### [print\_ssv](/reference/functions/print_ssv)

[→](/reference/functions/print_ssv)

Prints a record as a space-separated string of values.

```tql
record.print_ssv()
```

### [print\_tsv](/reference/functions/print_tsv)

[→](/reference/functions/print_tsv)

Prints a record as a tab-separated string of values.

```tql
record.print_tsv()
```

### [print\_xsv](/reference/functions/print_xsv)

[→](/reference/functions/print_xsv)

Prints a record as a delimited sequence of values.

```tql
record.print_tsv()
```

### [print\_yaml](/reference/functions/print_yaml)

[→](/reference/functions/print_yaml)

Prints a value as a YAML document.

```tql
record.print_yaml()
```

## Record

[Section titled “Record”](#record)

### [get](/reference/functions/get)

[→](/reference/functions/get)

Gets a field from a record or an element from a list

```tql
xs.get(index, fallback)
```

### [has](/reference/functions/has)

[→](/reference/functions/has)

Checks whether a record has a specified field.

```tql
record.has("field")
```

### [keys](/reference/functions/keys)

[→](/reference/functions/keys)

Retrieves a list of field names from a record.

```tql
record.keys()
```

### [merge](/reference/functions/merge)

[→](/reference/functions/merge)

Combines two records into a single record by merging their fields.

```tql
merge(foo, bar)
```

### [sort](/reference/functions/sort)

[→](/reference/functions/sort)

Sorts lists and record fields.

```tql
xs.sort()
```

## Runtime

[Section titled “Runtime”](#runtime)

### [config](/reference/functions/config)

[→](/reference/functions/config)

Reads Tenzir's configuration file.

```tql
config()
```

### [env](/reference/functions/env)

[→](/reference/functions/env)

Reads an environment variable.

```tql
env("PATH")
```

### [secret](/reference/functions/secret)

[→](/reference/functions/secret)

Use the value of a secret.

```tql
secret("KEY")
```

## Subnet

[Section titled “Subnet”](#subnet)

### [network](/reference/functions/network)

[→](/reference/functions/network)

Retrieves the network address of a subnet.

```tql
10.0.0.0/8.network()
```

## Time & Date

[Section titled “Time & Date”](#time--date)

### [count\_days](/reference/functions/count_days)

[→](/reference/functions/count_days)

Counts the number of `days` in a duration.

```tql
count_days(100d)
```

### [count\_hours](/reference/functions/count_hours)

[→](/reference/functions/count_hours)

Counts the number of `hours` in a duration.

```tql
count_hours(100d)
```

### [count\_microseconds](/reference/functions/count_microseconds)

[→](/reference/functions/count_microseconds)

Counts the number of `microseconds` in a duration.

```tql
count_microseconds(100d)
```

### [count\_milliseconds](/reference/functions/count_milliseconds)

[→](/reference/functions/count_milliseconds)

Counts the number of `milliseconds` in a duration.

```tql
count_milliseconds(100d)
```

### [count\_minutes](/reference/functions/count_minutes)

[→](/reference/functions/count_minutes)

Counts the number of `minutes` in a duration.

```tql
count_minutes(100d)
```

### [count\_months](/reference/functions/count_months)

[→](/reference/functions/count_months)

Counts the number of `months` in a duration.

```tql
count_months(100d)
```

### [count\_nanoseconds](/reference/functions/count_nanoseconds)

[→](/reference/functions/count_nanoseconds)

Counts the number of `nanoseconds` in a duration.

```tql
count_nanoseconds(100d)
```

### [count\_seconds](/reference/functions/count_seconds)

[→](/reference/functions/count_seconds)

Counts the number of `seconds` in a duration.

```tql
count_seconds(100d)
```

### [count\_weeks](/reference/functions/count_weeks)

[→](/reference/functions/count_weeks)

Counts the number of `weeks` in a duration.

```tql
count_weeks(100d)
```

### [count\_years](/reference/functions/count_years)

[→](/reference/functions/count_years)

Counts the number of `years` in a duration.

```tql
count_years(100d)
```

### [day](/reference/functions/day)

[→](/reference/functions/day)

Extracts the day component from a timestamp.

```tql
ts.day()
```

### [days](/reference/functions/days)

[→](/reference/functions/days)

Converts a number to equivalent days.

```tql
days(100)
```

### [format\_time](/reference/functions/format_time)

[→](/reference/functions/format_time)

Formats a time into a string that follows a specific format.

```tql
ts.format_time("%d/ %m/%Y")
```

### [from\_epoch](/reference/functions/from_epoch)

[→](/reference/functions/from_epoch)

Interprets a duration as Unix time.

```tql
from_epoch(time_ms * 1ms)
```

### [hour](/reference/functions/hour)

[→](/reference/functions/hour)

Extracts the hour component from a timestamp.

```tql
ts.hour()
```

### [hours](/reference/functions/hours)

[→](/reference/functions/hours)

Converts a number to equivalent hours.

```tql
hours(100)
```

### [microseconds](/reference/functions/microseconds)

[→](/reference/functions/microseconds)

Converts a number to equivalent microseconds.

```tql
microseconds(100)
```

### [milliseconds](/reference/functions/milliseconds)

[→](/reference/functions/milliseconds)

Converts a number to equivalent milliseconds.

```tql
milliseconds(100)
```

### [minute](/reference/functions/minute)

[→](/reference/functions/minute)

Extracts the minute component from a timestamp.

```tql
ts.minute()
```

### [minutes](/reference/functions/minutes)

[→](/reference/functions/minutes)

Converts a number to equivalent minutes.

```tql
minutes(100)
```

### [month](/reference/functions/month)

[→](/reference/functions/month)

Extracts the month component from a timestamp.

```tql
ts.month()
```

### [months](/reference/functions/months)

[→](/reference/functions/months)

Converts a number to equivalent months.

```tql
months(100)
```

### [nanoseconds](/reference/functions/nanoseconds)

[→](/reference/functions/nanoseconds)

Converts a number to equivalent nanoseconds.

```tql
nanoseconds(100)
```

### [now](/reference/functions/now)

[→](/reference/functions/now)

Gets the current wallclock time.

```tql
now()
```

### [parse\_time](/reference/functions/parse_time)

[→](/reference/functions/parse_time)

Parses a time from a string that follows a specific format.

```tql
"10/11/2012".parse_time("%d/%m/%Y")
```

### [second](/reference/functions/second)

[→](/reference/functions/second)

Extracts the second component from a timestamp with subsecond precision.

```tql
ts.second()
```

### [seconds](/reference/functions/seconds)

[→](/reference/functions/seconds)

Converts a number to equivalent seconds.

```tql
seconds(100)
```

### [since\_epoch](/reference/functions/since_epoch)

[→](/reference/functions/since_epoch)

Interprets a time value as duration since the Unix epoch.

```tql
since_epoch(2021-02-24)
```

### [weeks](/reference/functions/weeks)

[→](/reference/functions/weeks)

Converts a number to equivalent weeks.

```tql
weeks(100)
```

### [year](/reference/functions/year)

[→](/reference/functions/year)

Extracts the year component from a timestamp.

```tql
ts.year()
```

### [years](/reference/functions/years)

[→](/reference/functions/years)

Converts a number to equivalent years.

```tql
years(100)
```

## Utility

[Section titled “Utility”](#utility)

### [contains](/reference/functions/contains)

[→](/reference/functions/contains)

Searches for a value within data structures recursively.

```tql
this.contains("value")
```

### [contains\_null](/reference/functions/contains_null)

[→](/reference/functions/contains_null)

Checks whether the input contains any `null` values.

```tql
{x: 1, y: null}.contains_null() == true
```

### [is\_empty](/reference/functions/is_empty)

[→](/reference/functions/is_empty)

Checks whether a value is empty.

```tql
"".is_empty()
```

### [random](/reference/functions/random)

[→](/reference/functions/random)

Generates a random number in *\[0,1]*.

```tql
random()
```

### [uuid](/reference/functions/uuid)

[→](/reference/functions/uuid)

Generates a Universally Unique Identifier (UUID) string.

```tql
uuid()
```

## String

[Section titled “String”](#string)

### Filesystem

[Section titled “Filesystem”](#filesystem)

### [file\_contents](/reference/functions/file_contents)

[→](/reference/functions/file_contents)

Reads a file's contents.

```tql
file_contents("/path/to/file")
```

### [file\_name](/reference/functions/file_name)

[→](/reference/functions/file_name)

Extracts the file name from a file path.

```tql
file_name("/path/to/log.json")
```

### [parent\_dir](/reference/functions/parent_dir)

[→](/reference/functions/parent_dir)

Extracts the parent directory from a file path.

```tql
parent_dir("/path/to/log.json")
```

### Inspection

[Section titled “Inspection”](#inspection)

### [ends\_with](/reference/functions/ends_with)

[→](/reference/functions/ends_with)

Checks if a string ends with a specified substring.

```tql
"hello".ends_with("lo")
```

### [is\_alnum](/reference/functions/is_alnum)

[→](/reference/functions/is_alnum)

Checks if a string is alphanumeric.

```tql
"hello123".is_alnum()
```

### [is\_alpha](/reference/functions/is_alpha)

[→](/reference/functions/is_alpha)

Checks if a string contains only alphabetic characters.

```tql
"hello".is_alpha()
```

### [is\_lower](/reference/functions/is_lower)

[→](/reference/functions/is_lower)

Checks if a string is in lowercase.

```tql
"hello".is_lower()
```

### [is\_numeric](/reference/functions/is_numeric)

[→](/reference/functions/is_numeric)

Checks if a string contains only numeric characters.

```tql
"1234".is_numeric()
```

### [is\_printable](/reference/functions/is_printable)

[→](/reference/functions/is_printable)

Checks if a string contains only printable characters.

```tql
"hello".is_printable()
```

### [is\_title](/reference/functions/is_title)

[→](/reference/functions/is_title)

Checks if a string follows title case.

```tql
"Hello World".is_title()
```

### [is\_upper](/reference/functions/is_upper)

[→](/reference/functions/is_upper)

Checks if a string is in uppercase.

```tql
"HELLO".is_upper()
```

### [length\_bytes](/reference/functions/length_bytes)

[→](/reference/functions/length_bytes)

Returns the length of a string in bytes.

```tql
"hello".length_bytes()
```

### [length\_chars](/reference/functions/length_chars)

[→](/reference/functions/length_chars)

Returns the length of a string in characters.

```tql
"hello".length_chars()
```

### [match\_regex](/reference/functions/match_regex)

[→](/reference/functions/match_regex)

Checks if a string partially matches a regular expression.

```tql
"Hi".match_regex("[Hh]i")
```

### [slice](/reference/functions/slice)

[→](/reference/functions/slice)

Slices a string with offsets and strides.

```tql
"Hi".slice(begin=2, stride=4)
```

### [starts\_with](/reference/functions/starts_with)

[→](/reference/functions/starts_with)

Checks if a string starts with a specified substring.

```tql
"hello".starts_with("he")
```

### Transformation

[Section titled “Transformation”](#transformation)

### [capitalize](/reference/functions/capitalize)

[→](/reference/functions/capitalize)

Capitalizes the first character of a string.

```tql
"hello".capitalize()
```

### [join](/reference/functions/join)

[→](/reference/functions/join)

Joins a list of strings into a single string using a separator.

```tql
join(["a", "b", "c"], ",")
```

### [pad\_end](/reference/functions/pad_end)

[→](/reference/functions/pad_end)

Pads a string at the end to a specified length.

```tql
"hello".pad_end(10)
```

### [pad\_start](/reference/functions/pad_start)

[→](/reference/functions/pad_start)

Pads a string at the start to a specified length.

```tql
"hello".pad_start(10)
```

### [replace](/reference/functions/replace)

[→](/reference/functions/replace)

Replaces characters within a string.

```tql
"hello".replace("o", "a")
```

### [replace\_regex](/reference/functions/replace_regex)

[→](/reference/functions/replace_regex)

Replaces characters within a string based on a regular expression.

```tql
"hello".replace("l+o", "y")
```

### [reverse](/reference/functions/reverse)

[→](/reference/functions/reverse)

Reverses the characters of a string.

```tql
"hello".reverse()
```

### [split](/reference/functions/split)

[→](/reference/functions/split)

Splits a string into substrings.

```tql
split("a,b,c", ",")
```

### [split\_regex](/reference/functions/split_regex)

[→](/reference/functions/split_regex)

Splits a string into substrings with a regex.

```tql
split_regex("a1b2c", r"\d")
```

### [to\_lower](/reference/functions/to_lower)

[→](/reference/functions/to_lower)

Converts a string to lowercase.

```tql
"HELLO".to_lower()
```

### [to\_title](/reference/functions/to_title)

[→](/reference/functions/to_title)

Converts a string to title case.

```tql
"hello world".to_title()
```

### [to\_upper](/reference/functions/to_upper)

[→](/reference/functions/to_upper)

Converts a string to uppercase.

```tql
"hello".to_upper()
```

### [trim](/reference/functions/trim)

[→](/reference/functions/trim)

Trims whitespace or specified characters from both ends of a string.

```tql
" hello ".trim()
```

### [trim\_end](/reference/functions/trim_end)

[→](/reference/functions/trim_end)

Trims whitespace or specified characters from the end of a string.

```tql
"hello ".trim_end()
```

### [trim\_start](/reference/functions/trim_start)

[→](/reference/functions/trim_start)

Trims whitespace or specified characters from the start of a string.

```tql
" hello".trim_start()
```

## Type System

[Section titled “Type System”](#type-system)

### Conversion

[Section titled “Conversion”](#conversion)

### [duration](/reference/functions/duration)

[→](/reference/functions/duration)

Casts an expression to a duration value.

```tql
duration("1.34w")
```

### [float](/reference/functions/float)

[→](/reference/functions/float)

Casts an expression to a float.

```tql
float(42)
```

### [int](/reference/functions/int)

[→](/reference/functions/int)

Casts an expression to an integer.

```tql
int(-4.2)
```

### [ip](/reference/functions/ip)

[→](/reference/functions/ip)

Casts an expression to an IP address.

```tql
ip("1.2.3.4")
```

### [string](/reference/functions/string)

[→](/reference/functions/string)

Casts an expression to a string.

```tql
string(1.2.3.4)
```

### [subnet](/reference/functions/subnet)

[→](/reference/functions/subnet)

Casts an expression to a subnet value.

```tql
subnet("1.2.3.4/16")
```

### [time](/reference/functions/time)

[→](/reference/functions/time)

Casts an expression to a time value.

```tql
time("2020-03-15")
```

### [uint](/reference/functions/uint)

[→](/reference/functions/uint)

Casts an expression to an unsigned integer.

```tql
uint(4.2)
```

### Introspection

[Section titled “Introspection”](#introspection)

### [type\_id](/reference/functions/type_id)

[→](/reference/functions/type_id)

Retrieves the type id of an expression.

```tql
type_id(1 + 3.2)
```

### [type\_of](/reference/functions/type_of)

[→](/reference/functions/type_of)

Retrieves the type definition of an expression.

```tql
type_of(this)
```

### Transposition

[Section titled “Transposition”](#transposition)

### [flatten](/reference/functions/flatten)

[→](/reference/functions/flatten)

Flattens nested data.

```tql
flatten(this)
```

### [unflatten](/reference/functions/unflatten)

[→](/reference/functions/unflatten)

Unflattens nested data.

```tql
unflatten(this)
```

# abs

Returns the absolute value.

```tql
abs(x:number) -> number
abs(x:duration) -> duration
```

## Description

[Section titled “Description”](#description)

This function returns the [absolute value](https://en.wikipedia.org/wiki/Absolute_value) for a number or a duration.

### `x: duration|number`

[Section titled “x: duration|number”](#x-durationnumber)

The value to compute absolute value for.

## Examples

[Section titled “Examples”](#examples)

```tql
from {x: -13.3}
x = x.abs()
```

```tql
{x: 13.3}
```

# add

Adds an element into a list if it doesn’t already exist (set-insertion).

```tql
add(xs:list, x:any) -> list
```

## Description

[Section titled “Description”](#description)

The `add` function returns the list `xs` with `x` added at the end, but only if `x` is not already present in the list. This performs a set-insertion operation, ensuring no duplicate values in the resulting list.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The list to add to.

### `x: any`

[Section titled “x: any”](#x-any)

An element to add to the list. If this is of a type incompatible with the list, it will be considered as `null`.

## Examples

[Section titled “Examples”](#examples)

### Add a new element to a list

[Section titled “Add a new element to a list”](#add-a-new-element-to-a-list)

```tql
from {xs: [1, 2, 3]}
xs = xs.add(4)
```

```tql
{xs: [1, 2, 3, 4]}
```

### Try to add an existing element

[Section titled “Try to add an existing element”](#try-to-add-an-existing-element)

```tql
from {xs: [1, 2, 3]}
xs = xs.add(2)
```

```tql
{xs: [1, 2, 3]}
```

### Add to an empty list

[Section titled “Add to an empty list”](#add-to-an-empty-list)

```tql
from {xs: []}
xs = xs.add("hello")
```

```tql
{xs: ["hello"]}
```

## See Also

[Section titled “See Also”](#see-also)

[`append`](/reference/functions/append), [`prepend`](/reference/functions/prepend), [`remove`](/reference/functions/remove) [`distinct`](/reference/functions/distinct)

# all

Computes the conjunction (AND) of all grouped boolean values.

```tql
all(xs:list) -> bool
```

## Description

[Section titled “Description”](#description)

The `all` function returns `true` if all values in `xs` are `true` and `false` otherwise.

### `xs: list`

[Section titled “xs: list”](#xs-list)

A list of boolean values.

## Examples

[Section titled “Examples”](#examples)

### Check if all values are true

[Section titled “Check if all values are true”](#check-if-all-values-are-true)

```tql
from {x: true}, {x: true}, {x: false}
summarize result=all(x)
```

```tql
{result: false}
```

## See Also

[Section titled “See Also”](#see-also)

[`any`](/reference/functions/any)

# any

Computes the disjunction (OR) of all grouped boolean values.

```tql
any(xs:list) -> bool
```

## Description

[Section titled “Description”](#description)

The `any` function returns `true` if any value in `xs` is `true` and `false` otherwise.

### `xs: list`

[Section titled “xs: list”](#xs-list)

A list of boolean values.

## Examples

[Section titled “Examples”](#examples)

### Check if any value is true

[Section titled “Check if any value is true”](#check-if-any-value-is-true)

```tql
from {x: false}, {x: false}, {x: true}
summarize result=any(x)
```

```tql
{result: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`all`](/reference/functions/all)

# append

Inserts an element at the back of a list.

```tql
append(xs:list, x:any) -> list
```

## Description

[Section titled “Description”](#description)

The `append` function returns the list `xs` with `x` inserted at the end. The expression `xs.append(x)` is equivalent to `[...xs, x]`.

## Examples

[Section titled “Examples”](#examples)

### Append a number to a list

[Section titled “Append a number to a list”](#append-a-number-to-a-list)

```tql
from {xs: [1, 2]}
xs = xs.append(3)
```

```tql
{xs: [1, 2, 3]}
```

## See Also

[Section titled “See Also”](#see-also)

[`add`](/reference/functions/add), [`concatenate`](/reference/functions/concatenate), [`prepend`](/reference/functions/prepend), [`remove`](/reference/functions/remove)

# bit_and

Computes the bit-wise AND of its arguments.

```tql
bit_and(lhs:number, rhs:number) -> number
```

## Description

[Section titled “Description”](#description)

The `bit_and` function computes the bit-wise AND of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers.

### `lhs: number`

[Section titled “lhs: number”](#lhs-number)

The left-hand side operand.

### `rhs: number`

[Section titled “rhs: number”](#rhs-number)

The right-hand side operand.

## Examples

[Section titled “Examples”](#examples)

### Perform bit-wise AND on integers

[Section titled “Perform bit-wise AND on integers”](#perform-bit-wise-and-on-integers)

```tql
from {x: bit_and(5, 3)}
```

```tql
{x: 1}
```

## See Also

[Section titled “See Also”](#see-also)

[`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_not

Computes the bit-wise NOT of its argument.

```tql
bit_not(x:number) -> number
```

## Description

[Section titled “Description”](#description)

The `bit_not` function computes the bit-wise NOT of `x`. The operation inverts each bit in the binary representation of the number.

### `x: number`

[Section titled “x: number”](#x-number)

The number to perform bit-wise NOT on.

## Examples

[Section titled “Examples”](#examples)

### Perform bit-wise NOT on an integer

[Section titled “Perform bit-wise NOT on an integer”](#perform-bit-wise-not-on-an-integer)

```tql
from {x: bit_not(5)}
```

```tql
{x: -6}
```

## See Also

[Section titled “See Also”](#see-also)

[`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_or

Computes the bit-wise OR of its arguments.

```tql
bit_or(lhs:number, rhs:number) -> number
```

## Description

[Section titled “Description”](#description)

The `bit_or` function computes the bit-wise OR of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers.

### `lhs: number`

[Section titled “lhs: number”](#lhs-number)

The left-hand side operand.

### `rhs: number`

[Section titled “rhs: number”](#rhs-number)

The right-hand side operand.

## Examples

[Section titled “Examples”](#examples)

### Perform bit-wise OR on integers

[Section titled “Perform bit-wise OR on integers”](#perform-bit-wise-or-on-integers)

```tql
from {x: bit_or(5, 3)}
```

```tql
{x: 7}
```

## See Also

[Section titled “See Also”](#see-also)

[`bit_and`](/reference/functions/bit_and), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_xor

Computes the bit-wise XOR of its arguments.

```tql
bit_xor(lhs:number, rhs:number) -> number
```

## Description

[Section titled “Description”](#description)

The `bit_xor` function computes the bit-wise XOR (exclusive OR) of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers.

### `lhs: number`

[Section titled “lhs: number”](#lhs-number)

The left-hand side operand.

### `rhs: number`

[Section titled “rhs: number”](#rhs-number)

The right-hand side operand.

## Examples

[Section titled “Examples”](#examples)

### Perform bit-wise XOR on integers

[Section titled “Perform bit-wise XOR on integers”](#perform-bit-wise-xor-on-integers)

```tql
from {x: bit_xor(5, 3)}
```

```tql
{x: 6}
```

## See Also

[Section titled “See Also”](#see-also)

[`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# capitalize

Capitalizes the first character of a string.

```tql
capitalize(x:string) -> string
```

## Description

[Section titled “Description”](#description)

The `capitalize` function returns the input string with the first character converted to uppercase and the rest to lowercase.

## Examples

[Section titled “Examples”](#examples)

### Capitalize a lowercase string

[Section titled “Capitalize a lowercase string”](#capitalize-a-lowercase-string)

```tql
from {x: "hello world".capitalize()}
```

```tql
{x: "Hello world"}
```

## See Also

[Section titled “See Also”](#see-also)

[`to_upper`](/reference/functions/to_upper), [`to_lower`](/reference/functions/to_lower), [`to_title`](/reference/functions/to_title)

# ceil

Computes the ceiling of a number or a time/duration with a specified unit.

```tql
ceil(x:number)
ceil(x:time, unit:duration)
ceil(x:duration, unit:duration)
```

## Description

[Section titled “Description”](#description)

The `ceil` function takes the [ceiling](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions) of a number `x`.

For time and duration values, use the second `unit` argument to define the rounding unit.

## Examples

[Section titled “Examples”](#examples)

### Take the ceiling of integers

[Section titled “Take the ceiling of integers”](#take-the-ceiling-of-integers)

```tql
from {
  x: ceil(3.4),
  y: ceil(3.5),
  z: ceil(-3.4),
}
```

```tql
{
  x: 4,
  y: 4,
  z: -3,
}
```

### Round time and duration values up to a unit

[Section titled “Round time and duration values up to a unit”](#round-time-and-duration-values-up-to-a-unit)

```tql
from {
  x: ceil(2024-02-24, 1y),
  y: ceil(10m, 1h)
}
```

```tql
{
  x: 2025-01-01,
  y: 1h,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`floor`](/reference/functions/floor), [`round`](/reference/functions/round)

# collect

Creates a list of all non-null grouped values, preserving duplicates.

```tql
collect(xs:list) -> list
```

## Description

[Section titled “Description”](#description)

The `collect` function returns a list of all non-null values in `xs`, including duplicates.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to collect.

## Examples

[Section titled “Examples”](#examples)

### Collect values into a list

[Section titled “Collect values into a list”](#collect-values-into-a-list)

```tql
from {x: 1}, {x: 2}, {x: 2}, {x: 3}
summarize values=collect(x)
```

```tql
{values: [1, 2, 2, 3]}
```

## See Also

[Section titled “See Also”](#see-also)

[`distinct`](/reference/functions/distinct), [`sum`](/reference/functions/sum)

# community_id

Computes the Community ID for a network connection/flow.

```tql
community_id(src_ip=ip, dst_ip=ip, proto=string,
             [src_port=int, dst_port=int, seed=int]) -> str
```

## Description

[Section titled “Description”](#description)

The `community_id` function computes a unique hash digest of a network connection according to the [Community ID](https://github.com/corelight/community-id-spec) spec. The digest is useful for pivoting between multiple events that belong to the same connection.

The `src_ip` and `dst_ip` parameters are required. The `proto` string is also required and must be `tcp`, `udp`, `icmp` or `icmp6`. `src_port` and `dst_port` may only be specified if the other one is. `seed` can be used to set the initial hashing seed.

## Examples

[Section titled “Examples”](#examples)

### Compute a Community ID from a flow 5-tuple

[Section titled “Compute a Community ID from a flow 5-tuple”](#compute-a-community-id-from-a-flow-5-tuple)

```tql
from {
  x: community_id(src_ip=1.2.3.4, src_port=4584, dst_ip=43.3.132.3,
                  dst_port=3483, proto="tcp")
}
```

```tql
{x: "1:koNcqhFRD5kb254ZrLsdv630jCM="}
```

### Compute a Community ID from a host pair

[Section titled “Compute a Community ID from a host pair”](#compute-a-community-id-from-a-host-pair)

Because source and destination port are optional, it suffices to provide two IP addreses to compute a valid Community ID.

```tql
from {x: community_id(src_ip=1.2.3.4, dst_ip=43.3.132.3, proto="udp")}
```

```tql
{x: "1:7TrrMeH98PrUKC0ySu3RNmpUr48="}
```

# concatenate

Merges two lists.

```tql
concatenate(xs:list, ys:list) -> list
```

## Description

[Section titled “Description”](#description)

The `concatenate` function returns a list containing all elements from the lists `xs` and `ys` in order. The expression `concatenate(xs, ys)` is equivalent to `[...xs, ...ys]`.

## Examples

[Section titled “Examples”](#examples)

### Concatenate two lists

[Section titled “Concatenate two lists”](#concatenate-two-lists)

```tql
from {xs: [1, 2], ys: [3, 4]}
zs = concatenate(xs, ys)
```

```tql
{
  xs: [1, 2],
  ys: [3, 4],
  zs: [1, 2, 3, 4]
}
```

## See Also

[Section titled “See Also”](#see-also)

[`append`](/reference/functions/append), [`merge`](/reference/functions/merge), [`prepend`](/reference/functions/prepend), [`zip`](/reference/functions/zip)

# config

Reads Tenzir’s configuration file.

```tql
config() -> record
```

## Description

[Section titled “Description”](#description)

The `config` function retrieves Tenzir’s configuration, including values from various `tenzir.yaml` files, plugin-specific configuration files, environment variables, and command-line options.

Note that the `tenzir.secrets`, `tenzir.token` and `caf` options are omitted from the returned record. The former to avoid leaking secrets, the latter as it only contains internal performance-related that are developer-facing and should not be relied upon within TQL.

## Examples

[Section titled “Examples”](#examples)

### Provide a name mapping in the config file

[Section titled “Provide a name mapping in the config file”](#provide-a-name-mapping-in-the-config-file)

/opt/tenzir/etc/tenzir/tenzir.yaml

```yaml
flags:
  de: 🇩🇪
  us: 🇺🇸
```

```tql
let $flags = config().flags
from (
  {country: "de"},
  {country: "us"},
  {country: "uk"},
)
select flag = $flags.get(country, "unknown")
```

```tql
{flag: "🇩🇪"}
{flag: "🇺🇸"}
{flag: "unknown"}
```

## See also

[Section titled “See also”](#see-also)

[`env`](/reference/functions/env), [`secret`](/reference/functions/secret)

# contains

Searches for a value within data structures recursively.

```tql
contains(input:any, target:any, [exact:bool]) -> bool
```

## Description

[Section titled “Description”](#description)

The `contains` function returns `true` if the `target` value is found anywhere within the `input` data structure, and `false` otherwise. The search is performed recursively, meaning it will look inside nested records, lists, and other compound data structures.

By default, strings match via substring search and subnets use containment checks. When `exact` is set to `true`, only exact matches are considered.

### `input: any`

[Section titled “input: any”](#input-any)

The data structure to search within. Can be any type including primitives, records, lists, and nested structures.

### `target: any`

[Section titled “target: any”](#target-any)

The value to search for. Cannot be a list or record.

### `exact: bool` (optional)

[Section titled “exact: bool (optional)”](#exact-bool-optional)

Controls the matching behavior:

* When `false` (default): strings match via substring search, and subnets/IPs use containment checks
* When `true`: only exact equality matches are considered

## Examples

[Section titled “Examples”](#examples)

### Search within records

[Section titled “Search within records”](#search-within-records)

```tql
from {name: "Alice", age: 30, active: true}
found_alice = contains(this, "Alice")
found_bob = contains(this, "Bob")
found_30 = contains(this, 30)
```

```tql
{
  name: "Alice",
  age: 30,
  active: true,
  found_alice: true,
  found_bob: false,
  found_30: true,
}
```

### Search within nested structures

[Section titled “Search within nested structures”](#search-within-nested-structures)

```tql
from {user: {profile: {name: "John", settings: {theme: "dark"}}}}
found_john = contains(this, "John")
found_theme = contains(user, "dark")
found_missing = contains(this, "light")
```

```tql
{
  user: {
    profile: {
      name: "John",
      settings: {
        theme: "dark",
      },
    },
  },
  found_john: true,
  found_theme: true,
  found_missing: false,
}
```

### Search within lists

[Section titled “Search within lists”](#search-within-lists)

```tql
from {numbers: [1, 2, 3, 42], tags: ["important", "urgent"]}
found_42 = contains(numbers, 42)
found_important = contains(tags, "important")
found_missing = contains(numbers, 99)
```

```tql
{
  numbers: [1, 2, 3, 42],
  tags: ["important", "urgent"],
  found_42: true,
  found_important: true,
  found_missing: false,
}
```

### Search with numeric type compatibility

[Section titled “Search with numeric type compatibility”](#search-with-numeric-type-compatibility)

```tql
from {values: {int_val: 42, uint_val: 42.uint(), double_val: 42.0}}
search_int = contains(values, 42)
search_uint = contains(values, 42.uint())
search_double = contains(values, 42.0)
```

```tql
{
  values: {
    int_val: 42,
    uint_val: 42,
    double_val: 42.0,
  },
  search_int: true,
  search_uint: true,
  search_double: true,
}
```

### Search in deeply nested structures

[Section titled “Search in deeply nested structures”](#search-in-deeply-nested-structures)

```tql
from {
  data: {
    level1: {
      level2: {
        level3: {
          target: "found"
        }
      }
    }
  }
}
deep_search = contains(data, "found")
```

```tql
{
  data: {
    level1: {
      level2: {
        level3: {
          target: "found",
        },
      },
    },
  },
  deep_search: true,
}
```

### Substring search in strings

[Section titled “Substring search in strings”](#substring-search-in-strings)

```tql
from {message: "Hello, World!"}
substring_match = contains(message, "World")
exact_match = contains(message, "World", exact=true)
partial_no_match = contains(message, "Universe")
exact_no_match = contains(message, "Hello, World", exact=true)
```

```tql
{
  message: "Hello, World!",
  substring_match: true,
  exact_match: false,
  partial_no_match: false,
  exact_no_match: false,
}
```

### Subnet and IP containment

[Section titled “Subnet and IP containment”](#subnet-and-ip-containment)

```tql
from {subnet: 10.0.0.0/8}
contains_ip = contains(subnet, 10.1.2.3)
contains_subnet = contains(subnet, 10.0.0.0/16)
exact_subnet = contains(subnet, 10.0.0.0/8, exact=true)
```

```tql
{
  subnet: 10.0.0.0/8,
  contains_ip: true,
  contains_subnet: true,
  exact_subnet: true,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`has`](/reference/functions/has), [`match_regex`](/reference/functions/match_regex)

# contains_null

Checks whether the input contains any `null` values.

```tql
contains_null(x:any) -> bool
```

## Description

[Section titled “Description”](#description)

The `contains_null` function checks if the input contains any `null` values recursively.

### `x: any`

[Section titled “x: any”](#x-any)

The input to check for `null` values.

## Examples

[Section titled “Examples”](#examples)

### Check if list has null values

[Section titled “Check if list has null values”](#check-if-list-has-null-values)

```tql
from {x: [{a: 1}, {}]}
contains_null = x.contains_null()
```

```tql
{
  x: [
    {
      a: 1,
    },
    {
      a: null,
    },
  ],
  contains_null: true,
}
```

### Check a record with null values

[Section titled “Check a record with null values”](#check-a-record-with-null-values)

```tql
from {x: "foo", y: null}
contains_null = this.contains_null()
```

```tql
{
  x: "foo",
  y: null,
  contains_null: true,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`has`](/reference/functions/has), [`is_empty`](/reference/functions/is_empty) [`contains`](/reference/functions/contains)

# count

Counts the events or non-null grouped values.

```tql
count(xs:list) -> int
```

## Description

[Section titled “Description”](#description)

The `count` function returns the number of non-null values in `xs`. When used without arguments, it counts the total number of events.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to count.

## Examples

[Section titled “Examples”](#examples)

### Count the number of non-null values

[Section titled “Count the number of non-null values”](#count-the-number-of-non-null-values)

```tql
from {x: 1}, {x: null}, {x: 2}
summarize total=count(x)
```

```tql
{total: 2}
```

## See Also

[Section titled “See Also”](#see-also)

[`count_distinct`](/reference/functions/count_distinct)

# count_days

Counts the number of `days` in a duration.

```tql
count_days(x:duration) -> float
```

## Description

This function returns the number of days in a duration, i.e., `duration / 1d`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_distinct

Counts all distinct non-null grouped values.

```tql
count_distinct(xs:list) -> int
```

## Description

[Section titled “Description”](#description)

The `count_distinct` function returns the number of unique, non-null values in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to count.

## Examples

[Section titled “Examples”](#examples)

### Count distinct values

[Section titled “Count distinct values”](#count-distinct-values)

```tql
from {x: 1}, {x: 2}, {x: 2}, {x: 3}
summarize unique=count_distinct(x)
```

```tql
{unique: 3}
```

## See Also

[Section titled “See Also”](#see-also)

[`count`](/reference/functions/count), [`distinct`](/reference/functions/distinct)

# count_hours

Counts the number of `hours` in a duration.

```tql
count_hours(x:duration) -> float
```

## Description

This function returns the number of hours in a duration, i.e., `duration / 1h`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_if

Counts the events or non-null grouped values matching a given predicate.

```tql
count_if(xs:list, predicate:any -> bool) -> int
```

## Description

[Section titled “Description”](#description)

The `count_if` function returns the number of non-null values in `xs` that satisfy the given `predicate`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to count.

### `predicate: any -> bool`

[Section titled “predicate: any -> bool”](#predicate-any---bool)

The predicate to apply to each value to check whether it should be counted.

## Examples

[Section titled “Examples”](#examples)

### Count the number of values greater than 1

[Section titled “Count the number of values greater than 1”](#count-the-number-of-values-greater-than-1)

```tql
from {x: 1}, {x: null}, {x: 2}
summarize total=x.count_if(x => x > 1)
```

```tql
{total: 1}
```

## See Also

[Section titled “See Also”](#see-also)

[`count`](/reference/functions/count)

# count_microseconds

Counts the number of `microseconds` in a duration.

```tql
count_microseconds(x:duration) -> float
```

## Description

This function returns the number of microseconds in a duration, i.e., `duration / 1us`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_milliseconds

Counts the number of `milliseconds` in a duration.

```tql
count_milliseconds(x:duration) -> float
```

## Description

This function returns the number of milliseconds in a duration, i.e., `duration / 1ms`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_minutes

Counts the number of `minutes` in a duration.

```tql
count_minutes(x:duration) -> float
```

## Description

This function returns the number of minutes in a duration, i.e., `duration / 1min`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_months

Counts the number of `months` in a duration.

```tql
count_months(x:duration) -> float
```

## Description

This function returns the number of months in a duration, i.e., `duration / 1/12 * 1y`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_nanoseconds

Counts the number of `nanoseconds` in a duration.

```tql
count_nanoseconds(x:duration) -> int
```

## Description

This function returns the number of nanoseconds in a duration, i.e., `duration / 1ns`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_seconds

Counts the number of `seconds` in a duration.

```tql
count_seconds(x:duration) -> float
```

## Description

This function returns the number of seconds in a duration, i.e., `duration / 1s`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_weeks

Counts the number of `weeks` in a duration.

```tql
count_weeks(x:duration) -> float
```

## Description

This function returns the number of weeks in a duration, i.e., `duration / 1w`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_years

Counts the number of `years` in a duration.

```tql
count_years(x:duration) -> float
```

## Description

This function returns the number of years in a duration, i.e., `duration / 1y`.

### `x: duration`

The duration to count in.

## See Also

[`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# day

Extracts the day component from a timestamp.

```tql
day(x: time) -> int
```

## Description

[Section titled “Description”](#description)

The `day` function extracts the day component from a timestamp as an integer (1-31).

### `x: time`

[Section titled “x: time”](#x-time)

The timestamp from which to extract the day.

## Examples

[Section titled “Examples”](#examples)

### Extract the day from a timestamp

[Section titled “Extract the day from a timestamp”](#extract-the-day-from-a-timestamp)

```tql
from {
  ts: 2024-06-15T14:30:45.123456,
}
day = ts.day()
```

```tql
{
  ts: 2024-06-15T14:30:45.123456,
  day: 15,
}
```

## See also

[Section titled “See also”](#see-also)

[`year`](/reference/functions/year), [`month`](/reference/functions/month), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# days

Converts a number to equivalent days.

```tql
days(x:number) -> duration
```

## Description

This function returns days equivalent to a number, i.e., `number * 1d`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# decapsulate

Decapsulates packet data at link, network, and transport layer.

```tql
decapsulate(packet:record) -> record
```

## Description

[Section titled “Description”](#description)

The `decapsulate` function decodes binary PCAP packet data by extracting link, network, and transport layer information. The function takes a `packet` record as argument as produced by the [`read_pcap`](/reference/operators/read_pcap) operator, which may look like this:

```tql
{
  linktype: 1,
  timestamp: 2021-11-17T13:32:43.249525,
  captured_packet_length: 66,
  original_packet_length: 66,
  data: "ZJ7zvttmABY88f1tCABFAAA0LzBAAEAGRzjGR/dbgA6GqgBQ4HzXXzhE3N8/r4AQAfyWoQAAAQEICqMYaE9Mw7SY",
}
```

This entire record serves as input to `decapsulate` since the `linktype` determines how to intepret the binary `data` field containing the raw packet data.

Wireshark?

With `decapsulate`, we aim to provide a *minimal* packet parsing up to the transport layer so that you can work with packets in pipelines and implement use cases such as alert-based PCAP. The goal is *not* to comprehensively parse all protocol fields at great depth. If this is your objective, consider [Zeek](https://zeek.org), [Suricata](https://suricata.io), or [Wireshark](https://wireshark.org).

### VLAN Tags

[Section titled “VLAN Tags”](#vlan-tags)

The `decapsulate` function also extracts [802.1Q](https://en.wikipedia.org/wiki/IEEE_802.1Q) VLAN tags into a nested `vlan` record, consisting of an `outer` and `inner` field for the respective tags. The value of the VLAN tag corresponds to the 12-bit VLAN identifier (VID). Special values include `0` (frame does not carry a VLAN ID) and `0xFFF` (reserved value; sometimes wildcard match).

## Examples

[Section titled “Examples”](#examples)

### Decapsulate packets from a PCAP file

[Section titled “Decapsulate packets from a PCAP file”](#decapsulate-packets-from-a-pcap-file)

```tql
from "/path/to/trace.pcap"
this = decapsulate(this)
```

```tql
{
  ether: {
    src: "00-08-02-1C-47-AE",
    dst: "20-E5-2A-B6-93-F1",
    type: 2048,
  },
  ip: {
    src: 10.12.14.101,
    dst: 92.119.157.10,
    type: 6,
  },
  tcp: {
    src_port: 62589,
    dst_port: 4443,
  },
  community_id: "1:tSl1HyzM7qS0o3OpbOgxQJYCKCc=",
  udp: null,
  icmp: null,
}
```

If the trace contains 802.1Q traffic, then the output includes a `vlan` record:

```tql
{
  ether: {
    src: "00-17-5A-ED-7A-F0",
    dst: "FF-FF-FF-FF-FF-FF",
    type: 2048,
  },
  vlan: {
    outer: 1,
    inner: 20,
  },
  ip: {
    src: 192.168.1.1,
    dst: 255.255.255.255,
    type: 1,
  },
  icmp: {
    type: 8,
    code: 0,
  },
  community_id: "1:1eiKaTUjqP9UT1/1yu/o0frHlCk=",
}
```

# decode_base64

Decodes bytes as Base64.

```tql
decode_base64(bytes: blob|string) -> blob
```

## Description

[Section titled “Description”](#description)

Decodes bytes as Base64.

### `bytes: blob|string`

[Section titled “bytes: blob|string”](#bytes-blobstring)

The value to decode as Base64.

## Examples

[Section titled “Examples”](#examples)

### Decode a Base64 encoded string

[Section titled “Decode a Base64 encoded string”](#decode-a-base64-encoded-string)

```tql
from {bytes: "VGVuemly"}
decoded = bytes.decode_base64()
```

```tql
{bytes: "VGVuemly", decoded: "Tenzir"}
```

## See Also

[Section titled “See Also”](#see-also)

[`encode_base64`](/reference/functions/encode_base64)

# decode_hex

Decodes bytes from their hexadecimal representation.

```tql
decode_hex(bytes: blob|string) -> blob
```

## Description

[Section titled “Description”](#description)

Decodes bytes from their hexadecimal representation.

### `bytes: blob|string`

[Section titled “bytes: blob|string”](#bytes-blobstring)

The value to decode.

## Examples

[Section titled “Examples”](#examples)

### Decode a blob from hex

[Section titled “Decode a blob from hex”](#decode-a-blob-from-hex)

```tql
from {bytes: "54656E7A6972"}
decoded = bytes.decode_hex()
```

```tql
{bytes: "54656E7A6972", decoded: "Tenzir"}
```

### Decode a mixed-case hex string

[Section titled “Decode a mixed-case hex string”](#decode-a-mixed-case-hex-string)

```tql
from {bytes: "4e6f6E6365"}
decoded = bytes.decode_hex()
```

```tql
{bytes: "4e6f6E6365", decoded: "Nonce"}
```

## See Also

[Section titled “See Also”](#see-also)

[`encode_hex`](/reference/functions/encode_hex)

# decode_url

Decodes URL encoded strings.

```tql
decode_url(string: blob|string) -> blob
```

## Description

[Section titled “Description”](#description)

Decodes URL encoded strings or blobs, converting percent-encoded sequences back to their original characters.

### `string: blob|string`

[Section titled “string: blob|string”](#string-blobstring)

The URL encoded string to decode.

## Examples

[Section titled “Examples”](#examples)

### Decode a URL encoded string

[Section titled “Decode a URL encoded string”](#decode-a-url-encoded-string)

```tql
from {input: "Hello%20World%20%26%20Special%2FChars%3F"}
decoded = input.decode_url()
```

```tql
{
  input: "Hello%20World%20%26%20Special%2FChars%3F",
  decoded: "Hello World & Special/Chars?",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`encode_url`](/reference/functions/encode_url)

# distinct

Creates a sorted list without duplicates of non-null grouped values.

```tql
distinct(xs:list) -> list
```

## Description

[Section titled “Description”](#description)

The `distinct` function returns a sorted list containing unique, non-null values in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to deduplicate.

## Examples

[Section titled “Examples”](#examples)

### Get distinct values in a list

[Section titled “Get distinct values in a list”](#get-distinct-values-in-a-list)

```tql
from {x: 1}, {x: 2}, {x: 2}, {x: 3}
summarize unique=distinct(x)
```

```tql
{unique: [1, 2, 3]}
```

## See Also

[Section titled “See Also”](#see-also)

[`collect`](/reference/functions/collect), [`count_distinct`](/reference/functions/count_distinct), [`value_counts`](/reference/functions/value_counts)

# duration

Casts an expression to a duration value.

```tql
duration(x:string) -> duration
```

## Description

[Section titled “Description”](#description)

The `duration` function casts the given string `x` to a duration value.

## Examples

[Section titled “Examples”](#examples)

### Cast a string to a duration

[Section titled “Cast a string to a duration”](#cast-a-string-to-a-duration)

```tql
from {str: "1ms"}
dur = duration(str)
```

```tql
{str: "1ms", dur: 1ms}
```

## See Also

[Section titled “See Also”](#see-also)

[time](/reference/functions/time)

# encode_base64

Encodes bytes as Base64.

```tql
encode_base64(bytes: blob|string) -> string
```

## Description

[Section titled “Description”](#description)

Encodes bytes as Base64.

### `bytes: blob|string`

[Section titled “bytes: blob|string”](#bytes-blobstring)

The value to encode as Base64.

## Examples

[Section titled “Examples”](#examples)

### Encode a string as Base64

[Section titled “Encode a string as Base64”](#encode-a-string-as-base64)

```tql
from {bytes: "Tenzir"}
encoded = bytes.encode_base64()
```

```tql
{bytes: "Tenzir", encoded: "VGVuemly"}
```

## See Also

[Section titled “See Also”](#see-also)

[`decode_base64`](/reference/functions/decode_base64)

# encode_hex

Encodes bytes into their hexadecimal representation.

```tql
encode_hex(bytes: blob|string) -> string
```

## Description

[Section titled “Description”](#description)

Encodes bytes into their hexadecimal representation.

### `bytes: blob|string`

[Section titled “bytes: blob|string”](#bytes-blobstring)

The value to encode.

## Examples

[Section titled “Examples”](#examples)

### Encode a string to hex

[Section titled “Encode a string to hex”](#encode-a-string-to-hex)

```tql
from {bytes: "Tenzir"}
encoded = bytes.encode_hex()
```

```tql
{bytes: "Tenzir", encoded: "54656E7A6972"}
```

## See Also

[Section titled “See Also”](#see-also)

[`decode_hex`](/reference/functions/decode_hex)

# encode_url

Encodes strings using URL encoding.

```tql
encode_url(bytes: blob|string) -> string
```

## Description

[Section titled “Description”](#description)

Encodes strings or blobs using URL encoding, replacing special characters with their percent-encoded equivalents.

### `bytes: blob|string`

[Section titled “bytes: blob|string”](#bytes-blobstring)

The input to URL encode.

## Examples

[Section titled “Examples”](#examples)

### Encode a string as URL encoded

[Section titled “Encode a string as URL encoded”](#encode-a-string-as-url-encoded)

```tql
from {input: "Hello World & Special/Chars?"}
encoded = input.encode_url()
```

```tql
{
  input: "Hello World & Special/Chars?",
  encoded: "Hello%20World%20%26%20Special%2FChars%3F",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`decode_url`](/reference/functions/decode_url)

# encrypt_cryptopan

Encrypts an IP address via Crypto-PAn.

```tql
encrypt_cryptopan(address:ip, [seed=string])
```

## Description

[Section titled “Description”](#description)

The `encrypt_cryptopan` function encrypts the IP `address` using the [Crypto-PAn](https://en.wikipedia.org/wiki/Crypto-PAn) algorithm.

### `address: ip`

[Section titled “address: ip”](#address-ip)

The IP address to encrypt.

### `seed = string (optional)`

[Section titled “seed = string (optional)”](#seed--string-optional)

A 64-byte seed that describes a hexadecimal value. When the seed is shorter than 64 bytes, the function appends zeros to match the size; when it is longer, it truncates the seed.

## Examples

[Section titled “Examples”](#examples)

### Encrypt IP address fields

[Section titled “Encrypt IP address fields”](#encrypt-ip-address-fields)

```tql
let $seed = "deadbeef" // use secret() function in practice
from {
  src: encrypt_cryptopan(114.13.11.35, seed=$seed),
  dst: encrypt_cryptopan(114.56.11.200, seed=$seed),
}
```

```tql
{
  src: 117.179.11.60,
  dst: 117.135.244.180,
}
```

# ends_with

Checks if a string ends with a specified substring.

```tql
ends_with(x:string, suffix:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `ends_with` function returns `true` if `x` ends with `suffix` and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a string ends with a substring

[Section titled “Check if a string ends with a substring”](#check-if-a-string-ends-with-a-substring)

```tql
from {x: "hello".ends_with("lo")}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`starts_with`](/reference/functions/starts_with)

# entropy

Computes the Shannon entropy of all grouped values.

```tql
entropy(xs:list, [normalize=bool]) -> float
```

## Description

[Section titled “Description”](#description)

The `entropy` function calculates the Shannon entropy of the values in `xs`, which measures the amount of uncertainty or randomness in the data. Higher entropy values indicate more randomness, while lower values indicate more predictability.

The entropy is calculated as: `H(x) = -sum(p(x[i]) * log(p(x[i])))`, where `p(x[i])` is the probability of each unique value.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

### `normalize: bool (optional)`

[Section titled “normalize: bool (optional)”](#normalize-bool-optional)

Optional parameter to normalize the entropy between 0 and 1. When `true`, the entropy is divided by `log(number of unique values)`. Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Compute the entropy of values

[Section titled “Compute the entropy of values”](#compute-the-entropy-of-values)

```tql
from {x: 1}, {x: 1}, {x: 2}, {x: 3}
summarize entropy_value=entropy(x)
```

```tql
{
  entropy_value: 1.0397207708399179,
}
```

### Compute the normalized entropy

[Section titled “Compute the normalized entropy”](#compute-the-normalized-entropy)

```tql
from {x: 1}, {x: 1}, {x: 2}, {x: 3}
summarize normalized_entropy=entropy(x, normalize=true)
```

```tql
{
  normalized_entropy: 0.946394630357186,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`mode`](/reference/functions/mode), [`value_counts`](/reference/functions/value_counts)

# env

Reads an environment variable.

```tql
env(x:string) -> string
```

## Description

[Section titled “Description”](#description)

The `env` function retrieves the value of an environment variable `x`. If the variable does not exist, it returns `null`.

## Examples

[Section titled “Examples”](#examples)

### Read the `PATH` environment variable

[Section titled “Read the PATH environment variable”](#read-the-path-environment-variable)

```tql
from {x: env("PATH")}
```

```tql
{x: "/usr/local/bin:/usr/bin:/bin"}
```

## See also

[Section titled “See also”](#see-also)

[`config`](/reference/functions/config), [`secret`](/reference/functions/secret)

# file_contents

Reads a file’s contents.

```tql
file_contents(path:string, [binary=bool]) -> blob|string
```

## Description

[Section titled “Description”](#description)

The `file_contents` function reads a file’s contents.

### `path: string`

[Section titled “path: string”](#path-string)

Absolute path of file to read.

### `binary = bool (optional)`

[Section titled “binary = bool (optional)”](#binary--bool-optional)

Whether to read the file contents as a `blob`, instead of a `string`.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

```tql
let $secops_config = file_contents("/path/to/file.json").parse_json()
…
to_google_secops client_email=$secops_config.client_email, …
```

# file_name

Extracts the file name from a file path.

```tql
file_name(x:string) -> string
```

## Description

[Section titled “Description”](#description)

The `file_name` function returns the file name component of a file path, excluding the parent directories.

## Examples

[Section titled “Examples”](#examples)

### Extract the file name from a file path

[Section titled “Extract the file name from a file path”](#extract-the-file-name-from-a-file-path)

```tql
from {x: file_name("/path/to/log.json")}
```

```tql
{x: "log.json"}
```

## See Also

[Section titled “See Also”](#see-also)

[`parent_dir`](/reference/functions/parent_dir)

# first

Takes the first non-null grouped value.

```tql
first(xs:list) -> any
```

## Description

[Section titled “Description”](#description)

The `first` function returns the first non-null value in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to search.

## Examples

[Section titled “Examples”](#examples)

### Get the first non-null value

[Section titled “Get the first non-null value”](#get-the-first-non-null-value)

```tql
from {x: null}, {x: 2}, {x: 3}
summarize first_value=first(x)
```

```tql
{first_value: 2}
```

## See Also

[Section titled “See Also”](#see-also)

[`last`](/reference/functions/last)

# flatten

Flattens nested data.

```tql
flatten(x:record, separtor=string) -> record
```

## Description

[Section titled “Description”](#description)

The `flatten` function takes a record and performs actions on contained container types:

1. **Records**: Join nested records with a separator (`.` by default). For example, if a field named `x` is a record with fields `a` and `b`, flattening will lift the nested record into the parent scope by creating two new fields `x.a` and `x.b`.
2. **Lists**: Merge nested lists into a single (flat) list. For example, `[[[2]], [[3, 1]], [[4]]]` becomes `[2, 3, 1, 4]`.

For records inside lists, `flatten` “pushes lists down” into one list per record field. For example, the record

```tql
{
  foo: [
    {
      a: 2,
      b: 1,
    },
    {
      a: 4,
    },
  ],
}
```

becomes

```tql
{
  "foo.a": [
    2,
    4,
  ],
  "foo.b": [
    1,
    null,
  ],
}
```

Lists nested in records that are nested in lists will also be flattened. For example, the record

```tql
{
  foo: [
    {
      a: [
        [2, 23],
        [1,16],
      ],
      b: [1],
    },
    {
      a: [[4]],
    },
  ],
}
```

becomes

```tql
{
  "foo.a": [
    2,
    23,
    1,
    16,
    4
  ],
  "foo.b": [
    1
  ]
}
```

As you can see from the above examples, flattening also removes `null` values.

### `x: record`

[Section titled “x: record”](#x-record)

The record you want to flatten.

### `separator: string (optional)`

[Section titled “separator: string (optional)”](#separator-string-optional)

The separator to use for joining field names.

Defaults to `"."`.

## Examples

[Section titled “Examples”](#examples)

### Flatten fields with the dot character

[Section titled “Flatten fields with the dot character”](#flatten-fields-with-the-dot-character)

```tql
from {
  src_ip: 147.32.84.165,
  src_port: 1141,
  dest_ip: 147.32.80.9,
  dest_port: 53,
  event_type: "dns",
  dns: {
    type: "query",
    id: 553,
    rrname: "irc.freenode.net",
    rrtype: "A",
    tx_id: 0,
    grouped: {
      A: [
        "tenzir.com",
      ],
    },
  },
}
this = flatten(this)
```

```tql
{
  src_ip: 147.32.84.165,
  src_port: 1141,
  dest_ip: 147.32.80.9,
  dest_port: 53,
  event_type: "dns",
  "dns.type": "query",
  "dns.id": 553,
  "dns.rrname": "irc.freenode.net",
  "dns.rrtype": "A",
  "dns.tx_id": 0,
  "dns.grouped.A": ["tenzir.com"],
}
```

## See Also

[Section titled “See Also”](#see-also)

[`unflatten`](/reference/functions/unflatten)

# float

Casts an expression to a float.

```tql
float(x:any) -> float
```

## Description

[Section titled “Description”](#description)

The `float` function converts the given value `x` to a floating-point value.

## Examples

[Section titled “Examples”](#examples)

### Cast an integer to a float

[Section titled “Cast an integer to a float”](#cast-an-integer-to-a-float)

```tql
from {x: float(42)}
```

```tql
{x: 42.0}
```

### Cast a string to a float

[Section titled “Cast a string to a float”](#cast-a-string-to-a-float)

```tql
from {x: float("4.2")}
```

```tql
{x: 4.2}
```

## See Also

[Section titled “See Also”](#see-also)

[`ip`](/reference/functions/ip), [`string`](/reference/functions/string), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [int](/reference/functions/int)

# floor

Computes the floor of a number or a time/duration with a specified unit.

```tql
floor(x:number)
floor(x:time, unit:duration)
floor(x:duration, unit:duration)
```

## Description

[Section titled “Description”](#description)

The `floor` function takes the [floor](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions) of a number `x`.

For time and duration values, use the second `unit` argument to define the rounding unit.

## Examples

[Section titled “Examples”](#examples)

### Take the floor of integers

[Section titled “Take the floor of integers”](#take-the-floor-of-integers)

```tql
from {
  x: floor(3.4),
  y: floor(3.5),
  z: floor(-3.4),
}
```

```tql
{
  x: 3,
  y: 3,
  z: -4,
}
```

### Round time and duration values down to a unit

[Section titled “Round time and duration values down to a unit”](#round-time-and-duration-values-down-to-a-unit)

```tql
from {
  x: floor(2024-02-24, 1y),
  y: floor(1h52m, 1h)
}
```

```tql
{
  x: 2024-01-01,
  y: 1h,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`ceil`](/reference/functions/ceil), [`round`](/reference/functions/round)

# format_time

Formats a time into a string that follows a specific format.

```tql
format_time(input: time, format: string) -> string
```

## Description

[Section titled “Description”](#description)

The `format_time` function formats the given `input` time into a string by using the given `format`.

### `input: time`

[Section titled “input: time”](#input-time)

The input time for which a string should be constructed.

### `format: string`

[Section titled “format: string”](#format-string)

The string that specifies the desired output format, for example `"%m-%d-%Y"`. The allowed format specifiers are the same as for `strftime(3)`:

| Specifier | Description                            | Example                   |
| :-------: | :------------------------------------- | :------------------------ |
|    `%%`   | A literal `%` character                | `%`                       |
|    `%a`   | Abbreviated or full weekday name       | `Mon`, `Monday`           |
|    `%A`   | Equivalent to `%a`                     | `Mon`, `Monday`           |
|    `%b`   | Abbreviated or full month name         | `Jan`, `January`          |
|    `%B`   | Equivalent to `%b`                     | `Jan`, `January`          |
|    `%c`   | Date and time representation           | `Mon Jan 1 12:00:00 2024` |
|    `%C`   | Century as a decimal number            | `20`                      |
|    `%d`   | Day of the month with zero padding     | `01`, `31`                |
|    `%D`   | Equivalent to `%m/%d/%y`               | `01/31/24`                |
|    `%e`   | Day of the month with space padding    | `1`, `31`                 |
|    `%F`   | Equivalent to `%Y-%m-%d`               | `2024-01-31`              |
|    `%g`   | Last two digits of ISO week-based year | `24`                      |
|    `%G`   | ISO week-based year                    | `2024`                    |
|    `%h`   | Equivalent to `%b`                     | `Jan`                     |
|    `%H`   | Hour in 24-hour format                 | `00`, `23`                |
|    `%I`   | Hour in 12-hour format                 | `01`, `12`                |
|    `%j`   | Day of year                            | `001`, `365`              |
|    `%m`   | Month number                           | `01`, `12`                |
|    `%M`   | Minutes                                | `00`, `59`                |
|    `%n`   | Newline character                      | `\n`                      |
|    `%p`   | AM/PM designation                      | `AM`, `PM`                |
|    `%r`   | 12-hour clock time                     | `12:00:00 PM`             |
|    `%R`   | Equivalent to `%H:%M`                  | `23:59`                   |
|    `%S`   | Seconds                                | `00`, `59`                |
|    `%t`   | Tab character                          | `\t`                      |
|    `%T`   | Equivalent to `%H:%M:%S`               | `23:59:59`                |
|    `%u`   | ISO weekday (Monday=1)                 | `1`, `7`                  |
|    `%U`   | Week number (Sunday as first day)      | `00`, `52`                |
|    `%V`   | ISO week number                        | `01`, `53`                |
|    `%w`   | Weekday (Sunday=0)                     | `0`, `6`                  |
|    `%W`   | Week number (Monday as first day)      | `00`, `52`                |
|    `%x`   | Date representation                    | `01/31/24`                |
|    `%X`   | Time representation                    | `23:59:59`                |
|    `%y`   | Year without century                   | `24`                      |
|    `%Y`   | Year with century                      | `2024`                    |
|    `%z`   | UTC offset                             | `+0000`, `-0430`          |
|    `%Z`   | Time zone abbreviation                 | `UTC`, `EST`              |

## Examples

[Section titled “Examples”](#examples)

### Format a timestamp

[Section titled “Format a timestamp”](#format-a-timestamp)

```tql
from {
  x: 2024-12-31T12:59:42,
}
x = x.format_time("%d.%m.%Y")
```

```tql
{x: "31.12.2024"}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_time`](/reference/functions/parse_time)

# from_epoch

Interprets a duration as Unix time.

```tql
from_epoch(x:duration) -> time
```

## Description

[Section titled “Description”](#description)

The `from_epoch` function interprets a duration as [Unix time](https://en.wikipedia.org/wiki/Unix_time).

### `x: duration`

[Section titled “x: duration”](#x-duration)

The duration since the Unix epoch, i.e., 00:00:00 UTC on 1 January 1970.

## Examples

[Section titled “Examples”](#examples)

### Convert an integral Unix time

[Section titled “Convert an integral Unix time”](#convert-an-integral-unix-time)

```tql
from {time: 1736525429}
time = from_epoch(time * 1s)
```

```tql
{time: 2025-01-10T16:10:29+00:00}
```

### Interpret a duration as Unix time

[Section titled “Interpret a duration as Unix time”](#interpret-a-duration-as-unix-time)

```tql
from {x: from_epoch(50y + 12w + 20m)}
```

```tql
{x: 2020-03-13T00:20:00.000000}
```

## See Also

[Section titled “See Also”](#see-also)

[`now`](/reference/functions/now), [`since_epoch`](/reference/functions/since_epoch)

# get

Gets a field from a record or an element from a list

```tql
get(x:record, field:string, [fallback:any]) -> any
get(x:record|list, index:number, [fallback:any]) -> any
```

## Description

[Section titled “Description”](#description)

The `get` function returns the record field with the name `field` or the list element with the index `index`. If `fallback` is provided, the function gracefully returns the fallback value instead of emitting a warning and returning `null`.

### `xs: record|list`

[Section titled “xs: record|list”](#xs-recordlist)

A `record` or list you want to access.

### `index: int`/`field: string`

[Section titled “index: int/field: string”](#index-intfield-string)

An index or field to access. If the function’s subject `xs` is a `list`, `index` refers to the position in the list. If the subject `xs` is a `record`, `index` refers to the field index. If the subject is a `record`, you can also use the fields name as a `string` to refer to it.

If the given `index` or `field` are do not exist in the subject and no `fallback` was provided, a warning will be raised and the function will return `null`.

### `fallback: any (optional)`

[Section titled “fallback: any (optional)”](#fallback-any-optional)

A fallback value to return if the given `index` or `field` do not exist in the subject. Providing a `fallback` avoids a warning.

## Examples

[Section titled “Examples”](#examples)

### Get the first element of a list, or a fallback value

[Section titled “Get the first element of a list, or a fallback value”](#get-the-first-element-of-a-list-or-a-fallback-value)

```tql
from (
  {xs: [1, 2, 3]},
  {xs: []},
}
select first = xs.get(0, -1)
```

```tql
{first: 1}
{first: -1}
```

### Access a field of a record, or a fallback value

[Section titled “Access a field of a record, or a fallback value”](#access-a-field-of-a-record-or-a-fallback-value)

```tql
from (
  {x: 1, y: 2},
  {x: 3},
}
select x = this.get("x", -1), y = this.get("y", -1)
```

```tql
{x: 1, y: 2}
{x: 3, y: -1}
```

## See Also

[Section titled “See Also”](#see-also)

[`keys`](/reference/functions/keys)

# has

Checks whether a record has a specified field.

```tql
has(x:record, field:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `has` function returns `true` if the record contains the specified field and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a record has a specific field

[Section titled “Check if a record has a specific field”](#check-if-a-record-has-a-specific-field)

```tql
from {
  x: "foo",
  y: null,
}
this = {
  has_x: this.has("x"),
  has_y: this.has("y"),
  has_z: this.has("z"),
}
```

```tql
{
  has_x: true,
  has_y: true,
  has_z: false,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_empty`](/reference/functions/is_empty), [`keys`](/reference/functions/keys)

# hash_md5

Computes an MD5 hash digest.

```tql
hash_md5(x:any, [seed=string])
```

## Description

[Section titled “Description”](#description)

The `hash` function calculates a hash digest of a given value `x`.

### `x: any`

[Section titled “x: any”](#x-any)

The value to hash.

### `seed = string (optional)`

[Section titled “seed = string (optional)”](#seed--string-optional)

The seed for the hash.

## Examples

[Section titled “Examples”](#examples)

### Compute an MD5 digest of a string

[Section titled “Compute an MD5 digest of a string”](#compute-an-md5-digest-of-a-string)

```tql
from { x: hash_md5("foo") }
```

```tql
{ x: "acbd18db4cc2f85cedef654fccc4a4d8" }
```

## See Also

[Section titled “See Also”](#see-also)

[`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha1

Computes a SHA-1 hash digest.

```tql
hash_sha1(x:any, [seed=string]) -> string
```

## Description

[Section titled “Description”](#description)

The `hash_sha1` function calculates a SHA-1 hash digest for the given value `x`.

## Examples

[Section titled “Examples”](#examples)

### Compute a SHA-1 digest of a string

[Section titled “Compute a SHA-1 digest of a string”](#compute-a-sha-1-digest-of-a-string)

```tql
from {x: hash_sha1("foo")}
```

```tql
{x: "0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33"}
```

## See Also

[Section titled “See Also”](#see-also)

[`hash_md5`](/reference/functions/hash_md5), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha224

Computes a SHA-224 hash digest.

```tql
hash_sha224(x:any, [seed=string]) -> string
```

## Description

[Section titled “Description”](#description)

The `hash_sha224` function calculates a SHA-224 hash digest for the given value `x`.

## Examples

[Section titled “Examples”](#examples)

### Compute a SHA-224 digest of a string

[Section titled “Compute a SHA-224 digest of a string”](#compute-a-sha-224-digest-of-a-string)

```tql
from {x: hash_sha224("foo")}
```

```tql
{x: "0808f64e60d58979fcb676c96ec938270dea42445aeefcd3a4e6f8db"}
```

## See Also

[Section titled “See Also”](#see-also)

[`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha256

Computes a SHA-256 hash digest.

```tql
hash_sha256(x:any, [seed=string]) -> string
```

## Description

[Section titled “Description”](#description)

The `hash_sha256` function calculates a SHA-256 hash digest for the given value `x`.

## Examples

[Section titled “Examples”](#examples)

### Compute a SHA-256 digest of a string

[Section titled “Compute a SHA-256 digest of a string”](#compute-a-sha-256-digest-of-a-string)

```tql
from {x: hash_sha256("foo")}
```

```tql
{x: "2c26b46b68ffc68ff99b453c1d30413413422e6e6c8ee90c3abeac38044e8a8c1b0"}
```

## See Also

[Section titled “See Also”](#see-also)

[`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha384

Computes a SHA-384 hash digest.

```tql
hash_sha384(x:any, [seed=string]) -> string
```

## Description

[Section titled “Description”](#description)

The `hash_sha384` function calculates a SHA-384 hash digest for the given value `x`.

## Examples

[Section titled “Examples”](#examples)

### Compute a SHA-384 digest of a string

[Section titled “Compute a SHA-384 digest of a string”](#compute-a-sha-384-digest-of-a-string)

```tql
from {x: hash_sha384("foo")}
```

```tql
{x: "98c11ffdfdd540676b1a137cb1a22b2a70350c9a44171d6b1180c6be5cbb2ee3f79d532c8a1dd9ef2e8e08e752a3babb"}
```

## See Also

[Section titled “See Also”](#see-also)

[`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha512

Computes a SHA-512 hash digest.

```tql
hash_sha512(x:any, [seed=string]) -> string
```

## Description

[Section titled “Description”](#description)

The `hash_sha512` function calculates a SHA-512 hash digest for the given value `x`.

## Examples

[Section titled “Examples”](#examples)

### Compute a SHA-512 digest of a string

[Section titled “Compute a SHA-512 digest of a string”](#compute-a-sha-512-digest-of-a-string)

```tql
from {x: hash_sha512("foo")}
```

```tql
{x: "f7fbba6e0636f890e56fbbf3283e524c6fa3204ae298382d624741d0dc6638326e282c41be5e4254d8820772c5518a2c5a8c0c7f7eda19594a7eb539453e1ed7"}
```

## See Also

[Section titled “See Also”](#see-also)

[`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_xxh3

Computes an XXH3 hash digest.

```tql
hash_xxh3(x:any, [seed=string]) -> string
```

## Description

[Section titled “Description”](#description)

The `hash_xxh3` function calculates a 64-bit XXH3 hash digest for the given value `x`.

## Examples

[Section titled “Examples”](#examples)

### Compute an XXH3 digest of a string

[Section titled “Compute an XXH3 digest of a string”](#compute-an-xxh3-digest-of-a-string)

```tql
from {x: hash_xxh3("foo")}
```

```tql
{x: "ab6e5f64077e7d8a"}
```

## See Also

[Section titled “See Also”](#see-also)

[`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512)

# hour

Extracts the hour component from a timestamp.

```tql
hour(x: time) -> int
```

## Description

[Section titled “Description”](#description)

The `hour` function extracts the hour component from a timestamp as an integer (0-23).

### `x: time`

[Section titled “x: time”](#x-time)

The timestamp from which to extract the hour.

## Examples

[Section titled “Examples”](#examples)

### Extract the hour from a timestamp

[Section titled “Extract the hour from a timestamp”](#extract-the-hour-from-a-timestamp)

```tql
from {
  ts: 2024-06-15T14:30:45.123456,
}
hour = ts.hour()
```

```tql
{
  ts: 2024-06-15T14:30:45.123456,
  hour: 14,
}
```

## See also

[Section titled “See also”](#see-also)

[`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# hours

Converts a number to equivalent hours.

```tql
hours(x:number) -> duration
```

## Description

This function returns hours equivalent to a number, i.e., `number * 1h`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# int

Casts an expression to an integer.

```tql
int(x:number|string, base=int) -> int
```

## Description

[Section titled “Description”](#description)

The `int` function casts the provided value `x` to an integer. Non-integer values are truncated.

### `x: number|string`

[Section titled “x: number|string”](#x-numberstring)

The input to convert.

### `base = int`

[Section titled “base = int”](#base--int)

Base (radix) to parse a string as. Can be `10` or `16`.

If `16`, the string inputs may be optionally prefixed by `0x` or `0X`, e.g., `-0x134`.

Defaults to `10`.

## Examples

[Section titled “Examples”](#examples)

### Cast a floating-point number to an integer

[Section titled “Cast a floating-point number to an integer”](#cast-a-floating-point-number-to-an-integer)

```tql
from {x: int(4.2)}
```

```tql
{x: 4}
```

### Convert a string to an integer

[Section titled “Convert a string to an integer”](#convert-a-string-to-an-integer)

```tql
from {x: int("42")}
```

```tql
{x: 42}
```

### Parse a hexadecimal number

[Section titled “Parse a hexadecimal number”](#parse-a-hexadecimal-number)

```tql
from {x: int("0x42", base=16)}
```

```tql
{x: 66}
```

## See Also

[Section titled “See Also”](#see-also)

[`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [string](/reference/functions/string)

# ip

Casts an expression to an IP address.

```tql
ip(x:string) -> ip
```

## Description

[Section titled “Description”](#description)

The `ip` function casts the provided string `x` to an IP address.

## Examples

[Section titled “Examples”](#examples)

### Cast a string to an IP address

[Section titled “Cast a string to an IP address”](#cast-a-string-to-an-ip-address)

```tql
from {x: ip("1.2.3.4")}
```

```tql
{x: 1.2.3.4}
```

## See Also

[Section titled “See Also”](#see-also)

[`subnet`](/reference/functions/subnet), [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [int](/reference/functions/int), [string](/reference/functions/string)

# ip_category

Returns the type classification of an IP address.

```tql
ip_category(x:ip) -> string
```

## Description

[Section titled “Description”](#description)

The `ip_category` function returns the category classification of a given IP address `x` as a string. The possible return values are:

* `"unspecified"` - Unspecified address (0.0.0.0 or ::)
* `"loopback"` - Loopback address (127.0.0.0/8 or ::1)
* `"link_local"` - Link-local address (169.254.0.0/16 or fe80::/10)
* `"multicast"` - Multicast address (224.0.0.0/4 or ff00::/8)
* `"broadcast"` - Broadcast address (255.255.255.255, IPv4 only)
* `"private"` - Private address (RFC 1918 for IPv4, RFC 4193 for IPv6)
* `"global"` - Global (publicly routable) address

The function returns the most specific classification that applies to the address. For example, 127.0.0.1 is classified as `"loopback"` rather than just non-global.

## Examples

[Section titled “Examples”](#examples)

### Get IP address types

[Section titled “Get IP address types”](#get-ip-address-types)

```tql
from {
  global_ipv4: ip_category(8.8.8.8),
  private_ipv4: ip_category(192.168.1.1),
  loopback: ip_category(127.0.0.1),
  multicast: ip_category(224.0.0.1),
  link_local: ip_category(169.254.1.1),
}
```

```tql
{
  global_ipv4: "global",
  private_ipv4: "private",
  loopback: "loopback",
  multicast: "multicast",
  link_local: "link_local",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local)

# is_alnum

Checks if a string is alphanumeric.

```tql
is_alnum(x:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_alnum` function returns `true` if `x` contains only alphanumeric characters and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a string is alphanumeric

[Section titled “Check if a string is alphanumeric”](#check-if-a-string-is-alphanumeric)

```tql
from {x: "hello123".is_alnum()}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_alpha`](/reference/functions/is_alpha), [`is_numeric`](/reference/functions/is_numeric), [`is_printable`](/reference/functions/is_printable)

# is_alpha

Checks if a string contains only alphabetic characters.

```tql
is_alpha(x:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_alpha` function returns `true` if `x` contains only alphabetic characters and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a string is alphabetic

[Section titled “Check if a string is alphabetic”](#check-if-a-string-is-alphabetic)

```tql
from {x: "hello".is_alpha()}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_alnum`](/reference/functions/is_alnum), [`is_lower`](/reference/functions/is_lower), [`is_numeric`](/reference/functions/is_numeric), [`is_printable`](/reference/functions/is_printable), [`is_upper`](/reference/functions/is_upper)

# is_empty

Checks whether a value is empty.

```tql
is_empty(x:string|list|record) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_empty` function returns `true` if the input value is empty and `false` otherwise.

### `x: string|list|record`

[Section titled “x: string|list|record”](#x-stringlistrecord)

The value to check for emptiness.

The function works on three types:

* **Strings**: Returns `true` for empty strings (`""`)
* **Lists**: Returns `true` for empty lists (`[]`)
* **Records**: Returns `true` for empty records (`{}`)

For `null` values, the function returns `null`. For unsupported types, the function emits a warning and returns `null`.

## Examples

[Section titled “Examples”](#examples)

### Check if a string is empty

[Section titled “Check if a string is empty”](#check-if-a-string-is-empty)

```tql
from {
  empty: "".is_empty(),
  not_empty: "hello".is_empty(),
}
```

```tql
{
  empty: true,
  not_empty: false,
}
```

### Check if a list is empty

[Section titled “Check if a list is empty”](#check-if-a-list-is-empty)

```tql
from {
  empty: [].is_empty(),
  not_empty: [1, 2, 3].is_empty(),
}
```

```tql
{
  empty: true,
  not_empty: false,
}
```

### Check if a record is empty

[Section titled “Check if a record is empty”](#check-if-a-record-is-empty)

```tql
from {
  empty: {}.is_empty(),
  not_empty: {a: 1, b: 2}.is_empty(),
}
```

```tql
{
  empty: true,
  not_empty: false,
}
```

### Null handling

[Section titled “Null handling”](#null-handling)

```tql
from {
  result: null.is_empty(),
}
```

```tql
{
  result: null,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`length`](/reference/functions/length), [`has`](/reference/functions/has)

# is_global

Checks whether an IP address is a global address.

```tql
is_global(x:ip) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_global` function checks whether a given IP address `x` is a global address. A global address is a publicly routable address that is not:

* Loopback (127.0.0.0/8 for IPv4, ::1 for IPv6)
* Private (RFC 1918 for IPv4, RFC 4193 for IPv6)
* Link-local (169.254.0.0/16 for IPv4, fe80::/10 for IPv6)
* Multicast (224.0.0.0/4 for IPv4, ff00::/8 for IPv6)
* Broadcast (255.255.255.255 for IPv4)
* Unspecified (0.0.0.0 for IPv4, :: for IPv6)

## Examples

[Section titled “Examples”](#examples)

### Check if an IP is global

[Section titled “Check if an IP is global”](#check-if-an-ip-is-global)

```tql
from {
  global_ipv4: 8.8.8.8.is_global(),
  global_ipv6: 2001:4860:4860::8888.is_global(),
  private: 192.168.1.1.is_global(),
  loopback: 127.0.0.1.is_global(),
  link_local: 169.254.1.1.is_global(),
}
```

```tql
{
  global_ipv4: true,
  global_ipv6: true,
  private: false,
  loopback: false,
  link_local: false,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_link_local

Checks whether an IP address is a link-local address.

```tql
is_link_local(x:ip) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_link_local` function checks whether a given IP address `x` is a link-local address.

For IPv4, link-local addresses are in the range 169.254.0.0 to 169.254.255.255 (169.254.0.0/16).

For IPv6, link-local addresses are in the range fe80::/10.

Link-local addresses are used for communication between nodes on the same network segment and are not routable on the internet.

## Examples

[Section titled “Examples”](#examples)

### Check if an IP is link-local

[Section titled “Check if an IP is link-local”](#check-if-an-ip-is-link-local)

```tql
from {
  ipv4_link_local: 169.254.1.1.is_link_local(),
  ipv6_link_local: fe80::1.is_link_local(),
  not_link_local: 192.168.1.1.is_link_local(),
}
```

```tql
{
  ipv4_link_local: true,
  ipv6_link_local: true,
  not_link_local: false,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`ip_category`](/reference/functions/ip_category)

# is_loopback

Checks whether an IP address is a loopback address.

```tql
is_loopback(x:ip) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_loopback` function checks whether a given IP address `x` is a loopback address.

For IPv4, loopback addresses are in the range 127.0.0.0 to 127.255.255.255 (127.0.0.0/8).

For IPv6, the loopback address is ::1.

## Examples

[Section titled “Examples”](#examples)

### Check if an IP is loopback

[Section titled “Check if an IP is loopback”](#check-if-an-ip-is-loopback)

```tql
from {
  ipv4_loopback: 127.0.0.1.is_loopback(),
  ipv6_loopback: ::1.is_loopback(),
  not_loopback: 8.8.8.8.is_loopback(),
}
```

```tql
{
  ipv4_loopback: true,
  ipv6_loopback: true,
  not_loopback: false,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_lower

Checks if a string is in lowercase.

```tql
is_lower(x:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_lower` function returns `true` if `x` is entirely in lowercase and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a string is lowercase

[Section titled “Check if a string is lowercase”](#check-if-a-string-is-lowercase)

```tql
from {x: "hello".is_lower()}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_alpha`](/reference/functions/is_alpha), [`is_upper`](/reference/functions/is_upper), [`to_lower`](/reference/functions/to_lower)

# is_multicast

Checks whether an IP address is a multicast address.

```tql
is_multicast(x:ip) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_multicast` function checks whether a given IP address `x` is a multicast address.

For IPv4, multicast addresses are in the range 224.0.0.0 to 239.255.255.255 (224.0.0.0/4).

For IPv6, multicast addresses start with the prefix ff00::/8.

## Examples

[Section titled “Examples”](#examples)

### Check if an IP is multicast

[Section titled “Check if an IP is multicast”](#check-if-an-ip-is-multicast)

```tql
from {
  ipv4_multicast: 224.0.0.1.is_multicast(),
  ipv6_multicast: ff02::1.is_multicast(),
  not_multicast: 8.8.8.8.is_multicast(),
}
```

```tql
{
  ipv4_multicast: true,
  ipv6_multicast: true,
  not_multicast: false,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_numeric

Checks if a string contains only numeric characters.

```tql
is_numeric(x:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_numeric` function returns `true` if `x` contains only numeric characters and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a string is numeric

[Section titled “Check if a string is numeric”](#check-if-a-string-is-numeric)

```tql
from {x: "1234".is_numeric()}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_alpha`](/reference/functions/is_alpha), [`is_alnum`](/reference/functions/is_alnum)

# is_printable

Checks if a string contains only printable characters.

```tql
is_printable(x:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_printable` function returns `true` if `x` contains only printable characters and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a string is printable

[Section titled “Check if a string is printable”](#check-if-a-string-is-printable)

```tql
from {x: "hello".is_printable()}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_alnum`](/reference/functions/is_alnum), [`is_alpha`](/reference/functions/is_alpha)

# is_private

Checks whether an IP address is a private address.

```tql
is_private(x:ip) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_private` function checks whether a given IP address `x` is a private address according to RFC 1918 (IPv4) and RFC 4193 (IPv6).

For IPv4, private addresses are:

* 10.0.0.0/8 (10.0.0.0 - 10.255.255.255)
* 172.16.0.0/12 (172.16.0.0 - 172.31.255.255)
* 192.168.0.0/16 (192.168.0.0 - 192.168.255.255)

For IPv6, private addresses are:

* fc00::/7 (Unique Local Addresses)

Note: Link-local addresses (169.254.0.0/16 for IPv4 and fe80::/10 for IPv6) are **not** considered private addresses by this function.

## Examples

[Section titled “Examples”](#examples)

### Check if an IP is private

[Section titled “Check if an IP is private”](#check-if-an-ip-is-private)

```tql
from {
  private_10: 10.0.0.1.is_private(),
  private_172: 172.16.0.1.is_private(),
  private_192: 192.168.1.1.is_private(),
  private_ipv6: fc00::1.is_private(),
  link_local: 169.254.1.1.is_private(),
  public: 8.8.8.8.is_private(),
}
```

```tql
{
  private_10: true,
  private_172: true,
  private_192: true,
  private_ipv6: true,
  link_local: false,
  public: false,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_title

Checks if a string follows title case.

```tql
is_title(x:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_title` function returns `true` if `x` is in title case and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a string is in title case

[Section titled “Check if a string is in title case”](#check-if-a-string-is-in-title-case)

```tql
from {x: "Hello World".is_title()}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`to_title`](/reference/functions/to_title)

# is_upper

Checks if a string is in uppercase.

```tql
is_upper(x:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_upper` function returns `true` if `x` is entirely in uppercase; otherwise, it returns `false`.

## Examples

[Section titled “Examples”](#examples)

### Check if a string is uppercase

[Section titled “Check if a string is uppercase”](#check-if-a-string-is-uppercase)

```tql
from {x: "HELLO".is_upper()}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_alpha`](/reference/functions/is_alpha), [`is_lower`](/reference/functions/is_lower), [`to_upper`](/reference/functions/to_upper)

# is_v4

Checks whether an IP address has version number 4.

```tql
is_v4(x:ip) -> bool
```

## Description

[Section titled “Description”](#description)

The `ipv4` function checks whether the version number of a given IP address `x` is 4.

## Examples

[Section titled “Examples”](#examples)

### Check if an IP is IPv4

[Section titled “Check if an IP is IPv4”](#check-if-an-ip-is-ipv4)

```tql
from {
  x: 1.2.3.4.is_v4(),
  y: ::1.is_v4(),
}
```

```tql
{
  x: true,
  y: false,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_v6

Checks whether an IP address has version number 6.

```tql
is_v6(x:ip) -> bool
```

## Description

[Section titled “Description”](#description)

The `is_v6` function checks whether the version number of a given IP address `x` is 6.

## Examples

[Section titled “Examples”](#examples)

### Check if an IP is IPv6

[Section titled “Check if an IP is IPv6”](#check-if-an-ip-is-ipv6)

```tql
from {
  x: 1.2.3.4.is_v6(),
  y: ::1.is_v6(),
}
```

```tql
{
  x: false,
  y: true,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_v4`](/reference/functions/is_v4), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# join

Joins a list of strings into a single string using a separator.

```tql
join(xs:list, [separator:string]) -> string
```

## Description

[Section titled “Description”](#description)

The `join` function concatenates the elements of the input list `xs` into a single string, separated by the specified `separator`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

A list of strings to join.

### `separator: string (optional)`

[Section titled “separator: string (optional)”](#separator-string-optional)

The string used to separate elements in the result.

Defaults to `""`.

## Examples

[Section titled “Examples”](#examples)

### Join a list of strings with a comma

[Section titled “Join a list of strings with a comma”](#join-a-list-of-strings-with-a-comma)

```tql
from {x: join(["a", "b", "c"], "-")}
```

```tql
{x: "a-b-c"}
```

## See Also

[Section titled “See Also”](#see-also)

[`split`](/reference/functions/split), [`split_regex`](/reference/functions/split_regex)

# keys

Retrieves a list of field names from a record.

```tql
keys(x:record) -> list<string>
```

## Description

[Section titled “Description”](#description)

The `keys` function returns a list of strings containing all field names from the input record `x`.

### `x: record`

[Section titled “x: record”](#x-record)

The record whose field names you want to retrieve.

## Examples

[Section titled “Examples”](#examples)

### Get all field names from a record

[Section titled “Get all field names from a record”](#get-all-field-names-from-a-record)

```tql
from {x: 1, y: "hello", z: true}
select field_names = this.keys()
```

```tql
{
  field_names: ["x", "y", "z"],
}
```

### Use keys to dynamically access record fields

[Section titled “Use keys to dynamically access record fields”](#use-keys-to-dynamically-access-record-fields)

You can combine `keys` with sorting and element access to dynamically select fields from records. For example, the following collects the values from the element with the first key alphabetically in each record:

```tql
from (
  {foo: 10, bar: 20, baz: 30},
  {foo: 100, bar: 200},
  {baz: 300, qux: 400},
  {},
)
summarize first_sorted_key = this[this.keys().sort().first()]?.collect()
```

```tql
{
  first_sorted_key: [20, 200, 300],
}
```

### Use keys to get distribution of available fields

[Section titled “Use keys to get distribution of available fields”](#use-keys-to-get-distribution-of-available-fields)

```tql
from (
  {foo: 10, bar: 20, baz: 30},
  {foo: 100, bar: 200},
  {baz: 300, qux: 400},
  {},
)
select key = this.keys()
unroll key
top key
```

```tql
{name: "foo", count: 2}
{name: "bar", count: 2}
{name: "baz", count: 2}
{name: "qux", count: 1}
```

## See also

[Section titled “See also”](#see-also)

[`has`](/reference/functions/has), [`get`](/reference/functions/get)

# last

Takes the last non-null grouped value.

```tql
last(xs:list) -> any
```

## Description

[Section titled “Description”](#description)

The `last` function returns the last non-null value in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to search.

## Examples

[Section titled “Examples”](#examples)

### Get the last non-null value

[Section titled “Get the last non-null value”](#get-the-last-non-null-value)

```tql
from {x: 1}, {x: 2}, {x: null}
summarize last_value=last(x)
```

```tql
{last_value: 2}
```

## See Also

[Section titled “See Also”](#see-also)

[`first`](/reference/functions/first)

# length

Retrieves the length of a list.

```tql
length(xs:list) -> int
```

## Description

[Section titled “Description”](#description)

The `length` function returns the number of elements in the list `xs`.

## Examples

[Section titled “Examples”](#examples)

### Get the length of a list

[Section titled “Get the length of a list”](#get-the-length-of-a-list)

```tql
from {n: [1, 2, 3].length()}
```

```tql
{n: 3}
```

## See Also

[Section titled “See Also”](#see-also)

[`is_empty`](/reference/functions/is_empty), [`length_bytes`](/reference/functions/length_bytes), [`length_chars`](/reference/functions/length_chars)

# length_bytes

Returns the length of a string in bytes.

```tql
length_bytes(x:string) -> int
```

## Description

[Section titled “Description”](#description)

The `length_bytes` function returns the byte length of the `x` string.

## Examples

[Section titled “Examples”](#examples)

### Get the byte length of a string

[Section titled “Get the byte length of a string”](#get-the-byte-length-of-a-string)

For ASCII strings, the byte length is the same as the number of characters:

```tql
from {x: "hello".length_bytes()}
```

```tql
{x: 5}
```

For Unicode, this may not be the case:

```tql
from {x: "👻".length_bytes()}
```

```tql
{x: 4}
```

## See Also

[Section titled “See Also”](#see-also)

[`length`](/reference/functions/length), [`length_chars`](/reference/functions/length_chars)

# length_chars

Returns the length of a string in characters.

```tql
length_chars(x:string) -> int
```

## Description

[Section titled “Description”](#description)

The `length_chars` function returns the character length of the `x` string.

## Examples

[Section titled “Examples”](#examples)

### Get the character length of a string

[Section titled “Get the character length of a string”](#get-the-character-length-of-a-string)

For ASCII strings, the character length is the same as the number of bytes:

```tql
from {x: "hello".length_chars()}
```

```tql
{x: 5}
```

For Unicode, this may not be the case:

```tql
from {x: "👻".length_chars()}
```

```tql
{x: 1}
```

## See Also

[Section titled “See Also”](#see-also)

[`length`](/reference/functions/length), [`length_bytes`](/reference/functions/length_bytes)

# map

Maps each list element to an expression.

```tql
map(xs:list, capture:field, any->any) -> list
```

## Description

[Section titled “Description”](#description)

The `map` function applies an expression to each element within a list, returning a list of the same length.

### `xs: list`

[Section titled “xs: list”](#xs-list)

A list of values.

### `function: any -> any`

[Section titled “function: any -> any”](#function-any---any)

A lambda function that is applied to each list element.

If the lambda evaluates to different but compatible types for the elements of the list a unification is performed. For example, records are compatible with other records, and the resulting record will have the keys of both.

If the lambda evaluates to incompatible types for different elements of the list, the largest possible group of compatible values will be chosen and all other values will be `null`.

## Examples

[Section titled “Examples”](#examples)

### Check a predicate for all members of a list

[Section titled “Check a predicate for all members of a list”](#check-a-predicate-for-all-members-of-a-list)

```tql
from {
  hosts: [1.2.3.4, 127.0.0.1, 10.0.0.127]
}
hosts = hosts.map(x => x in 10.0.0.0/8)
```

```tql
{
  hosts: [false, false, true]
}
```

### Reshape a record inside a list

[Section titled “Reshape a record inside a list”](#reshape-a-record-inside-a-list)

```tql
from {
  answers: [
    {
      rdata: 76.76.21.21,
      rrname: "tenzir.com"
    }
  ]
}
answers = answers.map(x => {hostname: x.rrname, ip: x.rdata})
```

```tql
{
  answers: [
    {
      hostname: "tenzir.com",
      ip: "76.76.21.21",
    }
  ]
}
```

### Null values

[Section titled “Null values”](#null-values)

In the below example, the first entry does not match the given grok pattern, causing `parse_grok` to emit a `null`.

`map` will promote `null` values to typed null values, allowing you to still get all valid parts of the list mapped.

```tql
let $pattern = "%{WORD:w} %{NUMBER:n}"


from {
  l: ["hello", "world 42"]
}
l = l.map(str => str.parse_grok($pattern))
```

```tql
{
  l: [
    null,
    { w: "world", n: 42, },
  ],
}
```

### Incompatible types between elements

[Section titled “Incompatible types between elements”](#incompatible-types-between-elements)

In the below example the list `l` contains three strings. Two of those are JSON objects and one is a JSON list. While all three can be parsed as JSON by `parse_json`, the resulting `record` and `list` are incompatible types.

`map` will resolve this by picking the “largest compatible group”, in this case preferring the two `record`s over one `list`.

```tql
from {
  l: [
    r#"{ "x": 0 }"#,
    r#"{ "y": 0 }"#,
    r#"[ 3 ]"#,
  ]
}
l = l.map(str => str.parse_json())
```

```tql
{
  l: [
    {x: 0, y: null},
    {x: null, y: 0},
    null,
  ]
}
```

## See Also

[Section titled “See Also”](#see-also)

[`where`](/reference/functions/where), [`zip`](/reference/functions/zip)

# match_regex

Checks if a string partially matches a regular expression.

```tql
match_regex(input:string, regex:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `match_regex` function returns `true` if `regex` matches a substring of `input`.

To check whether the full string matches, you can use `^` and `$` to signify start and end of the string.

### `input: string`

[Section titled “input: string”](#input-string)

The string to partially match.

### `regex: string`

[Section titled “regex: string”](#regex-string)

The regular expression try and match.

The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `match_regex` at the moment.

## Examples

[Section titled “Examples”](#examples)

### Check contains a matching substring

[Section titled “Check contains a matching substring”](#check-contains-a-matching-substring)

```tql
from {input: "Hello There World"},
  {input: "hi there!"},
  {input: "Good Morning" }
output = input.match_regex("[T|t]here")
```

```tql
{input: "Hello There World", output: true}
{input: "hi there!", output: true}
{input: "Good Morning", output: false}
```

### Check if a string matches fully

[Section titled “Check if a string matches fully”](#check-if-a-string-matches-fully)

```tql
from {input: "example"},
  {input: "Example!"},
  {input: "example?" }
output = input.match_regex("^[E|e]xample[!]?$")
```

```tql
{input: "example", output: true}
{input: "example!", output: true}
{input: "example?", output: false}
```

# max

Computes the maximum of all grouped values.

```tql
max(xs:list) -> number
```

## Description

[Section titled “Description”](#description)

The `max` function returns the largest numeric value in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

## Examples

[Section titled “Examples”](#examples)

### Find the maximum value

[Section titled “Find the maximum value”](#find-the-maximum-value)

```tql
from {x: 1}, {x: 2}, {x: 3}
summarize max_value=max(x)
```

```tql
{max_value: 3}
```

## See Also

[Section titled “See Also”](#see-also)

[`min`](/reference/functions/min), [`mean`](/reference/functions/mean), [`sum`](/reference/functions/sum)

# mean

Computes the mean of all grouped values.

```tql
mean(xs:list) -> float
```

## Description

[Section titled “Description”](#description)

The `mean` function returns the average of all numeric values in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to average.

## Examples

[Section titled “Examples”](#examples)

### Compute the mean value

[Section titled “Compute the mean value”](#compute-the-mean-value)

```tql
from {x: 1}, {x: 2}, {x: 3}
summarize avg=mean(x)
```

```tql
{avg: 2.0}
```

## See Also

[Section titled “See Also”](#see-also)

[`max`](/reference/functions/max), [`median`](/reference/functions/median), [`min`](/reference/functions/min), [`quantile`](/reference/functions/quantile), [`stddev`](/reference/functions/stddev), [`sum`](/reference/functions/sum), [`variance`](/reference/functions/variance)

# median

Computes the approximate median of all grouped values using a t-digest algorithm.

```tql
median(xs:list) -> float
```

## Description

[Section titled “Description”](#description)

The `median` function returns an approximate median of all numeric values in `xs`, computed with a t-digest algorithm.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

## Examples

[Section titled “Examples”](#examples)

### Compute the median value

[Section titled “Compute the median value”](#compute-the-median-value)

```tql
from {x: 1}, {x: 2}, {x: 3}, {x: 4}
summarize median_value=median(x)
```

```tql
{median_value: 2.5}
```

## See Also

[Section titled “See Also”](#see-also)

[`mean`](/reference/functions/mean), [`mode`](/reference/functions/mode), [`quantile`](/reference/functions/quantile)

# merge

Combines two records into a single record by merging their fields.

```tql
merge(x: record, y: record) -> record
```

## Description

[Section titled “Description”](#description)

The `merge` function takes two records and returns a new record containing all fields from both records. If both records contain the same field, the value from the second record takes precedence.

## Examples

[Section titled “Examples”](#examples)

### Basic record merging

[Section titled “Basic record merging”](#basic-record-merging)

```tql
from {x: {a: 1, b: 2}, y: {c: 3, d: 4}}
select result = merge(x, y)
```

```tql
{
  result: {
    a: 1,
    b: 2,
    c: 3,
    d: 4
  }
}
```

### Handling overlapping fields

[Section titled “Handling overlapping fields”](#handling-overlapping-fields)

When fields exist in both records, the second record’s values take precedence:

```tql
from {
  r1: {name: "Alice", age: 30},
  r2: {name: "Bob", location: "NY"},
}
select result = merge(r1, r2)
```

```tql
{
  result: {
    name: "Bob",
    age: 30,
    location: "NY"
  }
}
```

### Handling null values

[Section titled “Handling null values”](#handling-null-values)

If either input is null, the input will be ignored.

```tql
from {x: {a: 1}, y: null}
select result = merge(x, y)
```

```tql
{
  result: {
    a: 1
  }
}
```

## See Also

[Section titled “See Also”](#see-also)

[`concatenate`](/reference/functions/concatenate)

# microseconds

Converts a number to equivalent microseconds.

```tql
microseconds(x:number) -> duration
```

## Description

This function returns microseconds equivalent to a number, i.e., `number * 1us`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# milliseconds

Converts a number to equivalent milliseconds.

```tql
milliseconds(x:number) -> duration
```

## Description

This function returns milliseconds equivalent to a number, i.e., `number * 1ms`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# min

Computes the minimum of all grouped values.

```tql
min(xs:list) -> number
```

## Description

[Section titled “Description”](#description)

The `min` function returns the smallest numeric value in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

## Examples

[Section titled “Examples”](#examples)

### Find the minimum value

[Section titled “Find the minimum value”](#find-the-minimum-value)

```tql
from {x: 1}, {x: 2}, {x: 3}
summarize min_value=min(x)
```

```tql
{min_value: 1}
```

## See Also

[Section titled “See Also”](#see-also)

[`max`](/reference/functions/max), [`mean`](/reference/functions/mean), [`sum`](/reference/functions/sum)

# minute

Extracts the minute component from a timestamp.

```tql
minute(x: time) -> int
```

## Description

[Section titled “Description”](#description)

The `minute` function extracts the minute component from a timestamp as an integer (0-59).

### `x: time`

[Section titled “x: time”](#x-time)

The timestamp from which to extract the minute.

## Examples

[Section titled “Examples”](#examples)

### Extract the minute from a timestamp

[Section titled “Extract the minute from a timestamp”](#extract-the-minute-from-a-timestamp)

```tql
from {
  ts: 2024-06-15T14:30:45.123456,
}
minute = ts.minute()
```

```tql
{
  ts: 2024-06-15T14:30:45.123456,
  minute: 30,
}
```

## See also

[Section titled “See also”](#see-also)

[`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`second`](/reference/functions/second)

# minutes

Converts a number to equivalent minutes.

```tql
minutes(x:number) -> duration
```

## Description

This function returns minutes equivalent to a number, i.e., `number * 1min`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# mode

Takes the most common non-null grouped value.

```tql
mode(xs:list) -> any
```

## Description

[Section titled “Description”](#description)

The `mode` function returns the most frequently occurring non-null value in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

## Examples

[Section titled “Examples”](#examples)

### Find the mode of values

[Section titled “Find the mode of values”](#find-the-mode-of-values)

```tql
from {x: 1}, {x: 1}, {x: 2}, {x: 3}
summarize mode_value=mode(x)
```

```tql
{mode_value: 1}
```

## See Also

[Section titled “See Also”](#see-also)

[`median`](/reference/functions/median), [`value_counts`](/reference/functions/value_counts)

# month

Extracts the month component from a timestamp.

```tql
month(x: time) -> int
```

## Description

[Section titled “Description”](#description)

The `month` function extracts the month component from a timestamp as an integer (1-12).

### `x: time`

[Section titled “x: time”](#x-time)

The timestamp from which to extract the month.

## Examples

[Section titled “Examples”](#examples)

### Extract the month from a timestamp

[Section titled “Extract the month from a timestamp”](#extract-the-month-from-a-timestamp)

```tql
from {
  ts: 2024-06-15T14:30:45.123456,
}
month = ts.month()
```

```tql
{
  ts: 2024-06-15T14:30:45.123456,
  month: 6,
}
```

## See also

[Section titled “See also”](#see-also)

[`year`](/reference/functions/year), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# months

Converts a number to equivalent months.

```tql
months(x:number) -> duration
```

## Description

This function returns months equivalent to a number, i.e., `number * 1/12 * 1y`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# nanoseconds

Converts a number to equivalent nanoseconds.

```tql
nanoseconds(x:number) -> duration
```

## Description

This function returns nanoseconds equivalent to a number, i.e., `number * 1ns`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# network

Retrieves the network address of a subnet.

```tql
network(x:subnet) -> ip
```

## Description

[Section titled “Description”](#description)

The `network` function returns the network address of a subnet.

## Examples

[Section titled “Examples”](#examples)

### Get the network address of a subnet

[Section titled “Get the network address of a subnet”](#get-the-network-address-of-a-subnet)

```tql
from {subnet: 192.168.0.0/16}
select ip = subnet.network()
```

```tql
{ip: 192.168.0.0}
```

# now

Gets the current wallclock time.

```tql
now() -> time
```

## Description

[Section titled “Description”](#description)

The `now` function returns the current wallclock time.

## Examples

[Section titled “Examples”](#examples)

### Get the time in UTC

[Section titled “Get the time in UTC”](#get-the-time-in-utc)

```tql
let $now = now()
from { x: $now }
```

```tql
{x: 2024-10-28T13:27:33.957987}
```

### Compute a field with the current time

[Section titled “Compute a field with the current time”](#compute-a-field-with-the-current-time)

```tql
subscribe "my-topic"
select ts=now()
```

```tql
{ts: 2024-10-30T15:03:04.85298}
{ts: 2024-10-30T15:03:06.31878}
{ts: 2024-10-30T15:03:07.59813}
```

## See Also

[Section titled “See Also”](#see-also)

[`from_epoch`](/reference/functions/from_epoch), [`since_epoch`](/reference/functions/since_epoch)

# ocsf::category_name

Returns the `category_name` for a given `category_uid`.

```tql
ocsf::category_uid(id:int) -> string
```

## Description

[Section titled “Description”](#description)

### `id: int`

[Section titled “id: int”](#id-int)

The `category_uid` for which `category_name` should be returned.

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::category_uid`](/reference/functions/ocsf/category_uid)

# ocsf::category_uid

Returns the `category_uid` for a given `category_name`.

```tql
ocsf::category_uid(name:string) -> int
```

## Description

[Section titled “Description”](#description)

### `name: string`

[Section titled “name: string”](#name-string)

The `category_name` for which `category_uid` should be returned.

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::category_name`](/reference/functions/ocsf/category_name)

# ocsf::class_name

Returns the `class_name` for a given `class_uid`.

```tql
ocsf::class_uid(id:int) -> string
```

## Description

[Section titled “Description”](#description)

### `id: int`

[Section titled “id: int”](#id-int)

The `class_uid` for which `class_name` should be returned.

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::class_uid`](/reference/functions/ocsf/class_uid)

# ocsf::class_uid

Returns the `class_uid` for a given `class_name`.

```tql
ocsf::class_uid(name:string) -> int
```

## Description

[Section titled “Description”](#description)

### `name: string`

[Section titled “name: string”](#name-string)

The `class_name` for which `class_uid` should be returned.

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::class_name`](/reference/functions/ocsf/class_name)

# ocsf::type_name

Returns the `type_name` for a given `type_uid`.

```tql
ocsf::type_name(id:int) -> string
```

## Description

[Section titled “Description”](#description)

### `id: int`

[Section titled “id: int”](#id-int)

The `type_uid` for which `type_name` should be returned.

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::type_uid`](/reference/functions/ocsf/type_uid)

# ocsf::type_uid

Returns the `type_uid` for a given `type_name`.

```tql
ocsf::type_uid(name:string) -> int
```

## Description

[Section titled “Description”](#description)

### `name: string`

[Section titled “name: string”](#name-string)

The `type_name` for which `type_uid` should be returned.

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::type_name`](/reference/functions/ocsf/type_name)

# otherwise

Returns a `fallback` value if `primary` is `null`.

```tql
otherwise(primary:any, fallback:any) -> any
```

## Description

[Section titled “Description”](#description)

The `otherwise` function evaluates its arguments and replaces `primary` with `fallback` where `primary` would be `null`.

### `primary: any`

[Section titled “primary: any”](#primary-any)

The expression to return if not `null`.

### `fallback: any`

[Section titled “fallback: any”](#fallback-any)

The expression to return if `primary` evaluates to `null`.

## Examples

[Section titled “Examples”](#examples)

### Set a default value for a key

[Section titled “Set a default value for a key”](#set-a-default-value-for-a-key)

```tql
from {x: 1}, {x: 2}, {}
x = x.otherwise(-1)
```

```tql
{x: 1}
{x: 2}
{x: -1}
```

# pad_end

Pads a string at the end to a specified length.

```tql
pad_end(x:string, length:int, [pad_char:string]) -> string
```

## Description

[Section titled “Description”](#description)

The `pad_end` function pads the string `x` at the end with `pad_char` (default: space) until it reaches the specified `length`. If the string is already longer than or equal to the specified length, it returns the original string unchanged.

### `x: string`

[Section titled “x: string”](#x-string)

The string to pad.

### `length: int`

[Section titled “length: int”](#length-int)

The target length of the resulting string.

### `pad_char: string`

[Section titled “pad\_char: string”](#pad_char-string)

The character to use for padding. Must be a single character. Defaults to a space.

Defaults to `" "`.

## Examples

[Section titled “Examples”](#examples)

### Pad with spaces

[Section titled “Pad with spaces”](#pad-with-spaces)

```tql
from {x: "hello".pad_end(10)}
```

```tql
{x: "hello     "}
```

### Pad with custom character

[Section titled “Pad with custom character”](#pad-with-custom-character)

```tql
from {x: "hello".pad_end(10, ".")}
```

```tql
{x: "hello....."}
```

### String already long enough

[Section titled “String already long enough”](#string-already-long-enough)

```tql
from {x: "hello world".pad_end(5)}
```

```tql
{x: "hello world"}
```

## See Also

[Section titled “See Also”](#see-also)

[`pad_start`](/reference/functions/pad_start), [`trim`](/reference/functions/trim), [`trim_end`](/reference/functions/trim_end)

# pad_start

Pads a string at the start to a specified length.

```tql
pad_start(x:string, length:int, [pad_char:string]) -> string
```

## Description

[Section titled “Description”](#description)

The `pad_start` function pads the string `x` at the start with `pad_char` (default: space) until it reaches the specified `length`. If the string is already longer than or equal to the specified length, it returns the original string unchanged.

### `x: string`

[Section titled “x: string”](#x-string)

The string to pad.

### `length: int`

[Section titled “length: int”](#length-int)

The target length of the resulting string.

### `pad_char: string`

[Section titled “pad\_char: string”](#pad_char-string)

The character to use for padding. Must be a single character. Defaults to a space.

Defaults to `" "`.

## Examples

[Section titled “Examples”](#examples)

### Pad with spaces

[Section titled “Pad with spaces”](#pad-with-spaces)

```tql
from {x: "hello".pad_start(10)}
```

```tql
{x: "     hello"}
```

### Pad with custom character

[Section titled “Pad with custom character”](#pad-with-custom-character)

```tql
from {x: "42".pad_start(5, "0")}
```

```tql
{x: "00042"}
```

### String already long enough

[Section titled “String already long enough”](#string-already-long-enough)

```tql
from {x: "hello world".pad_start(5)}
```

```tql
{x: "hello world"}
```

## See Also

[Section titled “See Also”](#see-also)

[`pad_end`](/reference/functions/pad_end), [`trim`](/reference/functions/trim), [`trim_start`](/reference/functions/trim_start)

# parent_dir

Extracts the parent directory from a file path.

```tql
parent_dir(x:string) -> string
```

## Description

[Section titled “Description”](#description)

The `parent_dir` function returns the parent directory path of the given file path, excluding the file name.

## Examples

[Section titled “Examples”](#examples)

### Extract the parent directory from a file path

[Section titled “Extract the parent directory from a file path”](#extract-the-parent-directory-from-a-file-path)

```tql
from {x: parent_dir("/path/to/log.json")}
```

```tql
{x: "/path/to"}
```

## See Also

[Section titled “See Also”](#see-also)

[`file_name`](/reference/functions/file_name)

# parse_cef

Parses a string as a CEF message

```tql
parse_cef(input:string, [schema=string, selector=string, schema_only=bool,
          raw=bool, unflatten_separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

The `parse_cef` function parses a string as a CEF message

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

```tql
from { x: "CEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8|key=value" }
y = x.parse_cef()
```

```tql
{
  cef_version: 0,
  device_vendor: "Cynet",
  device_product: "Cynet 360",
  device_version: "4.5.4.22139",
  signature_id: "0",
  name: "Memory Pattern - Cobalt Strike Beacon ReflectiveLoader",
  severity: "8",
  extension: {
    key: "value"
  }
}
```

# See Also

[Section titled “See Also”](#see-also)

[`read_cef`](/reference/operators/read_cef), [`print_cef`](/reference/functions/print_cef), [`read_syslog`](/reference/operators/read_syslog)

# parse_csv

Parses a string as CSV (Comma-Separated Values).

```tql
parse_csv(input:string, header=list<string>|string,
         [list_separator=string, null_value=string,
          auto_expand=bool, quotes=string, schema=string,
          selector=string, schema_only=bool, raw=bool,
          unflatten_separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

The `parse_csv` function parses a string as [CSV](https://en.wikipedia.org/wiki/Comma-separated_values).

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `header = list<string>|string`

[Section titled “header = list\<string>|string”](#header--liststringstring)

A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values.

### `list_separator = string (optional)`

[Section titled “list\_separator = string (optional)”](#list_separator--string-optional)

The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled.

Defaults to `;`.

### `null_value = string (optional)`

[Section titled “null\_value = string (optional)”](#null_value--string-optional)

The string denoting an absent value.

### `auto_expand = bool (optional)`

[Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional)

Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

```tql
from { input: "1,2,3" }
output = input.parse_csv(header=["a","b","c"])
```

```tql
{
  input: "1,2,3",
  output: {
    a: 1,
    b: 2,
    c: 3,
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_csv`](/reference/operators/read_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_tsv`](/reference/functions/parse_tsv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_grok

Parses a string according to a grok pattern.

```tql
parse_grok(input:string, pattern:string, [pattern_definitions=record|string,
           indexed_captures=bool, include_unnamed=bool,
           schema=string, selector=string, schema_only=bool,
           raw=bool, unflatten_separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

`parse_grok` uses a regular expression based parser similar to the [Logstash `grok` plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) in Elasticsearch. Tenzir ships with the same built-in patterns as Elasticsearch, found [here](https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns/ecs-v1).

In short, `pattern` consists of replacement fields, that look like `%{SYNTAX[:SEMANTIC[:CONVERSION]]}`, where:

* `SYNTAX` is a reference to a pattern, either built-in or user-defined through the `pattern_defintions` option.
* `SEMANTIC` is an identifier that names the field in the parsed record.
* `CONVERSION` is either `infer` (default), `string` (default with `raw=true`), `int`, or `float`.

The supported regular expression syntax is the one supported by [Boost.Regex](https://www.boost.org/doc/libs/1_81_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html), which is effectively Perl-compatible.

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `pattern: string`

[Section titled “pattern: string”](#pattern-string)

The `grok` pattern used for matching. Must match the input in its entirety.

### `pattern_definitions = record|string (optional)`

[Section titled “pattern\_definitions = record|string (optional)”](#pattern_definitions--recordstring-optional)

New pattern definitions to use. This may be a record of the form

```tql
{
  pattern_name: "pattern", …
}
```

For example, the built-in pattern `INT` would be defined as

```tql
{ INT: "(?:[+-]?(?:[0-9]+))" }
```

Alternatively, this may be a user-defined newline-delimited list of patterns, where a line starts with the pattern name, followed by a space, and the `grok`-pattern for that pattern. For example, the built-in pattern `INT` is defined as follows:

```plaintext
INT (?:[+-]?(?:[0-9]+))
```

### `indexed_captures = bool (optional)`

[Section titled “indexed\_captures = bool (optional)”](#indexed_captures--bool-optional)

All subexpression captures are included in the output, with the `SEMANTIC` used as the field name if possible, and the capture index otherwise.

### `include_unnamed = bool (optional)`

[Section titled “include\_unnamed = bool (optional)”](#include_unnamed--bool-optional)

By default, only fields that were given a name with `SEMANTIC`, or with the regular expression named capture syntax `(?<name>...)` are included in the resulting record.

With `include_unnamed=true`, replacement fields without a `SEMANTIC` are included in the output, using their `SYNTAX` value as the record field name.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

```tql
let $pattern = "%{IP:client} %{WORD} %{URIPATHPARAM:req} %{NUMBER:bytes} %{NUMBER:dur}"
from { input: "55.3.244.1 GET /index.html 15824 0.043" }
output = input.parse_grok($pattern)
output.dur = output.dur * 1s
```

```tql
{
  input: "55.3.244.1 GET /index.html 15824 0.043",
  output: {
    client: 55.3.244.1,
    req: "/index.html",
    bytes: 15824,
    dur: 43.0ms
  }
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_grok`](/reference/operators/read_grok)

# parse_json

Parses a string as a JSON value.

```tql
parse_json(input:string, [schema=string, selector=string, schema_only=bool,
           raw=bool, unflatten_separator=string]) -> any
```

## Description

[Section titled “Description”](#description)

The `parse_json` function parses a string as a JSON value.

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Parse a JSON record

[Section titled “Parse a JSON record”](#parse-a-json-record)

```tql
from { input: r#"{ "a": 42, "b": "text"}"# }
output = input.parse_json()
```

```tql
{
  input: "{ \"a\": 42, \"b\": \"text\"}",
  output: {
    a: 42,
    b: "text"
  }
}
```

### Parse a JSON list

[Section titled “Parse a JSON list”](#parse-a-json-list)

```tql
from { input: "[0,1]" }
output = input.parse_json()
```

```tql
{
  input: "[0,1]",
  output: [0, 1]
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_json`](/reference/operators/read_json)

# parse_kv

Parses a string as key-value pairs.

```tql
parse_kv(input:string, [field_split=string, value_split=string, quotes=string,
         schema=string, selector=string, schema_only=bool,
         raw=bool, unflatten_separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

The `parse_kv` function parses a string as key-value pairs.

The string is first split into fields according to `field_split`. This can be a regular expression. For example, the input `foo: bar, baz: 42` can be split into `foo: bar` and `baz: 42` with the regular expression `r",\s*"` (a comma, followed by any amount of whitespace) as the field splitter. Note that the matched separators are removed when splitting a string.

Afterwards, the extracted fields are split into their key and value by `value_split`, which can again be a regular expression. In our example, `r":\s*"` could be used to split `foo: bar` into the key `foo` and its value `bar`, and similarly `baz: 42` into `baz` and `42`. The result would thus be `{"foo": "bar", "baz": 42}`. If the regex matches multiple substrings, only the first match is used. If no match is found, the “field” is considered an extension of the previous fields value.

The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `parse_kv` at the moment. However, if the regular expression has a capture group, it is assumed that only the content of the capture group shall be used as the separator. This means that unsupported regular expressions such as `(?=foo)bar(?<=baz)` can be effectively expressed as `foo(bar)baz` instead.

### Quoted Values

[Section titled “Quoted Values”](#quoted-values)

The parser is aware of double-quotes (`"`). If the `field_split` or `value_split` are found within enclosing quotes, they are not considered matches. This means that both the key and the value may be enclosed in double-quotes.

For example, given `field_split` `\s*,\s*` and `value_split` `=`, the input

```plaintext
"key"="nested = value",key2="value, and more"
```

will parse as

```tql
{
  key: "nested = value",
  key2: "value, and more",
}
```

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `field_split = string (optional)`

[Section titled “field\_split = string (optional)”](#field_split--string-optional)

The regular expression used to separate individual fields.

Defaults to `r"\s"`.

### `value_split = string (optional)`

[Section titled “value\_split = string (optional)”](#value_split--string-optional)

The regular expression used to separate a key from its value.

Defaults to `"="`.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Parse comma-separated key-value pairs

[Section titled “Parse comma-separated key-value pairs”](#parse-comma-separated-key-value-pairs)

```tql
from {
  input: "surname: John, family_name: Smith, date_of_birth: 1995-05-26"
}
output = input.parse_kv(field_split=r"\s*,\s*", value_split=r"\s*:\s*")
```

```tql
{
  input: "surname: John, family_name: Smith, date_of_birth: 1995-05-26",
  output: {
    surname: "John",
    family_name: "Smith",
    date_of_birth: 1995-05-26,
  },
}
```

### Fields without a `value_split`

[Section titled “Fields without a value\_split”](#fields-without-a-value_split)

```tql
from  { input: "x=1 y=2 z=3 4 5 a=6" }
this = { ...input.parse_kv() }
```

```tql
{
  x: 1,
  y: 2,
  z: "3 4 5",
  a: 6,
}
```

# See Also

[Section titled “See Also”](#see-also)

[`read_kv`](/reference/operators/read_kv), [`print_kv`](/reference/functions/print_kv)

# parse_leef

Parses a string as a LEEF message

```tql
parse_leef(input:string, [schema=string, selector=string, schema_only=bool,
           raw=bool, unflatten_separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

The `parse_leef` function parses a string as a LEEF message

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

```tql
from { input: "LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1  dst=2.10.20.20  spt=1200" }
output = input.parse_leef()
```

```tql
{
  input: "LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1\tdst=2.10.20.20\tspt=1200",
  output: {
    leef_version: "1.0",
    vendor: "Microsoft",
    product_name: "MSExchange",
    product_version: "2016",
    event_class_id: "15345",
    attributes: {
      src: 10.50.1.1,
      dst: 2.10.20.20,
      spt: 1200,
    },
  },
}
```

# See Also

[Section titled “See Also”](#see-also)

[`read_leef`](/reference/operators/read_leef), [`print_leef`](/reference/functions/print_leef), [`parse_cef`](/reference/functions/parse_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# parse_ssv

Parses a string as space separated values.

```tql
parse_ssv(input:string, header=list<string>|string,
         [list_separator:string, null_value:string,
          auto_expand=bool, quotes=string, schema=string,
          selector=string, schema_only=bool, raw=bool,
          unflatten_separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

The `parse_ssv` function parses a string as space separated values.

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `header = list<string>|string`

[Section titled “header = list\<string>|string”](#header--liststringstring)

A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values.

### `list_separator = string (optional)`

[Section titled “list\_separator = string (optional)”](#list_separator--string-optional)

The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled.

Defaults to `,`.

### `null_value = string (optional)`

[Section titled “null\_value = string (optional)”](#null_value--string-optional)

The string denoting an absent value.

### `auto_expand = bool (optional)`

[Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional)

Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

```tql
from { input: "1 2 3" }
output = input.parse_ssv(header=["a","b","c"])
```

```tql
{
  input: "1 2 3",
  output: {
    a: 1,
    b: 2,
    c: 3,
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_ssv`](/reference/operators/read_ssv), [`parse_csv`](/reference/functions/parse_csv), [`parse_tsv`](/reference/functions/parse_tsv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_syslog

Parses a string as a Syslog message.

```tql
parse_syslog [raw=bool, schema=string, selector=string, schema_only=bool,
              unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

Parses a string as a [Syslog](https://en.wikipedia.org/wiki/Syslog) message.

Tenzir supports reading syslog messages in both the standardized “Syslog Protocol” format ([RFC 5424](https://tools.ietf.org/html/rfc5424)), and the older “BSD syslog Protocol” format ([RFC 3164](https://tools.ietf.org/html/rfc3164)).

## `input: string`

[Section titled “input: string”](#input-string)

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Parse a RFC5424 syslog string

[Section titled “Parse a RFC5424 syslog string”](#parse-a-rfc5424-syslog-string)

```tql
from { input: r#"<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource="Application" eventID="1011"] Event log entry"#}
output = input.parse_syslog()
```

```tql
{
  input: "<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource=\"Application\" eventID=\"1011\"] Event log entry",
  output: {
    facility: 20,
    severity: 5,
    version: 8,
    timestamp: 2023-10-11T22:14:15.003Z,
    hostname: "mymachineexamplecom",
    app_name: "evntslog",
    process_id: "1370",
    message_id: "ID47",
    structured_data: {
      "exampleSDID@32473": {
        eventSource: "Application",
        eventID: 1011,
      },
    },
    message: "Event log entry",
  },
}
```

# parse_time

Parses a time from a string that follows a specific format.

```tql
parse_time(input: string, format: string) -> time
```

## Description

[Section titled “Description”](#description)

The `parse_time` function matches the given `input` string against the `format` to construct a timestamp.

### `input: string`

[Section titled “input: string”](#input-string)

The input string from which the timestamp should be extracted.

### `format: string`

[Section titled “format: string”](#format-string)

The string that specifies the format of `input`, for example `"%m-%d-%Y"`. The allowed format specifiers are the same as for `strptime(3)`:

| Specifier | Description                            | Example                   |
| :-------: | :------------------------------------- | :------------------------ |
|    `%%`   | A literal `%` character                | `%`                       |
|    `%a`   | Abbreviated weekday name               | `Mon`                     |
|    `%A`   | Full weekday name                      | `Monday`                  |
|    `%b`   | Abbreviated month name                 | `Jan`                     |
|    `%B`   | Full month name                        | `January`                 |
|    `%c`   | Date and time representation           | `Mon Jan 1 12:00:00 2024` |
|    `%C`   | Century (year divided by 100)          | `20`                      |
|    `%d`   | Day of month with zero padding         | `01`, `31`                |
|    `%D`   | Equivalent to `%m/%d/%y`               | `01/31/24`                |
|    `%e`   | Day of month with space padding        | `1`, `31`                 |
|    `%F`   | Equivalent to `%Y-%m-%d`               | `2024-01-31`              |
|    `%g`   | Last two digits of ISO week-based year | `24`                      |
|    `%G`   | ISO week-based year                    | `2024`                    |
|    `%h`   | Equivalent to `%b`                     | `Jan`                     |
|    `%H`   | Hour in 24-hour format                 | `00`, `23`                |
|    `%I`   | Hour in 12-hour format                 | `01`, `12`                |
|    `%j`   | Day of year                            | `001`, `365`              |
|    `%m`   | Month number                           | `01`, `12`                |
|    `%M`   | Minute                                 | `00`, `59`                |
|    `%n`   | Newline character                      | `\n`                      |
|    `%p`   | AM/PM designation                      | `AM`, `PM`                |
|    `%r`   | 12-hour clock time                     | `12:00:00 PM`             |
|    `%R`   | Equivalent to `%H:%M`                  | `23:59`                   |
|    `%S`   | Seconds                                | `00`, `59`                |
|    `%t`   | Tab character                          | `\t`                      |
|    `%T`   | Equivalent to `%H:%M:%S`               | `23:59:59`                |
|    `%u`   | ISO weekday (Monday=1)                 | `1`, `7`                  |
|    `%U`   | Week number (Sunday as first day)      | `00`, `52`                |
|    `%V`   | ISO week number                        | `01`, `53`                |
|    `%w`   | Weekday (Sunday=0)                     | `0`, `6`                  |
|    `%W`   | Week number (Monday as first day)      | `00`, `52`                |
|    `%x`   | Date representation                    | `01/31/24`                |
|    `%X`   | Time representation                    | `23:59:59`                |
|    `%y`   | Year without century                   | `24`                      |
|    `%Y`   | Year with century                      | `2024`                    |
|    `%z`   | UTC offset                             | `+0000`, `-0430`          |
|    `%Z`   | Time zone abbreviation                 | `UTC`, `EST`              |

## Examples

[Section titled “Examples”](#examples)

### Parse a timestamp

[Section titled “Parse a timestamp”](#parse-a-timestamp)

```tql
from {
  x: "2024-12-31+12:59:42",
}
x = x.parse_time("%Y-%m-%d+%H:%M:%S")
```

```tql
{x: 2024-12-31T12:59:42.000000}
```

## See Also

[Section titled “See Also”](#see-also)

[`format_time`](/reference/functions/format_time)

# parse_tsv

Parses a string as tab separated values.

```tql
parse_tsv(input:string, header=list<string>|string,
         [list_separator:string, null_value:string,
          auto_expand=bool, quotes=string, schema=string,
          selector=string, schema_only=bool, raw=bool,
          unflatten_separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

The `parse_tsv` function parses a string as [TSV](https://en.wikipedia.org/wiki/Tab-separated_values).

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `header = list<string>|string`

[Section titled “header = list\<string>|string”](#header--liststringstring)

A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values.

### `list_separator = string (optional)`

[Section titled “list\_separator = string (optional)”](#list_separator--string-optional)

The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled.

Defaults to `,`.

### `null_value = string (optional)`

[Section titled “null\_value = string (optional)”](#null_value--string-optional)

The string denoting an absent value.

### `auto_expand = bool (optional)`

[Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional)

Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

```tql
from {
  input: "1\t2\t3"
}
output = input.parse_tsv(header=["a","b","c"])
```

```tql
{
  input: "1\t2\t3",
  output: {
    a: 1,
    b: 2,
    c: 3,
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_tsv`](/reference/operators/read_tsv), [`parse_csv`](/reference/functions/parse_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_xsv

Parses a string as delimiter separated values.

```tql
parse_xsv(input:string, field_separator=string, list_separator=string, null_value=string,
          header=list<string>|string,
         [auto_expand=bool, quotes=string, schema=string,
          selector=string, schema_only=bool, raw=bool,
          unflatten_separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

The `parse_xsv` function parses a string as [XSV](https://en.wikipedia.org/wiki/Delimiter-separated_values), a generalization of CSV with a more flexible separator specification.

The following table lists existing XSV configurations:

| Format                                  | Field Separator | List Separator | Null Value |
| --------------------------------------- | :-------------: | :------------: | :--------: |
| [`csv`](/reference/functions/parse_csv) |       `,`       |       `;`      |    empty   |
| [`ssv`](/reference/functions/parse_ssv) |    `<space>`    |       `,`      |     `-`    |
| [`tsv`](/reference/functions/parse_tsv) |       `\t`      |       `,`      |     `-`    |

### `header = list<string>|string`

[Section titled “header = list\<string>|string”](#header--liststringstring)

A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values.

### `field_separator = string`

[Section titled “field\_separator = string”](#field_separator--string)

The string separating different fields.

### `list_separator = string`

[Section titled “list\_separator = string”](#list_separator--string)

The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled.

### `null_value = string`

[Section titled “null\_value = string”](#null_value--string)

The string denoting an absent value.

### `auto_expand = bool (optional)`

[Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional)

Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

```tql
from { input: "1,2,3" }
output = input.parse_xsv(
  field_separator=",",
  list_separator= ";",
  null_value="",
  header=["a","b","c"],
)
```

```tql
{
  input: "1,2,3",
  output: {
    a: 1,
    b: 2,
    c: 3,
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_xsv`](/reference/operators/read_xsv), [`parse_csv`](/reference/functions/parse_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_tsv`](/reference/functions/parse_tsv)

# parse_yaml

Parses a string as a YAML value.

```tql
parse_yaml(input:string, [schema=string, selector=string, schema_only=bool,
           raw=bool, unflatten_separator=string]) -> any
```

## Description

[Section titled “Description”](#description)

The `parse_yaml` function parses a string as a YAML value.

### `input: string`

[Section titled “input: string”](#input-string)

The string to parse.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Parse a YAML map containing a list

[Section titled “Parse a YAML map containing a list”](#parse-a-yaml-map-containing-a-list)

```tql
from { x: r#"yarp:
  - darp
  - larp"# }
x = x.parse_yaml()
```

```tql
{
  x: {
    yarp: [
      "darp",
      "larp",
    ],
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_yaml`](/reference/operators/read_yaml), [`print_yaml`](/reference/functions/print_yaml)

# prepend

Inserts an element at the start of a list.

```tql
prepend(xs:list, x:any) -> list
```

## Description

[Section titled “Description”](#description)

The `prepend` function returns the list `xs` with `x` inserted at the front. The expression `xs.prepend(y)` is equivalent to `[x, ...xs]`.

## Examples

[Section titled “Examples”](#examples)

### Prepend a number to a list

[Section titled “Prepend a number to a list”](#prepend-a-number-to-a-list)

```tql
from {xs: [1, 2]}
xs = xs.prepend(3)
```

```tql
{xs: [3, 1, 2]}
```

## See Also

[Section titled “See Also”](#see-also)

[`add`](/reference/functions/add), [`append`](/reference/functions/append), [`concatenate`](/reference/functions/concatenate), [`remove`](/reference/functions/remove)

# print_cef

Prints records as Common Event Format (CEF) messages

```tql
print_cef(extension:record, cef_version=str, device_vendor=str,
          device_product=str, device_version=str, signature_id=str,
          name=str, severity=str, [flatten_separator=str, null_value=str]) -> str
```

## Description

[Section titled “Description”](#description)

Prints records as the attributes of a CEF message.

### `extension: record`

[Section titled “extension: record”](#extension-record)

The record to print as the extension of the CEF message

### `cef_version = str`

[Section titled “cef\_version = str”](#cef_version--str)

The CEF version in the CEF header.

### `device_vendor = str`

[Section titled “device\_vendor = str”](#device_vendor--str)

The vendor in the CEF header.

### `device_product = str`

[Section titled “device\_product = str”](#device_product--str)

The product name in the CEF header.

### `device_version = str`

[Section titled “device\_version = str”](#device_version--str)

The product version in the CEF header.

### `signature_id = str`

[Section titled “signature\_id = str”](#signature_id--str)

The event (class) ID in the CEF header.

### `name = str`

[Section titled “name = str”](#name--str)

The name field in the CEF header, i.e. the human readable description.

### `severity = str`

[Section titled “severity = str”](#severity--str)

The severity in the CEF header.

### `null_value = str (optional)`

[Section titled “null\_value = str (optional)”](#null_value--str-optional)

A string to use if any of the values in `extension` are `null`.

Defaults to the empty string.

### `flatten_separator = str (optional)`

[Section titled “flatten\_separator = str (optional)”](#flatten_separator--str-optional)

A string used to flatten nested records in `attributes`.

Defaults to `"."`.

## Examples

[Section titled “Examples”](#examples)

### Write a CEF

[Section titled “Write a CEF”](#write-a-cef)

```tql
from {
  extension: {
    a: 42,
    b: "Hello"
  },
  signature_id: "MyCustomSignature",
  severity: "8"
}
r = extension.print_cef(
    cef_version="0",
    device_vendor="Tenzir", device_product="Tenzir Node", device_version="5.5.0",
    signature_id=signature_id, severity=severity,
    name= signature_id + " written by Tenzir"
)
select r
write_lines
```

```txt
CEF:0|Tenzir|Tenzir Node|5.5.0|MyCustomSignature|MyCustomSignature written by Tenzir|8|a=42 b=Hello
```

### Upgrade a nested CEF message in Syslog

[Section titled “Upgrade a nested CEF message in Syslog”](#upgrade-a-nested-cef-message-in-syslog)

```tql
from "my.log" {
  read_syslog // produces the expected shape for `write_syslog`
}
// read the message into a structured form
message = message.parse_cef()
// re-write the message with modifications
message = message.extension.print_cef(
  cef_version=message.cef_version,
  device_vendor=message.device_vendor, device_product=message.device_product,
  device_version=message.device_version, signature_id=signature_id, severity="9"
  name=message.name
)
write_syslog
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_cef`](/reference/functions/parse_cef), [`read_cef`](/reference/operators/read_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# print_csv

Prints a record as a comma-separated string of values.

```tql
print_csv(input:record, [list_separator=str, null_value=str]) -> string
```

## Description

The `print_csv` function prints a record’s values as a comma separated string.

### `input: record`

The record you want to print.

### `list_separator = str (optional)`

The string separating the elements in list fields.

Defaults to `";"`.

### `null_value = str (optional)`

The string denoting an absent value.

Defaults to `""`.

## Examples

[Section titled “Examples”](#examples)

```tql
from {
  x:1,
  y:true,
  z: "String"
}
output = this.print_csv()
```

```tql
{
  x: 1,
  y: true,
  z: "String",
  output: "1,true,String",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_csv`](/reference/functions/parse_csv), [`write_csv`](/reference/operators/write_csv)

# print_json

Transforms a value into a JSON string.

```tql
print_json(input:any, [strip=bool, color=bool, arrays_of_objects=bool,
                       strip_null_fields=bool, strip_nulls_in_lists=bool,
                       strip_empty_records=bool, strip_empty_lists=bool]) -> string
```

## Description

[Section titled “Description”](#description)

Transforms a value into a JSON string.

### `input: any`

The value to print as a JSON value.

### `strip = bool (optional)`

Enables all `strip_*` options.

Defaults to `false`.

### `color = bool (optional)`

Colorize the output.

Defaults to `false`.

### `strip_null_fields = bool (optional)`

Strips all fields with a `null` value from records.

Defaults to `false`.

### `strip_nulls_in_lists = bool (optional)`

Strips all `null` values from lists.

Defaults to `false`.

### `strip_empty_records = bool (optional)`

Strips empty records, including those that only became empty by stripping.

Defaults to `false`.

### `strip_empty_lists = bool (optional)`

Strips empty lists, including those that only became empty by stripping.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Print without null fields

[Section titled “Print without null fields”](#print-without-null-fields)

```tql
from {x: 0}, {x:null}, {x: { x: 0, y: 1 } }, { x: [0,1,2,] }
x = x.print_json(strip_null_fields=true)
```

```tql
{
  x: "0",
}
{
  x: null,
}
{
  x: "{\n  \"x\": 0,\n  \"y\": 1\n}",
}
{
  x: "[\n  0,\n  1,\n  2\n]",
}
```

## See also

[Section titled “See also”](#see-also)

[`write_json`](/reference/operators/write_json), [`print_ndjson`](/reference/functions/print_ndjson), [`parse_json`](/reference/functions/parse_json)

# print_kv

Prints records in a key-value format.

```tql
print_kv( input:record, [field_separator=str, value_separator=str,
                         list_separator=str, flatten_separator=str,
                         null_value=str] ) -> str
```

## Description

[Section titled “Description”](#description)

Prints records in a Key-Value format. Nested data will be flattend, keys or values containing the given separators will be quoted and the special characters `\n`, `\r`, `\` and `"` will be escaped.

### `input: record`

[Section titled “input: record”](#input-record)

The record to print as a string.

### `field_separator = str (optional)`

[Section titled “field\_separator = str (optional)”](#field_separator--str-optional)

A string that shall separate the key-value pairs.

Must not be an empty string.

Defaults to `" "`.

### `value_separator = str (optional)`

[Section titled “value\_separator = str (optional)”](#value_separator--str-optional)

A string that shall separate key and value within key-value pair.

Must not be an empty string.

Defaults to `"="`.

### `list_separator = str (optional)`

[Section titled “list\_separator = str (optional)”](#list_separator--str-optional)

Must not be an empty string.

Defaults to `","`.

### `flatten_separator = str (optional)`

[Section titled “flatten\_separator = str (optional)”](#flatten_separator--str-optional)

A string to join the keys of nested records with. For example, given `flatten="."`

Defaults to `"."`.

### `null_value = str (optional)`

[Section titled “null\_value = str (optional)”](#null_value--str-optional)

A string to represent null values.

Defaults to the empty string.

## Examples

[Section titled “Examples”](#examples)

### Format a record as key-value pair

[Section titled “Format a record as key-value pair”](#format-a-record-as-key-value-pair)

```tql
from {
  input: {key: "value"}
}
output = input.print_kv()
```

```tql
{
  input: {
    key: "value",
  },
  output: "key=value",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_kv`](/reference/functions/parse_kv), [`write_kv`](/reference/operators/read_kv), [`write_kv`](/reference/operators/write_kv)

# print_leef

Prints records as LEEF messages

```tql
print_leef(attributes:record, vendor=str, product_name=str, product_version=str,
           event_class_id=str,
          [delimiter=str, null_value=str, flatten_separator=str]) -> str
```

## Description

[Section titled “Description”](#description)

Prints records as the attributes of a [LEEF](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) message.

### `attributes: record`

[Section titled “attributes: record”](#attributes-record)

The record to print as the attributes of a LEEF message

### `vendor = str`

[Section titled “vendor = str”](#vendor--str)

The vendor in the LEEF header.

### `product_name = str`

[Section titled “product\_name = str”](#product_name--str)

The product name in the LEEF header.

### `product_version = str`

[Section titled “product\_version = str”](#product_version--str)

The product version in the LEEF header.

### `event_class_id = str`

[Section titled “event\_class\_id = str”](#event_class_id--str)

The event (class) ID in the LEEF header.

### `delimiter = str (optional)`

[Section titled “delimiter = str (optional)”](#delimiter--str-optional)

This delimiter will be used to separate the key-value pairs in the attributes. It must be a single character. If the chosen delimiter is not `"\t"`, the message will be a LEEF:2.0 message, otherwise it will be LEEF:1.0.

Defaults to `"\t"`.

### `null_value = str (optional)`

[Section titled “null\_value = str (optional)”](#null_value--str-optional)

A string to use if any of the header values evaluate to null.

Defaults to an empty string.

### `flatten_separator = str (optional)`

[Section titled “flatten\_separator = str (optional)”](#flatten_separator--str-optional)

A string used to flatten nested records in `attributes`.

Defaults to `"."`.

## Examples

[Section titled “Examples”](#examples)

### Write a LEEF:1.0 message

[Section titled “Write a LEEF:1.0 message”](#write-a-leef10-message)

```tql
from {
  attributes: {
    a: 42, b: "Hello"
  }, event_class_id: "critical"
}
r = attributes.print_leef(
    vendor="Tenzir",
    product_name="Tenzir Node",
    product_version="5.5.0",
    event_class_id=event_class_id)
select r
write_lines
```

```txt
LEEF:1.0|Tenzir Node|5.5.0|critical|a=42  b=Hello
```

### Reformat a nested LEEF message

[Section titled “Reformat a nested LEEF message”](#reformat-a-nested-leef-message)

```tql
from "my.log" {
  read_syslog // produces the expected shape for `write_syslog`
}
message = message.parse_leef()
message = message.attributes.print_leef(
  vendor=message.vendor,
  product_name=message.product_name,
  product_version=message.product_version,
  event_class_id=message.event_class_id,
  delimiter="^"
)
write_syslog
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_leef`](/reference/functions/parse_leef), [`read_leef`](/reference/operators/read_leef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# print_ndjson

Transforms a value into a single-line JSON string.

```tql
print_ndjson(input:any, [strip=bool, color=bool, arrays_of_objects=bool,
                         strip_null_fields=bool, strip_nulls_in_lists=bool,
                         strip_empty_records=bool, strip_empty_lists=bool]) -> string
```

## Description

[Section titled “Description”](#description)

Transforms a value into a single-line JSON string.

### `input: any`

The value to print as a JSON value.

### `strip = bool (optional)`

Enables all `strip_*` options.

Defaults to `false`.

### `color = bool (optional)`

Colorize the output.

Defaults to `false`.

### `strip_null_fields = bool (optional)`

Strips all fields with a `null` value from records.

Defaults to `false`.

### `strip_nulls_in_lists = bool (optional)`

Strips all `null` values from lists.

Defaults to `false`.

### `strip_empty_records = bool (optional)`

Strips empty records, including those that only became empty by stripping.

Defaults to `false`.

### `strip_empty_lists = bool (optional)`

Strips empty lists, including those that only became empty by stripping.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Print without null fields

[Section titled “Print without null fields”](#print-without-null-fields)

```tql
from {x: 0},
     {x:null},
     {x: {x: 0, y: 1}},
     {x: [0,1,2,]}
x = x.print_ndjson(strip_null_fields=true)
```

```tql
{
  x: "0",
}
{
  x: null,
}
{
  x: "{\"x\": 0, \"y\": 1}",
}
{
  x: "[0, 1, 2]",
}
```

## See also

[Section titled “See also”](#see-also)

[`write_ndjson`](/reference/operators/write_ndjson), [`print_json`](/reference/functions/print_json), [`parse_json`](/reference/functions/parse_json)

# print_ssv

Prints a record as a space-separated string of values.

```tql
print_ssv(input:record, [list_separator=str, null_value=str]) -> string
```

## Description

The `print_ssv` function prints a record’s values as a space separated string.

### `input: record`

The record you want to print.

### `list_separator = str (optional)`

The string separating the elements in list fields.

Defaults to `","`.

### `null_value = str (optional)`

The string denoting an absent value.

Defaults to `"-"`.

## Examples

[Section titled “Examples”](#examples)

### Print a record as space

[Section titled “Print a record as space”](#print-a-record-as-space)

```tql
from {x:1, y:true, z: "String"}
output = this.print_ssv()
```

```tql
{
  x: 1,
  y: true,
  z: "String",
  output: "1 true String",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_ssv`](/reference/functions/parse_ssv), [`write_ssv`](/reference/operators/write_ssv)

# print_tsv

Prints a record as a tab-separated string of values.

```tql
print_tsv(input:record, [list_separator=str, null_value=str]) -> string
```

## Description

The `print_tsv` function prints a record’s values as a tab separated string.

### `input: record`

The record you want to print.

### `list_separator = str (optional)`

The string separating the elements in list fields.

Defaults to `","`.

### `null_value = str (optional)`

The string denoting an absent value.

Defaults to `"-"`.

## Examples

[Section titled “Examples”](#examples)

```tql
from {
  x:1,
  y:true,
  z: "String"
}
output = this.print_tsv()
```

```tql
{
  x: 1,
  y: true,
  z: "String",
  output: "1\ttrue\tString",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_tsv`](/reference/functions/parse_tsv), [`write_tsv`](/reference/operators/write_tsv)

# print_xsv

Prints a record as a delimited sequence of values.

```tql
print_xsv(input:record, field_separator=str, list_separator=str, null_value=str) -> string
```

## Description

[Section titled “Description”](#description)

The `parse_xsv` function prints a record’s values as delimiter separated string.

The following table lists existing XSV configurations:

| Format                                  | Field Separator | List Separator | Null Value |
| --------------------------------------- | :-------------: | :------------: | :--------: |
| [`csv`](/reference/functions/print_csv) |       `,`       |       `;`      |    empty   |
| [`ssv`](/reference/functions/print_ssv) |    `<space>`    |       `,`      |     `-`    |
| [`tsv`](/reference/functions/print_tsv) |       `\t`      |       `,`      |     `-`    |

### `field_separator = str`

[Section titled “field\_separator = str”](#field_separator--str)

The string separating different fields.

### `list_separator = str`

[Section titled “list\_separator = str”](#list_separator--str)

The string separating different elements in a list within a single field.

### `null_value = str`

[Section titled “null\_value = str”](#null_value--str)

The string denoting an absent value.

## Examples

[Section titled “Examples”](#examples)

```tql
from {
  x:1,
  y:true,
  z: "String",
}
output = this.print_xsv(
  field_separator=",",
  list_separator=";",
  null_value="null")
```

```tql
{
  x: 1,
  y: true,
  z: "String",
  output: "1,true,String",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_xsv`](/reference/functions/parse_xsv), [`write_xsv`](/reference/operators/write_xsv)

# print_yaml

Prints a value as a YAML document.

```tql
print_yaml( input:any, [include_document_markers=bool] )
```

## Description

[Section titled “Description”](#description)

### `input:any`

[Section titled “input:any”](#inputany)

The value to print as YAML.

### `include_document_markers = bool (optional)`

[Section titled “include\_document\_markers = bool (optional)”](#include_document_markers--bool-optional)

Includes the “start of document” and “end of document” markers in the result.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

```tql
from {x: { x: 0, y: 1 } }, { x: [0,1,2,] }
x = x.print_yaml()
```

```tql
{
  x: "x: 0\ny: 1",
}
{
  x: "- 0\n- 1\n- 2",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`write_yaml`](/reference/operators/write_yaml), [`parse_yaml`](/reference/functions/parse_yaml)

# quantile

Computes the specified quantile of all grouped values.

```tql
quantile(xs:list, q=float) -> float
```

## Description

[Section titled “Description”](#description)

The `quantile` function returns the quantile of all numeric values in `xs`, specified by the argument `q`, which should be a value between 0 and 1.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

### `q: float`

[Section titled “q: float”](#q-float)

The quantile to compute, where `q=0.5` represents the median.

## Examples

[Section titled “Examples”](#examples)

### Compute the 0.5 quantile (median) of values

[Section titled “Compute the 0.5 quantile (median) of values”](#compute-the-05-quantile-median-of-values)

```tql
from {x: 1}, {x: 2}, {x: 3}, {x: 4}
summarize median_value=quantile(x, q=0.5)
```

```tql
{median_value: 2.5}
```

## See Also

[Section titled “See Also”](#see-also)

[`median`](/reference/functions/median), [`mean`](/reference/functions/mean)

# random

Generates a random number in *\[0,1]*.

```tql
random() -> float
```

## Description

[Section titled “Description”](#description)

The `random` function generates a random number by drawing from a [uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) in the interval *\[0,1]*.

## Examples

[Section titled “Examples”](#examples)

### Generate a random number

[Section titled “Generate a random number”](#generate-a-random-number)

```tql
from {x: random()}
```

```tql
{x: 0.19634716885782455}
```

## See Also

[Section titled “See Also”](#see-also)

[`uuid`](/reference/functions/uuid)

# remove

Removes all occurrences of an element from a list.

```tql
remove(xs:list, x:any) -> list
```

## Description

[Section titled “Description”](#description)

The `remove` function returns the list `xs` with all occurrences of `x` removed. If `x` is not present in the list, the original list is returned unchanged.

### `xs: list`

[Section titled “xs: list”](#xs-list)

A list to remove elements from.

### `x: any`

[Section titled “x: any”](#x-any)

The value to remove from the list.

## Examples

[Section titled “Examples”](#examples)

### Remove an element from a list

[Section titled “Remove an element from a list”](#remove-an-element-from-a-list)

```tql
from {xs: [1, 2, 3, 2, 4]}
xs = xs.remove(2)
```

```tql
{xs: [1, 3, 4]}
```

### Remove a non-existent element

[Section titled “Remove a non-existent element”](#remove-a-non-existent-element)

```tql
from {xs: [1, 2, 3]}
xs = xs.remove(5)
```

```tql
{xs: [1, 2, 3]}
```

### Remove from a list with strings

[Section titled “Remove from a list with strings”](#remove-from-a-list-with-strings)

```tql
from {xs: ["apple", "banana", "apple", "orange"]}
xs = xs.remove("apple")
```

```tql
{xs: ["banana", "orange"]}
```

## See Also

[Section titled “See Also”](#see-also)

[`add`](/reference/functions/add), [`append`](/reference/functions/append), [`prepend`](/reference/functions/prepend) [`distinct`](/reference/functions/distinct)

# replace

Replaces characters within a string.

```tql
replace(x:string, pattern:string, replacement:string, [max=int]) -> string
```

## Description

[Section titled “Description”](#description)

The `replace` function returns a new string where occurrences of `pattern` in `x` are replaced with `replacement`, up to `max` times. If `max` is omitted, all occurrences are replaced.

### `x: string`

[Section titled “x: string”](#x-string)

The subject to replace the action on.

### `pattern: string`

[Section titled “pattern: string”](#pattern-string)

The pattern to replace in `x`.

### `replacement: string`

[Section titled “replacement: string”](#replacement-string)

The replacement value for `pattern`.

### `max = int (optional)`

[Section titled “max = int (optional)”](#max--int-optional)

The maximum number of replacements to perform.

If the option is not set, all occurrences are replaced.

## Examples

[Section titled “Examples”](#examples)

### Replace all occurrences of a character

[Section titled “Replace all occurrences of a character”](#replace-all-occurrences-of-a-character)

```tql
from {x: "hello".replace("l", "r")}
```

```tql
{x: "herro"}
```

### Replace a limited number of occurrences

[Section titled “Replace a limited number of occurrences”](#replace-a-limited-number-of-occurrences)

```tql
from {x: "hello".replace("l", "r", max=1)}
```

```tql
{x: "herlo"}
```

## See Also

[Section titled “See Also”](#see-also)

[`replace_regex`](/reference/functions/replace_regex), [`replace`](/reference/operators/replace)

# replace_regex

Replaces characters within a string based on a regular expression.

```tql
replace_regex(x:string, pattern:string, replacement:string, [max=int]) -> string
```

## Description

[Section titled “Description”](#description)

The `replace_regex` function returns a new string where substrings in `x` that match `pattern` are replaced with `replacement`, up to `max` times. If `max` is omitted, all matches are replaced.

### `x: string`

[Section titled “x: string”](#x-string)

The subject to replace the action on.

### `pattern: string`

[Section titled “pattern: string”](#pattern-string)

The pattern (as regular expression) to replace in `x`.

### `replacement: string`

[Section titled “replacement: string”](#replacement-string)

The replacement value for `pattern`.

### `max = int (optional)`

[Section titled “max = int (optional)”](#max--int-optional)

The maximum number of replacements to perform.

If the option is not set, all occurrences are replaced.

## Examples

[Section titled “Examples”](#examples)

### Replace all matches of a regular expression

[Section titled “Replace all matches of a regular expression”](#replace-all-matches-of-a-regular-expression)

```tql
from {x: replace_regex("hello", "l+", "y")}
```

```tql
{x: "heyo"}
```

### Replace a limited number of matches

[Section titled “Replace a limited number of matches”](#replace-a-limited-number-of-matches)

```tql
from {x: replace_regex("hellolo", "l+", "y", max=1)}
```

```tql
{x: "heyolo"}
```

## See Also

[Section titled “See Also”](#see-also)

[`replace`](/reference/functions/replace)

# reverse

Reverses the characters of a string.

```tql
reverse(x:string) -> string
```

## Description

[Section titled “Description”](#description)

The `reverse` function returns a new string with the characters of `x` in reverse order.

This function operates on Unicode codepoints, not grapheme clusters. Hence, it will not correctly reverse grapheme clusters composed of multiple codepoints.

## Examples

[Section titled “Examples”](#examples)

### Reverse a string

[Section titled “Reverse a string”](#reverse-a-string)

```tql
from {x: reverse("hello")}
```

```tql
{x: "olleh"}
```

# round

Rounds a number or a time/duration with a specified unit.

```tql
round(x:number)
round(x:time, unit:duration)
round(x:duration, unit:duration)
```

## Description

[Section titled “Description”](#description)

The `round` function rounds a number `x` to an integer.

For time and duration values, use the second `unit` argument to define the rounding unit.

## Examples

[Section titled “Examples”](#examples)

### Round integers

[Section titled “Round integers”](#round-integers)

```tql
from {
  x: round(3.4),
  y: round(3.5),
  z: round(-3.4),
}
```

```tql
{
  x: 3,
  y: 4,
  z: -3,
}
```

### Round time and duration values

[Section titled “Round time and duration values”](#round-time-and-duration-values)

```tql
from {
  x: round(2024-08-23, 1y),
  y: round(42m, 1h)
}
```

```tql
{
  x: 2025-01-01,
  y: 1h,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`ceil`](/reference/functions/ceil), [`floor`](/reference/functions/floor)

# second

Extracts the second component from a timestamp with subsecond precision.

```tql
second(x: time) -> float
```

## Description

[Section titled “Description”](#description)

The `second` function extracts the second component from a timestamp as a floating-point number (0-59.999…) that includes subsecond precision.

### `x: time`

[Section titled “x: time”](#x-time)

The timestamp from which to extract the second.

## Examples

[Section titled “Examples”](#examples)

### Extract the second from a timestamp

[Section titled “Extract the second from a timestamp”](#extract-the-second-from-a-timestamp)

```tql
from {
  ts: 2024-06-15T14:30:45.123456,
}
second = ts.second()
```

```tql
{
  ts: 2024-06-15T14:30:45.123456,
  second: 45.123456,
}
```

### Extract only the full second component without subsecond precision

[Section titled “Extract only the full second component without subsecond precision”](#extract-only-the-full-second-component-without-subsecond-precision)

```tql
from {
  ts: 2024-06-15T14:30:45.123456,
}
full_second = ts.second().floor()
```

```tql
{
  ts: 2024-06-15T14:30:45.123456,
  full_second: 45,
}
```

## See also

[Section titled “See also”](#see-also)

[`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute)

# seconds

Converts a number to equivalent seconds.

```tql
seconds(x:number) -> duration
```

## Description

This function returns seconds equivalent to a number, i.e., `number * 1s`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# secret

Use the value of a secret.

```tql
secret(name:string) -> secret
```

## Description

[Section titled “Description”](#description)

An operator accepting a secret will first try and lookup the value in the environment or configuration of the Tenzir Node. A `tenzir` client process can use secrets only if it has a Tenzir Node to connect to.

If the secret is not found in the node, a request is made to the Tenzir Platform. Should the platform also not be able to find the secret, an error is raised.

See the [explanation page for secrets](/explanations/secrets) for more details.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the secret to use. This must be a constant.

## Legacy Model

[Section titled “Legacy Model”](#legacy-model)

The configuration option `tenzir.legacy-secret-model` changes the behavior of the `secret` function to return a `string` instead of a `secret`.

The legacy model only allows using secrets from the Tenzir Node’s configuration. No secrets from the Tenzir Platform’s secret store will be available.

We do not recommend enabling this option.

## Examples

[Section titled “Examples”](#examples)

### Using secrets in an operator

[Section titled “Using secrets in an operator”](#using-secrets-in-an-operator)

```tql
load_tcp "127.0.0.1:4000" {
  read_ndjson
}
to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token")
```

### Secrets are not rendered in output

[Section titled “Secrets are not rendered in output”](#secrets-are-not-rendered-in-output)

```tql
from {x: secret("geheim")}
```

```tql
{x: "***" }
```

## See also

[Section titled “See also”](#see-also)

[`config`](/reference/functions/config), [`env`](/reference/functions/env)

# shift_left

Performs a bit-wise left shift.

```tql
shift_left(lhs:number, rhs:number) -> number
```

## Description

[Section titled “Description”](#description)

The `shift_left` function performs a bit-wise left shift of `lhs` by `rhs` bit positions. Each left shift multiplies the number by 2.

### `lhs: number`

[Section titled “lhs: number”](#lhs-number)

The number to be shifted.

### `rhs: number`

[Section titled “rhs: number”](#rhs-number)

The number of bit positions to shift to the left.

## Examples

[Section titled “Examples”](#examples)

### Shift bits to the left

[Section titled “Shift bits to the left”](#shift-bits-to-the-left)

```tql
from {x: shift_left(5, 2)}
```

```tql
{x: 20}
```

## See Also

[Section titled “See Also”](#see-also)

[`shift_right`](/reference/functions/shift_right), [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not)

# shift_right

Performs a bit-wise right shift.

```tql
shift_right(lhs:number, rhs:number) -> number
```

## Description

[Section titled “Description”](#description)

The `shift_right` function performs a bit-wise right shift of `lhs` by `rhs` bit positions. Each right shift divides the number by 2, truncating any fractional part.

### `lhs: number`

[Section titled “lhs: number”](#lhs-number)

The number to be shifted.

### `rhs: number`

[Section titled “rhs: number”](#rhs-number)

The number of bit positions to shift to the right.

## Examples

[Section titled “Examples”](#examples)

### Shift bits to the right

[Section titled “Shift bits to the right”](#shift-bits-to-the-right)

```tql
from {x: shift_right(20, 2)}
```

```tql
{x: 5}
```

## See Also

[Section titled “See Also”](#see-also)

[`shift_left`](/reference/functions/shift_left), [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not)

# since_epoch

Interprets a time value as duration since the Unix epoch.

```tql
since_epoch(x:time) -> duration
```

## Description

[Section titled “Description”](#description)

The `since_epoch` function turns a time value into a duration since the [Unix epoch](https://en.wikipedia.org/wiki/Unix_time), i.e., since 00:00:00 UTC on January 1970.

## Examples

[Section titled “Examples”](#examples)

### Retrive the Unix time for a given date

[Section titled “Retrive the Unix time for a given date”](#retrive-the-unix-time-for-a-given-date)

```tql
from { x: since_epoch(2021-02-24) }
```

```tql
{x: 18682.0d}
```

## See Also

[Section titled “See Also”](#see-also)

[`from_epoch`](/reference/functions/from_epoch), [`now`](/reference/functions/now)

# slice

Slices a string with offsets and strides.

```tql
slice(x:string, [begin=int, end=int, stride=int])
```

## Description

[Section titled “Description”](#description)

The `slice` function takes a string as input and selects parts from it.

### `x: string`

[Section titled “x: string”](#x-string)

The string to slice.

### `begin = int (optional)`

[Section titled “begin = int (optional)”](#begin--int-optional)

The offset to start slice from.

If negative, offset is calculated from the end of string.

Defaults to `0`.

### `end = int (optional)`

[Section titled “end = int (optional)”](#end--int-optional)

The offset to end the slice at.

If negative, offset is calculated from the end of string.

If unspecified, ends at the `input`’s end.

### `stride = int (optional)`

[Section titled “stride = int (optional)”](#stride--int-optional)

The difference between the current character to take and the next character to take.

If negative, characters are chosen in reverse.

Defaults to `1`.

## Examples

[Section titled “Examples”](#examples)

### Get the first 3 characters of a string

[Section titled “Get the first 3 characters of a string”](#get-the-first-3-characters-of-a-string)

```tql
from {x: "123456789"}
x = x.slice(end=3)
```

```tql
{x: "123"}
```

### Get the 1st, 3rd, and 5th characters

[Section titled “Get the 1st, 3rd, and 5th characters”](#get-the-1st-3rd-and-5th-characters)

```tql
from {x: "1234567890"}
x = x.slice(stride=2, end=6)
```

```tql
{x: "135"}
```

### Select a substring from the 2nd character up to the 8th character

[Section titled “Select a substring from the 2nd character up to the 8th character”](#select-a-substring-from-the-2nd-character-up-to-the-8th-character)

```tql
from {x: "1234567890"}
x = x.slice(begin=1, end=8)
```

```tql
{x: "2345678"}
```

# sort

Sorts lists and record fields.

```tql
sort(xs:list|record) -> list|record
```

## Description

[Section titled “Description”](#description)

The `sort` function takes either a list or record as input, ordering lists by value and records by their field name.

### `xs: list|record`

[Section titled “xs: list|record”](#xs-listrecord)

The list or record to sort.

## Examples

[Section titled “Examples”](#examples)

### Sort values in a list

[Section titled “Sort values in a list”](#sort-values-in-a-list)

```tql
from {xs: [1, 3, 2]}
xs = xs.sort()
```

```tql
{xs: [1, 2, 3]}
```

### Sort a record by its field names

[Section titled “Sort a record by its field names”](#sort-a-record-by-its-field-names)

```tql
from {a: 1, c: 3, b: {y: true, x: false}}
this = this.sort()
```

```tql
{a: 1, b: {y: true, x: false}, c: 3}
```

Note that nested records are not automatically sorted. Use `b = b.sort()` to sort it manually.

# split

Splits a string into substrings.

```tql
split(x:string, pattern:string, [max:int], [reverse:bool]) -> list
```

## Description

[Section titled “Description”](#description)

The `split` function splits the input string `x` into a list of substrings using the specified `pattern`. Optional arguments allow limiting the number of splits (`max`) and reversing the splitting direction (`reverse`).

### `x: string`

[Section titled “x: string”](#x-string)

The string to split.

### `pattern: string`

[Section titled “pattern: string”](#pattern-string)

The delimiter or pattern used for splitting.

### `max: int (optional)`

[Section titled “max: int (optional)”](#max-int-optional)

The maximum number of splits to perform.

Defaults to `0`, meaning no limit.

### `reverse: bool (optional)`

[Section titled “reverse: bool (optional)”](#reverse-bool-optional)

If `true`, splits from the end of the string.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Split a string by a delimiter

[Section titled “Split a string by a delimiter”](#split-a-string-by-a-delimiter)

```tql
from {xs: split("a,b,c", ",")}
```

```tql
{xs: ["a", "b", "c"]}
```

### Limit the number of splits

[Section titled “Limit the number of splits”](#limit-the-number-of-splits)

```tql
from {xs: split("a-b-c", "-", max=1)}
```

```tql
{xs: ["a", "b-c"]}
```

## See Also

[Section titled “See Also”](#see-also)

[`split_regex`](/reference/functions/split_regex), [`join`](/reference/functions/join)

# split_regex

Splits a string into substrings with a regex.

```tql
split_regex(x:string, pattern:string, [max:int], [reverse:bool]) -> list
```

## Description

[Section titled “Description”](#description)

The `split_regex` function splits the input string `x` into a list of substrings using the specified regular expression `pattern`. Optional arguments allow limiting the number of splits (`max`) and reversing the splitting direction (`reverse`).

### `x: string`

[Section titled “x: string”](#x-string)

The string to split.

### `pattern: string`

[Section titled “pattern: string”](#pattern-string)

The regular expression used for splitting.

### `max: int (optional)`

[Section titled “max: int (optional)”](#max-int-optional)

The maximum number of splits to perform.

Defaults to `0`, meaning no limit.

### `reverse: bool (optional)`

[Section titled “reverse: bool (optional)”](#reverse-bool-optional)

If `true`, splits from the end of the string.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Split a string using a regex pattern

[Section titled “Split a string using a regex pattern”](#split-a-string-using-a-regex-pattern)

```tql
from {xs: split_regex("a1b2c", r"\d")}
```

```tql
{xs: ["a", "b", "c", ""]}
```

### Limit the number of splits

[Section titled “Limit the number of splits”](#limit-the-number-of-splits)

```tql
from {xs: split_regex("a1b2c3", r"\d", max=1)}
```

```tql
{xs: ["a", "b2c3"]}
```

## See Also

[Section titled “See Also”](#see-also)

[`split`](/reference/functions/split), [`join`](/reference/functions/join)

# sqrt

Computes the square root of a number.

```tql
sqrt(x:number) -> float
```

## Description

[Section titled “Description”](#description)

The `sqrt` function computes the [square root](https://en.wikipedia.org/wiki/Square_root) of any non-negative number `x`.

## Examples

[Section titled “Examples”](#examples)

### Compute the square root of an integer

[Section titled “Compute the square root of an integer”](#compute-the-square-root-of-an-integer)

```tql
from {x: sqrt(49)}
```

```tql
{x: 7.0}
```

### Fail to compute the square root of a negative number

[Section titled “Fail to compute the square root of a negative number”](#fail-to-compute-the-square-root-of-a-negative-number)

```tql
from {x: sqrt(-1)}
```

```tql
{x: null}
```

# starts_with

Checks if a string starts with a specified substring.

```tql
starts_with(x:string, prefix:string) -> bool
```

## Description

[Section titled “Description”](#description)

The `starts_with` function returns `true` if `x` starts with `prefix` and `false` otherwise.

## Examples

[Section titled “Examples”](#examples)

### Check if a string starts with a substring

[Section titled “Check if a string starts with a substring”](#check-if-a-string-starts-with-a-substring)

```tql
from {x: "hello".starts_with("he")}
```

```tql
{x: true}
```

## See Also

[Section titled “See Also”](#see-also)

[`ends_with`](/reference/functions/ends_with)

# stddev

Computes the standard deviation of all grouped values.

```tql
stddev(xs:list) -> float
```

## Description

[Section titled “Description”](#description)

The `stddev` function returns the standard deviation of all numeric values in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

## Examples

[Section titled “Examples”](#examples)

### Compute the standard deviation of values

[Section titled “Compute the standard deviation of values”](#compute-the-standard-deviation-of-values)

```tql
from {x: 1}, {x: 2}, {x: 3}
summarize stddev_value=stddev(x)
```

```tql
{stddev_value: 0.816}
```

## See Also

[Section titled “See Also”](#see-also)

[`variance`](/reference/functions/variance), [`mean`](/reference/functions/mean)

# string

Casts an expression to a string.

```tql
string(x:any) -> string
```

## Description

[Section titled “Description”](#description)

The `string` function casts the given value `x` to a string.

## Examples

[Section titled “Examples”](#examples)

### Cast an IP address to a string

[Section titled “Cast an IP address to a string”](#cast-an-ip-address-to-a-string)

```tql
from {x: string(1.2.3.4)}
```

```tql
{x: "1.2.3.4"}
```

## See Also

[Section titled “See Also”](#see-also)

[`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [int](/reference/functions/int)

# subnet

Casts an expression to a subnet value.

```tql
subnet(x:string) -> subnet
```

## Description

[Section titled “Description”](#description)

The `subnet` function casts an expression to a subnet.

### `x: string`

[Section titled “x: string”](#x-string)

The string expression to cast.

## Examples

[Section titled “Examples”](#examples)

### Cast a string to a subnet

[Section titled “Cast a string to a subnet”](#cast-a-string-to-a-subnet)

```tql
from {x: subnet("1.2.3.4/16")}
```

```tql
{x: 1.2.0.0/16}
```

## See Also

[Section titled “See Also”](#see-also)

[`int`](/reference/functions/int), [`ip`](/reference/functions/ip), [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [string](/reference/functions/string)

# sum

Computes the sum of all values.

```tql
sum(xs:list) -> int
```

## Description

[Section titled “Description”](#description)

The `sum` function computes the total of all number values.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to aggregate.

## Examples

[Section titled “Examples”](#examples)

### Compute a sum over a group of events

[Section titled “Compute a sum over a group of events”](#compute-a-sum-over-a-group-of-events)

```tql
from {x: 1}, {x: 2}, {x: 3}
summarize n=sum(x)
```

```tql
{n: 6}
```

## See Also

[Section titled “See Also”](#see-also)

[`collect`](/reference/functions/collect), [`max`](/reference/functions/max), [`mean`](/reference/functions/mean), [`min`](/reference/functions/min)

# time

Casts an expression to a time value.

```tql
time(x:any) -> time
```

## Description

[Section titled “Description”](#description)

The `time` function casts the given string or number `x` to a time value.

## Examples

[Section titled “Examples”](#examples)

### Cast a string to a time value

[Section titled “Cast a string to a time value”](#cast-a-string-to-a-time-value)

```tql
from {x: time("2020-03-15")}
```

```tql
{x: 2020-03-15T00:00:00.000000}
```

## See Also

[Section titled “See Also”](#see-also)

[`ip`](/reference/functions/ip), [`string`](/reference/functions/string), [`subnet`](/reference/functions/subnet), [`uint`](/reference/functions/uint), [duration](/reference/functions/duration), [float](/reference/functions/float), [int](/reference/functions/int)

# to_lower

Converts a string to lowercase.

```tql
to_lower(x:string) -> string
```

## Description

[Section titled “Description”](#description)

The `to_lower` function converts all characters in `x` to lowercase.

## Examples

[Section titled “Examples”](#examples)

### Convert a string to lowercase

[Section titled “Convert a string to lowercase”](#convert-a-string-to-lowercase)

```tql
from {x: "HELLO".to_lower()}
```

```tql
{x: "hello"}
```

## See Also

[Section titled “See Also”](#see-also)

[`capitalize`](/reference/functions/capitalize), [`is_lower`](/reference/functions/is_lower), [`to_title`](/reference/functions/to_title), [`to_upper`](/reference/functions/to_upper)

# to_title

Converts a string to title case.

```tql
to_title(x:string) -> string
```

## Description

[Section titled “Description”](#description)

The `to_title` function converts all words in `x` to title case.

## Examples

[Section titled “Examples”](#examples)

### Convert a string to title case

[Section titled “Convert a string to title case”](#convert-a-string-to-title-case)

```tql
from {x: "hello world".to_title()}
```

```tql
{x: "Hello World"}
```

## See Also

[Section titled “See Also”](#see-also)

[`capitalize`](/reference/functions/capitalize), [`is_title`](/reference/functions/is_title), [`to_lower`](/reference/functions/to_lower), [`to_upper`](/reference/functions/to_upper)

# to_upper

Converts a string to uppercase.

```tql
to_upper(x:string) -> string
```

## Description

[Section titled “Description”](#description)

The `to_upper` function converts all characters in `x` to uppercase.

## Examples

[Section titled “Examples”](#examples)

### Convert a string to uppercase

[Section titled “Convert a string to uppercase”](#convert-a-string-to-uppercase)

```tql
from {x: "hello".to_upper()}
```

```tql
{x: "HELLO"}
```

## See Also

[Section titled “See Also”](#see-also)

[`capitalize`](/reference/functions/capitalize), [`is_upper`](/reference/functions/is_upper), [`to_lower`](/reference/functions/to_lower), [`to_title`](/reference/functions/to_title)

# trim

Trims whitespace or specified characters from both ends of a string.

```tql
trim(x:string, [chars:string]) -> string
```

## Description

[Section titled “Description”](#description)

The `trim` function removes characters from both ends of `x`.

When called with one argument, it removes leading and trailing whitespace. When called with two arguments, it removes any characters found in `chars` from both ends of the string.

### `x: string`

[Section titled “x: string”](#x-string)

The string to trim.

### `chars: string (optional)`

[Section titled “chars: string (optional)”](#chars-string-optional)

A string where each character represents a character to remove. Any character found in this string will be trimmed from both ends.

Defaults to whitespace characters.

## Examples

[Section titled “Examples”](#examples)

### Trim whitespace from both ends

[Section titled “Trim whitespace from both ends”](#trim-whitespace-from-both-ends)

```tql
from {x: " hello ".trim()}
```

```tql
{x: "hello"}
```

### Trim specific characters

[Section titled “Trim specific characters”](#trim-specific-characters)

```tql
from {x: "/path/to/file/".trim("/")}
```

```tql
{x: "path/to/file"}
```

### Trim multiple characters

[Section titled “Trim multiple characters”](#trim-multiple-characters)

```tql
from {x: "--hello--world--".trim("-")}
```

```tql
{x: "hello--world"}
```

## See Also

[Section titled “See Also”](#see-also)

[`trim_start`](/reference/functions/trim_start), [`trim_end`](/reference/functions/trim_end)

# trim_end

Trims whitespace or specified characters from the end of a string.

```tql
trim_end(x:string, [chars:string]) -> string
```

## Description

[Section titled “Description”](#description)

The `trim_end` function removes characters from the end of `x`.

When called with one argument, it removes trailing whitespace. When called with two arguments, it removes any characters found in `chars` from the end of the string.

### `x: string`

[Section titled “x: string”](#x-string)

The string to trim.

### `chars: string (optional)`

[Section titled “chars: string (optional)”](#chars-string-optional)

A string where each character represents a character to remove. Any character found in this string will be trimmed from the end.

Defaults to whitespace characters.

## Examples

[Section titled “Examples”](#examples)

### Trim whitespace from the end

[Section titled “Trim whitespace from the end”](#trim-whitespace-from-the-end)

```tql
from {x: "hello ".trim_end()}
```

```tql
{x: "hello"}
```

### Trim specific characters

[Section titled “Trim specific characters”](#trim-specific-characters)

```tql
from {x: "/path/to/file/".trim_end("/")}
```

```tql
{x: "/path/to/file"}
```

### Trim multiple characters

[Section titled “Trim multiple characters”](#trim-multiple-characters)

```tql
from {x: "hello/-/".trim_end("/-")}
```

```tql
{x: "hello"}
```

## See Also

[Section titled “See Also”](#see-also)

[`trim`](/reference/functions/trim), [`trim_start`](/reference/functions/trim_start)

# trim_start

Trims whitespace or specified characters from the start of a string.

```tql
trim_start(x:string, [chars:string]) -> string
```

## Description

[Section titled “Description”](#description)

The `trim_start` function removes characters from the beginning of `x`.

When called with one argument, it removes leading whitespace. When called with two arguments, it removes any characters found in `chars` from the start of the string.

### `x: string`

[Section titled “x: string”](#x-string)

The string to trim.

### `chars: string (optional)`

[Section titled “chars: string (optional)”](#chars-string-optional)

A string where each character represents a character to remove. Any character found in this string will be trimmed from the start.

Defaults to whitespace characters.

## Examples

[Section titled “Examples”](#examples)

### Trim whitespace from the start

[Section titled “Trim whitespace from the start”](#trim-whitespace-from-the-start)

```tql
from {x: " hello".trim_start()}
```

```tql
{x: "hello"}
```

### Trim specific characters

[Section titled “Trim specific characters”](#trim-specific-characters)

```tql
from {x: "/path/to/file".trim_start("/")}
```

```tql
{x: "path/to/file"}
```

### Trim multiple characters

[Section titled “Trim multiple characters”](#trim-multiple-characters)

```tql
from {x: "/-/hello".trim_start("/-")}
```

```tql
{x: "hello"}
```

## See Also

[Section titled “See Also”](#see-also)

[`trim`](/reference/functions/trim), [`trim_end`](/reference/functions/trim_end)

# type_id

Retrieves the type id of an expression.

```tql
type_id(x:any) -> string
```

## Description

[Section titled “Description”](#description)

The `type_id` function returns the type id of the given value `x`.

## Examples

[Section titled “Examples”](#examples)

### Retrieve the type of a numeric expression

[Section titled “Retrieve the type of a numeric expression”](#retrieve-the-type-of-a-numeric-expression)

```tql
from {x: type_id(1 + 3.2)}
```

```tql
{x: "41615fdb30a38aaf"}
```

## See also

[Section titled “See also”](#see-also)

[`type_of`](/reference/functions/type_of)

# type_of

Retrieves the type definition of an expression.

```tql
type_of(x:any) -> record
```

## Description

[Section titled “Description”](#description)

The `type_of` function returns the type definition of the given value `x`.

Subject to change

This function is designed for internal use of the Tenzir Platform and its output format is subject to change without notice.

## Examples

[Section titled “Examples”](#examples)

### Retrieve the type definition of a schema

[Section titled “Retrieve the type definition of a schema”](#retrieve-the-type-definition-of-a-schema)

```tql
from {x: 1, y: "2"}
this = type_of(this)
```

```tql
{
  name: "tenzir.from",
  kind: "record",
  attributes: [],
  state: {
    fields: [
      {
        name: "x",
        type: {
          name: null,
          kind: "int64",
          attributes: [],
          state: null,
        },
      },
      {
        name: "y",
        type: {
          name: null,
          kind: "string",
          attributes: [],
          state: null,
        },
      },
    ],
  },
}
```

## See also

[Section titled “See also”](#see-also)

[`type_id`](/reference/functions/type_id)

# uint

Casts an expression to an unsigned integer.

```tql
uint(x:number|string, base=int) -> uint
```

## Description

[Section titled “Description”](#description)

The `uint` function casts the provided value `x` to an unsigned integer. Non-integer values are truncated.

### `x: number|string`

[Section titled “x: number|string”](#x-numberstring)

The input to convert.

### `base = int`

[Section titled “base = int”](#base--int)

Base (radix) to parse a string as. Can be `10` or `16`.

If `16`, the string inputs may be optionally prefixed by `0x` or `0X`, e.g., `0x134`.

Defaults to `10`.

## Examples

[Section titled “Examples”](#examples)

### Cast a floating-point number to an unsigned integer

[Section titled “Cast a floating-point number to an unsigned integer”](#cast-a-floating-point-number-to-an-unsigned-integer)

```tql
from {x: uint(4.2)}
```

```tql
{x: 4}
```

### Parse a hexadecimal number

[Section titled “Parse a hexadecimal number”](#parse-a-hexadecimal-number)

```tql
from {x: uint("0x42", base=16)}
```

```tql
{x: 66}
```

## See Also

[Section titled “See Also”](#see-also)

[`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [float](/reference/functions/float), [int](/reference/functions/int), [string](/reference/functions/string)

# unflatten

Unflattens nested data.

```tql
unflatten(x:record, [separator=string]) -> record
```

## Description

[Section titled “Description”](#description)

The `unflatten` function creates nested records out of fields whose names include a separator.

Note

`unflatten` uses a heuristic to determine the unflattened schema. Thus, the schema of a record that has been flattened using [`flatten`](/reference/functions/flatten) and unflattened afterwards may not be identical to the schema of the unmodified record.

### `x: record`

[Section titled “x: record”](#x-record)

The record you want to unflatten.

### `separator: string (optional)`

[Section titled “separator: string (optional)”](#separator-string-optional)

The separator to use for splitting field names.

Defaults to `"."`.

## Examples

[Section titled “Examples”](#examples)

### Unflatten fields at the dot character

[Section titled “Unflatten fields at the dot character”](#unflatten-fields-at-the-dot-character)

```tql
// Note the fields in double quotes that are single fields that contain a
// literal "." in their field name, as opposed to nested records.
from {
  src_ip: 147.32.84.165,
  src_port: 1141,
  dest_ip: 147.32.80.9,
  dest_port: 53,
  event_type: "dns",
  "dns.type": "query",
  "dns.id": 553,
  "dns.rrname": "irc.freenode.net",
  "dns.rrtype": "A",
  "dns.tx_id": 0,
  "dns.grouped.A": ["tenzir.com"],
}
this = unflatten(this)
```

```tql
{
  src_ip: 147.32.84.165,
  src_port: 1141,
  dest_ip: 147.32.80.9,
  dest_port: 53,
  event_type: "dns",
  dns: {
    type: "query",
    id: 553,
    rrname: "irc.freenode.net",
    rrtype: "A",
    tx_id: 0,
    grouped: {
      A: [
        "tenzir.com",
      ],
    },
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`flatten`](/reference/functions/flatten)

# uuid

Generates a Universally Unique Identifier (UUID) string.

```tql
uuid([version=string]) -> string
```

## Description

[Section titled “Description”](#description)

The `uuid` function generates a [Universally Unique Identifier (UUID)](https://en.wikipedia.org/wiki/Universally_unique_identifier) string. UUIDs, 128-bit numbers, uniquely identify information in computer systems.

This function generates several UUID versions based on relevant standards like [RFC 4122](https://www.rfc-editor.org/rfc/rfc4122.html) and the newer [RFC 9562](https://www.rfc-editor.org/rfc/rfc9562.html) which defines versions 6 and 7.

### `version = string (optional)`

[Section titled “version = string (optional)”](#version--string-optional)

Specifies the version of the UUID to generate. If you omit this argument, the function uses `"v4"` by default. It supports the following values:

* `"v1"`: Generates a time-based UUID using a timestamp and node ID.
* `"v4"`: Generates a randomly generated UUID using a cryptographically strong random number generator (see RFC 4122). **This is the default.**
* `"v6"`: Generates a time-based UUID, similar to v1 but reordered for better database index locality and lexical sorting (see RFC 9562).
* `"v7"`: Generates a time-based UUID using a Unix timestamp and random bits, designed to be monotonically increasing (suitable for primary keys, see RFC 9562).
* `"nil"`: Generates the special “nil” UUID, which consists entirely of zeros: `00000000-0000-0000-0000-000000000000`.

Defaults to `"v4"`.

## Examples

[Section titled “Examples”](#examples)

### Generate a random default (v4) UUID

[Section titled “Generate a random default (v4) UUID”](#generate-a-random-default-v4-uuid)

```tql
from {guid: uuid()}
```

```tql
{guid: "f47ac10b-58cc-4372-a567-0e02b2c3d479"}
```

### Generate a random version 7 UUID

[Section titled “Generate a random version 7 UUID”](#generate-a-random-version-7-uuid)

```tql
from {guid: uuid(version="v7")}
```

```tql
{guid: "018ecb4f-abc1-7123-8def-0123456789ab"}
```

### Generate the nil UUID

[Section titled “Generate the nil UUID”](#generate-the-nil-uuid)

```tql
from {guid: uuid(version="nil")}
```

```tql
{guid: "00000000-0000-0000-0000-000000000000"}
```

## See Also

[Section titled “See Also”](#see-also)

[`random`](/reference/functions/random)

# value_counts

Returns a list of all grouped values alongside their frequency.

```tql
value_counts(xs:list) -> list
```

## Description

[Section titled “Description”](#description)

The `value_counts` function returns a list of all unique non-null values in `xs` alongside their occurrence count.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

## Examples

[Section titled “Examples”](#examples)

### Get value counts

[Section titled “Get value counts”](#get-value-counts)

```tql
from {x: 1}, {x: 2}, {x: 2}, {x: 3}
summarize counts=value_counts(x)
```

```tql
{counts: [{value: 1, count: 1}, {value: 2, count: 2}, {value: 3, count: 1}]}
```

## See Also

[Section titled “See Also”](#see-also)

[`mode`](/reference/functions/mode), [`distinct`](/reference/functions/distinct)

# variance

Computes the variance of all grouped values.

```tql
variance(xs:list) -> float
```

## Description

[Section titled “Description”](#description)

The `variance` function returns the variance of all numeric values in `xs`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

The values to evaluate.

## Examples

[Section titled “Examples”](#examples)

### Compute the variance of values

[Section titled “Compute the variance of values”](#compute-the-variance-of-values)

```tql
from {x: 1}, {x: 2}, {x: 3}
summarize variance_value=variance(x)
```

```tql
{variance_value: 0.666}
```

## See Also

[Section titled “See Also”](#see-also)

[`stddev`](/reference/functions/stddev), [`mean`](/reference/functions/mean)

# weeks

Converts a number to equivalent weeks.

```tql
weeks(x:number) -> duration
```

## Description

This function returns weeks equivalent to a number, i.e., `number * 1w`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# where

Filters list elements based on a predicate.

```tql
where(xs:list, any->bool) -> list
```

## Description

[Section titled “Description”](#description)

The `where` function keeps only elements of a list for which a predicate evaluates to `true`.

### `xs: list`

[Section titled “xs: list”](#xs-list)

A list of values.

### `function: any -> bool`

[Section titled “function: any -> bool”](#function-any---bool)

A lambda function that is evaluated for each list element.

## Examples

[Section titled “Examples”](#examples)

### Keep only elements greater than 3

[Section titled “Keep only elements greater than 3”](#keep-only-elements-greater-than-3)

```tql
from {
  xs: [1, 2, 3, 4, 5]
}
xs = xs.where(x => x > 3)
```

```tql
{
  xs: [4, 5]
}
```

## See Also

[Section titled “See Also”](#see-also)

[`map`](/reference/functions/map)

# year

Extracts the year component from a timestamp.

```tql
year(x: time) -> int
```

## Description

[Section titled “Description”](#description)

The `year` function extracts the year component from a timestamp as an integer.

### `x: time`

[Section titled “x: time”](#x-time)

The timestamp from which to extract the year.

## Examples

[Section titled “Examples”](#examples)

### Extract the year from a timestamp

[Section titled “Extract the year from a timestamp”](#extract-the-year-from-a-timestamp)

```tql
from {
  ts: 2024-06-15T14:30:45.123456,
}
year = ts.year()
```

```tql
{
  ts: 2024-06-15T14:30:45.123456,
  year: 2024,
}
```

## See also

[Section titled “See also”](#see-also)

[`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# years

Converts a number to equivalent years.

```tql
years(x:number) -> duration
```

## Description

This function returns years equivalent to a number, i.e., `number * 1y`.

### `x: number`

The number to convert.

## See Also

[`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# zip

Combines two lists into a list of pairs.

```tql
zip(xs:list, ys:list) -> list
```

## Description

[Section titled “Description”](#description)

The `zip` function returns a list containing records with two fields `left` and `right`, each containing the respective elements of the input lists.

If both lists are null, `zip` returns null. If one of the lists is null or has a mismatching length, missing values are filled in with nulls, using the longer list’s length, and a warning is emitted.

## Examples

[Section titled “Examples”](#examples)

### Combine two lists

[Section titled “Combine two lists”](#combine-two-lists)

```tql
from {xs: [1, 2], ys: [3, 4]}
select zs = zip(xs, ys)
```

```tql
{
  zs: [
    {left: 1, right: 3},
    {left: 2, right: 4}
  ]
}
```

## See Also

[Section titled “See Also”](#see-also)

[`concatenate`](/reference/functions/concatenate), [`map`](/reference/functions/map)

# MCP Server

The [Tenzir MCP Server](https://github.com/tenzir/mcp) enables AI assistants to interact with Tenzir through the [Model Context Protocol](https://modelcontextprotocol.io) (MCP).

Quick Start

Check our [installation guide](/guides/mcp-setup/install-mcp-server) to get up and running with the MCP Server.

## Tools

[Section titled “Tools”](#tools)

The MCP server provides several *tools* that your AI agent can call.

Work in progress

The MCP server is changing rapidly at the moment. Stay tuned for more content here soon!

# Node Configuration

The below example configuration ships with every Tenzir package. Head over to the [explanation of the configuration](/explanations/configuration) for details on how the various settings work.

tenzir.yaml

```yaml
# This is an example configuration file for Tenzir that shows all available
# options. Options in angle brackets have their default value determined at
# runtime.


# Options that concern Tenzir.
tenzir:
  # The token that is offered when connecting to the Tenzir Platform.
  # It is used to identify the node and assign it to the correct workspace.
  # This setting is ignored in the open-source edition of Tenzir, which does
  # not contain the platform plugin.
  token: tnz_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX


  # The host and port to listen at for node-to-node connections in the form
  # `<host>:<port>`. Host or port may be emitted to use their defaults, which
  # are localhost and 5158, respectively. Set the port to zero to automatically
  # choose a port. Set to false to disable exposing an endpoint.
  endpoint: localhost:5158


  # The timeout for connecting to a Tenzir server. Set to 0 seconds to wait
  # indefinitely.
  connection-timeout: 5m


  # The delay between two connection attempts. Set to 0s to try connecting
  # without retries.
  connection-retry-delay: 3s


  # Configure retention policies.
  retention:
    # How long to keep metrics for. Set to 0s to disable metrics retention
    # entirely.
    # WARNING: A low retention period may negatively impact the usability of
    # pipeline activity in the Tenzir Platform.
    #metrics: 7d


    # How long to keep diagnostics for. Set to 0s to disable diagnostics
    # retention entirely.
    # WARNING: A low retention period may negatively impact the usability of
    # diagnostics in the Tenzir Platform.
    #diagnostics: 30d


  # Configure the behavior of the `cache` operator. The Tenzir Platform uses the
  # cache operator to store and retrieve data efficiently.
  cache:
    # Specifies the default lifetime for the `cache` operator.
    #lifetime: 10min


    # Specifies an upper bound for the total memory usage in bytes across all
    # caches in a node. If the memory usage exceeds this limit, the node will
    # start evicting caches to make room for new data. The node requires a
    # minimum total cache capacity of 64MiB.
    #capacity: 1Gi


  # A certificate file used as the default for operators accepting a `cacert`
  # option. This will default to an appropriate directory for the system. For
  # example:
  #   - /etc/ssl/certs/ca-bundle.crt on RedHat
  #   - /etc/ssl/certs/ca-certificates.crt on Ubuntu
  #cacert:


  # The file system path used for persistent state.
  # Defaults to one of the following paths, selecting the first that is
  # available:
  #   - $STATE_DIRECTORY
  #   - $PWD/tenzir.db
  #state-directory:


  # The file system path used for recoverable state.
  # In a node process, defaults to the first of the following paths that is
  # available:
  #   - $CACHE_DIRECTORY
  #   - $XDG_CACHE_HOME
  #   - $XDG_HOME_DIR/.cache/tenzir (linux) or $XDG_HOME_DIR/Libraries/caches/tenzir (mac)
  #   - $HOME/.cache/tenzir (linux) or $HOME/Libraries/caches/tenzir (mac)
  #   - $TEMPORARY_DIRECTORY/tenzir-cache-<uid>
  # To determine $TEMPORARY_DIRECTORY, the values of TMPDIR, TMP, TEMP, TEMPDIR are
  # checked in that order, and as a last resort "/tmp" is used.
  # In a client process, this setting is ignored and
  # `$TEMPORARY_DIRECTORY/tenzir-client-cache-<uid>` is used as cache directory.
  #cache-directory:


  # The file system path used for log files.
  # Defaults to one of the following paths, selecting the first that is
  # available:
  #   - $LOGS_DIRECTORY/server.log
  #   - <state-directory>/server.log
  #log-file:


  # The file system path used for client log files relative to the current
  # working directory of the client. Note that this is disabled by default.
  # If not specified no log files are written for clients at all.
  client-log-file: "client.log"


  # Format for printing individual log entries to the log-file.
  # For a list of valid format specifiers, see spdlog format specification
  # at https://github.com/gabime/spdlog/wiki/3.-Custom-formatting.
  file-format: "[%Y-%m-%dT%T.%e%z] [%n] [%l] [%s:%#] %v"


  # Configures the minimum severity of messages written to the log file.
  # Possible values: quiet, error, warning, info, verbose, debug, trace.
  # File logging is only available for commands that start a node (e.g.,
  # tenzir-node). The levels above 'verbose' are usually not available in
  # release builds.
  file-verbosity: debug


  # Whether to enable automatic log rotation. If set to false, a new log file
  # will be created when the size of the current log file exceeds 10 MiB.
  disable-log-rotation: false


  # The size limit when a log file should be rotated.
  log-rotation-threshold: 10MiB


  # Maximum number of log messages in the logger queue.
  log-queue-size: 1000000


  # The sink type to use for console logging. Possible values: stderr,
  # syslog, journald. Note that 'journald' can only be selected on linux
  # systems, and only if Tenzir was built with journald support.
  # The journald sink is used as default if Tenzir is started as a systemd
  # service and the service is configured to use the journal for stderr,
  # otherwise the default is the unstructured stderr sink.
  #console-sink: stderr/journald


  # Mode for console log output generation. Automatic renders color only when
  # writing to a tty.
  # Possible values: always, automatic, never. (default automatic)
  console: automatic


  # Format for printing individual log entries to the console. For a list
  # of valid format specifiers, see spdlog format specification at
  # https://github.com/gabime/spdlog/wiki/3.-Custom-formatting.
  console-format: "%^[%T.%e] %v%$"


  # Configures the minimum severity of messages written to the console.
  # For a list of valid log levels, see file-verbosity.
  console-verbosity: info


  # List of directories to look for schema files in ascending order of
  # priority.
  schema-dirs: []


  # Additional directories to load plugins specified using `tenzir.plugins`
  # from.
  plugin-dirs: []


  # List of paths that contain statically configured packages.
  # This setting is ignored unless the package manager plugin is enabled.
  package-dirs: []


  # The plugins to load at startup. For relative paths, Tenzir tries to find
  # the files in the specified `tenzir.plugin-dirs`. The special values
  # 'bundled' and 'all' enable autoloading of bundled and all plugins
  # respectively. Note: Add `example` or `/path/to/libtenzir-plugin-example.so`
  # to load the example plugin.
  plugins: []


  # Names of plugins and builtins to explicitly forbid from being used in
  # Tenzir. For example, adding `shell` will prohibit use of the `shell`
  # operator builtin, and adding `kafka` will prohibit use of the `kafka`
  # connector plugin.
  disable-plugins: []


  # Forbid unsafe location overrides for pipelines with the 'local' and 'remote'
  # keywords, e.g., remotely reading from a file.
  no-location-overrides: false


  # Prevent all pipelines from automatically starting when the node starts.
  no-autostart: false


  # Do not move pipeline operators to subprocesses.
  disable-pipeline-subprocesses: false


  # The size of an index shard, expressed in number of events. This should
  # be a power of 2.
  max-partition-size: 4Mi


  # Timeout after which the importer forwards events to subscribers like `export
  # live=true` or `metrics live=true`. Set to 0s for an unbuffered mode. A
  # higher value increases performance, and a lower value reduces latency.
  import-buffer-timeout: 1s


  # Timeout after which an active partition is forcibly flushed, regardless of
  # its size.
  active-partition-timeout: 30s


  # Maximum number of events across all active partitions. This indirectly
  # controls the maximum memory usage when importing events.
  max-buffered-events: 12Mi


  # Automatically rebuild undersized and outdated partitions in the background.
  # The given number controls how much resources to spend on it. Set to 0 to
  # disable.
  automatic-rebuild: 1


  # Timeout after which an automatic rebuild is triggered.
  rebuild-interval: 30min


  # Zstd compression level applied to the Feather store backend.
  # zstd-compression-level: <default>


  # The URL of the control endpoint when connecting to a self-hosted
  # instance of the Tenzir Platform.
  platform-control-endpoint: wss://ws.tenzir.app/production


  # Whether to undermine the security of the TLS connection to the
  # Tenzir Platform by disabling certificate validation.
  # Setting this to `true` is strongly discouraged.
  platform-skip-peer-verification: false


  # The name to use when connecting to the platform as an ephemeral node.
  # This setting is ignored unless a workspace token is used to connect to
  # the platform. Workspace tokens are currently only available for the
  # Sovereign Edition of the Tenzir Platform.
  platform-ephemeral-node-name: Ephemeral Node


  # Control how operator's calculate demand from their upstream operator. Note
  # that this is an expert feature and should only be changed if you know what
  # you are doing. The configured values can also be changed per operator by
  # using the `_tune` operator.
  demand:
    # Issue demand only if room for at least this many elements is available.
    # Must be greater than zero. Values may either be set to a number, or to a
    # record containing `bytes` and `events` fields with numbers depending on
    # the operator's input type.
    min-elements:
      bytes: 128Ki
      events: 8Ki
    # Controls how many elements may be buffered until the operator stops
    # issuing demand. Must be greater or equal to min-elements. Values may
    # either be set to a number, or to a record containing `bytes` and `events`
    # fields with numbers depending on the operator's input type.
    max-elements:
      bytes: 4Mi
      events: 254Ki
    # Controls how many batches of elements may be buffered until the operator
    # stops issuing demand. Must be greater than zero.
    max-batches: 20
    # Controls the minimum backoff duration after an operator is detected to be
    # idle. Must be at least 10ms.
    min-backoff: 10ms
    # Controls the maximum backoff duration after an operator is detected to be
    # idle. Must be at least 10ms.
    max-backoff: 1s
    # Controls the growth rate of the backoff duration for operators that
    # continue to be idle. Must be at least 1.0. Note that setting a growth rate
    # of 1.0 causes the `max-backoff` duration to be ignored, replacing the
    # exponential growth with a constant value.
    backoff-rate: 2.0


  # Context configured as part of the configuration that are always available.
  contexts:
    # A unique name for the context that's used in the context, enrich, and
    # lookup operators to refer to the context.
    indicators:
      # The type of the context.
      type: bloom-filter
      # Arguments for creating the context, depending on the type. Refer to the
      # documentation of the individual context types to see the arguments they
      # require. Note that changes to these arguments to not apply to any
      # contexts that were previously created.
      arguments:
        capacity: 1B
        fp-probability: 0.001


  # The `index` key is used to adjust the false-positive rate of
  # the first-level lookup data structures (called synopses) in the
  # catalog. The lower the false-positive rate the more space will be
  # required, so this setting can be used to manually tune the trade-off
  # of performance vs. space.
  index:
    # The default false-positive rate for type synopses.
    default-fp-rate: 0.01
    # rules:
    #   Every rule adjusts the behaviour of Tenzir for a set of targets.
    #   Tenzir creates one synopsis per target. Targets can be either types
    #   or field names.
    #
    #   fp-rate - false positive rate. Has effect on string and address type
    #             targets
    #
    #   partition-index - Tenzir will not create dense index when set to false
    #   - targets: [:ip]
    #     fp-rate: 0.01


  # The `tenzir-ctl start` command starts a new Tenzir server process.
  start:


    # Prints the endpoint for clients when the server is ready to accept
    # connections. This comes in handy when letting the OS choose an
    # available random port, i.e., when specifying 0 as port value.
    print-endpoint: false


    # Writes the endpoint for clients when the server is ready to accept
    # connections to the specified destination. This comes in handy when letting
    # the OS choose an available random port, i.e., when specifying 0 as port
    # value, and `print-endpoint` is not sufficient.
    #write-endpoint: /tmp/tenzir-node-endpoint


    # An ordered list of commands to run inside the node after starting.
    # As an example, to configure an auto-starting PCAP source that listens
    # on the interface 'en0' and lives inside the Tenzir node, add `spawn
    # source pcap -i en0`.
    # Note that commands are not executed sequentially but in parallel.
    commands: []


    # Triggers removal of old data when the disk budget is exceeded.
    disk-budget-high: 0GiB


    # When the budget was exceeded, data is erased until the disk space is
    # below this value.
    disk-budget-low: 0GiB


    # Seconds between successive disk space checks.
    disk-budget-check-interval: 90


    # When erasing, how many partitions to erase in one go before rechecking
    # the size of the database directory.
    disk-budget-step-size: 1


    # Binary to use for checking the size of the database directory. If left
    # unset, Tenzir will recursively add up the size of all files in the
    # database directory to compute the size. Mainly useful for e.g.
    # compressed filesystem where raw file size is not the correct metric.
    # Must be the absolute path to an executable file, which will get passed
    # the database directory as its first and only argument.
    #disk-budget-check-binary: /opt/tenzir/libexec/tenzir-df-percent.sh


  # User-defined operators.
  operators:
    # The Zeek operator is an example that takes raw bytes in the form of a
    # PCAP and then parses Zeek's output via the `zeek-json` format to generate
    # a stream of events.
    zeek: |
      shell "zeek -r - LogAscii::output_to_stdout=T
             JSONStreaming::disable_default_logs=T
             JSONStreaming::enable_log_rotation=F
             json-streaming-logs"
      read_zeek_json
    # The Suricata operator is analogous to the above Zeek example, with the
    # difference that we are using Suricata. The commmand line configures
    # Suricata such that it reads PCAP on stdin and produces EVE JSON logs on
    # stdout, which we then parse with the `suricata` format.
    suricata: |
     shell "suricata -r /dev/stdin
            --set outputs.1.eve-log.filename=/dev/stdout
            --set logging.outputs.0.console.enabled=no"
     read_suricata


  # In addition to running pipelines interactively, you can also deploy
  # *Pipelines as Code*. This infrastrucutre-as-code-like method differs from
  # pipelines run on the command-line or through app.tenzir.com in two ways:
  # 1. Pipelines deployed as code always start alongside the Tenzir node.
  # 2. Deletion via the user interface is not allowed for pipelines configured
  #    as code.
  pipelines:
    # A unique identifier for the pipeline that's used for metrics, diagnostics,
    # and API calls interacting with the pipeline.
    publish-suricata:
      # An optional user-facing name for the pipeline. Defaults to the id.
      name: Import Suricata from TCP
      # The definition of the pipeline. Configured pipelines that fail to start
      # cause the node to fail to start.
      definition: |
        load_tcp "0.0.0.0:34343" { read_suricata schema_only=true }
        | where event_type != "stats"
        | publish "suricata"
      # Pipelines that encounter an error stop running and show an error state.
      # This option causes pipelines to automatically restart when they
      # encounter an error instead. The first restart happens immediately, and
      # subsequent restarts after the configured delay, defaulting to 1 minute.
      # The following values are valid for this option:
      # - Omit the option, or set it to null or false to disable.
      # - Set the option to true to enable with the default delay of 1 minute.
      # - Set the option to a valid duration to enable with a custom delay.
      restart-on-error: 1 minute
      # Pipelines that are unstoppable will run automatically and indefinitely.
      # They are not able to pause or stop.
      # If they do complete, they will end up in a failed state.
      # If `restart-on-error` is enabled, they will restart after the specified
      # duration.
      unstoppable: false


  # Use the legacy secret model. Under this model, the `secret` function yields
  # plain `string`s and can only look up secrets from the `tenzir.secrets`
  # section in this config, but not from the Tenzir Platform's secret store.
  legacy-secret-model: false


  # Enables the `secret_assert` operator. This operator can be used for our
  # integration tests and may be useful to test local setups.
  # Since it theoretically allows for brute-forcing a secret's value, it is
  # disabled by default.
  enable-assert-secret-operator: false


  # Local secrets, defined as key - value pairs. The values must be strings
  secrets:
    # my-secret-name: my-secret-value


# The below settings are internal to CAF, and aren't checked by Tenzir directly.
# Please be careful when changing these options. Note that some CAF options may
# be in conflict with Tenzir options, and are only listed here for completeness.
caf:


  # Options affecting the internal scheduler.
  scheduler:


    # Accepted alternative: "sharing".
    policy: stealing


    # Configures whether the scheduler generates profiling output.
    enable-profiling: false


    # Output file for profiler data (only if profiling is enabled).
    #profiling-output-file: </dev/null>


    # Measurement resolution in milliseconds (only if profiling is enabled).
    profiling-resolution: 100ms


    # Forces a fixed number of threads if set. Defaults to the number of
    # available CPU cores if starting a Tenzir node, or *2* for client commands.
    #max-threads: <number of cores>


    # Maximum number of messages actors can consume in one run.
    max-throughput: 500


  # When using "stealing" as scheduler policy.
  work-stealing:


    # Number of zero-sleep-interval polling attempts.
    aggressive-poll-attempts: 100


    # Frequency of steal attempts during aggressive polling.
    aggressive-steal-interval: 10


    # Number of moderately aggressive polling attempts.
    moderate-poll-attempts: 500


    # Frequency of steal attempts during moderate polling.
    moderate-steal-interval: 5


    # Sleep interval between poll attempts.
    moderate-sleep-duration: 50us


    # Frequency of steal attempts during relaxed polling.
    relaxed-steal-interval: 1


    # Sleep interval between poll attempts.
    relaxed-sleep-duration: 10ms


  stream:


    # Maximum delay for partial batches.
    max-batch-delay: 15ms


    # Selects an implementation for credit computation.
    # Accepted alternative: "token-based".
    credit-policy: token-based


    # When using "size-based" as credit-policy.
    size-based-policy:


      # Desired batch size in bytes.
      bytes-per-batch: 32


      # Maximum input buffer size in bytes.
      buffer-capacity: 256


      # Frequency of collecting batch sizes.
      sampling-rate: 100


      # Frequency of re-calibrations.
      calibration-interval: 1


      # Factor for discounting older samples.
      smoothing-factor: 2.5


    # When using "token-based" as credit-policy.
    token-based-policy:


      # Number of elements per batch.
      batch-size: 1


      # Max. number of elements in the input buffer.
      buffer-size: 64


  # Collecting metrics can be resource consuming. This section is used for
  # filtering what should and what should not be collected
  metrics-filters:


    # Rules for actor based metrics filtering.
    actors:


      # List of selected actors for run-time metrics.
      includes: []


      # List of excluded actors from run-time metrics.
      excludes: []


  # Configure using OpenSSL for node-to-node connections.
  # NOTE: Use the tenzir.endpoint variable to configure the endpoint.
  openssl:


    # Path to the PEM-formatted certificate file.
    certificate:


    # Path to the private key file for this node.
    key:


    # Passphrase to decrypt the private key.
    passphrase:


    # Path to an OpenSSL-style directory of trusted certificates.
    capath:


    # Path to a file of concatenated PEM-formatted certificates.
    cafile:


    # Colon-separated list of OpenSSL cipher strings to use.
    cipher-list:
```

# Operators

Tenzir comes with a wide range of built-in pipeline operators.

## Analyze

[Section titled “Analyze”](#analyze)

### [rare](/reference/operators/rare)

[→](/reference/operators/rare)

Shows the least common values.

```tql
rare auth.token
```

### [reverse](/reference/operators/reverse)

[→](/reference/operators/reverse)

Reverses the event order.

```tql
reverse
```

### [sort](/reference/operators/sort)

[→](/reference/operators/sort)

Sorts events by the given expressions.

```tql
sort name, -abs(transaction)
```

### [summarize](/reference/operators/summarize)

[→](/reference/operators/summarize)

Groups events and applies aggregate functions to each group.

```tql
summarize name, sum(amount)
```

### [top](/reference/operators/top)

[→](/reference/operators/top)

Shows the most common values.

```tql
top user
```

## Charts

[Section titled “Charts”](#charts)

### [chart\_area](/reference/operators/chart_area)

[→](/reference/operators/chart_area)

Plots events on an area chart.

```tql
chart_area …
```

### [chart\_bar](/reference/operators/chart_bar)

[→](/reference/operators/chart_bar)

Plots events on an bar chart.

```tql
chart_bar …
```

### [chart\_line](/reference/operators/chart_line)

[→](/reference/operators/chart_line)

Plots events on an line chart.

```tql
chart_line …
```

### [chart\_pie](/reference/operators/chart_pie)

[→](/reference/operators/chart_pie)

Plots events on an pie chart.

```tql
chart_pie …
```

## Connecting Pipelines

[Section titled “Connecting Pipelines”](#connecting-pipelines)

### [publish](/reference/operators/publish)

[→](/reference/operators/publish)

Publishes events to a channel with a topic.

```tql
publish "topic"
```

### [subscribe](/reference/operators/subscribe)

[→](/reference/operators/subscribe)

Subscribes to events from a channel with a topic.

```tql
subscribe "topic"
```

## Contexts

[Section titled “Contexts”](#contexts)

### [context::create\_bloom\_filter](/reference/operators/context/create_bloom_filter)

[→](/reference/operators/context/create_bloom_filter)

Creates a Bloom filter context.

```tql
context::create_bloom_filter "ctx", capacity=1Mi, fp_probability=0.01
```

### [context::create\_geoip](/reference/operators/context/create_geoip)

[→](/reference/operators/context/create_geoip)

Creates a GeoIP context.

```tql
context::create_geoip "ctx", db_path="GeoLite2-City.mmdb"
```

### [context::create\_lookup\_table](/reference/operators/context/create_lookup_table)

[→](/reference/operators/context/create_lookup_table)

Creates a lookup table context.

```tql
context::create_lookup_table "ctx"
```

### [context::enrich](/reference/operators/context/enrich)

[→](/reference/operators/context/enrich)

Enriches events with data from a context.

```tql
context::enrich "ctx", key=x
```

### [context::erase](/reference/operators/context/erase)

[→](/reference/operators/context/erase)

Removes entries from a context.

```tql
context::erase "ctx", key=x
```

### [context::inspect](/reference/operators/context/inspect)

[→](/reference/operators/context/inspect)

Resets a context.

```tql
context::inspect "ctx"
```

### [context::list](/reference/operators/context/list)

[→](/reference/operators/context/list)

Lists all contexts

```tql
context::list
```

### [context::load](/reference/operators/context/load)

[→](/reference/operators/context/load)

Loads context state.

```tql
context::load "ctx"
```

### [context::remove](/reference/operators/context/remove)

[→](/reference/operators/context/remove)

Deletes a context.

```tql
context::remove "ctx"
```

### [context::reset](/reference/operators/context/reset)

[→](/reference/operators/context/reset)

Resets a context.

```tql
context::reset "ctx"
```

### [context::save](/reference/operators/context/save)

[→](/reference/operators/context/save)

Saves context state.

```tql
context::save "ctx"
```

### [context::update](/reference/operators/context/update)

[→](/reference/operators/context/update)

Updates a context with new data.

```tql
context::update "ctx", key=x, value=y
```

## Detection

[Section titled “Detection”](#detection)

### [sigma](/reference/operators/sigma)

[→](/reference/operators/sigma)

Filter the input with Sigma rules and output matching events.

```tql
sigma "/tmp/rules/"
```

### [yara](/reference/operators/yara)

[→](/reference/operators/yara)

Executes YARA rules on byte streams.

```tql
yara "/path/to/rules", blockwise=true
```

## Encode & Decode

[Section titled “Encode & Decode”](#encode--decode)

### [compress](/reference/operators/compress)

[→](/reference/operators/compress)

Compresses a stream of bytes.

```tql
compress "zstd"
```

### [compress\_brotli](/reference/operators/compress_brotli)

[→](/reference/operators/compress_brotli)

Compresses a stream of bytes using Brotli compression.

```tql
compress_brotli, level=10
```

### [compress\_bz2](/reference/operators/compress_bz2)

[→](/reference/operators/compress_bz2)

Compresses a stream of bytes using bz2 compression.

```tql
compress_bz2, level=9
```

### [compress\_gzip](/reference/operators/compress_gzip)

[→](/reference/operators/compress_gzip)

Compresses a stream of bytes using gzip compression.

```tql
compress_gzip, level=8
```

### [compress\_lz4](/reference/operators/compress_lz4)

[→](/reference/operators/compress_lz4)

Compresses a stream of bytes using lz4 compression.

```tql
compress_lz4, level=7
```

### [compress\_zstd](/reference/operators/compress_zstd)

[→](/reference/operators/compress_zstd)

Compresses a stream of bytes using zstd compression.

```tql
compress_zstd, level=6
```

### [decompress](/reference/operators/decompress)

[→](/reference/operators/decompress)

Decompresses a stream of bytes.

```tql
decompress "gzip"
```

### [decompress\_brotli](/reference/operators/decompress_brotli)

[→](/reference/operators/decompress_brotli)

Decompresses a stream of bytes in the Brotli format.

```tql
decompress_brotli
```

### [decompress\_bz2](/reference/operators/decompress_bz2)

[→](/reference/operators/decompress_bz2)

Decompresses a stream of bytes in the Bzip2 format.

```tql
decompress_bz2
```

### [decompress\_gzip](/reference/operators/decompress_gzip)

[→](/reference/operators/decompress_gzip)

Decompresses a stream of bytes in the Gzip format.

```tql
decompress_gzip
```

### [decompress\_lz4](/reference/operators/decompress_lz4)

[→](/reference/operators/decompress_lz4)

Decompresses a stream of bytes in the Lz4 format.

```tql
decompress_lz4
```

### [decompress\_zstd](/reference/operators/decompress_zstd)

[→](/reference/operators/decompress_zstd)

Decompresses a stream of bytes in the Zstd format.

```tql
decompress_zstd
```

## Escape Hatches

[Section titled “Escape Hatches”](#escape-hatches)

### [python](/reference/operators/python)

[→](/reference/operators/python)

Executes Python code against each event of the input.

```tql
python "self.x = self.y"
```

### [shell](/reference/operators/shell)

[→](/reference/operators/shell)

Executes a system command and hooks its stdin and stdout into the pipeline.

```tql
shell "echo hello"
```

## Filter

[Section titled “Filter”](#filter)

### [assert](/reference/operators/assert)

[→](/reference/operators/assert)

Drops events and emits a warning if the invariant is violated.

```tql
assert name.starts_with("John")
```

### [assert\_throughput](/reference/operators/assert_throughput)

[→](/reference/operators/assert_throughput)

Emits a warning if the pipeline does not have the expected throughput

```tql
assert_throughput 1000, within=1s
```

### [deduplicate](/reference/operators/deduplicate)

[→](/reference/operators/deduplicate)

Removes duplicate events based on a common key.

```tql
deduplicate src_ip
```

### [head](/reference/operators/head)

[→](/reference/operators/head)

Limits the input to the first `n` events.

```tql
head 20
```

### [sample](/reference/operators/sample)

[→](/reference/operators/sample)

Dynamically samples events from a event stream.

```tql
sample 30s, max_samples=2k
```

### [slice](/reference/operators/slice)

[→](/reference/operators/slice)

Keeps a range of events within the interval `[begin, end)` stepping by `stride`.

```tql
slice begin=10, end=30
```

### [tail](/reference/operators/tail)

[→](/reference/operators/tail)

Limits the input to the last `n` events.

```tql
tail 20
```

### [taste](/reference/operators/taste)

[→](/reference/operators/taste)

Limits the input to `n` events per unique schema.

```tql
taste 1
```

### [where](/reference/operators/where)

[→](/reference/operators/where)

Keeps only events for which the given predicate is true.

```tql
where name.starts_with("John")
```

## Flow Control

[Section titled “Flow Control”](#flow-control)

### [cron](/reference/operators/cron)

[→](/reference/operators/cron)

Runs a pipeline periodically according to a cron expression.

```tql
cron "* */10 * * * MON-FRI" { from "https://example.org" }
```

### [delay](/reference/operators/delay)

[→](/reference/operators/delay)

Delays events relative to a given start time, with an optional speedup.

```tql
delay ts, speed=2.5
```

### [discard](/reference/operators/discard)

[→](/reference/operators/discard)

Discards all incoming events.

```tql
discard
```

### [every](/reference/operators/every)

[→](/reference/operators/every)

Runs a pipeline periodically at a fixed interval.

```tql
every 10s { summarize sum(amount) }
```

### [fork](/reference/operators/fork)

[→](/reference/operators/fork)

Executes a subpipeline with a copy of the input.

```tql
fork { to "copy.json" }
```

### [load\_balance](/reference/operators/load_balance)

[→](/reference/operators/load_balance)

Routes the data to one of multiple subpipelines.

```tql
load_balance $over { publish $over }
```

### [pass](/reference/operators/pass)

[→](/reference/operators/pass)

Does nothing with the input.

```tql
pass
```

### [repeat](/reference/operators/repeat)

[→](/reference/operators/repeat)

Repeats the input a number of times.

```tql
repeat 100
```

### [throttle](/reference/operators/throttle)

[→](/reference/operators/throttle)

Limits the bandwidth of a pipeline.

```tql
throttle 100M, within=1min
```

## Host Inspection

[Section titled “Host Inspection”](#host-inspection)

### [files](/reference/operators/files)

[→](/reference/operators/files)

Shows file information for a given directory.

```tql
files "/var/log/", recurse=true
```

### [nics](/reference/operators/nics)

[→](/reference/operators/nics)

Shows a snapshot of available network interfaces.

```tql
nics
```

### [processes](/reference/operators/processes)

[→](/reference/operators/processes)

Shows a snapshot of running processes.

```tql
processes
```

### [sockets](/reference/operators/sockets)

[→](/reference/operators/sockets)

Shows a snapshot of open sockets.

```tql
sockets
```

## Internals

[Section titled “Internals”](#internals)

### [api](/reference/operators/api)

[→](/reference/operators/api)

Use Tenzir's REST API directly from a pipeline.

```tql
api "/pipeline/list"
```

### [batch](/reference/operators/batch)

[→](/reference/operators/batch)

The `batch` operator controls the batch size of events.

```tql
batch timeout=1s
```

### [buffer](/reference/operators/buffer)

[→](/reference/operators/buffer)

An in-memory buffer to improve handling of data spikes in upstream operators.

```tql
buffer 10M, policy="drop"
```

### [cache](/reference/operators/cache)

[→](/reference/operators/cache)

An in-memory cache shared between pipelines.

```tql
cache "w01wyhTZm3", ttl=10min
```

### [local](/reference/operators/local)

[→](/reference/operators/local)

Forces a pipeline to run locally.

```tql
local { sort foo }
```

### [measure](/reference/operators/measure)

[→](/reference/operators/measure)

Replaces the input with metrics describing the input.

```tql
measure
```

### [remote](/reference/operators/remote)

[→](/reference/operators/remote)

Forces a pipeline to run remotely at a node.

```tql
remote { version }
```

### [serve](/reference/operators/serve)

[→](/reference/operators/serve)

Make events available under the `/serve` REST API endpoint

```tql
serve "abcde12345"
```

### [strict](/reference/operators/strict)

[→](/reference/operators/strict)

Treats all warnings as errors.

```tql
strict { assert false }
```

### [unordered](/reference/operators/unordered)

[→](/reference/operators/unordered)

Removes ordering assumptions from a pipeline.

```tql
unordered { read_ndjson }
```

## Modify

[Section titled “Modify”](#modify)

### [dns\_lookup](/reference/operators/dns_lookup)

[→](/reference/operators/dns_lookup)

Performs DNS lookups to resolve IP addresses to hostnames or hostnames to IP addresses.

```tql
dns_lookup ip_address, result=dns_info
```

### [drop](/reference/operators/drop)

[→](/reference/operators/drop)

Removes fields from the event.

```tql
drop name, metadata.id
```

### [drop\_null\_fields](/reference/operators/drop_null_fields)

[→](/reference/operators/drop_null_fields)

Removes fields containing null values from the event.

```tql
drop_null_fields name, metadata.id
```

### [enumerate](/reference/operators/enumerate)

[→](/reference/operators/enumerate)

Add a field with the number of preceding events.

```tql
enumerate num
```

### [http](/reference/operators/http)

[→](/reference/operators/http)

Sends HTTP/1.1 requests and forwards the response.

```tql
http "example.com"
```

### [move](/reference/operators/move)

[→](/reference/operators/move)

Moves values from one field to another, removing the original field.

```tql
move id=parsed_id, ctx.message=incoming.status
```

### [replace](/reference/operators/replace)

[→](/reference/operators/replace)

Replaces all occurrences of a value with another value.

```tql
replace what=42, with=null
```

### [select](/reference/operators/select)

[→](/reference/operators/select)

Selects some values and discards the rest.

```tql
select name, id=metadata.id
```

### [set](/reference/operators/set)

[→](/reference/operators/set)

Assigns a value to a field, creating it if necessary.

```tql
name = "Tenzir"
```

### [timeshift](/reference/operators/timeshift)

[→](/reference/operators/timeshift)

Adjusts timestamps relative to a given start time, with an optional speedup.

```tql
timeshift ts, start=2020-01-01
```

### [unroll](/reference/operators/unroll)

[→](/reference/operators/unroll)

Returns a new event for each member of a list or a record in an event, duplicating the surrounding event.

```tql
unroll names
```

## OCSF

[Section titled “OCSF”](#ocsf)

### [ocsf::apply](/reference/operators/ocsf/apply)

[→](/reference/operators/ocsf/apply)

Casts incoming events to their OCSF type.

```tql
ocsf::apply
```

### [ocsf::derive](/reference/operators/ocsf/derive)

[→](/reference/operators/ocsf/derive)

Automatically assigns enum strings from their integer counterparts and vice versa.

```tql
ocsf::derive
```

### [ocsf::trim](/reference/operators/ocsf/trim)

[→](/reference/operators/ocsf/trim)

Drops fields from OCSF events to reduce their size.

```tql
ocsf::trim
```

## Packages

[Section titled “Packages”](#packages)

### [package::add](/reference/operators/package/add)

[→](/reference/operators/package/add)

Installs a package.

```tql
package::add "suricata-ocsf"
```

### [package::list](/reference/operators/package/list)

[→](/reference/operators/package/list)

Shows installed packages.

```tql
package::list
```

### [package::remove](/reference/operators/package/remove)

[→](/reference/operators/package/remove)

Uninstalls a package.

```tql
package::remove "suricata-ocsf"
```

## Parsing

[Section titled “Parsing”](#parsing)

### [read\_all](/reference/operators/read_all)

[→](/reference/operators/read_all)

Parses an incoming bytes stream into a single event.

```tql
read_all binary=true
```

### [read\_bitz](/reference/operators/read_bitz)

[→](/reference/operators/read_bitz)

Parses bytes as *BITZ* format.

```tql
read_bitz
```

### [read\_cef](/reference/operators/read_cef)

[→](/reference/operators/read_cef)

Parses an incoming Common Event Format (CEF) stream into events.

```tql
read_cef
```

### [read\_csv](/reference/operators/read_csv)

[→](/reference/operators/read_csv)

Read CSV (Comma-Separated Values) from a byte stream.

```tql
read_csv null_value="-"
```

### [read\_delimited](/reference/operators/read_delimited)

[→](/reference/operators/read_delimited)

Parses an incoming bytes stream into events using a string as delimiter.

```tql
read_delimited "|"
```

### [read\_delimited\_regex](/reference/operators/read_delimited_regex)

[→](/reference/operators/read_delimited_regex)

Parses an incoming bytes stream into events using a regular expression as delimiter.

```tql
read_delimited_regex r"\s+"
```

### [read\_feather](/reference/operators/read_feather)

[→](/reference/operators/read_feather)

Parses an incoming Feather byte stream into events.

```tql
read_feather
```

### [read\_gelf](/reference/operators/read_gelf)

[→](/reference/operators/read_gelf)

Parses an incoming GELF stream into events.

```tql
read_gelf
```

### [read\_grok](/reference/operators/read_grok)

[→](/reference/operators/read_grok)

Parses lines of input with a grok pattern.

```tql
read_grok "%{IP:client} %{WORD:action}"
```

### [read\_json](/reference/operators/read_json)

[→](/reference/operators/read_json)

Parses an incoming JSON stream into events.

```tql
read_json arrays_of_objects=true
```

### [read\_kv](/reference/operators/read_kv)

[→](/reference/operators/read_kv)

Read Key-Value pairs from a byte stream.

```tql
read_kv r"(\s+)[A-Z_]+:", r":\s*"
```

### [read\_leef](/reference/operators/read_leef)

[→](/reference/operators/read_leef)

Parses an incoming \[LEEF]\[leef] stream into events.

```tql
read_leef
```

### [read\_lines](/reference/operators/read_lines)

[→](/reference/operators/read_lines)

Parses an incoming bytes stream into events.

```tql
read_lines
```

### [read\_ndjson](/reference/operators/read_ndjson)

[→](/reference/operators/read_ndjson)

Parses an incoming NDJSON (newline-delimited JSON) stream into events.

```tql
read_ndjson
```

### [read\_parquet](/reference/operators/read_parquet)

[→](/reference/operators/read_parquet)

Reads events from a Parquet byte stream.

```tql
read_parquet
```

### [read\_pcap](/reference/operators/read_pcap)

[→](/reference/operators/read_pcap)

Reads raw network packets in PCAP file format.

```tql
read_pcap
```

### [read\_ssv](/reference/operators/read_ssv)

[→](/reference/operators/read_ssv)

Read SSV (Space-Separated Values) from a byte stream.

```tql
read_ssv header="name count"
```

### [read\_suricata](/reference/operators/read_suricata)

[→](/reference/operators/read_suricata)

Parse an incoming \[Suricata EVE JSON]\[eve-json] stream into events.

```tql
read_suricata
```

### [read\_syslog](/reference/operators/read_syslog)

[→](/reference/operators/read_syslog)

Parses an incoming Syslog stream into events.

```tql
read_syslog
```

### [read\_tsv](/reference/operators/read_tsv)

[→](/reference/operators/read_tsv)

Read TSV (Tab-Separated Values) from a byte stream.

```tql
read_tsv auto_expand=true
```

### [read\_xsv](/reference/operators/read_xsv)

[→](/reference/operators/read_xsv)

Read XSV from a byte stream.

```tql
read_xsv ";", ":", "N/A"
```

### [read\_yaml](/reference/operators/read_yaml)

[→](/reference/operators/read_yaml)

Parses an incoming YAML stream into events.

```tql
read_yaml
```

### [read\_zeek\_json](/reference/operators/read_zeek_json)

[→](/reference/operators/read_zeek_json)

Parse an incoming Zeek JSON stream into events.

```tql
read_zeek_json
```

### [read\_zeek\_tsv](/reference/operators/read_zeek_tsv)

[→](/reference/operators/read_zeek_tsv)

Parses an incoming `Zeek TSV` stream into events.

```tql
read_zeek_tsv
```

## Pipelines

[Section titled “Pipelines”](#pipelines)

### [pipeline::activity](/reference/operators/pipeline/activity)

[→](/reference/operators/pipeline/activity)

Summarizes the activity of pipelines.

```tql
pipeline::activity range=1d, interval=1h
```

### [pipeline::detach](/reference/operators/pipeline/detach)

[→](/reference/operators/pipeline/detach)

Starts a pipeline in the node.

```tql
pipeline::detach { … }
```

### [pipeline::list](/reference/operators/pipeline/list)

[→](/reference/operators/pipeline/list)

Shows managed pipelines.

```tql
pipeline::list
```

### [pipeline::run](/reference/operators/pipeline/run)

[→](/reference/operators/pipeline/run)

Starts a pipeline in the node and waits for it to complete.

```tql
pipeline::run { … }
```

## Printing

[Section titled “Printing”](#printing)

### [write\_bitz](/reference/operators/write_bitz)

[→](/reference/operators/write_bitz)

Writes events in *BITZ* format.

```tql
write_bitz
```

### [write\_csv](/reference/operators/write_csv)

[→](/reference/operators/write_csv)

Transforms event stream to CSV (Comma-Separated Values) byte stream.

```tql
write_csv
```

### [write\_feather](/reference/operators/write_feather)

[→](/reference/operators/write_feather)

Transforms the input event stream to Feather byte stream.

```tql
write_feather
```

### [write\_json](/reference/operators/write_json)

[→](/reference/operators/write_json)

Transforms the input event stream to a JSON byte stream.

```tql
write_json
```

### [write\_kv](/reference/operators/write_kv)

[→](/reference/operators/write_kv)

Writes events in a Key-Value format.

```tql
write_kv
```

### [write\_lines](/reference/operators/write_lines)

[→](/reference/operators/write_lines)

Writes events as key-value pairsthe *values* of an event.

```tql
write_lines
```

### [write\_ndjson](/reference/operators/write_ndjson)

[→](/reference/operators/write_ndjson)

Transforms the input event stream to a Newline-Delimited JSON byte stream.

```tql
write_ndjson
```

### [write\_parquet](/reference/operators/write_parquet)

[→](/reference/operators/write_parquet)

Transforms event stream to a Parquet byte stream.

```tql
write_parquet
```

### [write\_pcap](/reference/operators/write_pcap)

[→](/reference/operators/write_pcap)

Transforms event stream to PCAP byte stream.

```tql
write_pcap
```

### [write\_ssv](/reference/operators/write_ssv)

[→](/reference/operators/write_ssv)

Transforms event stream to SSV (Space-Separated Values) byte stream.

```tql
write_ssv
```

### [write\_syslog](/reference/operators/write_syslog)

[→](/reference/operators/write_syslog)

Writes events as syslog.

```tql
write_syslog
```

### [write\_tql](/reference/operators/write_tql)

[→](/reference/operators/write_tql)

Transforms the input event stream to a TQL notation byte stream.

```tql
write_tql
```

### [write\_tsv](/reference/operators/write_tsv)

[→](/reference/operators/write_tsv)

Transforms event stream to TSV (Tab-Separated Values) byte stream.

```tql
write_tsv
```

### [write\_xsv](/reference/operators/write_xsv)

[→](/reference/operators/write_xsv)

Transforms event stream to XSV byte stream.

```tql
write_xsv
```

### [write\_yaml](/reference/operators/write_yaml)

[→](/reference/operators/write_yaml)

Transforms the input event stream to YAML byte stream.

```tql
write_yaml
```

### [write\_zeek\_tsv](/reference/operators/write_zeek_tsv)

[→](/reference/operators/write_zeek_tsv)

Transforms event stream into Zeek Tab-Separated Value byte stream.

```tql
write_zeek_tsv
```

## Inputs

[Section titled “Inputs”](#inputs)

### Bytes

[Section titled “Bytes”](#bytes)

### [load\_amqp](/reference/operators/load_amqp)

[→](/reference/operators/load_amqp)

Loads a byte stream via AMQP messages.

```tql
load_amqp
```

### [load\_azure\_blob\_storage](/reference/operators/load_azure_blob_storage)

[→](/reference/operators/load_azure_blob_storage)

Loads bytes from Azure Blob Storage.

```tql
load_azure_blob_storage "abfs://container/file"
```

### [load\_file](/reference/operators/load_file)

[→](/reference/operators/load_file)

Loads the contents of the file at `path` as a byte stream.

```tql
load_file "/tmp/data.json"
```

### [load\_ftp](/reference/operators/load_ftp)

[→](/reference/operators/load_ftp)

Loads a byte stream via FTP.

```tql
load_ftp "ftp.example.org"
```

### [load\_gcs](/reference/operators/load_gcs)

[→](/reference/operators/load_gcs)

Loads bytes from a Google Cloud Storage object.

```tql
load_gcs "gs://bucket/object.json"
```

### [load\_google\_cloud\_pubsub](/reference/operators/load_google_cloud_pubsub)

[→](/reference/operators/load_google_cloud_pubsub)

Subscribes to a Google Cloud Pub/Sub subscription and obtains bytes.

```tql
load_google_cloud_pubsub project_id="my-project"
```

### [load\_http](/reference/operators/load_http)

[→](/reference/operators/load_http)

Loads a byte stream via HTTP.

```tql
load_http "example.org", params={n: 5}
```

### [load\_kafka](/reference/operators/load_kafka)

[→](/reference/operators/load_kafka)

Loads a byte stream from an Apache Kafka topic.

```tql
load_kafka topic="example"
```

### [load\_nic](/reference/operators/load_nic)

[→](/reference/operators/load_nic)

Loads bytes from a network interface card (NIC).

```tql
load_nic "eth0"
```

### [load\_s3](/reference/operators/load_s3)

[→](/reference/operators/load_s3)

Loads from an Amazon S3 object.

```tql
load_s3 "s3://my-bucket/obj.csv"
```

### [load\_sqs](/reference/operators/load_sqs)

[→](/reference/operators/load_sqs)

Loads bytes from \[Amazon SQS]\[sqs] queues.

```tql
load_sqs "sqs://tenzir"
```

### [load\_stdin](/reference/operators/load_stdin)

[→](/reference/operators/load_stdin)

Accepts bytes from standard input.

```tql
load_stdin
```

### [load\_tcp](/reference/operators/load_tcp)

[→](/reference/operators/load_tcp)

Loads bytes from a TCP or TLS connection.

```tql
load_tcp "0.0.0.0:8090" { read_json }
```

### [load\_udp](/reference/operators/load_udp)

[→](/reference/operators/load_udp)

Loads bytes from a UDP socket.

```tql
load_udp "0.0.0.0:8090"
```

### [load\_zmq](/reference/operators/load_zmq)

[→](/reference/operators/load_zmq)

Receives ZeroMQ messages.

```tql
load_zmq
```

### Events

[Section titled “Events”](#events)

### [from](/reference/operators/from)

[→](/reference/operators/from)

Obtains events from an URI, inferring the source, compression and format.

```tql
from "data.json"
```

### [from\_azure\_blob\_storage](/reference/operators/from_azure_blob_storage)

[→](/reference/operators/from_azure_blob_storage)

Reads one or multiple files from Azure Blob Storage.

```tql
from_azure_blob_storage "abfs://container/data/**.json"
```

### [from\_file](/reference/operators/from_file)

[→](/reference/operators/from_file)

Reads one or multiple files from a filesystem.

```tql
from_file "s3://data/**.json"
```

### [from\_fluent\_bit](/reference/operators/from_fluent_bit)

[→](/reference/operators/from_fluent_bit)

Receives events via Fluent Bit.

```tql
from_fluent_bit "opentelemetry"
```

### [from\_gcs](/reference/operators/from_gcs)

[→](/reference/operators/from_gcs)

Reads one or multiple files from Google Cloud Storage.

```tql
from_gcs "gs://my-bucket/data/**.json"
```

### [from\_http](/reference/operators/from_http)

[→](/reference/operators/from_http)

Sends and receives HTTP/1.1 requests.

```tql
from_http "0.0.0.0:8080"
```

### [from\_opensearch](/reference/operators/from_opensearch)

[→](/reference/operators/from_opensearch)

Receives events via Opensearch Bulk API.

```tql
from_opensearch
```

### [from\_s3](/reference/operators/from_s3)

[→](/reference/operators/from_s3)

Reads one or multiple files from Amazon S3.

```tql
from_s3 "s3://my-bucket/data/**.json"
```

### [from\_udp](/reference/operators/from_udp)

[→](/reference/operators/from_udp)

Receives UDP datagrams and outputs structured events.

```tql
from_udp "0.0.0.0:8090"
```

### [from\_velociraptor](/reference/operators/from_velociraptor)

[→](/reference/operators/from_velociraptor)

Submits VQL to a Velociraptor server and returns the response as events.

```tql
from_velociraptor subscribe="Windows"
```

## Node

[Section titled “Node”](#node)

### Inspection

[Section titled “Inspection”](#inspection)

### [diagnostics](/reference/operators/diagnostics)

[→](/reference/operators/diagnostics)

Retrieves diagnostic events from a Tenzir node.

```tql
diagnostics
```

### [metrics](/reference/operators/metrics)

[→](/reference/operators/metrics)

Retrieves metrics events from a Tenzir node.

```tql
metrics "cpu"
```

### [openapi](/reference/operators/openapi)

[→](/reference/operators/openapi)

Shows the node's OpenAPI specification.

```tql
openapi
```

### [plugins](/reference/operators/plugins)

[→](/reference/operators/plugins)

Shows all available plugins and built-ins.

```tql
plugins
```

### [version](/reference/operators/version)

[→](/reference/operators/version)

Shows the current version.

```tql
version
```

### Storage Engine

[Section titled “Storage Engine”](#storage-engine)

### [export](/reference/operators/export)

[→](/reference/operators/export)

Retrieves events from a Tenzir node.

```tql
export
```

### [fields](/reference/operators/fields)

[→](/reference/operators/fields)

Retrieves all fields stored at a node.

```tql
fields
```

### [import](/reference/operators/import)

[→](/reference/operators/import)

Imports events into a Tenzir node.

```tql
import
```

### [partitions](/reference/operators/partitions)

[→](/reference/operators/partitions)

Retrieves metadata about events stored at a node.

```tql
partitions src_ip == 1.2.3.4
```

### [schemas](/reference/operators/schemas)

[→](/reference/operators/schemas)

Retrieves all schemas for events stored at a node.

```tql
schemas
```

## Outputs

[Section titled “Outputs”](#outputs)

### Bytes

[Section titled “Bytes”](#bytes-1)

### [save\_amqp](/reference/operators/save_amqp)

[→](/reference/operators/save_amqp)

Saves a byte stream via AMQP messages.

```tql
save_amqp
```

### [save\_azure\_blob\_storage](/reference/operators/save_azure_blob_storage)

[→](/reference/operators/save_azure_blob_storage)

Saves bytes to Azure Blob Storage.

```tql
save_azure_blob_storage "abfs://container/file"
```

### [save\_email](/reference/operators/save_email)

[→](/reference/operators/save_email)

Saves bytes through an SMTP server.

```tql
save_email "user@example.org"
```

### [save\_file](/reference/operators/save_file)

[→](/reference/operators/save_file)

Writes a byte stream to a file.

```tql
save_file "/tmp/out.json"
```

### [save\_ftp](/reference/operators/save_ftp)

[→](/reference/operators/save_ftp)

Saves a byte stream via FTP.

```tql
save_ftp "ftp.example.org"
```

### [save\_gcs](/reference/operators/save_gcs)

[→](/reference/operators/save_gcs)

Saves bytes to a Google Cloud Storage object.

```tql
save_gcs "gs://bucket/object.json"
```

### [save\_google\_cloud\_pubsub](/reference/operators/save_google_cloud_pubsub)

[→](/reference/operators/save_google_cloud_pubsub)

Publishes to a Google Cloud Pub/Sub topic.

```tql
save_google_cloud_pubsub project_id="my-project"
```

### [save\_http](/reference/operators/save_http)

[→](/reference/operators/save_http)

Sends a byte stream via HTTP.

```tql
save_http "example.org/api"
```

### [save\_kafka](/reference/operators/save_kafka)

[→](/reference/operators/save_kafka)

Saves a byte stream to a Apache Kafka topic.

```tql
save_kafka topic="example"
```

### [save\_s3](/reference/operators/save_s3)

[→](/reference/operators/save_s3)

Saves bytes to an Amazon S3 object.

```tql
save_s3 "s3://my-bucket/obj.csv"
```

### [save\_sqs](/reference/operators/save_sqs)

[→](/reference/operators/save_sqs)

Saves bytes to \[Amazon SQS]\[sqs] queues.

```tql
save_sqs "sqs://tenzir"
```

### [save\_stdout](/reference/operators/save_stdout)

[→](/reference/operators/save_stdout)

Writes a byte stream to standard output.

```tql
save_stdout
```

### [save\_tcp](/reference/operators/save_tcp)

[→](/reference/operators/save_tcp)

Saves bytes to a TCP or TLS connection.

```tql
save_tcp "0.0.0.0:8090", tls=true
```

### [save\_udp](/reference/operators/save_udp)

[→](/reference/operators/save_udp)

Saves bytes to a UDP socket.

```tql
save_udp "0.0.0.0:8090"
```

### [save\_zmq](/reference/operators/save_zmq)

[→](/reference/operators/save_zmq)

Sends bytes as ZeroMQ messages.

```tql
save_zmq
```

### Events

[Section titled “Events”](#events-1)

### [to](/reference/operators/to)

[→](/reference/operators/to)

Saves to an URI, inferring the destination, compression and format.

```tql
to "output.json"
```

### [to\_amazon\_security\_lake](/reference/operators/to_amazon_security_lake)

[→](/reference/operators/to_amazon_security_lake)

Sends OCSF events to Amazon Security Lake.

```tql
to_amazon_security_lake "s3://…"
```

### [to\_azure\_log\_analytics](/reference/operators/to_azure_log_analytics)

[→](/reference/operators/to_azure_log_analytics)

Sends events to the Microsoft Azure Logs Ingestion API.

```tql
to_azure_log_analytics tenant_id="...", workspace_id="..."
```

### [to\_clickhouse](/reference/operators/to_clickhouse)

[→](/reference/operators/to_clickhouse)

Sends events to a ClickHouse table.

```tql
to_clickhouse table="my_table"
```

### [to\_fluent\_bit](/reference/operators/to_fluent_bit)

[→](/reference/operators/to_fluent_bit)

Sends events via Fluent Bit.

```tql
to_fluent_bit "elasticsearch" …
```

### [to\_google\_cloud\_logging](/reference/operators/to_google_cloud_logging)

[→](/reference/operators/to_google_cloud_logging)

Sends events to Google Cloud Logging.

```tql
to_google_cloud_logging …
```

### [to\_google\_secops](/reference/operators/to_google_secops)

[→](/reference/operators/to_google_secops)

Sends unstructured events to a Google SecOps Chronicle instance.

```tql
to_google_secops …
```

### [to\_hive](/reference/operators/to_hive)

[→](/reference/operators/to_hive)

Writes events to a URI using hive partitioning.

```tql
to_hive "s3://…", partition_by=[x]
```

### [to\_kafka](/reference/operators/to_kafka)

[→](/reference/operators/to_kafka)

Sends messages to an Apache Kafka topic.

```tql
to_kafka "topic", message=this.print_json()
```

### [to\_opensearch](/reference/operators/to_opensearch)

[→](/reference/operators/to_opensearch)

Sends events to an OpenSearch-compatible Bulk API.

```tql
to_opensearch "localhost:9200", …
```

### [to\_sentinelone\_data\_lake](/reference/operators/to_sentinelone_data_lake)

[→](/reference/operators/to_sentinelone_data_lake)

Sends security events to SentinelOne Singularity Data Lake via REST API.

```tql
to_sentinelone_data_lake "https://…", …
```

### [to\_snowflake](/reference/operators/to_snowflake)

[→](/reference/operators/to_snowflake)

Sends events to a Snowflake database.

```tql
to_snowflake account_identifier="…
```

### [to\_splunk](/reference/operators/to_splunk)

[→](/reference/operators/to_splunk)

Sends events to a Splunk \[HTTP Event Collector (HEC)]\[hec].

```tql
to_splunk "localhost:8088", …
```

# api

Use Tenzir’s REST API directly from a pipeline.

```tql
api endpoint:string, [request_body:string]
```

## Description

[Section titled “Description”](#description)

The `api` operator interacts with Tenzir’s REST API without needing to spin up a web server, making all APIs accessible from within pipelines.

### `endpoint: string`

[Section titled “endpoint: string”](#endpoint-string)

The endpoint to request, e.g., `/pipeline/list` to list all managed pipelines.

Tenzir’s [REST API specification](/reference/node/api) lists all available endpoints.

### `request_body: string (optional)`

[Section titled “request\_body: string (optional)”](#request_body-string-optional)

A single string containing the JSON request body to send with the request.

## Examples

[Section titled “Examples”](#examples)

### List all running pipelines

[Section titled “List all running pipelines”](#list-all-running-pipelines)

```tql
api "/pipeline/list"
```

### Create a new pipeline and start it immediately

[Section titled “Create a new pipeline and start it immediately”](#create-a-new-pipeline-and-start-it-immediately)

```tql
api "/pipeline/create", {
  name: "Suricata Import",
  definition: "from file /tmp/eve.sock read suricata",
  autostart: { created: true },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`openapi`](/reference/operators/openapi), [`serve`](/reference/operators/serve)

# assert

Drops events and emits a warning if the invariant is violated.

```tql
assert invariant:bool, [message=any]
```

## Description

[Section titled “Description”](#description)

The `assert` operator asserts that `invariant` is `true` for events. In case an event does not satisfy the invariant, it is dropped and a warning is emitted.

Consider using `where` instead

If you only want to filter events, use `where` instead of `assert`. The `where` operator does not emit a warning when the expression evaluates to false, hence it is more suitable for normal filtering purposes. It is also much faster than `assert` in some situations due to optimizations such as predicate pushdown.

### `invariant: bool`

[Section titled “invariant: bool”](#invariant-bool)

Condition to assert being `true`.

### `message = any (optional)`

[Section titled “message = any (optional)”](#message--any-optional)

Context to associate with the assertion failure.

## Examples

[Section titled “Examples”](#examples)

### Make sure that `x != 2`

[Section titled “Make sure that x != 2”](#make-sure-that-x--2)

```tql
from {x: 1}, {x: 2}, {x: 3}
assert x != 2
```

```tql
{x: 1}
// warning: assertion failure
{x: 3}
```

### Check that a topic only contains certain events

[Section titled “Check that a topic only contains certain events”](#check-that-a-topic-only-contains-certain-events)

```tql
subscribe "network"
assert @name == "ocsf.network_activity"
// continue processing
```

## See Also

[Section titled “See Also”](#see-also)

[`assert_throughput`](/reference/operators/assert_throughput), [`where`](/reference/operators/where)

# assert_throughput

Emits a warning if the pipeline does not have the expected throughput

```tql
assert_throughput min_events:int, within=duration, [retries=int]
```

## Description

[Section titled “Description”](#description)

The `assert_throughput` operator checks a pipeline’s throughput, emitting a warning if the minimum specified throughput is unmet, and optionally an error if the number of retries is exceeded.

## Examples

[Section titled “Examples”](#examples)

### Require 1,000 events per second, failing if the issue persists for 30s

[Section titled “Require 1,000 events per second, failing if the issue persists for 30s”](#require-1000-events-per-second-failing-if-the-issue-persists-for-30s)

```tql
from "udp://0.0.0.0:514" { read_syslog }
assert_throughput 1k, within=1s, retries=30
```

## See Also

[Section titled “See Also”](#see-also)

[`assert`](/reference/operators/assert)

# batch

The `batch` operator controls the batch size of events.

```tql
batch [limit:int, timeout=duration]
```

## Description

[Section titled “Description”](#description)

The `batch` operator takes its input and rewrites it into batches of up to the desired size.

Expert Operator

The `batch` operator is a lower-level building block that lets users explicitly control batching, which otherwise is controlled automatically by Tenzir’s underlying pipeline execution engine. Use with caution!

### `limit: int (optional)`

[Section titled “limit: int (optional)”](#limit-int-optional)

How many events to put into one batch at most.

Defaults to `65536`.

### `timeout = duration (optional)`

[Section titled “timeout = duration (optional)”](#timeout--duration-optional)

Specifies a maximum latency for events passing through the batch operator. When unspecified, an infinite duration is used.

# buffer

An in-memory buffer to improve handling of data spikes in upstream operators.

```tql
buffer [capacity:int, policy=string]
```

## Description

[Section titled “Description”](#description)

The `buffer` operator buffers up to the specified number of events or bytes in memory.

By default, operators in a pipeline run only when their downstream operators want to receive input. This mechanism is called back pressure. The `buffer` operator effectively breaks back pressure by storing up to the specified number of events in memory, always requesting more input, which allows upstream operators to run uninterruptedly even in case the downstream operators of the buffer are unable to keep up. This allows pipelines to handle data spikes more easily.

### `capacity: int (optional)`

[Section titled “capacity: int (optional)”](#capacity-int-optional)

The number of events or bytes that may be kept at most in the buffer.

Note that every operator already buffers up to `254Ki` events before it starts applying back pressure. Smaller buffers may decrease performance.

### `policy = string (optional)`

[Section titled “policy = string (optional)”](#policy--string-optional)

Specifies what the operator does when the buffer runs full.

* `"drop"`: Drop events that do not fit into the buffer. This policy is not supported for bytes inputs.
* `"block"`: Use back pressure to slow down upstream operators.

When buffering events, this option defaults to `"block"` for pipelines visible on the overview page on [app.tenzir.com](https://app.tenzir.com), and to `"drop"` otherwise. When buffering bytes, this option always defaults to `"block"`.

## Examples

[Section titled “Examples”](#examples)

### Buffer up to 10 million events or bytes

[Section titled “Buffer up to 10 million events or bytes”](#buffer-up-to-10-million-events-or-bytes)

Buffer and drop events if downstream cannot keep up:

```tql
buffer 10M, policy="drop"
```

## See Also

[Section titled “See Also”](#see-also)

[`cache`](/reference/operators/cache)

# cache

An in-memory cache shared between pipelines.

```tql
cache id:string, [mode=string, capacity=int, read_timeout=duration, write_timeout=duration]
```

Used by the Tenzir Platform

The Tenzir Platform heavily relies on the `cache` operator to make data access faster and more reliable in both the Explorer and on Dashboards.

## Description

[Section titled “Description”](#description)

The `cache` operator caches events in an in-memory buffer at a node. Caches must have a user-provided unique ID.

The first pipeline to use a cache writes into the cache. All further pipelines using the same cache will read from the cache instead of executing the operators before the `cache` operator in the same pipeline.

Cache Policy

Set the `tenzir.cache` configuration section controls how caches behave in Tenzir Nodes:

* `tenzir.cache.lifetime` sets the default write timeout for newly created caches.
* `tenzir.cache.capacity` sets an upper bound for the estimated total memory usage in bytes across all caches in a node. If the memory usage exceeds this limit, the node will start evicting caches to make room for new data. This eviction process happens at most once every 30 seconds. The node requires a minimum total cache capacity of 64MiB.

```yaml
tenzir:
  cache:
    lifetime: 10min
    capacity: 1Gi
```

### `id: string`

[Section titled “id: string”](#id-string)

An arbitrary string that uniquely identifies the cache.

### `mode = string (optional)`

[Section titled “mode = string (optional)”](#mode--string-optional)

Configures whether the operator is used an input, an output, or a transformation. The following modes are available currently:

* `"read"`: The operators acts as an input operator reading from a cache that is requires to already exist.
* `"write"`: The operator acts as an output operator writing into a cache that must not already exist.
* `"readwrite"`: The operator acts as a transformation passing through events, lazily creating a cache if it does not already exist. If a cache exists, upstream operators will not be run and instead the cache is read.

Defaults to `"readwrite"`.

### `capacity = int (optional)`

[Section titled “capacity = int (optional)”](#capacity--int-optional)

Stores how many events the cache can hold. Caches stop accepting events if the capacity is reached and emit a warning.

Defaults to unlimited.

### `read_timeout = duration (optional)`

[Section titled “read\_timeout = duration (optional)”](#read_timeout--duration-optional)

Defines the maximum inactivity time until the cache is evicted from memory. The timer starts when writing the cache completes (or runs into the capacity limit), and resets whenever the cache is read from.

Defaults to `10min`, or the value specified in the `tenzir.cache.lifetime` option.

### `write_timeout = duration (optional)`

[Section titled “write\_timeout = duration (optional)”](#write_timeout--duration-optional)

If set, defines an upper bound for the lifetime of the cache. Unlike the `read_timeout` option, this does not refresh when the cache is accessed.

## Examples

[Section titled “Examples”](#examples)

### Cache the results of an expensive query

[Section titled “Cache the results of an expensive query”](#cache-the-results-of-an-expensive-query)

```tql
export
where @name == "suricata.flow"
summarize total=sum(bytes_toserver), src_ip, dest_ip
cache "some-unique-identifier"
```

### Get high-level statistics about a query

[Section titled “Get high-level statistics about a query”](#get-high-level-statistics-about-a-query)

This calculates the cache again only if the query does not exist anymore, and delete the cache if it’s unused for more than a minute.

```tql
export
where @name == "suricata.flow"
summarize src_ip, total=sum(bytes_toserver), dest_ip
cache "some-unique-identifier", read_timeout=1min
summarize src_ip, total=sum(total), destinations=count(dest_ip)
```

Get the same statistics, assuming the cache still exists:

```tql
cache "some-unique-identifier", mode="read"
summarize src_ip, total=sum(total), destinations=count(dest_ip)
```

## See Also

[Section titled “See Also”](#see-also)

[`buffer`](/reference/operators/buffer)

# chart_area

Plots events on an area chart.

```tql
chart_area x=field, y=any, [x_min=any, x_max=any, y_min=any, y_max=any,
                            resolution=duration, fill=any, x_log=bool,
                            y_log=bool, group=any, position=string]
```

## Description

[Section titled “Description”](#description)

Visualizes events with an area chart on the [Tenzir Platform](https://app.tenzir.com).

### `x = field`

[Section titled “x = field”](#x--field)

Positions on the x-axis for each data point.

### `y = any`

[Section titled “y = any”](#y--any)

Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation).

Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`.

For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`.

### `x_min = any (optional)`

[Section titled “x\_min = any (optional)”](#x_min--any-optional)

If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket.

### `x_max = any (optional)`

[Section titled “x\_max = any (optional)”](#x_max--any-optional)

If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket.

### `y_min = any (optional)`

[Section titled “y\_min = any (optional)”](#y_min--any-optional)

If specified, any `y` values less than `y_min` will appear clipped out of the chart.

### `y_max = any (optional)`

[Section titled “y\_max = any (optional)”](#y_max--any-optional)

If specified, any `y` values greater than `y_max` will appear clipped out of the chart.

### `resolution = duration (optional)`

[Section titled “resolution = duration (optional)”](#resolution--duration-optional)

This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified.

For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket.

### `fill = any (optional)`

[Section titled “fill = any (optional)”](#fill--any-optional)

Optional value to fill gaps and replace `null`s with.

### `x_log = bool (optional)`

[Section titled “x\_log = bool (optional)”](#x_log--bool-optional)

If `true`, use a logarithmic scale for the x-axis.

Defaults to `false`.

### `y_log = bool (optional)`

[Section titled “y\_log = bool (optional)”](#y_log--bool-optional)

If `true`, use a logarithmic scale for the y-axis.

Defaults to `false`.

### `group = any (optional)`

[Section titled “group = any (optional)”](#group--any-optional)

Optional expression to group the aggregations with.

### `position = string (optional)`

[Section titled “position = string (optional)”](#position--string-optional)

Determines how the `y` values are displayed. Possible values:

* `grouped`
* `stacked`

Defaults to `grouped`.

## Examples

[Section titled “Examples”](#examples)

### Chart TCP metrics

[Section titled “Chart TCP metrics”](#chart-tcp-metrics)

This pipeline charts MBs read and written by different pipelines over TCP in hourly intervals for the past 24 hours.

```tql
metrics "tcp"
chart_area x=timestamp,
    y={tx: sum(bytes_written/1M), rx: sum(bytes_read/1M)},
    x_min=now()-1d,
    resolution=1h,
    group=pipeline_id
```

## See Also

[Section titled “See Also”](#see-also)

[`chart_bar`](/reference/operators/chart_bar), [`chart_line`](/reference/operators/chart_line), [`chart_pie`](/reference/operators/chart_pie)

# chart_bar

Plots events on an bar chart.

```tql
chart_bar x|label=field, y|value=any, [x_min=any, x_max=any,
          y_min=any, y_max=any, resolution=duration, fill=any,
          x_log=bool, y_log=bool, group=any, position=string]
```

## Description

[Section titled “Description”](#description)

Visualizes events with an bar chart on the [Tenzir Platform](https://app.tenzir.com).

### `x|label = field`

[Section titled “x|label = field”](#xlabel--field)

Label for each bar.

### `y|value = any`

[Section titled “y|value = any”](#yvalue--any)

Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation).

Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`.

For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`.

### `x_min = any (optional)`

[Section titled “x\_min = any (optional)”](#x_min--any-optional)

If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket.

### `x_max = any (optional)`

[Section titled “x\_max = any (optional)”](#x_max--any-optional)

If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket.

### `y_min = any (optional)`

[Section titled “y\_min = any (optional)”](#y_min--any-optional)

If specified, any `y` values less than `y_min` will appear clipped out of the chart.

### `y_max = any (optional)`

[Section titled “y\_max = any (optional)”](#y_max--any-optional)

If specified, any `y` values greater than `y_max` will appear clipped out of the chart.

### `resolution = duration (optional)`

[Section titled “resolution = duration (optional)”](#resolution--duration-optional)

This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified.

For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket.

### `fill = any (optional)`

[Section titled “fill = any (optional)”](#fill--any-optional)

Optional value to fill gaps and replace `null`s with.

### `x_log = bool (optional)`

[Section titled “x\_log = bool (optional)”](#x_log--bool-optional)

If `true`, use a logarithmic scale for the x-axis.

Defaults to `false`.

### `y_log = bool (optional)`

[Section titled “y\_log = bool (optional)”](#y_log--bool-optional)

If `true`, use a logarithmic scale for the y-axis.

Defaults to `false`.

### `group = any (optional)`

[Section titled “group = any (optional)”](#group--any-optional)

Optional expression to group the aggregations with.

### `position = string (optional)`

[Section titled “position = string (optional)”](#position--string-optional)

Determines how the `y` values are displayed. Possible values:

* `grouped`
* `stacked`

Defaults to `grouped`.

## Examples

[Section titled “Examples”](#examples)

### Chart count of events imported for every unique schema

[Section titled “Chart count of events imported for every unique schema”](#chart-count-of-events-imported-for-every-unique-schema)

```tql
metrics "import"
chart_bar x=schema, y=sum(events), x_min=now()-1d
```

## See Also

[Section titled “See Also”](#see-also)

[`chart_area`](/reference/operators/chart_area), [`chart_line`](/reference/operators/chart_line), [`chart_pie`](/reference/operators/chart_pie)

# chart_line

Plots events on an line chart.

```tql
chart_line x=field, y=any, [x_min=any, x_max=any, y_min=any, y_max=any,
                            resolution=duration, fill=any, x_log=bool,
                            y_log=bool, group=any]
```

## Description

[Section titled “Description”](#description)

Visualizes events with an line chart on the [Tenzir Platform](https://app.tenzir.com).

### `x = field`

[Section titled “x = field”](#x--field)

Positions on the x-axis for each data point.

### `y = any`

[Section titled “y = any”](#y--any)

Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation).

Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`.

For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`.

### `x_min = any (optional)`

[Section titled “x\_min = any (optional)”](#x_min--any-optional)

If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket.

### `x_max = any (optional)`

[Section titled “x\_max = any (optional)”](#x_max--any-optional)

If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket.

### `y_min = any (optional)`

[Section titled “y\_min = any (optional)”](#y_min--any-optional)

If specified, any `y` values less than `y_min` will appear clipped out of the chart.

### `y_max = any (optional)`

[Section titled “y\_max = any (optional)”](#y_max--any-optional)

If specified, any `y` values greater than `y_max` will appear clipped out of the chart.

### `resolution = duration (optional)`

[Section titled “resolution = duration (optional)”](#resolution--duration-optional)

This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified.

For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket.

### `fill = any (optional)`

[Section titled “fill = any (optional)”](#fill--any-optional)

Optional value to fill gaps and replace `null`s with.

### `x_log = bool (optional)`

[Section titled “x\_log = bool (optional)”](#x_log--bool-optional)

If `true`, use a logarithmic scale for the x-axis.

Defaults to `false`.

### `y_log = bool (optional)`

[Section titled “y\_log = bool (optional)”](#y_log--bool-optional)

If `true`, use a logarithmic scale for the y-axis.

Defaults to `false`.

### `group = any (optional)`

[Section titled “group = any (optional)”](#group--any-optional)

Optional expression to group the aggregations with.

## Examples

[Section titled “Examples”](#examples)

### Chart published events

[Section titled “Chart published events”](#chart-published-events)

This pipeline charts number of events published by each pipeline over 30 minute intervals for the past 24 hours.

```tql
metrics "publish"
chart_line x=timestamp,
    y=sum(events),
    x_min=now()-1d,
    group=pipeline_id,
    resolution=30min
```

## See Also

[Section titled “See Also”](#see-also)

[`chart_area`](/reference/operators/chart_area), [`chart_bar`](/reference/operators/chart_bar), [`chart_pie`](/reference/operators/chart_pie)

# chart_pie

Plots events on an pie chart.

```tql
chart_pie x|label=field, y|value=any, [group=any]
```

## Description

[Section titled “Description”](#description)

Visualizes events with an pie chart on the [Tenzir Platform](https://app.tenzir.com).

### `x|label = field`

[Section titled “x|label = field”](#xlabel--field)

Name of each slice on the chart.

### `y|value = any`

[Section titled “y|value = any”](#yvalue--any)

Value of each slice on the chart. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation).

Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`.

For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`.

### `group = any (optional)`

[Section titled “group = any (optional)”](#group--any-optional)

Optional expression to group the aggregations with.

## Examples

[Section titled “Examples”](#examples)

### Chart count of events imported for every unique schema

[Section titled “Chart count of events imported for every unique schema”](#chart-count-of-events-imported-for-every-unique-schema)

```tql
metrics "import"
where timestamp > now() - 1d
chart_pie label=schema, value=sum(events)
```

## See Also

[Section titled “See Also”](#see-also)

[`chart_area`](/reference/operators/chart_area), [`chart_bar`](/reference/operators/chart_bar), [`chart_line`](/reference/operators/chart_line)

# compress

Compresses a stream of bytes.

```tql
compress codec:string, [level=int]
```

Deprecated

The `compress` operator is deprecated. You should use the [bespoke operators](/reference/operators#encode--decode) instead. These operators offer more options for some of the formats.

## Description

[Section titled “Description”](#description)

The `compress` operator compresses bytes in a pipeline incrementally with a known codec.

Streaming Compression

The operator uses [Apache Arrow’s compression utilities](https://arrow.apache.org/docs/cpp/api/utilities.html#compression) under the hood, and transparently supports all options that Apache Arrow supports for streaming compression.

Besides the supported `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`, Apache Arrow also ships with codecs for `lzo`, `lz4_raw`, `lz4_hadoop` and `snappy`, which only support oneshot compression. Support for them is not currently implemented.

### `codec: string`

[Section titled “codec: string”](#codec-string)

An identifier of the codec to use. Currently supported are `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`.

### `level = int (optional)`

[Section titled “level = int (optional)”](#level--int-optional)

The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used.

## Examples

[Section titled “Examples”](#examples)

### Export all events in a Gzip-compressed NDJSON file

[Section titled “Export all events in a Gzip-compressed NDJSON file”](#export-all-events-in-a-gzip-compressed-ndjson-file)

```tql
export
write_ndjson
compress "gzip"
save_file "/tmp/backup.json.gz"
```

### Recompress a Zstd-compressed file at a higher compression level

[Section titled “Recompress a Zstd-compressed file at a higher compression level”](#recompress-a-zstd-compressed-file-at-a-higher-compression-level)

```tql
load_file "in.zst"
decompress "zstd"
compress "zstd", level=18
save_file "out.zst"
```

# compress_brotli

Compresses a stream of bytes using Brotli compression.

```tql
compress_brotli [level=int, window_bits=int]
```

## Description

[Section titled “Description”](#description)

The `compress_brotli` operator compresses bytes in a pipeline incrementally.

### `level = int (optional)`

[Section titled “level = int (optional)”](#level--int-optional)

The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used.

### `window_bits = int (optional)`

[Section titled “window\_bits = int (optional)”](#window_bits--int-optional)

A number representing the encoder window bits.

## Examples

[Section titled “Examples”](#examples)

### Export all events in a Brotli-compressed NDJSON file

[Section titled “Export all events in a Brotli-compressed NDJSON file”](#export-all-events-in-a-brotli-compressed-ndjson-file)

```tql
export
write_ndjson
compress_brotli
save_file "/tmp/backup.json.bt"
```

### Recompress a Brotli-compressed file at a different compression level

[Section titled “Recompress a Brotli-compressed file at a different compression level”](#recompress-a-brotli-compressed-file-at-a-different-compression-level)

```tql
load_file "in.brotli"
decompress_brotli
compress_brotli level=18
save_file "out.brotli"
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_brotli`](/reference/operators/decompress_brotli)

# compress_bz2

Compresses a stream of bytes using bz2 compression.

```tql
compress_bz2 [level=int]
```

## Description

[Section titled “Description”](#description)

The `compress_bz2` operator compresses bytes in a pipeline incrementally.

### `level = int (optional)`

[Section titled “level = int (optional)”](#level--int-optional)

The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used.

## Examples

[Section titled “Examples”](#examples)

### Export all events in a Bzip2-compressed NDJSON file

[Section titled “Export all events in a Bzip2-compressed NDJSON file”](#export-all-events-in-a-bzip2-compressed-ndjson-file)

```tql
export
write_ndjson
compress_bz2
save_file "/tmp/backup.json.bz2"
```

### Recompress a Bzip2-compressed file at a different compression level

[Section titled “Recompress a Bzip2-compressed file at a different compression level”](#recompress-a-bzip2-compressed-file-at-a-different-compression-level)

```tql
load_file "in.bz2"
decompress_bz2
compress_bz2 level=18
save_file "out.bz2"
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_brotli`](/reference/operators/compress_brotli), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_bz2`](/reference/operators/decompress_bz2)

# compress_gzip

Compresses a stream of bytes using gzip compression.

```tql
compress_gzip [level=int, window_bits=int, format=string]
```

## Description

[Section titled “Description”](#description)

The `compress_gzip` operator compresses bytes in a pipeline incrementally.

### `level = int (optional)`

[Section titled “level = int (optional)”](#level--int-optional)

The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used.

### `window_bits = int (optional)`

[Section titled “window\_bits = int (optional)”](#window_bits--int-optional)

A number representing the encoder window bits.

### `format = string (optional)`

[Section titled “format = string (optional)”](#format--string-optional)

A string representing the used format. Possible values are `zlib`, `deflate` and `gzip`.

Defaults to `gzip`.

## Examples

[Section titled “Examples”](#examples)

### Export all events in a Gzip-compressed NDJSON file

[Section titled “Export all events in a Gzip-compressed NDJSON file”](#export-all-events-in-a-gzip-compressed-ndjson-file)

```tql
export
write_ndjson
compress_gzip
save_file "/tmp/backup.json.gz"
```

### Compress using Gzip deflate

[Section titled “Compress using Gzip deflate”](#compress-using-gzip-deflate)

```tql
export
write_ndjson
compress_gzip format="deflate"
```

### Recompress a Gzip-compressed file at a different compression level

[Section titled “Recompress a Gzip-compressed file at a different compression level”](#recompress-a-gzip-compressed-file-at-a-different-compression-level)

```tql
load_file "in.gzip"
decompress_gzip
compress_gzip level=18
save_file "out.gzip"
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_gzip`](/reference/operators/decompress_gzip)

# compress_lz4

Compresses a stream of bytes using lz4 compression.

```tql
compress_lz4 [level=int]
```

## Description

[Section titled “Description”](#description)

The `compress_lz4` operator compresses bytes in a pipeline incrementally.

### `level = int (optional)`

[Section titled “level = int (optional)”](#level--int-optional)

The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used.

## Examples

[Section titled “Examples”](#examples)

### Export all events in a Lz4-compressed NDJSON file

[Section titled “Export all events in a Lz4-compressed NDJSON file”](#export-all-events-in-a-lz4-compressed-ndjson-file)

```tql
export
write_ndjson
compress_lz4
save_file "/tmp/backup.json.lz4"
```

### Recompress a Lz4-compressed file at a different compression level

[Section titled “Recompress a Lz4-compressed file at a different compression level”](#recompress-a-lz4-compressed-file-at-a-different-compression-level)

```tql
load_file "in.lz4"
decompress_lz4
compress_lz4 level=18
save_file "out.lz4"
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_lz4`](/reference/operators/decompress_lz4)

# compress_zstd

Compresses a stream of bytes using zstd compression.

```tql
compress_zstd [level=int]
```

## Description

[Section titled “Description”](#description)

The `compress_zstd` operator compresses bytes in a pipeline incrementally.

### `level = int (optional)`

[Section titled “level = int (optional)”](#level--int-optional)

The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used.

## Examples

[Section titled “Examples”](#examples)

### Export all events in a Zstd-compressed NDJSON file

[Section titled “Export all events in a Zstd-compressed NDJSON file”](#export-all-events-in-a-zstd-compressed-ndjson-file)

```tql
export
write_ndjson
compress_zstd
save_file "/tmp/backup.json.zstd"
```

### Recompress a Zstd-compressed file at a different compression level

[Section titled “Recompress a Zstd-compressed file at a different compression level”](#recompress-a-zstd-compressed-file-at-a-different-compression-level)

```tql
load_file "in.zstd"
decompress_zstd
compress_zstd level=18
save_file "out.zstd"
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# context::create_bloom_filter

Creates a Bloom filter context.

```tql
context::create_bloom_filter name:string, capacity=int, fp_probability=float
```

## Description

[Section titled “Description”](#description)

The `context::create_bloom_filter` operator constructs a new context of type [Bloom filter](/explanations/enrichment#bloom-filter).

To find suitable values for the capacity and false-positive probability, consult Thomas Hurst’s [Bloom Filter Calculator](https://hur.st/bloomfilter/). The parameter `n` corresponds to `capacity` and `p` to `fp_probability`.

You can also create a Bloom filter context as code by adding it to `tenzir.contexts` in your `tenzir.yaml`:

\<prefix>/etc/tenzir/tenzir.yaml

```yaml
tenzir:
  contexts:
    my-iocs:
      type: bloom-filter
      arguments:
        capacity: 1B
        fp-probability: 0.001
```

Making changes to `arguments` of an already created context has no effect.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the new Bloom filter.

### `capacity = uint`

[Section titled “capacity = uint”](#capacity--uint)

The maximum number of items in the filter that maintain the false positive probability. Adding more elements does not yield an error, but lookups will more likely return false positives.

### `fp_probability = float`

[Section titled “fp\_probability = float”](#fp_probability--float)

The false-positive probability of the Bloom filter.

## Examples

[Section titled “Examples”](#examples)

### Create a new Bloom filter context

[Section titled “Create a new Bloom filter context”](#create-a-new-bloom-filter-context)

```tql
context::create_bloom_filter "ctx", capacity=1B, fp_probability=0.001
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::create_geoip

Creates a GeoIP context.

```tql
context::create_geoip name:string, [db_path=string]
```

## Description

[Section titled “Description”](#description)

The `context::create_geoip` operator constructs a new context of type [GeoIP](/explanations/enrichment#geoip-database).

You must either provide a database with the `db_path` argument or use [`context::load`](/reference/operators/context/load) to populate the context after creation.

You can also create a GeoIP context as code by adding it to `tenzir.contexts` in your `tenzir.yaml`:

\<prefix>/etc/tenzir/tenzir.yaml

```yaml
tenzir:
  contexts:
    my-geoips:
      type: geoip
      arguments:
        db-path: /usr/local/share/stuff/high-res-geoips.mmdb
```

Making changes to `arguments` of an already created context has no effect.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the new GeoIP context.

### `db_path = string (optional)`

[Section titled “db\_path = string (optional)”](#db_path--string-optional)

The path to the [MMDB](https://maxmind.github.io/MaxMind-DB/) database, relative to the node’s working directory.

## Examples

[Section titled “Examples”](#examples)

### Create a new GeoIP context

[Section titled “Create a new GeoIP context”](#create-a-new-geoip-context)

```tql
context::create_geoip "ctx", db_path="GeoLite2-City.mmdb"
```

### Populate a GeoIP context from a remote location

[Section titled “Populate a GeoIP context from a remote location”](#populate-a-geoip-context-from-a-remote-location)

Load [CIRCL’s Geo Open](https://data.public.lu/en/datasets/geo-open-ip-address-geolocation-per-country-in-mmdb-format/) dataset from November 12, 2024:

```tql
load_http "https://data.public.lu/fr/datasets/r/69064b5d-bf46-4244-b752-2096b16917a4"
context::load "ctx"
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::create_lookup_table

Creates a lookup table context.

```tql
context::create_lookup_table name:string
```

## Description

[Section titled “Description”](#description)

The `context::create_lookup_table` operator constructs a new context of type [lookup table](/explanations/enrichment#lookup-table).

You can also create a lookup table as code by adding it to `tenzir.contexts` in your `tenzir.yaml`:

\<prefix>/etc/tenzir/tenzir.yaml

```yaml
tenzir:
  contexts:
    my-table:
      type: lookup-table
```

### `name: string`

[Section titled “name: string”](#name-string)

The name of the new lookup table.

## Examples

[Section titled “Examples”](#examples)

### Create a new lookup table context

[Section titled “Create a new lookup table context”](#create-a-new-lookup-table-context)

```tql
context::create_lookup_table "ctx"
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::enrich

Enriches events with data from a context.

```tql
context::enrich name:string, key=any,
               [into=field, mode=string, format=string]
```

## Description

[Section titled “Description”](#description)

The `context::enrich` operator enriches events with data from a context.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the context to enrich with.

### `key = any`

[Section titled “key = any”](#key--any)

The field to use for the context lookup.

### `into = field (optional)`

[Section titled “into = field (optional)”](#into--field-optional)

The field into which to write the enrichment.

Defaults to the context name (`name`).

### `mode = string (optional)`

[Section titled “mode = string (optional)”](#mode--string-optional)

The mode of the enrichment operation:

* `set`: overwrites the field specified by `into`.
* `append`: appends into the list specified by `into`. If `into` is `null` or an `empty` list, a new list is created. If `into` is not a list, the enrichment will fail with a warning.

Defaults to `set`.

### `format = string (optional)`

[Section titled “format = string (optional)”](#format--string-optional)

The style of the enriched value:

* `plain`: formats the enrichment as retrieved from the context.
* `ocsf`: formats the enrichment as an [OCSF Enrichment](https://schema.ocsf.io/1.4.0-dev/objects/enrichment?extensions=) object with fields `data`, `provider`, `type`, and `value`.

Defaults to `plain`.

## Examples

[Section titled “Examples”](#examples)

### Enrich with a lookup table

[Section titled “Enrich with a lookup table”](#enrich-with-a-lookup-table)

Create a lookup table:

```tql
context::create_lookup_table "ctx"
```

Add data to the lookup table:

```tql
from {x:1, y:"a"},
     {x:2, y:"b"}
context::update "ctx", key=x, value=y
```

Enrich with the table:

```tql
from {x:1}
context::enrich "ctx", key=x
```

```tql
{
  x: 1,
  ctx: "a",
}
```

### Enrich as OCSF Enrichment

[Section titled “Enrich as OCSF Enrichment”](#enrich-as-ocsf-enrichment)

Assume the same table preparation as above, but followed by a different call to `context::enrich` using the `format` option:

```tql
from {x:1}
context::enrich "ctx", key=x, format="ocsf"
```

```tql
{
  x: 1,
  ctx: {
    created_time: 2024-11-18T16:35:48.069981,
    name: "x",
    value: 1,
    data: "a",
  }
}
```

### Enrich by appending to an array

[Section titled “Enrich by appending to an array”](#enrich-by-appending-to-an-array)

Enrich twice with the same context and accumulate enrichments into an array:

```tql
from {x:1}
context::enrich "ctx", key=x, into=enrichments, mode="append"
context::enrich "ctx", key=x, into=enrichments, mode="append"
```

```tql
{
  x: 1,
  enrichments: [
    "a",
    "a",
  ]
}
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::erase

Removes entries from a context.

```tql
context::erase name:string, key=any
```

## Description

[Section titled “Description”](#description)

The `context::erase` operator removes data from a context.

Use the `key` argument to specify the field in the input that should be deleted from the context.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the context to remove entries from.

### `key = any`

[Section titled “key = any”](#key--any)

The field that represents the enrichment key in the data.

## Examples

[Section titled “Examples”](#examples)

### Delete entries from a context

[Section titled “Delete entries from a context”](#delete-entries-from-a-context)

```plaintext
from {network: 10.0.0.1/16}
context::erase "network-classification", key=network
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::inspect

Resets a context.

```tql
context::inspect name:string
```

## Description

[Section titled “Description”](#description)

The `context::inspect` operator shows details about a specified context.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the context to inspect.

## Examples

[Section titled “Examples”](#examples)

### Inspect a context

[Section titled “Inspect a context”](#inspect-a-context)

Add data to the lookup table:

```tql
from {x:1, y:"a"},
     {x:2, y:"b"}
context::update "ctx", key=x, value=y
```

Retrieve the lookup table contents:

```tql
context::inspect "ctx"
```

```tql
{key: 2, value: "b"}
{key: 1, value: "a"}
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::erase`](/reference/operators/context/enrich), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::list

Lists all contexts

```tql
context::list
```

## Description

[Section titled “Description”](#description)

The `context::list` operator retrieves all contexts.

## Examples

[Section titled “Examples”](#examples)

### Show all contexts

[Section titled “Show all contexts”](#show-all-contexts)

```tql
context::list
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::load

Loads context state.

```tql
context::load name:string
```

## Description

[Section titled “Description”](#description)

The `context::load` operator replaces the state of the specified context with its (binary) input.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the context whose state to update.

## Examples

[Section titled “Examples”](#examples)

### Replace the database of a GeoIP context

[Section titled “Replace the database of a GeoIP context”](#replace-the-database-of-a-geoip-context)

```tql
load_file "ultra-high-res.mmdb", mmap=true
context::load "ctx"
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::remove

Deletes a context.

```tql
context::remove name:string
```

## Description

[Section titled “Description”](#description)

The `context::remove` operator deletes the specified context.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the context to delete.

## Examples

[Section titled “Examples”](#examples)

### Delete a context

[Section titled “Delete a context”](#delete-a-context)

```tql
context::delete "ctx"
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::reset

Resets a context.

```tql
context::reset name:string
```

## Description

[Section titled “Description”](#description)

The `context::reset` operator erases all data that has been added with `context::update`.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the context to reset.

## Examples

[Section titled “Examples”](#examples)

### Reset a context

[Section titled “Reset a context”](#reset-a-context)

```tql
context::reset "ctx"
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_geoip`](/reference/operators/context/create_geoip), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::save

Saves context state.

```tql
context::save name:string
```

## Description

[Section titled “Description”](#description)

The `context::save` operator dumps the state of the specified context into its (binary) output.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the context whose state to save.

## Examples

[Section titled “Examples”](#examples)

### Store the database of a GeoIP context

[Section titled “Store the database of a GeoIP context”](#store-the-database-of-a-geoip-context)

```tql
context::save "ctx"
save_file "snapshot.mmdb"
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::update

Updates a context with new data.

```tql
context::update name:string, key=any,
               [value=any, create_timeout=duration,
                write_timeout=duration, read_timeout=duration]
```

## Description

[Section titled “Description”](#description)

The `context::update` operator adds new data to a specified context.

Use the `key` argument to specify the field in the input that should be associated with the context. The [`context::enrich`](/reference/operators/context/enrich) operator uses this key to access the context. For contexts that support assigning a value with a given key, you can provide an expression to customize what’s being associated with the given key.

The three arguments `create_timeout`, `write_timeout`, and `read_timeout` only work with lookup tables and set the respective timeouts per table entry.

### `name: string`

[Section titled “name: string”](#name-string)

The name of the context to update.

### `key = any`

[Section titled “key = any”](#key--any)

The field that represents the enrichment key in the data.

### `value = any (optional)`

[Section titled “value = any (optional)”](#value--any-optional)

The field that represents the enrichment value to associate with `key`.

Defaults to `this`.

### `create_timeout = duration (optional)`

[Section titled “create\_timeout = duration (optional)”](#create_timeout--duration-optional)

Expires a context entry after a given duration since entry creation.

### `write_timeout = duration (optional)`

[Section titled “write\_timeout = duration (optional)”](#write_timeout--duration-optional)

Expires a context entry after a given duration since the last update time. Every Every call to `context::update` resets the timeout for the respective key.

### `read_timeout = duration (optional)`

[Section titled “read\_timeout = duration (optional)”](#read_timeout--duration-optional)

Expires a context entry after a given duration since the last access time. Every call to `context::enrich` resets the timeout for the respective key.

## Examples

[Section titled “Examples”](#examples)

### Populate a lookup table with data

[Section titled “Populate a lookup table with data”](#populate-a-lookup-table-with-data)

Create a lookup table:

```tql
context::create_lookup_table "ctx"
```

Add data to the lookup table via `context::update`:

```tql
from {x:1, y:"a"},
     {x:2, y:"b"}
context::update "ctx", key=x, value=y
```

Retrieve the lookup table contents:

```tql
context::inspect "ctx"
```

```tql
{key: 2, value: "b"}
{key: 1, value: "a"}
```

### Use a custom value as lookup table

[Section titled “Use a custom value as lookup table”](#use-a-custom-value-as-lookup-table)

```tql
from {x:1},
     {x:2}
context::update "ctx", key=x, value=x*x
```

```tql
{key: 2, value: 4}
{key: 1, value: 1}
```

## See Also

[Section titled “See Also”](#see-also)

[`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list)

# cron

Runs a pipeline periodically according to a cron expression.

```tql
cron schedule:string { … }
```

## Description

[Section titled “Description”](#description)

The `cron` operator performs scheduled execution of a pipeline indefinitely according to a [cron expression](https://en.wikipedia.org/wiki/Cron).

The executor spawns a new pipeline according to the cadence given by `schedule`. If the pipeline runs longer than the interval to the next scheduled time point, the next run immediately starts.

### `schedule: string`

[Section titled “schedule: string”](#schedule-string)

The cron expression with the following syntax:

```plaintext
<seconds> <minutes> <hours> <days of month> <months> <days of week>
```

The 6 fields are separated by a space. Allowed values for each field are:

| Field        | Value range\* | Special characters      | Alternative Literals |
| ------------ | ------------- | ----------------------- | -------------------- |
| seconds      | 0-59          | `*` `,` `-`             |                      |
| minutes      | 0-59          | `*` `,` `-`             |                      |
| hours        | 0-23          | `*` `,` `-`             |                      |
| days of      | 1-31          | `*` `,` `-` `?` `L` `W` |                      |
| months       | 1-12          | `*` `,` `-`             | `JAN` … `DEC`        |
| days of week | 0-6           | `*` `,` `-` `?` `L` `#` | `SUN` … `SAT`        |

The special characters have the following meaning:

| Special character | Meaning           | Description                                       |
| ----------------- | ----------------- | ------------------------------------------------- |
| `*`               | all values        | selects all values within a field                 |
| `?`               | no specific value | specify one field and leave the other unspecified |
| `-`               | range             | specify ranges                                    |
| `,`               | comma             | specify additional values                         |
| `/`               | slash             | specify increments                                |
| `L`               | last              | last day of the month or last day of the week     |
| `W`               | weekday           | the weekday nearest to the given day              |
| `#`               | nth               | specify the Nth day of the month                  |

## Examples

[Section titled “Examples”](#examples)

### Fetch the results from an API every 10 minutes

[Section titled “Fetch the results from an API every 10 minutes”](#fetch-the-results-from-an-api-every-10-minutes)

Pull an endpoint on every 10th minute, Monday through Friday:

```tql
cron "* */10 * * * MON-FRI" {
  from "https://example.org/api"
}
publish "api"
```

## See Also

[Section titled “See Also”](#see-also)

[`every`](/reference/operators/every)

# decompress

Decompresses a stream of bytes.

```tql
decompress codec:string
```

Deprecated

The `decompress` operator is deprecated. You should use the [bespoke operators](/reference/operators#encode--decode) instead.

## Description

[Section titled “Description”](#description)

The `decompress` operator decompresses bytes in a pipeline incrementally with a known codec. The operator supports decompressing multiple concatenated streams of the same codec transparently.

Streaming Decompression

The operator uses [Apache Arrow’s compression utilities](https://arrow.apache.org/docs/cpp/api/utilities.html#compression) under the hood, and transparently supports all options that Apache Arrow supports for streaming decompression.

Besides the supported `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`, Apache Arrow also ships with codecs for `lzo`, `lz4_raw`, `lz4_hadoop` and `snappy`, which only support oneshot decompression. Support for them is not currently implemented.

### `codec: string`

[Section titled “codec: string”](#codec-string)

An identifier of the codec to use. Currently supported are `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`.

## Examples

[Section titled “Examples”](#examples)

### Import Suricata events from a Zstd-compressed file

[Section titled “Import Suricata events from a Zstd-compressed file”](#import-suricata-events-from-a-zstd-compressed-file)

```tql
load_file "eve.json.zst"
decompress "zstd"
read_suricata
import
```

### Convert a Zstd-compressed file into an LZ4-compressed file

[Section titled “Convert a Zstd-compressed file into an LZ4-compressed file”](#convert-a-zstd-compressed-file-into-an-lz4-compressed-file)

```tql
load_file "in.zst"
decompress "zstd"
compress "lz4"
save_file "out.lz4"
```

# decompress_brotli

Decompresses a stream of bytes in the Brotli format.

```tql
decompress_brotli
```

## Description

[Section titled “Description”](#description)

The `decompress_brotli` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently.

## Examples

[Section titled “Examples”](#examples)

### Import Suricata events from a Brotli-compressed file

[Section titled “Import Suricata events from a Brotli-compressed file”](#import-suricata-events-from-a-brotli-compressed-file)

```tql
load_file "eve.json.brotli"
decompress_brotli
read_suricata
import
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_brotli`](/reference/operators/compress_brotli), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_bz2

Decompresses a stream of bytes in the Bzip2 format.

```tql
decompress_bz2
```

## Description

[Section titled “Description”](#description)

The `decompress_bz2` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently.

## Examples

[Section titled “Examples”](#examples)

### Import Suricata events from a Bzip2-compressed file

[Section titled “Import Suricata events from a Bzip2-compressed file”](#import-suricata-events-from-a-bzip2-compressed-file)

```tql
load_file "eve.json.bz"
decompress_bz2
read_suricata
import
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_bz2`](/reference/operators/compress_bz2), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_gzip

Decompresses a stream of bytes in the Gzip format.

```tql
decompress_gzip
```

## Description

[Section titled “Description”](#description)

The `decompress_gzip` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently.

## Examples

[Section titled “Examples”](#examples)

### Import Suricata events from a Gzip-compressed file

[Section titled “Import Suricata events from a Gzip-compressed file”](#import-suricata-events-from-a-gzip-compressed-file)

```tql
load_file "eve.json.gz"
decompress_brotli
decompress_gzip
import
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_gzip`](/reference/operators/compress_gzip), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_lz4

Decompresses a stream of bytes in the Lz4 format.

```tql
decompress_lz4
```

## Description

[Section titled “Description”](#description)

The `decompress_lz4` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently.

## Examples

[Section titled “Examples”](#examples)

### Import Suricata events from a LZ4-compressed file

[Section titled “Import Suricata events from a LZ4-compressed file”](#import-suricata-events-from-a-lz4-compressed-file)

```tql
load_file "eve.json.lz4"
decompress_lz4
read_suricata
import
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_lz4`](/reference/operators/compress_lz4), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_zstd

Decompresses a stream of bytes in the Zstd format.

```tql
decompress_zstd
```

## Description

[Section titled “Description”](#description)

The `decompress_zstd` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently.

## Examples

[Section titled “Examples”](#examples)

### Import Suricata events from a Zstd-compressed file

[Section titled “Import Suricata events from a Zstd-compressed file”](#import-suricata-events-from-a-zstd-compressed-file)

```tql
load_file "eve.json.zstd"
decompress_zstd
read_suricata
import
```

## See Also

[Section titled “See Also”](#see-also)

[`compress_zstd`](/reference/operators/compress_zstd), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# deduplicate

Removes duplicate events based on a common key.

```tql
deduplicate [key:any, limit=int, distance=int, create_timeout=duration,
             write_timeout=duration, read_timeout=duration]
```

## Description

[Section titled “Description”](#description)

The `deduplicate` operator removes duplicates from a stream of events, based on the value of one or more fields.

### `key: any (optional)`

[Section titled “key: any (optional)”](#key-any-optional)

The key to deduplicate. To deduplicate multiple fields, use a record expression like `{foo: bar, baz: qux}`.

Defaults to `this`, i.e., deduplicating entire events.

### `limit = int (optional)`

[Section titled “limit = int (optional)”](#limit--int-optional)

The number of duplicate keys allowed before an event is suppressed.

Defaults to `1`, which is equivalent to removing all duplicates.

### `distance = int (optional)`

[Section titled “distance = int (optional)”](#distance--int-optional)

Distance between two events that can be considered duplicates. A value of `1` means that only adjacent events can be considered duplicate.

When unspecified, the distance is infinite.

### `create_timeout = duration (optional)`

[Section titled “create\_timeout = duration (optional)”](#create_timeout--duration-optional)

The time that needs to pass until a surpressed event is no longer considered a duplicate. The timeout resets when the first event for a given key is let through.

### `write_timeout = duration (optional)`

[Section titled “write\_timeout = duration (optional)”](#write_timeout--duration-optional)

The time that needs to pass until a suppressed event is no longer considered a duplicate. The timeout resets when any event for a given key is let through.

For a limit of `1`, the write timeout is equivalent to the create timeout.

The write timeout must be smaller than the create timeout.

### `read_timeout = duration (optional)`

[Section titled “read\_timeout = duration (optional)”](#read_timeout--duration-optional)

The time that needs to pass until a suppressed event is no longer considered a duplicate. The timeout resets when a key is seen, even if the event is suppressed.

The read timeout must be smaller than the write and create timeouts.

## Examples

[Section titled “Examples”](#examples)

### Simple deduplication

[Section titled “Simple deduplication”](#simple-deduplication)

Consider the following data:

```tql
{foo: 1, bar: "a"}
{foo: 1, bar: "a"}
{foo: 1, bar: "a"}
{foo: 1, bar: "b"}
{foo: null, bar: "b"}
{bar: "b"}
{foo: null, bar: "b"}
{foo: null, bar: "b"}
```

For `deduplicate`, all duplicate events are removed:

```tql
{foo: 1, bar: "a"}
{foo: 1, bar: "b"}
{foo: null, bar: "b"}
{bar: "b"}
```

If `deduplicate bar` is used, only the field `bar` is considered when determining whether an event is a duplicate:

```tql
{foo: 1, bar: "a"}
{foo: 1, bar: "b"}
```

And for `deduplicate foo`, only the field `foo` is considered. Note, how the missing `foo` field is treated as if it had the value `null`, i.e., it’s not included in the output.

```tql
{foo: 1, bar: "a"}
{foo: null, bar: "b"}
```

### Get up to 10 warnings per hour for each run of a pipeline

[Section titled “Get up to 10 warnings per hour for each run of a pipeline”](#get-up-to-10-warnings-per-hour-for-each-run-of-a-pipeline)

```tql
diagnostics live=true
deduplicate {id: pipeline_id, run: run}, limit=10, create_timeout=1h
```

### Get an event whenever the node disconnected from the Tenzir Platform

[Section titled “Get an event whenever the node disconnected from the Tenzir Platform”](#get-an-event-whenever-the-node-disconnected-from-the-tenzir-platform)

```tql
metrics "platform", live=true
deduplicate connected, distance=1
where not connected
```

## See Also

[Section titled “See Also”](#see-also)

[`sample`](/reference/operators/sample)

# delay

Delays events relative to a given start time, with an optional speedup.

```tql
delay field:time, [start=time, speed=double]
```

## Description

[Section titled “Description”](#description)

The `delay` operator replays a dataflow according to a time field by introducing sleeping periods proportional to the inter-arrival times of the events.

With the `speed` option, you can adjust the sleep time of the time series induced by `field` with a multiplicative factor. This has the effect of making the time series “faster” for values great than 1 and “slower” for values less than 1. Unless you provide a start time with `start`, the operator will anchor the timestamps in `field` to begin with the current wall clock time, as if you provided `start=now()`.

The diagram below illustrates the effect of applying `delay` to dataflow. If an event in the stream has a timestamp the precedes the previous event, `delay` emits it instantly. Otherwise `delay` sleeps the amount of time to reach the next timestamp. As shown in the last illustration, the `speed` factor has a scaling effect on the inter-arrival times.

![Delay](/_astro/delay.excalidraw.BSSAawE0_19DKCs.svg)

The options `start` and `speed` work independently, i.e., you can use them separately or both together.

### `field: time`

[Section titled “field: time”](#field-time)

The field in the event containing the timestamp values.

### `start = time (optional)`

[Section titled “start = time (optional)”](#start--time-optional)

The timestamp to anchor the time values around.

Defaults to the first non-null timestamp in `field`.

### `speed = double (optional)`

[Section titled “speed = double (optional)”](#speed--double-optional)

A constant factor to be divided by the inter-arrival time. For example, 2.0 decreases the event gaps by a factor of two, resulting a twice as fast dataflow. A value of 0.1 creates dataflow that spans ten times the original time frame.

Defaults to 1.0.

## Examples

[Section titled “Examples”](#examples)

### Replay logs in real time

[Section titled “Replay logs in real time”](#replay-logs-in-real-time)

Replay the M57 Zeek logs with real-world inter-arrival times from the `ts` field. For example, if an event arrives at time *t* and the next event at time *u*, then the `delay` operator will wait time *u - t* between emitting the two events. If *t > u* then the operator immediately emits next event.

```tql
load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst"
read_zeek_tsv
delay ts
```

### Replay logs at 10.5 times the original speed

[Section titled “Replay logs at 10.5 times the original speed”](#replay-logs-at-105-times-the-original-speed)

```tql
load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst"
read_zeek_tsv
delay ts, speed=10.5
```

### Replay and delay after a given timestamp

[Section titled “Replay and delay after a given timestamp”](#replay-and-delay-after-a-given-timestamp)

Replay and start delaying only after `ts` exceeds `2021-11-17T16:35` and emit all events prior to that timestamp immediately.

```tql
load_file "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst"
read_zeek_tsv
delay ts, start=2021-11-17T16:35, speed=10.0
```

Adjust the timestamp to the present, and then start replaying in 2 hours from now:

```tql
load_file "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst"
decompress "zstd"
read_zeek_tsv
timeshift ts
delay ts, start=now()+2h
```

## See Also

[Section titled “See Also”](#see-also)

[`timeshift`](/reference/operators/timeshift)

# diagnostics

Retrieves diagnostic events from a Tenzir node.

```tql
diagnostics [live=bool, retro=bool]
```

## Description

[Section titled “Description”](#description)

The `diagnostics` operator retrieves diagnostic events from a Tenzir node.

Retention Policy

Set the `tenzir.retention.diagnostics` configuration option to change how long Tenzir Nodes store diagnostics:

```yaml
tenzir:
  retention:
    diagnostics: 30d
```

### `live = bool (optional)`

[Section titled “live = bool (optional)”](#live--bool-optional)

If `true`, emits diagnostic events as they are generated in real-time. Unless `retro=true` is also given, this makes it so that previous diagnostics events are not returned.

### `retro = bool (optional)`

[Section titled “retro = bool (optional)”](#retro--bool-optional)

Return diagnostic events that were generated in the past. Unless `live=true` is given, this is the default. If both are set to `true`, all previous events are returned before beginning with the live events.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir emits diagnostic information with the following schema:

### `tenzir.diagnostic`

[Section titled “tenzir.diagnostic”](#tenzirdiagnostic)

Contains detailed information about the diagnostic.

| Field         | Type           | Description                                                                                |
| :------------ | :------------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id` | `string`       | The ID of the pipeline that created the diagnostic.                                        |
| `hidden`      | `bool`         | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `run`         | `uint64`       | The number of the run, starting at 1 for the first run.                                    |
| `timestamp`   | `time`         | The exact timestamp of the diagnostic creation.                                            |
| `message`     | `string`       | The diagnostic message.                                                                    |
| `severity`    | `string`       | The diagnostic severity.                                                                   |
| `notes`       | `list<record>` | The diagnostic notes. Can be empty.                                                        |
| `annotations` | `list<record>` | The diagnostic annotations. Can be empty.                                                  |
| `rendered`    | `string`       | The rendered diagnostic, as printed on the command-line.                                   |

The records in `notes` have the following schema:

| Field     | Type     | Description                                                   |
| :-------- | :------- | :------------------------------------------------------------ |
| `kind`    | `string` | The kind of note, which is `note`, `usage`, `hint` or `docs`. |
| `message` | `string` | The message of this note.                                     |

The records in `annotations` have the following schema:

| Field     | Type     | Description                                                                                                  |
| :-------- | :------- | :----------------------------------------------------------------------------------------------------------- |
| `primary` | `bool`   | True if the `source` represents the underlying reason for the diagnostic, false if it is only related to it. |
| `text`    | `string` | A message for explanations. Can be empty.                                                                    |
| `source`  | `string` | The character range in the pipeline string that this annotation is associated to.                            |

## Examples

[Section titled “Examples”](#examples)

### View all diagnostics generated in the past 5 minutes

[Section titled “View all diagnostics generated in the past 5 minutes”](#view-all-diagnostics-generated-in-the-past-5-minutes)

```tql
diagnostics
where timestamp > now() - 5min
```

### Get a live feed of error diagnostics

[Section titled “Get a live feed of error diagnostics”](#get-a-live-feed-of-error-diagnostics)

```tql
diagnostics live=true
where severity == "error"
```

## See Also

[Section titled “See Also”](#see-also)

[`metrics`](/reference/operators/metrics)

# discard

Discards all incoming events.

```tql
discard
```

## Description

[Section titled “Description”](#description)

The `discard` operator discards all the incoming events immediately, without rendering them or doing any additional processing.

This operator is mainly used to test or benchmark pipelines.

## Examples

[Section titled “Examples”](#examples)

### Benchmark to see how long it takes to export everything

[Section titled “Benchmark to see how long it takes to export everything”](#benchmark-to-see-how-long-it-takes-to-export-everything)

```tql
export
discard
```

# dns_lookup

Performs DNS lookups to resolve IP addresses to hostnames or hostnames to IP addresses.

```tql
dns_lookup field, [result=field]
```

## Description

[Section titled “Description”](#description)

The `dns_lookup` operator performs DNS resolution on the specified field. It automatically detects whether to perform a forward lookup (hostname to IP) or reverse lookup (IP to hostname) based on the field’s content.

* **Reverse lookup**: When the field contains an IP address, the operator performs a PTR query to find the associated hostname.
* **Forward lookup**: When the field contains a string, the operator performs A and AAAA queries to find associated IP addresses.

The result is stored as a record in the specified result field.

### `field: ip|string`

[Section titled “field: ip|string”](#field-ipstring)

The field containing either an IP address or hostname to look up.

### `result = field (optional)`

[Section titled “result = field (optional)”](#result--field-optional)

The field where the DNS lookup result will be stored.

Defaults to `dns_lookup`.

The result is a record with the following structure:

For reverse lookups (IP to hostname):

```tql
{
  hostname: string
}
```

For forward lookups (hostname to IP):

```tql
list<record>
```

Where each record has the structure:

```tql
{
  address: ip,
  type: string,
  ttl: duration
}
```

If the lookup fails or times out, the result field will be `null`.

## Examples

[Section titled “Examples”](#examples)

### Reverse DNS lookup

[Section titled “Reverse DNS lookup”](#reverse-dns-lookup)

Resolve an IP address to its hostname:

```tql
from {src_ip: 8.8.8.8, dst_ip: 192.168.1.1}
dns_lookup src_ip, result=src_dns
```

```tql
{
  src_ip: 8.8.8.8,
  dst_ip: 192.168.1.1,
  src_dns: {
    hostname: "dns.google"
  }
}
```

### Forward DNS lookup

[Section titled “Forward DNS lookup”](#forward-dns-lookup)

Resolve a hostname to its IP addresses:

```tql
from {domain: "example.com", timestamp: 2024-01-15T10:30:00}
dns_lookup domain, result=ip_info
```

```tql
{
  domain: "example.com",
  timestamp: 2024-01-15T10:30:00,
  ip_info: [
    {address: 93.184.215.14, type: "A", ttl: 5m},
    {address: 2606:2800:21f:cb07:6820:80da:af6b:8b2c, type: "AAAA", ttl: 5m}
  ]
}
```

### Handling lookup failures

[Section titled “Handling lookup failures”](#handling-lookup-failures)

When a DNS lookup fails, the result field is set to `null`:

```tql
from {ip: 192.168.1.123}
dns_lookup ip, result=hostname_info
```

```tql
{
  ip: 192.168.1.123,
  hostname_info: null
}
```

### Multiple lookups in a pipeline

[Section titled “Multiple lookups in a pipeline”](#multiple-lookups-in-a-pipeline)

```tql
from {
  source: 1.1.1.1,
  destination: "tenzir.com"
}
dns_lookup source, result=source_dns
dns_lookup destination, result=dest_ips
```

```tql
{
  source: 1.1.1.1,
  destination: "tenzir.com",
  source_dns: {
    hostname: "one.one.one.one"
  },
  dest_ips: [
    {address: 185.199.108.153, type: "A", ttl: 1h},
    {address: 185.199.109.153, type: "A", ttl: 1h},
    {address: 185.199.110.153, type: "A", ttl: 1h},
    {address: 185.199.111.153, type: "A", ttl: 1h}
  ]
}
```

## See Also

[Section titled “See Also”](#see-also)

[`set`](/reference/operators/set)

# drop

Removes fields from the event.

```tql
drop field...
```

## Description

[Section titled “Description”](#description)

Removes the given fields from the events. Issues a warning if a field is not present.

## Examples

[Section titled “Examples”](#examples)

### Drop fields from the input

[Section titled “Drop fields from the input”](#drop-fields-from-the-input)

```tql
from {
  src: 192.168.0.4,
  dst: 192.168.0.31,
  role: "admin",
  info: {
    id: "cR32kdMD9",
    msg: 8411,
  },
}
drop role, info.id
```

```tql
{
  src: 192.168.0.4,
  dst: 192.168.0.31,
  info: {
    msg: 8411,
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`select`](/reference/operators/select), [`where`](/reference/operators/where)

# drop_null_fields

Removes fields containing null values from the event.

```tql
drop_null_fields [field...]
```

## Description

[Section titled “Description”](#description)

The `drop_null_fields` operator removes fields that have `null` values from events. Without arguments, it removes all fields with `null` values from the entire event. When provided with specific field paths, it removes those fields if they contain null values, and for record fields, it also recursively removes any null fields within them.

### `field... (optional)`

[Section titled “field... (optional)”](#field-optional)

A comma-separated list of field paths to process. When specified:

* If a field contains `null`, it will be removed
* If a field is a record, all null fields within it will be removed recursively
* Other null fields outside the specified paths will be preserved

Behavior with Lists

The `drop_null_fields` operator does not currently support dropping fields from records that are inside lists.

The operator does not remove `null` values from within lists. A field containing a list with `null` elements is not considered a null field.

## Examples

[Section titled “Examples”](#examples)

### Drop all null fields from the input

[Section titled “Drop all null fields from the input”](#drop-all-null-fields-from-the-input)

```tql
from {
  src: 192.168.0.4,
  dst: null,
  role: "admin",
  info: {
    id: null,
    msg: 8411,
  },
}
drop_null_fields
```

```tql
{
  src: 192.168.0.4,
  role: "admin",
  info: {
    msg: 8411,
  },
}
```

### Drop specific null fields

[Section titled “Drop specific null fields”](#drop-specific-null-fields)

```tql
from {
  src: 192.168.0.4,
  dst: null,
  role: null,
  info: {
    id: null,
    msg: 8411,
  },
}
drop_null_fields dst, info.id
```

```tql
{
  src: 192.168.0.4,
  role: null,
  info: {
    msg: 8411,
  },
}
```

### Drop null fields within a record field

[Section titled “Drop null fields within a record field”](#drop-null-fields-within-a-record-field)

When specifying a record field, all null fields within it are removed recursively:

```tql
from {
  metadata: {
    created: "2024-01-01",
    updated: null,
    tags: null,
    author: "admin"
  },
  data: {
    value: 42,
    comment: null
  }
}
drop_null_fields metadata
```

```tql
{
  metadata: {
    created: "2024-01-01",
    author: "admin",
  },
  data: {
    value: 42,
    comment: null,
  },
}
```

Note that `data.comment` remains null because only `metadata` was specified.

### Behavior with records inside lists

[Section titled “Behavior with records inside lists”](#behavior-with-records-inside-lists)

The `drop_null_fields` operator does not remove fields from records that are inside lists:

```tql
from {
  id: 1,
  items: [{name: "a", value: 1}, {name: "b", value: null}],
  metadata: null,
  tags: ["x", null, "y"]
}
drop_null_fields
```

```tql
{
  id: 1,
  items: [
    {
      name: "a",
      value: 1,
    },
    {
      name: "b",
      value: null,
    },
  ],
  tags: [
    "x",
    null,
    "y",
  ],
}
```

In this example:

* The `metadata` field is removed because it contains `null`
* The `items` field is kept with all its internal structure intact
* The `tags` field is kept even though it contains `null` elements
* Fields within records inside lists (like `value`) are not dropped even if they contain `null`

## See Also

[Section titled “See Also”](#see-also)

[`drop`](/reference/operators/drop), [`select`](/reference/operators/select), [`where`](/reference/operators/where)

# enumerate

Add a field with the number of preceding events.

```tql
enumerate [out:field, group=any]
```

## Description

[Section titled “Description”](#description)

The `enumerate` operator adds a new field with the number of preceding events to the beginning of the input record.

### `out: field (optional)`

[Section titled “out: field (optional)”](#out-field-optional)

Sets the name of the output field.

Defaults to `"#"`.

### `group: any (optional)`

[Section titled “group: any (optional)”](#group-any-optional)

Groups events by the specified expression and enumerates within each group. When provided, an independent enumeration counter is used for each unique value of the grouping expression.

## Examples

[Section titled “Examples”](#examples)

### Enumerate the input by prepending row numbers

[Section titled “Enumerate the input by prepending row numbers”](#enumerate-the-input-by-prepending-row-numbers)

```tql
from {x: "a"}, {x: "b"}, {x: "c"}
enumerate
```

```tql
{"#": 0, x: "a"}
{"#": 1, x: "b"}
{"#": 2, x: "c"}
```

### Use a custom field for the row numbers

[Section titled “Use a custom field for the row numbers”](#use-a-custom-field-for-the-row-numbers)

```tql
from {x: true}, {x: false}
enumerate index
```

```tql
{index: 0, x: true}
{index: 1, x: false}
```

### Count within groups

[Section titled “Count within groups”](#count-within-groups)

```tql
from {x: 1}, {x: 2}, {x: 1}, {x: 2}
enumerate count, group=x
count = count + 1
```

```tql
{count: 1, x: 1}
{count: 1, x: 2}
{count: 2, x: 1}
{count: 2, x: 2}
```

# every

Runs a pipeline periodically at a fixed interval.

```tql
every interval:duration { … }
```

## Description

[Section titled “Description”](#description)

The `every` operator repeats running a pipeline indefinitely at a fixed interval. The first run is starts directly when the outer pipeline itself starts.

Every `interval`, the executor spawns a new pipeline that runs to completion. If the pipeline runs longer than `interval`, the next run immediately starts.

## Examples

[Section titled “Examples”](#examples)

### Produce one event per second and enumerate the result

[Section titled “Produce one event per second and enumerate the result”](#produce-one-event-per-second-and-enumerate-the-result)

```tql
every 1s {
  from {}
}
enumerate
```

```tql
{"#": 0} // immediately
{"#": 1} // after 1s
{"#": 2} // after 2s
{"#": 3} // after 3s
// … continues like this
```

### Fetch the results from an API every 10 minutes

[Section titled “Fetch the results from an API every 10 minutes”](#fetch-the-results-from-an-api-every-10-minutes)

```tql
every 10min {
  load_http "example.org/api/threats"
  read_json
}
publish "threat-feed"
```

## See Also

[Section titled “See Also”](#see-also)

[`cron`](/reference/operators/cron)

# export

Retrieves events from a Tenzir node.

```tql
export [live=bool, retro=bool, internal=bool, parallel=int]
```

## Description

[Section titled “Description”](#description)

The `export` operator retrieves events from a Tenzir node.

This operator is the dual to [`import`](/reference/operators/import).

### `live = bool (optional)`

[Section titled “live = bool (optional)”](#live--bool-optional)

Work on all events that are imported with `import` operators in real-time instead of on events persisted at a Tenzir node.

Note that live exports may drop events if the following pipeline fails to keep up. To connect pipelines with back pressure, use the [`publish`](/reference/operators/publish) and [`subscribe`](/reference/operators/subscribe) operators.

### `retro = bool (optional)`

[Section titled “retro = bool (optional)”](#retro--bool-optional)

Export persistent events at a Tenzir node. Unless `live=true` is given, this is implied.

Use `retro=true, live=true` to export past events, and live events afterwards.

### `internal = bool (optional)`

[Section titled “internal = bool (optional)”](#internal--bool-optional)

Export internal events, such as metrics or diagnostics, instead. By default, `export` only returns events that were previously imported with `import`. In contrast, `export internal=true` exports internal events such as operator metrics.

### `parallel = int (optional)`

[Section titled “parallel = int (optional)”](#parallel--int-optional)

The parallel level controls how many worker threads the operator uses at most for querying historical events.

Defaults to 3.

## Examples

[Section titled “Examples”](#examples)

### Export all stored events as JSON

[Section titled “Export all stored events as JSON”](#export-all-stored-events-as-json)

```tql
export
write_json
```

### Get a subset of matching events

[Section titled “Get a subset of matching events”](#get-a-subset-of-matching-events)

```tql
export
where src_ip == 1.2.3.4
head 20
```

## See Also

[Section titled “See Also”](#see-also)

[`import`](/reference/operators/import), [`subscribe`](/reference/operators/subscribe)

# fields

Retrieves all fields stored at a node.

```tql
fields
```

## Description

[Section titled “Description”](#description)

The `fields` operator shows a list of all fields stored at a node across all available schemas.

## Examples

[Section titled “Examples”](#examples)

### Get the top-5 most frequently used fields across schemas

[Section titled “Get the top-5 most frequently used fields across schemas”](#get-the-top-5-most-frequently-used-fields-across-schemas)

```tql
fields
summarize field, count=count_distinct(schema), schemas=distinct(schema)
sort -count
head 5
```

# files

Shows file information for a given directory.

```tql
files [dir:string, recurse=bool, follow_symlinks=bool, skip_permission_denied=bool]
```

## Description

[Section titled “Description”](#description)

The `files` operator shows file information for all files in the given directory.

### `dir: string (optional)`

[Section titled “dir: string (optional)”](#dir-string-optional)

The directory to list files in.

Defaults to the current working directory.

### `recurse = bool (optional)`

[Section titled “recurse = bool (optional)”](#recurse--bool-optional)

Recursively list files in subdirectories.

### `follow_symlinks = bool (optional)`

[Section titled “follow\_symlinks = bool (optional)”](#follow_symlinks--bool-optional)

Follow directory symlinks.

### `skip_permission_denied = bool (optional)`

[Section titled “skip\_permission\_denied = bool (optional)”](#skip_permission_denied--bool-optional)

Skip directories that would otherwise result in permission denied errors.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir emits file information with the following schema.

### `tenzir.file`

[Section titled “tenzir.file”](#tenzirfile)

Contains detailed information about the file.

| Field             | Type     | Description                              |
| :---------------- | :------- | :--------------------------------------- |
| `path`            | `string` | The file path.                           |
| `type`            | `string` | The type of the file (see below).        |
| `permissions`     | `record` | The permissions of the file (see below). |
| `owner`           | `string` | The file’s owner.                        |
| `group`           | `string` | The file’s group.                        |
| `file_size`       | `uint64` | The file size in bytes.                  |
| `hard_link_count` | `uint64` | The number of hard links to the file.    |
| `last_write_time` | `time`   | The time of the last write to the file.  |

The `type` field can have one of the following values:

| Value       | Description                     |
| :---------- | :------------------------------ |
| `regular`   | The file is a regular file.     |
| `directory` | The file is a directory.        |
| `symlink`   | The file is a symbolic link.    |
| `block`     | The file is a block device.     |
| `character` | The file is a character device. |
| `fifo`      | The file is a named IPC pipe.   |
| `socket`    | The file is a named IPC socket. |
| `not_found` | The file does not exist.        |
| `unknown`   | The file has an unknown type.   |

The `permissions` record contains the following fields:

| Field    | Type     | Description                         |
| :------- | :------- | :---------------------------------- |
| `owner`  | `record` | The file permissions for the owner. |
| `group`  | `record` | The file permissions for the group. |
| `others` | `record` | The file permissions for others.    |

The `owner`, `group`, and `others` records contain the following fields:

| Field     | Type   | Description                     |
| :-------- | :----- | :------------------------------ |
| `read`    | `bool` | Whether the file is readable.   |
| `write`   | `bool` | Whether the file is writeable.  |
| `execute` | `bool` | Whether the file is executable. |

## Examples

[Section titled “Examples”](#examples)

### Compute the total file size of the current directory

[Section titled “Compute the total file size of the current directory”](#compute-the-total-file-size-of-the-current-directory)

```tql
files recurse=true
summarize total_size=sum(file_size)
```

### Find all named pipes in `/tmp`

[Section titled “Find all named pipes in /tmp”](#find-all-named-pipes-in-tmp)

```tql
files "/tmp", recurse=true, skip_permission_denied=true
where type == "fifo"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_file`](/reference/operators/load_file), [`processes`](/reference/operators/processes), [`save_file`](/reference/operators/save_file), [`sockets`](/reference/operators/sockets)

# fork

Executes a subpipeline with a copy of the input.

```tql
fork { … }
```

## Description

[Section titled “Description”](#description)

The `fork` operator executes a subpipeline with a copy of its input, that is: whenever an event arrives, it is sent both to the given pipeline and forwarded at the same time to the next operator.

### `{ … }`

[Section titled “{ … }”](#-)

The pipeline to execute. Must have a sink.

## Examples

[Section titled “Examples”](#examples)

### Publish incoming events while importing them simultaneously

[Section titled “Publish incoming events while importing them simultaneously”](#publish-incoming-events-while-importing-them-simultaneously)

```tql
fork {
  publish "imported-events"
}
import
```

# from

Obtains events from an URI, inferring the source, compression and format.

```tql
from uri:string, [loader_args… { … }]
from events…
```

## Description

[Section titled “Description”](#description)

The `from` operator is an easy way to get data into Tenzir. It will try to infer the connector, compression and format based on the given URI.

Alternatively, it can be used to create events from records.

Use `from` if you can

The `from` operator is designed as an easy way to get data into Tenzir, without having to manually write the separate steps of data ingestion manually.

### `uri: string`

[Section titled “uri: string”](#uri-string)

The URI to load from.

Note

The URI for `from` must be a constant string and cannot be a `secret`.

### `loader_args… (optional)`

[Section titled “loader\_args… (optional)”](#loader_args-optional)

An optional set of arguments passed to the loader. This can be used to e.g. pass credentials to a connector:

```tql
from "https://example.org/file.json", headers={Token: "XYZ"}
```

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

The optional pipeline argument allows for explicitly specifying how `from` decompresses and parses data. By default, the pipeline is inferred based on a set of [rules](#explanation).

If inference is not possible, or not sufficient, this argument can be used to control the decompression and parsing. Providing this pipeline disables the inference. [Examples](#load-a-file-with-parser-arguments)

### `events…`

[Section titled “events…”](#events)

Instead of a URI, you can also provide one or more records, which will be the operators output. This is mostly useful for testing pipelines without loading actual data.

## Explanation

[Section titled “Explanation”](#explanation)

Loading a resource into tenzir consists of three steps:

* [**Loading**](#loading) the raw bytes
* [**Decompressing**](#decompressing) (optional)
* [**Reading**](#reading) the bytes as structured data

The `from` operator tries to infer all three steps from the given URI.

### Loading

[Section titled “Loading”](#loading)

The connector is inferred based on the URI `scheme://`. See the [URI schemes section](#uri-schemes) for supported schemes. If no scheme is present, the connector attempts to load from the filesystem.

### Decompressing

[Section titled “Decompressing”](#decompressing)

The compression is inferred from the “file-ending” in the URI. Under the hood, this uses the [`decompress_*` operators](/reference/operators#encode--decode). Supported compressions can be found in the [list of compression extensions](#compression).

The decompression step is optional and will only happen if a compression could be inferred. If you know that the source is compressed and the compression cannot be inferred, you can use the [pipeline argument](#---optional) to specify the decompression manually.

### Reading

[Section titled “Reading”](#reading)

The format to read is, just as the compression, inferred from the file-ending. Supported file formats are the common file endings for our [`read_*` operators](/reference/operators#parsing).

If you want to provide additional arguments to the parser, you can use the [pipeline argument](#---optional) to specify the parsing manually. This can be useful, if you e.g. know that the input is `suricata` or `ndjson` instead of just plain `json`.

### The pipeline argument & its relation to the loader

[Section titled “The pipeline argument & its relation to the loader”](#the-pipeline-argument--its-relation-to-the-loader)

Some loaders, such as the [`load_tcp`](/reference/operators/load_tcp) operator, accept a sub-pipeline directly. If the selected loader accepts a sub-pipeline, the `from` operator will dispatch decompression and parsing into that sub-pipeline. If a an explicit pipeline argument is provided it is forwarded as-is. If the loader does not accept a sub-pipeline, the decompression and parsing steps are simply performed as part of the regular pipeline.

#### Example transformation:

[Section titled “Example transformation:”](#example-transformation)

from operator

```tql
from "myfile.json.gz"
```

Effective pipeline

```tql
load_file "myfile.json.gz"
decompress_gzip
read_json
```

#### Example with pipeline argument:

[Section titled “Example with pipeline argument:”](#example-with-pipeline-argument)

from operator

```tql
from "tcp://0.0.0.0:12345", parallel=10 {
  read_gelf
}
```

Effective pipeline

```tql
load_tcp "tcp://0.0.0.0:12345", parallel=10 {
  read_gelf
}
```

## Supported Deductions

[Section titled “Supported Deductions”](#supported-deductions)

### URI schemes

[Section titled “URI schemes”](#uri-schemes)

| Scheme          | Operator                                                                    | Example                                          |
| :-------------- | :-------------------------------------------------------------------------- | :----------------------------------------------- |
| `abfs`,`abfss`  | [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage)   | `from "abfs://path/to/file.json"`                |
| `amqp`          | [`load_amqp`](/reference/operators/load_amqp)                               | `from "amqp://…`                                 |
| `elasticsearch` | [`from_opensearch`](/reference/operators/from_opensearch)                   | `from "elasticsearch://1.2.3.4:9200`             |
| `file`          | [`load_file`](/reference/operators/load_file)                               | `from "file://path/to/file.json"`                |
| `fluent-bit`    | [`from_fluent_bit`](/reference/operators/from_fluent_bit)                   | `from "fluent-bit://elasticsearch"`              |
| `ftp`, `ftps`   | [`load_ftp`](/reference/operators/load_ftp)                                 | `from "ftp://example.com/file.json"`             |
| `gcps`          | [`load_google_cloud_pubsub`](/reference/operators/load_google_cloud_pubsub) | `from "gcps://project_id/subscription_id" { … }` |
| `gs`            | [`load_gcs`](/reference/operators/load_gcs)                                 | `from "gs://bucket/object.json"`                 |
| `http`, `https` | [`load_http`](/reference/operators/load_http)                               | `from "http://example.com/file.json"`            |
| `inproc`        | [`load_zmq`](/reference/operators/load_zmq)                                 | `from "inproc://127.0.0.1:56789" { read_json }`  |
| `kafka`         | [`load_kafka`](/reference/operators/load_kafka)                             | `from "kafka://topic" { read_json }`             |
| `opensearch`    | [`from_opensearch`](/reference/operators/from_opensearch)                   | `from "opensearch://1.2.3.4:9200`                |
| `s3`            | [`load_s3`](/reference/operators/load_s3)                                   | `from "s3://bucket/file.json"`                   |
| `sqs`           | [`load_sqs`](/reference/operators/load_sqs)                                 | `from "sqs://my-queue" { read_json }`            |
| `tcp`           | [`load_tcp`](/reference/operators/load_tcp)                                 | `from "tcp://127.0.0.1:13245" { read_json }`     |
| `udp`           | [`load_udp`](/reference/operators/load_udp)                                 | `from "udp://127.0.0.1:56789" { read_json }`     |
| `zmq`           | [`load_zmq`](/reference/operators/load_zmq)                                 | `from "zmq://127.0.0.1:56789" { read_json }`     |

Please see the respective operator pages for details on the URI’s locator format.

### File extensions

[Section titled “File extensions”](#file-extensions)

#### Format

[Section titled “Format”](#format)

The `from` operator can deduce the file format based on these file-endings:

| Format  | File Endings         | Operator                                            |
| :------ | :------------------- | :-------------------------------------------------- |
| CSV     | `.csv`               | [`read_csv`](/reference/operators/read_csv)         |
| Feather | `.feather`, `.arrow` | [`read_feather`](/reference/operators/read_feather) |
| JSON    | `.json`              | [`read_json`](/reference/operators/read_json)       |
| NDJSON  | `.ndjson`, `.jsonl`  | [`read_ndjson`](/reference/operators/read_ndjson)   |
| Parquet | `.parquet`           | [`read_parquet`](/reference/operators/read_parquet) |
| Pcap    | `.pcap`              | [`read_pcap`](/reference/operators/read_pcap)       |
| SSV     | `.ssv`               | [`read_ssv`](/reference/operators/read_ssv)         |
| TSV     | `.tsv`               | [`read_tsv`](/reference/operators/read_tsv)         |
| YAML    | `.yaml`              | [`read_yaml`](/reference/operators/read_yaml)       |

#### Compression

[Section titled “Compression”](#compression)

The `from` operator can deduce the following compressions based on these file-endings:

| Compression | File Endings     |
| :---------- | :--------------- |
| Brotli      | `.br`, `.brotli` |
| Bzip2       | `.bz2`           |
| Gzip        | `.gz`, `.gzip`   |
| LZ4         | `.lz4`           |
| Zstd        | `.zst`, `.zstd`  |

## Examples

[Section titled “Examples”](#examples)

### Load a local file

[Section titled “Load a local file”](#load-a-local-file)

```tql
from "path/to/my/load/file.csv"
```

### Load a compressed file

[Section titled “Load a compressed file”](#load-a-compressed-file)

```tql
from "path/to/my/load/file.json.bz2"
```

### Load a file with parser arguments

[Section titled “Load a file with parser arguments”](#load-a-file-with-parser-arguments)

Provide an explicit header to the CSV parser:

```tql
from "path/to/my/load/file.csv.bz2" {
  decompress_brotli // this is now necessary due to the pipeline argument
  read_csv header="col1,col2,col3"
}
```

### Pick a more suitable parser

[Section titled “Pick a more suitable parser”](#pick-a-more-suitable-parser)

The file `eve.json` contains Suricata logs, but the `from` operator does not know this. We provide an explicit `read_suricata` instead:

```tql
from "path/to/my/load/eve.json" {
  read_suricata
}
```

### Load from HTTP with a header

[Section titled “Load from HTTP with a header”](#load-from-http-with-a-header)

```tql
from "https://example.org/file.json", headers={Token: "1234"}
```

### Create events from records

[Section titled “Create events from records”](#create-events-from-records)

```tql
from {message: "Value", endpoint: {ip: 127.0.0.1, port: 42}},
     {message: "Value", endpoint: {ip: 127.0.0.1, port: 42}, raw: "text"},
     {message: "Value", endpoint: null}
```

```tql
{
  message: "Value",
  endpoint: {
    ip: 127.0.0.1,
    port: 42
  }
}
{
  message: "Value",
  endpoint: {
    ip: 127.0.0.1,
    port: 42
  },
  raw: "text"
}
{
  message: "Value",
  endpoint: null
}
```

## See Also

[Section titled “See Also”](#see-also)

[`from_file`](/reference/operators/from_file), [`to`](/reference/operators/to)

# from_azure_blob_storage

Reads one or multiple files from Azure Blob Storage.

```tql
from_azure_blob_storage url:string, [account_key=string, watch=bool,
  remove=bool, rename=string->string, path_field=field] { … }
```

## Description

[Section titled “Description”](#description)

The `from_azure_blob_storage` operator reads files from Azure Blob Storage, with support for glob patterns, automatic format detection, and file monitoring.

By default, authentication is handled by the Azure SDK’s credential chain which may read from multiple environment variables, such as:

* `AZURE_TENANT_ID`
* `AZURE_CLIENT_ID`
* `AZURE_CLIENT_SECRET`
* `AZURE_AUTHORITY_HOST`
* `AZURE_CLIENT_CERTIFICATE_PATH`
* `AZURE_FEDERATED_TOKEN_FILE`

### `url: string`

[Section titled “url: string”](#url-string)

URL identifying the Azure Blob Storage location where data should be read from.

The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `container/**/data` matches `container/data`.

Supported URI formats:

1. `abfs[s]://<account>.blob.core.windows.net[/<container>[/<path>]]`
2. `abfs[s]://<container>@<account>.dfs.core.windows.net[/<path>]`
3. `abfs[s]://[<account>@]<host>[.<domain>][:<port>][/<container>[/<path>]]`
4. `abfs[s]://[<account>@]<container>[/<path>]`

(1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2).

Authenticate with the Azure CLI

Run `az login` on the command-line to authenticate the current user with Azure’s command-line arguments.

### `account_key = string (optional)`

[Section titled “account\_key = string (optional)”](#account_key--string-optional)

Account key for authenticating with Azure Blob Storage.

### `watch = bool (optional)`

[Section titled “watch = bool (optional)”](#watch--bool-optional)

In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s.

Defaults to `false`.

### `remove = bool (optional)`

[Section titled “remove = bool (optional)”](#remove--bool-optional)

Deletes files after they have been read completely.

Defaults to `false`.

### `rename = string -> string (optional)`

[Section titled “rename = string -> string (optional)”](#rename--string---string-optional)

Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path.

If the target path already exists, the operator will overwrite the file.

The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path.

### `path_field = field (optional)`

[Section titled “path\_field = field (optional)”](#path_field--field-optional)

This makes the operator insert the path to the file where an event originated from before emitting it.

By default, paths will not be inserted into the outgoing events.

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable.

## Examples

[Section titled “Examples”](#examples)

### Read every JSON file from a container

[Section titled “Read every JSON file from a container”](#read-every-json-file-from-a-container)

```tql
from_azure_blob_storage "abfs://my-container/data/**.json"
```

### Read CSV files using account key authentication

[Section titled “Read CSV files using account key authentication”](#read-csv-files-using-account-key-authentication)

```tql
from_azure_blob_storage "abfs://container/data.csv", account_key="your-account-key"
```

### Read Suricata EVE JSON logs continuously

[Section titled “Read Suricata EVE JSON logs continuously”](#read-suricata-eve-json-logs-continuously)

```tql
from_azure_blob_storage "abfs://logs/suricata/**.json", watch=true {
  read_suricata
}
```

### Process files and move them to an archive container

[Section titled “Process files and move them to an archive container”](#process-files-and-move-them-to-an-archive-container)

```tql
from_azure_blob_storage "abfs://input/**.json",
  rename=(path => "/archive/" + path)
```

### Add source path to events

[Section titled “Add source path to events”](#add-source-path-to-events)

```tql
from_azure_blob_storage "abfs://data/**.json", path_field=source_file
```

## See Also

[Section titled “See Also”](#see-also)

[`from_file`](/reference/operators/from_file), [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage), [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage)

# from_file

Reads one or multiple files from a filesystem.

```tql
from_file url:string, [watch=bool, remove=bool, rename=string->string, path_field=field] { … }
```

## Description

[Section titled “Description”](#description)

The `from_file` operator reads files from local filesystems or cloud storage, with support for glob patterns, automatic format detection, and file monitoring.

### `url: string`

[Section titled “url: string”](#url-string)

URL or local filesystem path where data should be read from.

The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `foo/**/bar` matches `foo/bar`.

The URL can include additional options. For `s3://`, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`. For `gs://`, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`.

### `watch = bool (optional)`

[Section titled “watch = bool (optional)”](#watch--bool-optional)

In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s.

Defaults to `false`.

### `remove = bool (optional)`

[Section titled “remove = bool (optional)”](#remove--bool-optional)

Deletes files after they have been read completely.

Defaults to `false`.

### `rename = string -> string (optional)`

[Section titled “rename = string -> string (optional)”](#rename--string---string-optional)

Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path.

If the target path already exists, the operator will overwrite the file.

The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path.

### `path_field = field (optional)`

[Section titled “path\_field = field (optional)”](#path_field--field-optional)

This makes the operator insert the path to the file where an event originated from before emitting it.

By default, paths will not be inserted into the outgoing events.

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. This is using the same logic as [`from`](/reference/operators/from).

## Examples

[Section titled “Examples”](#examples)

### Read every `.csv` file from S3

[Section titled “Read every .csv file from S3”](#read-every-csv-file-from-s3)

```tql
from_file "s3://my-bucket/**.csv"
```

### Read every `.json` file in `/data` as Suricata EVE JSON

[Section titled “Read every .json file in /data as Suricata EVE JSON”](#read-every-json-file-in-data-as-suricata-eve-json)

```tql
from_file "/data/**.json" {
  read_suricata
}
```

### Read all files from S3 continuously and delete them afterwards

[Section titled “Read all files from S3 continuously and delete them afterwards”](#read-all-files-from-s3-continuously-and-delete-them-afterwards)

```tql
from_file "s3://my-bucket/**", watch=true, remove=true
```

### Move files to a directory, preserving filenames

[Section titled “Move files to a directory, preserving filenames”](#move-files-to-a-directory-preserving-filenames)

```tql
// The trailing slash automatically appends the original filename
from_file "/input/*.json", rename=path => "/output/"
```

## See Also

[Section titled “See Also”](#see-also)

[`from`](/reference/operators/from), [`load_file`](/reference/operators/load_file)

# from_fluent_bit

Receives events via Fluent Bit.

```tql
from_fluent_bit plugin:string, [options=record, fluent_bit_options=record,
                schema=string, selector=string, schema_only=bool, merge=bool,
                raw=bool, unflatten=string, tls=bool, cacert=string,
                certfile=string, keyfile=string, skip_peer_verification=bool]
```

## Description

[Section titled “Description”](#description)

The `from_fluent_bit` operator acts as a bridge into the [Fluent Bit](https://docs.fluentbit.io) ecosystem, making it possible to acquire events from a Fluent Bit [input plugin](https://docs.fluentbit.io/manual/pipeline/inputs).

An invocation of the `fluent-bit` commandline utility

```bash
fluent-bit -o plugin -p key1=value1 -p key2=value2 -p…
```

translates to our `from_fluent_bit` operator as follows:

```tql
from_fluent_bit "plugin", options={key1: value1, key2: value2, …}
```

Output to Fluent Bit

You can output events to Fluent Bit using the [`to_fluent_bit` operator](/reference/operators/to_fluent_bit).

### `plugin: string`

[Section titled “plugin: string”](#plugin-string)

The name of the Fluent Bit plugin.

Run `fluent-bit -h` and look under the **Inputs** section of the help text for available plugin names. The web documentation often comes with an example invocation near the bottom of the page, which also provides a good idea how you could use the operator.

### `options = record (optional)`

[Section titled “options = record (optional)”](#options--record-optional)

Sets plugin configuration properties.

The key-value pairs in this record are equivalent to `-p key=value` for the `fluent-bit` executable.

### `fluent_bit_options = record (optional)`

[Section titled “fluent\_bit\_options = record (optional)”](#fluent_bit_options--record-optional)

Sets global properties of the Fluent Bit service., e.g., `fluent_bit_options={flush:1, grace:3}`.

Consult the list of available [key-value pairs](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file#config_section) to configure Fluent Bit according to your needs.

We recommend factoring these options into the plugin-specific `fluent-bit.yaml` so that they are independent of the `fluent-bit` operator arguments.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

### `tls = bool (optional)`

Enables TLS.

Defaults to `false`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

## URI support & integration with `from`

[Section titled “URI support & integration with from”](#uri-support--integration-with-from)

The `from_fluent_bit` operator can also be used from the [`from`](/reference/operators/from) operator. For this, the `fluentbit://` scheme can be used. The URI is then translated:

```tql
from "fluentbit://plugin"
```

```tql
from_fluent_bit "plugin"
```

## Examples

[Section titled “Examples”](#examples)

### OpenTelemetry

[Section titled “OpenTelemetry”](#opentelemetry)

Ingest [OpenTelemetry](https://docs.fluentbit.io/manual/pipeline/inputs/slack) logs, metrics, and traces:

```tql
from_fluent_bit "opentelemetry"
```

You can then send JSON-encoded log data to a freshly created API endpoint:

```bash
curl \
  --header "Content-Type: application/json" \
  --request POST \
  --data '{"resourceLogs":[{"resource":{},"scopeLogs":[{"scope":{},"logRecords":[{"timeUnixNano":"1660296023390371588","body":{"stringValue":"{\"message\":\"dummy\"}"},"traceId":"","spanId":""}]}]}]}' \
  http://0.0.0.0:4318/v1/logs
```

### Splunk

[Section titled “Splunk”](#splunk)

Handle [Splunk](https://docs.fluentbit.io/manual/pipeline/inputs/splunk) HEC requests:

```tql
from_fluent_bit "splunk", options={port: 8088}
```

# from_gcs

Reads one or multiple files from Google Cloud Storage.

```tql
from_gcs url:string, [anonymous=bool, watch=bool, remove=bool,
  rename=string->string, path_field=field] { … }
```

## Description

[Section titled “Description”](#description)

The `from_gcs` operator reads files from Google Cloud Storage, with support for glob patterns, automatic format detection, and file monitoring.

By default, authentication is handled by Google’s Application Default Credentials (ADC) chain, which may read from multiple sources:

* `GOOGLE_APPLICATION_CREDENTIALS` environment variable pointing to a service account key file
* User credentials from `gcloud auth application-default login`
* Service account attached to the compute instance (Compute Engine, GKE)
* Google Cloud SDK credentials

### `url: string`

[Section titled “url: string”](#url-string)

URL identifying the Google Cloud Storage location where data should be read from.

The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `bucket/**/data` matches `bucket/data`.

The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist:

> For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`.

### `anonymous = bool (optional)`

[Section titled “anonymous = bool (optional)”](#anonymous--bool-optional)

Use anonymous credentials instead of any configured authentication. This only works for publicly readable buckets and objects.

Defaults to `false`.

### `watch = bool (optional)`

[Section titled “watch = bool (optional)”](#watch--bool-optional)

In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s.

Defaults to `false`.

### `remove = bool (optional)`

[Section titled “remove = bool (optional)”](#remove--bool-optional)

Deletes files after they have been read completely.

Defaults to `false`.

### `rename = string -> string (optional)`

[Section titled “rename = string -> string (optional)”](#rename--string---string-optional)

Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path.

If the target path already exists, the operator will overwrite the file.

The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path.

### `path_field = field (optional)`

[Section titled “path\_field = field (optional)”](#path_field--field-optional)

This makes the operator insert the path to the file where an event originated from before emitting it.

By default, paths will not be inserted into the outgoing events.

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable.

## Examples

[Section titled “Examples”](#examples)

### Read every JSON file from a bucket

[Section titled “Read every JSON file from a bucket”](#read-every-json-file-from-a-bucket)

```tql
from_gcs "gs://my-bucket/data/**.json"
```

### Read CSV files from a public bucket

[Section titled “Read CSV files from a public bucket”](#read-csv-files-from-a-public-bucket)

```tql
from_gcs "gs://public-dataset/data.csv", anonymous=true
```

### Read Zeek logs continuously

[Section titled “Read Zeek logs continuously”](#read-zeek-logs-continuously)

```tql
from_gcs "gs://logs/zeek/**.log", watch=true {
  read_zeek_tsv
}
```

### Add source path to events

[Section titled “Add source path to events”](#add-source-path-to-events)

```tql
from_gcs "gs://data-bucket/**.json", path_field=source_file
```

### Read Suricata EVE JSON logs with custom parsing

[Section titled “Read Suricata EVE JSON logs with custom parsing”](#read-suricata-eve-json-logs-with-custom-parsing)

```tql
from_gcs "gs://security-logs/suricata/**.json" {
  read_suricata
}
```

# from_http

Sends and receives HTTP/1.1 requests.

```tql
from_http url:string, [method=string, body=record|string|blob, encode=string,
          headers=record, metadata_field=field, error_field=field,
          paginate=record->string, paginate_delay=duration,
          connection_timeout=duration, max_retry_count=int,
          retry_delay=duration, tls=bool, certfile=string, keyfile=string,
          password=string { … }]
from_http url:string, server=true, [metadata_field=field, responses=record,
          max_request_size=int, tls=bool, certfile=string, keyfile=string,
          password=string { … }]
```

## Description

[Section titled “Description”](#description)

The `from_http` operator issues HTTP requests or spins up an HTTP/1.1 server on a given address and forwards received requests as events.

Format and Compression Inference

The `from_http` operator automatically infers the file format (such as JSON, CSV, Parquet, etc.) and compression type (such as gzip, zstd, etc.) directly from the URL’s file extension, just like the generic `from` operator. This makes it easier to load data from HTTP sources without manually specifying the format or decompression step.

If the format or compression cannot be determined from the URL, the operator will fall back to using the HTTP `Content-Type` and `Content-Encoding` response headers to determine how to parse and decompress the data.

If neither the URL nor the HTTP headers provide enough information, you can explicitly specify the decompression and parsing steps using a pipeline argument.

### `url: string`

[Section titled “url: string”](#url-string)

URL to listen on or to connect to.

Must have the form `<host>:<port>` when `server=true`.

### `method = string (optional)`

[Section titled “method = string (optional)”](#method--string-optional)

One of the following HTTP methods to use when using the client:

* `get`
* `head`
* `post`
* `put`
* `del`
* `connect`
* `options`
* `trace`

Defaults to `get`, or `post` if `body` is specified.

### `body = blob|record|string (optional)`

[Section titled “body = blob|record|string (optional)”](#body--blobrecordstring-optional)

Body to send with the HTTP request.

If the value is a `record`, then the body is encoded according to the `encode` option and an appropriate `Content-Type` is set for the request.

### `encode = string (optional)`

[Section titled “encode = string (optional)”](#encode--string-optional)

Specifies how to encode `record` bodies. Supported values:

* `json`
* `form`

Defaults to `json`.

### `headers = record (optional)`

[Section titled “headers = record (optional)”](#headers--record-optional)

Record of headers to send with the request.

### `metadata_field = field (optional)`

[Section titled “metadata\_field = field (optional)”](#metadata_field--field-optional)

Field to insert metadata into when using the parsing pipeline.

The response metadata (when using the client mode) has the following schema:

| Field     | Type     | Description                           |
| :-------- | :------- | :------------------------------------ |
| `code`    | `uint64` | The HTTP status code of the response. |
| `headers` | `record` | The response headers.                 |

The request metadata (when using the server mode) has the following schema:

| Field      | Type     | Description                          |
| :--------- | :------- | :----------------------------------- |
| `headers`  | `record` | The request headers.                 |
| `query`    | `record` | The query parameters of the request. |
| `path`     | `string` | The path requested.                  |
| `fragment` | `string` | The URI fragment of the request.     |
| `method`   | `string` | The HTTP method of the request.      |
| `version`  | `string` | The HTTP version of the request.     |

### `error_field = field (optional)`

[Section titled “error\_field = field (optional)”](#error_field--field-optional)

Field to insert the response body for HTTP error responses (status codes not in the 2xx or 3xx range).

When set, any HTTP response with a status code outside the 200–399 range will have its body stored in this field as a `blob`. Otherwise, error responses, alongside the original event, are skipped and an error is emitted.

### `paginate = record -> string (optional)`

[Section titled “paginate = record -> string (optional)”](#paginate--record---string-optional)

A lambda expression to evaluate against the result of the request (optionally parsed by the given pipeline). If the expression evaluation is successful and non-null, the resulting string is used as the URL for a new GET request with the same headers.

### `paginate_delay = duration (optional)`

[Section titled “paginate\_delay = duration (optional)”](#paginate_delay--duration-optional)

The duration to wait between consecutive pagination requests.

Defaults to `0s`.

### `connection_timeout = duration (optional)`

[Section titled “connection\_timeout = duration (optional)”](#connection_timeout--duration-optional)

Timeout for the connection.

Defaults to `5s`.

### `max_retry_count = int (optional)`

[Section titled “max\_retry\_count = int (optional)”](#max_retry_count--int-optional)

The maximum times to retry a failed request. Every request has its own retry count.

Defaults to `0`.

### `retry_delay = duration (optional)`

[Section titled “retry\_delay = duration (optional)”](#retry_delay--duration-optional)

The duration to wait between each retry.

Defaults to `1s`.

### `server = bool (optional)`

[Section titled “server = bool (optional)”](#server--bool-optional)

Whether to spin up an HTTP server or act as an HTTP client.

Defaults to `false`, i.e., the HTTP client.

### `responses = record (optional)`

[Section titled “responses = record (optional)”](#responses--record-optional)

Specify custom responses for endpoints on the server. For example,

```tql
responses = {
  "/resource/create": { code: 200, content_type: "text/html", body: "Created!" },
  "/resource/delete": { code: 401, content_type: "text/html", body: "Unauthorized!" }
}
```

creates two special routes on the server with different responses.

Requests to an unspecified endpoint are responded with HTTP Status `200 OK`.

### `max_request_size = int (optional)`

[Section titled “max\_request\_size = int (optional)”](#max_request_size--int-optional)

The maximum size of an incoming request to accept.

Defaults to `10MiB`.

### `tls = bool (optional)`

[Section titled “tls = bool (optional)”](#tls--bool-optional)

Enables TLS.

Defaults to `false`.

### `certfile = string (optional)`

[Section titled “certfile = string (optional)”](#certfile--string-optional)

Path to the client certificate. Required for server if `tls` is `true`.

### `keyfile = string (optional)`

[Section titled “keyfile = string (optional)”](#keyfile--string-optional)

Path to the key for the client certificate. Required for server if `tls` is `true`.

### `password = string (optional)`

[Section titled “password = string (optional)”](#password--string-optional)

Password for keyfile.

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

A pipeline that receives the response body as bytes, allowing parsing per request. This is especially useful in scenarios where the response body can be parsed into multiple events.

If not provided, the operator will attempt to infer the parsing operator from the `Content-Type` header. Should this inference fail (e.g., unsupported or missing `Content-Type`), the operator raises an error.

## Examples

[Section titled “Examples”](#examples)

### Make a GET request

[Section titled “Make a GET request”](#make-a-get-request)

Make a request to [urlscan.io](https://urlscan.io/docs/api#search) to search for scans for `tenzir.com` and get the first result.

```tql
from_http "https://urlscan.io/api/v1/search?q=tenzir.com"
unroll results
head 1
```

```tql
{
  results: {
    submitter: { ... },
    task: { ... },
    stats: { ... },
    page: { ... },
    _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30",
    _score: null,
    sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ],
    result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/",
    screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png",
  },
  total: 9,
  took: 296,
  has_more: false,
}
```

### Paginated API Requests

[Section titled “Paginated API Requests”](#paginated-api-requests)

Use the `paginate` parameter to handle paginated APIs:

```tql
from_http "https://api.example.com/data", paginate=(x => x.next_url?)
```

This sends a GET request to the initial URL and evaluates the `x.next_url` field in the response to determine the next URL for subsequent requests.

### Retry Failed Requests

[Section titled “Retry Failed Requests”](#retry-failed-requests)

Configure retries for failed requests:

```tql
from_http "https://api.example.com/data", max_retry_count=3, retry_delay=2s
```

This tries up to 3 times, waiting 2 seconds between each retry.

### Listen on port 8080

[Section titled “Listen on port 8080”](#listen-on-port-8080)

Spin up a server with:

```tql
from_http "0.0.0.0:8080", server=true, metadata_field=metadata
```

Send a request to the HTTP endpoint via `curl`:

```sh
echo '{"key": "value"}' | gzip | curl localhost:8080 --data-binary @- -H 'Content-Encoding: gzip' -H 'Content-Type: application/json'
```

Observe the request in the Tenzir pipeline, parsed and decompressed:

```tql
{
  key: "value",
  metadata: {
    headers: {
      Host: "localhost:8080",
      "User-Agent": "curl/8.13.0",
      Accept: "*/*",
      "Content-Encoding": "gzip",
      "Content-Length": "37",
      "Content-Type": "application/json",
    },
    path: "/",
    method: "post",
    version: "HTTP/1.1",
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`http`](/reference/operators/http), [`serve`](/reference/operators/serve)

# from_opensearch

Receives events via Opensearch Bulk API.

```tql
from_opensearch [url:string, keep_actions=bool, max_request_size=int, tls=bool,
                 certfile=string, keyfile=string, password=string]
```

## Description

[Section titled “Description”](#description)

The `from_opensearch` operator emulates simple situations for the [Opensearch Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/).

### `url: string (optional)`

[Section titled “url: string (optional)”](#url-string-optional)

URL to listen on.

Must have the form `host[:port]`.

Defaults to `"0.0.0.0:9200"`.

### `keep_actions = bool (optional)`

[Section titled “keep\_actions = bool (optional)”](#keep_actions--bool-optional)

Whether to keep the command objects such as `{"create": ...}`.

Defaults to `false`.

### `max_request_size = int (optional)`

[Section titled “max\_request\_size = int (optional)”](#max_request_size--int-optional)

The maximum size of an incoming request to accept.

Defaults to `10Mib`.

### `tls = bool (optional)`

[Section titled “tls = bool (optional)”](#tls--bool-optional)

Enables TLS.

Defaults to `false`.

### `certfile = string (optional)`

[Section titled “certfile = string (optional)”](#certfile--string-optional)

Path to the client certificate. Required if `tls` is `true`.

### `keyfile = string (optional)`

[Section titled “keyfile = string (optional)”](#keyfile--string-optional)

Path to the key for the client certificate. Required if `tls` is `true`.

### `password = string (optional)`

[Section titled “password = string (optional)”](#password--string-optional)

Password for keyfile.

## Examples

[Section titled “Examples”](#examples)

### Listen on port 8080 on an interface with IP 1.2.3.4

[Section titled “Listen on port 8080 on an interface with IP 1.2.3.4”](#listen-on-port-8080-on-an-interface-with-ip-1234)

```tql
from_opensearch "1.2.3.4:8080"
```

### Listen with TLS

[Section titled “Listen with TLS”](#listen-with-tls)

```tql
from_opensearch tls=true, certfile="server.crt", keyfile="private.key"
```

## See also

[Section titled “See also”](#see-also)

[`to_opensearch`](/reference/operators/to_opensearch)

# from_s3

Reads one or multiple files from Amazon S3.

```tql
from_s3 url:string, [anonymous=bool, access_key=string, secret_key=string,
  session_token=string, role=string, external_id=string, watch=bool,
  remove=bool, rename=string->string, path_field=field] { … }
```

## Description

[Section titled “Description”](#description)

The `from_s3` operator reads files from Amazon S3, with support for glob patterns, automatic format detection, and file monitoring.

By default, authentication is handled by AWS’s default credentials provider chain, which may read from multiple environment variables and credential files:

* `~/.aws/credentials` and `~/.aws/config`
* `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
* `AWS_SESSION_TOKEN`
* EC2 instance metadata service
* ECS container credentials

### `url: string`

[Section titled “url: string”](#url-string)

URL identifying the S3 location where data should be read from.

The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `bucket/**/data` matches `bucket/data`.

Supported URI format: `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)`

Options can be appended to the path as query parameters:

* `region`: AWS region (e.g., `us-east-1`)
* `scheme`: Connection scheme (`http` or `https`)
* `endpoint_override`: Custom S3-compatible endpoint
* `allow_bucket_creation`: Allow creating buckets if they don’t exist
* `allow_bucket_deletion`: Allow deleting buckets

### `anonymous = bool (optional)`

[Section titled “anonymous = bool (optional)”](#anonymous--bool-optional)

Use anonymous credentials instead of any configured authentication.

Defaults to `false`.

### `access_key = string (optional)`

[Section titled “access\_key = string (optional)”](#access_key--string-optional)

AWS access key ID for authentication.

### `secret_key = string (optional)`

[Section titled “secret\_key = string (optional)”](#secret_key--string-optional)

AWS secret access key for authentication. Required if `access_key` is provided.

### `session_token = string (optional)`

[Section titled “session\_token = string (optional)”](#session_token--string-optional)

AWS session token for temporary credentials.

### `role = string (optional)`

[Section titled “role = string (optional)”](#role--string-optional)

IAM role to assume when accessing S3.

### `external_id = string (optional)`

[Section titled “external\_id = string (optional)”](#external_id--string-optional)

External ID to use when assuming the specified `role`.

### `watch = bool (optional)`

[Section titled “watch = bool (optional)”](#watch--bool-optional)

In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s.

Defaults to `false`.

### `remove = bool (optional)`

[Section titled “remove = bool (optional)”](#remove--bool-optional)

Deletes files after they have been read completely.

Defaults to `false`.

### `rename = string -> string (optional)`

[Section titled “rename = string -> string (optional)”](#rename--string---string-optional)

Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path.

If the target path already exists, the operator will overwrite the file.

The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path.

### `path_field = field (optional)`

[Section titled “path\_field = field (optional)”](#path_field--field-optional)

This makes the operator insert the path to the file where an event originated from before emitting it.

By default, paths will not be inserted into the outgoing events.

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable.

## Examples

[Section titled “Examples”](#examples)

### Read every JSON file from a bucket

[Section titled “Read every JSON file from a bucket”](#read-every-json-file-from-a-bucket)

```tql
from_s3 "s3://my-bucket/data/**.json"
```

### Read CSV files using explicit credentials

[Section titled “Read CSV files using explicit credentials”](#read-csv-files-using-explicit-credentials)

```tql
from_s3 "s3://my-bucket/data.csv",
  access_key=secret("AWS_ACCESS_KEY"),
  secret_key=secret("AWS_SECRET_KEY")
```

### Read from S3-compatible service with custom endpoint

[Section titled “Read from S3-compatible service with custom endpoint”](#read-from-s3-compatible-service-with-custom-endpoint)

```tql
from_s3 "s3://my-bucket/data/**.json?endpoint_override=minio.example.com:9000&scheme=http"
```

### Read files continuously and assume IAM role

[Section titled “Read files continuously and assume IAM role”](#read-files-continuously-and-assume-iam-role)

```tql
from_s3 "s3://logs/application/**.json", watch=true, role="arn:aws:iam::123456789012:role/LogReaderRole"
```

### Process files and move them to an archive bucket

[Section titled “Process files and move them to an archive bucket”](#process-files-and-move-them-to-an-archive-bucket)

```tql
from_s3 "s3://input-bucket/**.json",
  rename=(path => "archive/" + path)
```

### Add source path to events

[Section titled “Add source path to events”](#add-source-path-to-events)

```tql
from_s3 "s3://data-bucket/**.json", path_field=source_file
```

### Read Zeek logs with anonymous access

[Section titled “Read Zeek logs with anonymous access”](#read-zeek-logs-with-anonymous-access)

```tql
from_s3 "s3://public-bucket/zeek/**.log", anonymous=true {
  read_zeek_tsv
}
```

## See Also

[Section titled “See Also”](#see-also)

[`from_file`](/reference/operators/from_file), [`load_s3`](/reference/operators/load_s3), [`save_s3`](/reference/operators/save_s3)

# from_udp

Receives UDP datagrams and outputs structured events.

```tql
from_udp endpoint:string, [resolve_hostnames=bool], [binary=bool]
```

## Description

[Section titled “Description”](#description)

Listens for UDP datagrams on the specified endpoint and outputs each datagram as a structured event containing the data and peer information.

Unlike [`load_udp`](/reference/operators/load_udp), which outputs raw bytes, `from_udp` produces structured events with metadata about the sender.

### `endpoint: string`

[Section titled “endpoint: string”](#endpoint-string)

The address to listen on. Must be of the format: `[udp://]host:port`.

Use `0.0.0.0` as the host to accept datagrams on all interfaces. The [`nics`](/reference/operators/nics) operator lists all available interfaces.

### `resolve_hostnames = bool (optional)`

[Section titled “resolve\_hostnames = bool (optional)”](#resolve_hostnames--bool-optional)

Perform DNS lookups to resolve hostnames for sender IP addresses.

Defaults to `false` since DNS lookups can be slow and may impact performance when receiving many datagrams.

### `binary = bool (optional)`

[Section titled “binary = bool (optional)”](#binary--bool-optional)

Output datagram data as binary (`blob`) instead of text (`string`).

Defaults to `false`. When `false`, the data field contains a UTF-8 string. When `true`, the data field contains raw bytes as a blob.

## Output Schema

[Section titled “Output Schema”](#output-schema)

Each UDP datagram produces one event with the following structure:

```json
{
  "data": <string|blob>, // string by default, blob when binary=true
  "peer": {
    "ip": <ip>,
    "port": <uint64>,
    "hostname": <string> // Does not exist when `resolve_hostnames=false`
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Receive UDP datagrams with sender information

[Section titled “Receive UDP datagrams with sender information”](#receive-udp-datagrams-with-sender-information)

```tql
from_udp "0.0.0.0:1234"
```

This might output events like:

```json
{
  "data": "Hello World",
  "peer": {
    "ip": "192.168.1.10",
    "port": 5678
  }
}
```

### Parse JSON data from UDP datagrams

[Section titled “Parse JSON data from UDP datagrams”](#parse-json-data-from-udp-datagrams)

```tql
from_udp "127.0.0.1:8080"
select data = data.parse_json()
```

### Filter by sender and decode data

[Section titled “Filter by sender and decode data”](#filter-by-sender-and-decode-data)

```tql
from_udp "0.0.0.0:9999"
where peer.ip == 192.168.1.100
select data
```

## See Also

[Section titled “See Also”](#see-also)

[`load_udp`](/reference/operators/load_udp), [`save_udp`](/reference/operators/save_udp)

# from_velociraptor

Submits VQL to a Velociraptor server and returns the response as events.

```tql
from_velociraptor [request_name=string, org_id=string, max_rows=int,
                   subscribe=string, query=string, max_wait=duration, profile=string]
```

## Description

[Section titled “Description”](#description)

The `from_velociraptor` input operator provides a request-response interface to a [Velociraptor](https://docs.velociraptor.app) server:

![Velociraptor](/_astro/velociraptor.excalidraw.DqyMhvp8_19DKCs.svg)

The pipeline operator is the client and it establishes a connection to a Velociraptor server. The client request contains a query written in the [Velociraptor Query Language (VQL)](https://docs.velociraptor.app/docs/vql), a SQL-inspired language with a `SELECT .. FROM .. WHERE` structure.

You can either send a raw VQL query via `from_velociraptor query "<vql>"` to a server and processs the response, or hook into a continuous feed of artifacts via `from_velociraptor subscribe <artifact>`. Whenever a hunt runs that contains this artifact, the server will forward it to the pipeline and emit the artifact payload in the response field `HuntResults`.

All Velociraptor client-to-server communication is mutually authenticated and encrypted via TLS certificates. This means you must provide client-side certificate, which you can generate as follows. (Velociraptor ships as a static binary that we refer to as `velociraptor-binary` here.)

1. Create a server configuration `server.yaml`:

   ```bash
   velociraptor-binary config generate > server.yaml
   ```

2. Create an API client:

   ```bash
   velociraptor-binary -c server.yaml config api_client name tenzir client.yaml
   ```

   Copy the generated `client.yaml` to your Tenzir plugin configuration directory as `velociraptor.yaml` so that the operator can find it:

   ```bash
   cp client.yaml /etc/tenzir/plugin/velociraptor.yaml
   ```

3. Run the frontend with the server configuration:

   ```bash
   velociraptor-binary -c server.yaml frontend
   ```

Now you are ready to run VQL queries!

### `request_name = string (optional)`

[Section titled “request\_name = string (optional)”](#request_name--string-optional)

An identifier for the request to the Velociraptor server.

Defaults to a randoum UUID.

### `org_id = string (optional)`

[Section titled “org\_id = string (optional)”](#org_id--string-optional)

The ID of the Velociraptor organization.

Defaults to `"root"`.

### `query = string (optional)`

[Section titled “query = string (optional)”](#query--string-optional)

The [VQL](https://docs.velociraptor.app/docs/vql) query string.

### `max_rows = int (optional)`

[Section titled “max\_rows = int (optional)”](#max_rows--int-optional)

The maxium number of rows to return in a the stream gRPC messages returned by the server.

Defaults to `1000`.

### `subscribe = string (optional)`

[Section titled “subscribe = string (optional)”](#subscribe--string-optional)

Subscribes to a flow artifact.

This option generates a larger VQL expression under the hood that creates one event per flow and artifact. The response contains a field `HuntResult` that contains the result of the hunt.

### `max_wait = duration (optional)`

[Section titled “max\_wait = duration (optional)”](#max_wait--duration-optional)

Controls how long to wait before releasing a partial result set.

Defaults to `1s`.

### `profile = string (optional)`

[Section titled “profile = string (optional)”](#profile--string-optional)

Specifies the configuration profile for the Velociraptor instance. This enables connecting to multiple Velociraptor instances from the same Tenzir node.

To use profiles, edit your `velociraptor.yaml` configuration like this, where `<config>` refers to the contents of the configuration file created by Velociraptor, and `<profile>` to the desired profile name.

```yaml
---
title: before
---


<config>


---
title: after
---


profiles:
  <profile>:
    <config>
```

If profiles are defined, the operator defaults to the first profile.

## Examples

[Section titled “Examples”](#examples)

### Show all processes

[Section titled “Show all processes”](#show-all-processes)

```tql
from_velociraptor query="select * from pslist()"
```

### Subscribe to a hunt flow containing the `Windows` artifact

[Section titled “Subscribe to a hunt flow containing the Windows artifact”](#subscribe-to-a-hunt-flow-containing-the-windows-artifact)

```tql
from_velociraptor subscribe="Windows"
```

# head

Limits the input to the first `n` events.

```tql
head [n:int]
```

## Description

[Section titled “Description”](#description)

Forwards the first `n` events and discards the rest.

`head n` is a shorthand notation for [`slice end=n`](/reference/operators/slice).

### `n: int (optional)`

[Section titled “n: int (optional)”](#n-int-optional)

The number of events to keep.

Defaults to `10`.

## Examples

[Section titled “Examples”](#examples)

### Get the first 10 events

[Section titled “Get the first 10 events”](#get-the-first-10-events)

```tql
head
```

### Get the first 5 events

[Section titled “Get the first 5 events”](#get-the-first-5-events)

```tql
head 5
```

## See Also

[Section titled “See Also”](#see-also)

[`slice`](/reference/operators/slice), [`tail`](/reference/operators/tail)

# http

Sends HTTP/1.1 requests and forwards the response.

```tql
http url:string, [method=string, body=record|string|blob, encode=string,
     headers=record, response_field=field, metadata_field=field,
     error_field=field, paginate=record->string, paginate_delay=duration,
     parallel=int, tls=bool, certfile=string, keyfile=string, password=string,
     connection_timeout=duration, max_retry_count=int, retry_delay=duration
     { … }]
```

## Description

[Section titled “Description”](#description)

The `http` operator issues HTTP/1.1 requests and forwards received responses as events.

Format and Compression Inference

The `http` operator automatically infers the file format (such as JSON, CSV, Parquet, etc.) and compression type (such as gzip, zstd, etc.) directly from the URL’s file extension, just like the generic `from` operator. This makes it easier to load data from HTTP sources without manually specifying the format or decompression step.

If the format or compression cannot be determined from the URL, the operator will fall back to using the HTTP `Content-Type` and `Content-Encoding` response headers to determine how to parse and decompress the data.

If neither the URL nor the HTTP headers provide enough information, you can explicitly specify the decompression and parsing steps using a pipeline argument.

### `url: string`

[Section titled “url: string”](#url-string)

URL to connect to.

### `method = string (optional)`

[Section titled “method = string (optional)”](#method--string-optional)

One of the following HTTP methods to use when using the client:

* `get`
* `head`
* `post`
* `put`
* `del`
* `connect`
* `options`
* `trace`

Defaults to `get`, or `post` if `body` is specified.

### `body = blob|record|string (optional)`

[Section titled “body = blob|record|string (optional)”](#body--blobrecordstring-optional)

Body to send with the HTTP request.

If the value is a `record`, then the body is encoded according to the `encode` option and an appropriate `Content-Type` is set for the request.

### `encode = string (optional)`

[Section titled “encode = string (optional)”](#encode--string-optional)

Specifies how to encode `record` bodies. Supported values:

* `json`
* `form`

Defaults to `json`.

### `headers = record (optional)`

[Section titled “headers = record (optional)”](#headers--record-optional)

Record of headers to send with the request.

### `response_field = field (optional)`

[Section titled “response\_field = field (optional)”](#response_field--field-optional)

Field to insert the response into.

Defaults to `this`.

### `metadata_field = field (optional)`

[Section titled “metadata\_field = field (optional)”](#metadata_field--field-optional)

Field to insert metadata into when using the parsing pipeline.

The metadata has the following schema:

| Field     | Type     | Description                           |
| :-------- | :------- | :------------------------------------ |
| `code`    | `uint64` | The HTTP status code of the response. |
| `headers` | `record` | The response headers.                 |

### `error_field = field (optional)`

[Section titled “error\_field = field (optional)”](#error_field--field-optional)

Field to insert the response body for HTTP error responses (status codes not in the 2xx or 3xx range).

When set, any HTTP response with a status code outside the 200–399 range will have its body stored in this field as a `blob`. Otherwise, error responses, alongside the original event, are skipped and a warning is emitted.

### `paginate = record -> string (optional)`

[Section titled “paginate = record -> string (optional)”](#paginate--record---string-optional)

A lambda expression to evaluate against the result of the request (optionally parsed by the given pipeline). If the expression evaluation is successful and non-null, the resulting string is used as the URL for a new GET request with the same headers.

### `paginate_delay = duration (optional)`

[Section titled “paginate\_delay = duration (optional)”](#paginate_delay--duration-optional)

The duration to wait between consecutive pagination requests.

Defaults to `0s`.

### `parallel = int (optional)`

[Section titled “parallel = int (optional)”](#parallel--int-optional)

Maximum amount of requests that can be in progress at any time.

Defaults to `1`.

### `tls = bool (optional)`

[Section titled “tls = bool (optional)”](#tls--bool-optional)

Enables TLS.

### `certfile = string (optional)`

[Section titled “certfile = string (optional)”](#certfile--string-optional)

Path to the client certificate.

### `keyfile = string (optional)`

[Section titled “keyfile = string (optional)”](#keyfile--string-optional)

Path to the key for the client certificate.

### `password = string (optional)`

[Section titled “password = string (optional)”](#password--string-optional)

Password file for keyfile.

### `connection_timeout = duration (optional)`

[Section titled “connection\_timeout = duration (optional)”](#connection_timeout--duration-optional)

Timeout for the connection.

Defaults to `5s`.

### `max_retry_count = int (optional)`

[Section titled “max\_retry\_count = int (optional)”](#max_retry_count--int-optional)

The maximum times to retry a failed request. Every request has its own retry count.

Defaults to `0`.

### `retry_delay = duration (optional)`

[Section titled “retry\_delay = duration (optional)”](#retry_delay--duration-optional)

The duration to wait between each retry.

Defaults to `1s`.

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

A pipeline that receives the response body as bytes, allowing parsing per request. This is especially useful in scenarios where the response body can be parsed into multiple events.

If not provided, the operator will attempt to infer the parsing operator from the `Content-Type` header. Should this inference fail (e.g., unsupported or missing `Content-Type`), the operator raises a warning and skips the request.

## Examples

[Section titled “Examples”](#examples)

### Make a GET request

[Section titled “Make a GET request”](#make-a-get-request)

Here we make a request to [urlscan.io](https://urlscan.io/docs/api#search) to search for scans for `tenzir.com` and get the first result.

```tql
from {}
http "https://urlscan.io/api/v1/search?q=tenzir.com"
unroll results
head 1
```

```tql
{
  results: {
    submitter: { ... },
    task: { ... },
    stats: { ... },
    page: { ... },
    _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30",
    _score: null,
    sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ],
    result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/",
    screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png",
  },
  total: 9,
  took: 296,
  has_more: false,
}
```

### Keeping input context

[Section titled “Keeping input context”](#keeping-input-context)

Frequently, the purpose of making real-time requests in a pipeline is to enrich the incoming data with additional context. In these cases, we want to keep the original event around. This can be done simply by specifying the `response_field` and `metadata_field` options as appropriate.

E.g. in the above example, let’s assume we had some initial context that we want to keep around:

```tql
from { ctx: {severity: "HIGH"}, domain: "tenzir.com", ip: 0.0.0.0 }
http "https://urlscan.io/api/v1/search?q=" + domain, response_field=scan
scan.results = scan.results[0]
```

```tql
{
  ctx: {
    severity: "HIGH",
  },
  domain: "tenzir.com",
  ip: 0.0.0.0,
  scan: {
    results: {
      submitter: { ... },
      task: { ... },
      stats: { ... },
      page: { ... },
      _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30",
      _score: null,
      sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ],
      result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/",
      screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png",
    },
    total: 9,
    took: 88,
    has_more: false,
  },
}
```

### Paginate an API

[Section titled “Paginate an API”](#paginate-an-api)

We can utilize the `sort` and `has_more` fields to get more pages from the API.

```tql
let $URL = "https://urlscan.io/api/v1/search?q=example.com"
from {}
http $URL, paginate=(x => $URL + "&search_after=" + results.last().sort.first() + "," + results.last().sort.last().slice(begin=1, end=-1) if has_more?)
head 10
```

Here we construct the next url for pagination by extracting values from the responses.

The query parameter `search_after` expects the two values from the `sort` key in the response to be joined with a `,`. Thus forming a URL like `https://urlscan.io/api/v1/search?q=example.com&search_after=1747796723608,0196f0cd-6fda-761a-81a6-ae1b18914e61`.

The `if has_more?` ensures pagination only continues till we have a `has_more` field that is `true`.

Additionally, we limit the maximum pages by a simple `head 10`.

## See Also

[Section titled “See Also”](#see-also)

[`from_http`](/reference/operators/from_http)

# import

Imports events into a Tenzir node.

```tql
import
```

## Description

[Section titled “Description”](#description)

The `import` operator persists events in a Tenzir node.

This operator is the dual to [`export`](/reference/operators/export).

## Examples

[Section titled “Examples”](#examples)

### Import Zeek connection logs in TSV format

[Section titled “Import Zeek connection logs in TSV format”](#import-zeek-connection-logs-in-tsv-format)

```tql
load_file "conn.log"
read_zeek_tsv
import
```

## See Also

[Section titled “See Also”](#see-also)

[`export`](/reference/operators/export), [`publish`](/reference/operators/publish)

# load_amqp

Loads a byte stream via AMQP messages.

```tql
load_amqp [url:str, channel=int, exchange=str, routing_key=str, queue=str,
           options=record, passive=bool, durable=bool, exclusive=bool,
           no_auto_delete=bool, no_local=bool, ack=bool]
```

## Description

[Section titled “Description”](#description)

The `load_amqp` operator is an [AMQP](https://www.amqp.org/) 0-9-1 client to receive messages from a queue.

### `url: str (optional)`

[Section titled “url: str (optional)”](#url-str-optional)

A URL that specifies the AMQP server. The URL must have the following format:

```plaintext
amqp://[USERNAME[:PASSWORD]@]HOSTNAME[:PORT]/[VHOST]
```

When the URL is present, it will overwrite the corresponding values of the configuration options.

### `channel = int (optional)`

[Section titled “channel = int (optional)”](#channel--int-optional)

The channel number to use.

Defaults to `1`.

### `exchange = str (optional)`

[Section titled “exchange = str (optional)”](#exchange--str-optional)

The exchange to interact with.

Defaults to `"amq.direct"`.

### `routing_key = str (optional)`

[Section titled “routing\_key = str (optional)”](#routing_key--str-optional)

The name of the routing key to bind a queue to an exchange.

Defaults to the empty string.

### `options = record (optional)`

[Section titled “options = record (optional)”](#options--record-optional)

An option record for RabbitMQ , e.g., `{max_channels: 42, frame_size: 1024, sasl_method: "external"}`.

Available options are:

```yaml
hostname: 127.0.0.1
port: 5672
ssl: false
vhost: /
max_channels: 2047
frame_size: 131072
heartbeat: 0
sasl_method: plain
username: guest
password: guest
```

### `queue = str (optional)`

[Section titled “queue = str (optional)”](#queue--str-optional)

The name of the queue to declare and then bind to.

Defaults to the empty string, resulting in auto-generated queue names, such as `"amq.gen-XNTLF0FwabIn9FFKKtQHzg"`.

### `passive = bool (optional)`

[Section titled “passive = bool (optional)”](#passive--bool-optional)

If `true`, the server will reply with OK if an exchange already exists with the same name, and raise an error otherwise.

Defaults to `false`.

### `durable = bool (optional)`

[Section titled “durable = bool (optional)”](#durable--bool-optional)

If `true` when creating a new exchange, the exchange will be marked as durable. Durable exchanges remain active when a server restarts. Non-durable exchanges (transient exchanges) are purged if/when a server restarts.

Defaults to `false`.

### `exclusive = bool (optional)`

[Section titled “exclusive = bool (optional)”](#exclusive--bool-optional)

If `true`, marks the queue as exclusive. Exclusive queues may only be accessed by the current connection, and are deleted when that connection closes. Passive declaration of an exclusive queue by other connections are not allowed.

Defaults to `false`.

### `no_auto_delete = bool (optional)`

[Section titled “no\_auto\_delete = bool (optional)”](#no_auto_delete--bool-optional)

If `true`, the exchange will *not* be deleted when all queues have finished using it.

Defaults to `false`.

### `no_local = bool (optional)`

[Section titled “no\_local = bool (optional)”](#no_local--bool-optional)

If `true`, the server will not send messages to the connection that published them.

Defaults to `false`.

### `ack = bool (optional)`

[Section titled “ack = bool (optional)”](#ack--bool-optional)

If `true`, the server expects acknowledgements for messages. Otherwise, when a message is delivered to the client the server assumes the delivery will succeed and immediately dequeues it. This functionality may decrease performance, while improving reliability. Without this flag, messages can get lost if a client dies before they are delivered to the application.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Consume a message from a specified AMQP queue

[Section titled “Consume a message from a specified AMQP queue”](#consume-a-message-from-a-specified-amqp-queue)

```tql
load_amqp "amqp://admin:pass@0.0.0.1:5672/vhost", queue="foo"
read_json
```

## See Also

[Section titled “See Also”](#see-also)

[`save_amqp`](/reference/operators/save_amqp)

# load_azure_blob_storage

Loads bytes from Azure Blob Storage.

```tql
load_azure_blob_storage uri:string, [account_key=string]
```

## Description

[Section titled “Description”](#description)

The `load_azure_blob_storage` operator loads bytes from an Azure Blob Storage.

By default, authentication is handled by the Azure SDK’s credential chain which may read from multiple environment variables, such as:

* `AZURE_TENANT_ID`
* `AZURE_CLIENT_ID`
* `AZURE_CLIENT_SECRET`
* `AZURE_AUTHORITY_HOST`
* `AZURE_CLIENT_CERTIFICATE_PATH`
* `AZURE_FEDERATED_TOKEN_FILE`

### `uri: string`

[Section titled “uri: string”](#uri-string)

A URI identifying the blob to load from.

Supported URI formats:

1. `abfs[s]://[:<password>@]<account>.blob.core.windows.net[/<container>[/<path>]]`
2. `abfs[s]://<container>[:<password>]@<account>.dfs.core.windows.net[/path]`
3. `abfs[s]://[<account[:<password>]@]<host[.domain]>[<:port>][/<container>[/path]]`
4. `abfs[s]://[<account[:<password>]@]<container>[/path]`

(1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs 1, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2).

Authenticate with the Azure CLI

Run `az login` on the command-line to authenticate the current user with Azure’s command-line arguments.

### `account_key = string (optional)`

[Section titled “account\_key = string (optional)”](#account_key--string-optional)

Account key to authenticate with.

## Examples

[Section titled “Examples”](#examples)

### Write JSON

[Section titled “Write JSON”](#write-json)

Read JSON from a blob `obj.json` in the blob container `container`, using the `tenzirdev` user:

```tql
load_azure_blob_storage "abfss://tenzirdev@container/obj.json"
read_json
```

## See Also

[Section titled “See Also”](#see-also)

[`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage)

# load_balance

Routes the data to one of multiple subpipelines.

```tql
load_balance over:list { … }
```

## Description

[Section titled “Description”](#description)

The `load_balance` operator spawns a nested pipeline for each element in the given list. Incoming events are distributed to exactly one of the nested pipelines. This operator may reorder the event stream.

### `over: list`

[Section titled “over: list”](#over-list)

This must be a `$`-variable, previously declared with `let`. For example, to load balance over a list of ports, use `let $cfg = [8080, 8081, 8082]` followed by `load_balance $cfg { … }`.

### `{ … }`

[Section titled “{ … }”](#-)

The nested pipeline to spawn. This pipeline can use the same variable as passed to `over`, which will be resolved to one of the list items. The following example spawns three nested pipelines, where `$port` is bound to `8080`, `8081` and `8082`, respectively.

```tql
let $cfg = [8080, 8081, 8082]
load_balance $cfg {
  let $port = $cfg
  // … continue here
}
```

The given subpipeline must end with a sink. This limitation might be removed in future versions.

## Examples

[Section titled “Examples”](#examples)

### Route data to multiple TCP ports

[Section titled “Route data to multiple TCP ports”](#route-data-to-multiple-tcp-ports)

```tql
let $cfg = ["192.168.0.30:8080", "192.168.0.30:8081"]


subscribe "input"
load_balance $cfg {
  write_json
  save_tcp $cfg
}
```

### Route data to multiple Splunk endpoints

[Section titled “Route data to multiple Splunk endpoints”](#route-data-to-multiple-splunk-endpoints)

```tql
let $cfg = [{
  ip: 192.168.0.30,
  token: "example-token-1234",
}, {
  ip: 192.168.0.31,
  token: "example-token-5678",
}]


subscribe "input"
load_balance $cfg {
  let $endpoint = string($cfg.ip) + ":8080"
  to_splunk $endpoint, hec_token=$cfg.token
}
```

# load_file

Loads the contents of the file at `path` as a byte stream.

```tql
load_file path:string, [follow=bool, mmap=bool, timeout=duration]
```

## Description

[Section titled “Description”](#description)

The `file` loader acquires raw bytes from a file.

### `path: string`

[Section titled “path: string”](#path-string)

The file path to load from. When `~` is the first character, it will be substituted with the value of the `$HOME` environment variable.

### `follow = bool (optional)`

[Section titled “follow = bool (optional)”](#follow--bool-optional)

Do not stop when the end of file is reached, but rather to wait for additional data to be appended to the input.

### `mmap = bool (optional)`

[Section titled “mmap = bool (optional)”](#mmap--bool-optional)

Use the `mmap(2)` system call to map the file and produce only one single chunk of bytes, instead of producing data piecemeal via `read(2)`. This option effectively gives the downstream parser full control over reads.

<!--
TODO: Add this back once they are ported.

For the [`feather`](TODO) and [`parquet`](TODO) parsers, this significantly
reduces memory usage and improves performance.
-->

### `timeout = duration (optional)`

[Section titled “timeout = duration (optional)”](#timeout--duration-optional)

Wait at most for the provided duration when performing a blocking system call. This flags comes in handy in combination with `follow=true` to produce a steady pulse of input in the pipeline execution, as input (even if empty) drives the processing forward.

## Examples

[Section titled “Examples”](#examples)

### Load the raw contents of a file

[Section titled “Load the raw contents of a file”](#load-the-raw-contents-of-a-file)

```tql
load_file "example.txt"
```

## See Also

[Section titled “See Also”](#see-also)

[`files`](/reference/operators/files), [`from_file`](/reference/operators/from_file), [`load_stdin`](/reference/operators/load_stdin), [`save_file`](/reference/operators/save_file)

# load_ftp

Loads a byte stream via FTP.

```tql
load_ftp url:str [tls=bool, cacert=string, certifle=string,
                  keyfile=string, skip_peer_verification=bool]
```

## Description

[Section titled “Description”](#description)

Loads a byte stream via FTP.

### `url: str`

[Section titled “url: str”](#url-str)

The URL to request from. The `ftp://` scheme can be omitted.

### `tls = bool (optional)`

Enables TLS.

Defaults to `false`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

```tql
load_ftp "ftp.example.org"
```

# load_gcs

Loads bytes from a Google Cloud Storage object.

```tql
load_gcs uri:string, [anonymous=bool]
```

## Description

[Section titled “Description”](#description)

The `load_gcs` operator connects to a GCS bucket to acquire raw bytes from a GCS object.

The connector tries to retrieve the appropriate credentials using Google’s [Application Default Credentials](https://google.aip.dev/auth/4110).

### `uri: string`

[Section titled “uri: string”](#uri-string)

The path to the GCS object.

The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist:

> For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`.

### `anonymous = bool (optional)`

[Section titled “anonymous = bool (optional)”](#anonymous--bool-optional)

Ignore any predefined credentials and try to use anonymous credentials.

## Examples

[Section titled “Examples”](#examples)

Read JSON from an object `log.json` in the folder `logs` in `bucket`.

```tql
load_gcs "gs://bucket/logs/log.json"
read_json
```

## See Also

[Section titled “See Also”](#see-also)

[`save_gcs`](/reference/operators/save_gcs)

# load_google_cloud_pubsub

Subscribes to a Google Cloud Pub/Sub subscription and obtains bytes.

```tql
load_google_cloud_pubsub project_id=string, subscription_id=string, [timeout=duration]
```

Authentication

The connector tries to retrieve the appropriate credentials using Google’s [Application Default Credentials](https://google.aip.dev/auth/4110).

## Description

[Section titled “Description”](#description)

The operator acquires raw bytes from a Google Cloud Pub/Sub subscription.

### `project_id = string`

[Section titled “project\_id = string”](#project_id--string)

The project to connect to. Note that this is the project id, not the display name.

### `subscription_id = string`

[Section titled “subscription\_id = string”](#subscription_id--string)

The subscription to subscribe to.

### `timeout = duration (optional)`

[Section titled “timeout = duration (optional)”](#timeout--duration-optional)

How long to wait for messages before ending the connection. A duration of zero means the operator will run forever.

The default value is `0s`.

## URI support & integration with `from`

[Section titled “URI support & integration with from”](#uri-support--integration-with-from)

The `load_google_cloud_pubsub` operator can also be used from the [`from`](/reference/operators/from) operator. For this, the `gcps://` scheme can be used. The URI is then translated:

```tql
from "gcps://my_project/my_subscription"
```

```tql
load_google_cloud_pubsub project_id="my_project", subscription_id="my_subscription"
```

## Examples

[Section titled “Examples”](#examples)

### Read JSON messages from a subscription

[Section titled “Read JSON messages from a subscription”](#read-json-messages-from-a-subscription)

Subscribe to `my-subscription` in the project `amazing-project-123456` and parse the messages as JSON:

```tql
load_google_cloud_pubsub project_id="amazing-project-123456", subscription_id="my-subscription"
read_json
```

## See Also

[Section titled “See Also”](#see-also)

[`save_google_cloud_pubsub`](/reference/operators/save_google_cloud_pubsub)

# load_http

Loads a byte stream via HTTP.

```tql
load_http url:string, [data=record, params=record, headers=record,
          method=string, form=bool, chunked=bool, multipart=bool,
          parallel=int, tls=bool, cacert=string, certifle=string,
          keyfile=string, skip_peer_verification=bool]
```

## Description

[Section titled “Description”](#description)

The `load_http` operator performs a HTTP request and returns the response.

### `url: string`

[Section titled “url: string”](#url-string)

The URL to request from. The `http://` scheme can be omitted.

### `method = string (optional)`

[Section titled “method = string (optional)”](#method--string-optional)

The HTTP method, such as `POST` or `GET`.

The default is `"GET"`.

### `params = record (optional)`

[Section titled “params = record (optional)”](#params--record-optional)

The query parameters for the request.

### `headers = record (optional)`

[Section titled “headers = record (optional)”](#headers--record-optional)

The headers for the request.

### `data = record (optional)`

[Section titled “data = record (optional)”](#data--record-optional)

The request body as a record of key-value pairs. The body is encoded as JSON unless `form=true` has been set.

### `form = bool (optional)`

[Section titled “form = bool (optional)”](#form--bool-optional)

Submits the HTTP request body as form-encoded data.

This automatically sets the `Content-Type` header to `application/x-www-form-urlencoded`.

Defaults to `false`.

### `chunked = bool (optional)`

[Section titled “chunked = bool (optional)”](#chunked--bool-optional)

Whether to enable [chunked transfer encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding). This is equivalent to manually setting the header `Transfer-Encoding: chunked`.

Defaults to `false`.

### `multipart = bool (optional)`

[Section titled “multipart = bool (optional)”](#multipart--bool-optional)

Whether to encode the HTTP request body as [multipart message](https://en.wikipedia.org/wiki/MIME#Multipart_messages).

This automatically sets the `Content-Type` header to `application/form-multipart; X` where `X` contains the MIME part boundary.

Defaults to `false`.

### `tls = bool (optional)`

Enables TLS.

Defaults to `true`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Perform an API call and get the response

[Section titled “Perform an API call and get the response”](#perform-an-api-call-and-get-the-response)

```tql
load_http "example.org/api", headers={"X-API-Token": "0000-0000-0000"}
```

## See Also

[Section titled “See Also”](#see-also)

[`save_http`](/reference/operators/save_http)

# load_kafka

Loads a byte stream from an Apache Kafka topic.

```tql
load_kafka topic:string, [count=int, exit=bool, offset=int|string, options=record,
           aws_iam=record, commit_batch_size=int, commit_timeout=duration]
```

## Description

[Section titled “Description”](#description)

The `load_kafka` operator reads bytes from a Kafka topic.

The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`.

The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them:

* `bootstrap.servers`: `localhost`
* `client.id`: `tenzir`
* `group.id`: `tenzir`
* `enable.auto.commit`: `false` (This option cannot be changed)

### `topic: string`

[Section titled “topic: string”](#topic-string)

The Kafka topic to use.

### `count = int (optional)`

[Section titled “count = int (optional)”](#count--int-optional)

Exit successfully after having consumed `count` messages.

### `exit = bool (optional)`

[Section titled “exit = bool (optional)”](#exit--bool-optional)

Exit successfully after having received the last message.

Without this option, the operator waits for new messages after consuming the last one.

### `offset = int|string (optional)`

[Section titled “offset = int|string (optional)”](#offset--intstring-optional)

The offset to start consuming from. Possible values are:

* `"beginning"`: first offset
* `"end"`: last offset
* `"stored"`: stored offset
* `<value>`: absolute offset
* `-<value>`: relative offset from end

The default is `"stored"`.

<!--
- `s@<value>`: timestamp in ms to start at
- `e@<value>`: timestamp in ms to stop at (not included)
-->

### `options = record (optional)`

[Section titled “options = record (optional)”](#options--record-optional)

A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"auto.offset.reset" : "earliest", "enable.partition.eof": true}`.

The `load_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs.

We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `load_kafka` arguments.

### `commit_batch_size = int (optional)`

[Section titled “commit\_batch\_size = int (optional)”](#commit_batch_size--int-optional)

The operator commits offsets after receiving `commit_batch_size` messages to improve throughput. If you need to ensure exactly-once semantics for your pipeline, set this option to `1` to commit every message individually.

Defaults to `1000`.

### `commit_timeout = duration (optional)`

[Section titled “commit\_timeout = duration (optional)”](#commit_timeout--duration-optional)

A timeout after which the operator commits messages, even if it accepted fewer than `commit_batch_size`. This helps with long-running, low-volume pipelines.

Defaults to `10s`.

### `aws_iam = record (optional)`

[Section titled “aws\_iam = record (optional)”](#aws_iam--record-optional)

If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified.

Available keys:

* `region`: Region of the MSK Clusters. Must be specified when using IAM.
* `assume_role`: Optional role ARN to assume.
* `session_name`: Optional session name to use when assuming a role.
* `external_id`: Optional external id to use when assuming a role.

The operator tries to get credentials in the following order:

1. Checks your environment variables for AWS Credentials.
2. Checks your `$HOME/.aws/credentials` file for a profile and credentials
3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`.
4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that isn’t directly supported by AWS.
5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set.
6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON.

## Examples

[Section titled “Examples”](#examples)

### Read 100 JSON messages from the topic `tenzir`

[Section titled “Read 100 JSON messages from the topic tenzir”](#read-100-json-messages-from-the-topic-tenzir)

```tql
load_kafka "tenzir", count=100
read_json
```

### Read Zeek Streaming JSON logs starting at the beginning

[Section titled “Read Zeek Streaming JSON logs starting at the beginning”](#read-zeek-streaming-json-logs-starting-at-the-beginning)

```tql
load_kafka "zeek", offset="beginning"
read_zeek_json
```

## See Also

[Section titled “See Also”](#see-also)

[`save_kafka`](/reference/operators/save_kafka)

# load_nic

Loads bytes from a network interface card (NIC).

```tql
load_nic iface:str, [snaplen=int, emit_file_headers=bool]
```

## Description

[Section titled “Description”](#description)

The `load_nic` operator uses libpcap to acquire packets from a network interface and packs them into blocks of bytes that represent PCAP packet records.

The received first packet triggers also emission of PCAP file header such that downstream operators can treat the packet stream as valid PCAP capture file.

### `iface: str`

[Section titled “iface: str”](#iface-str)

The interface to load bytes from.

### `snaplen = int (optional)`

[Section titled “snaplen = int (optional)”](#snaplen--int-optional)

Sets the snapshot length of the captured packets.

This value is an upper bound on the packet size. Packets larger than this size get truncated to `snaplen` bytes.

Defaults to `262144`.

### `emit_file_headers = bool (optional)`

[Section titled “emit\_file\_headers = bool (optional)”](#emit_file_headers--bool-optional)

Creates PCAP file headers for every flushed batch.

The operator emits chunk of bytes that represent a stream of packets. When setting `emit_file_headers` every chunk gets its own PCAP file header, as opposed to just the very first. This yields a continuous stream of concatenated PCAP files.

Our [`read_pcap`](/reference/operators/read_pcap) operator can handle such concatenated traces, and optionally re-emit thes file headers as separate events.

## Examples

[Section titled “Examples”](#examples)

### Read PCAP packets from `eth0`

[Section titled “Read PCAP packets from eth0”](#read-pcap-packets-from-eth0)

```tql
load_nic "eth0"
read_pcap
```

### Perform the equivalent of `tcpdump -i en0 -w trace.pcap`

[Section titled “Perform the equivalent of tcpdump -i en0 -w trace.pcap”](#perform-the-equivalent-of-tcpdump--i-en0--w-tracepcap)

```tql
load_nic "en0"
read_pcap
write_pcap
save_file "trace.pcap"
```

## See Also

[Section titled “See Also”](#see-also)

[`read_pcap`](/reference/operators/read_pcap), [`nics`](/reference/operators/nics), [`write_pcap`](/reference/operators/write_pcap)

# load_s3

Loads from an Amazon S3 object.

```tql
load_s3 uri:str, [anonymous=bool, role=string, external_id=string]
```

## Description

[Section titled “Description”](#description)

The `load_s3` operator connects to an S3 bucket to acquire raw bytes from an S3 object.

The connector tries to retrieve the appropriate credentials using AWS’s [default credentials provider chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html).

Note

Make sure to configure AWS credentials for the same user account that runs `tenzir` and `tenzir-node`. The AWS CLI creates configuration files for the current user under `~/.aws`, which can only be read by the same user account.

The `tenzir-node` systemd unit by default creates a `tenzir` user and runs as that user, meaning that the AWS credentials must also be configured for that user. The directory `~/.aws` must be readable for the `tenzir` user.

If a config file `<prefix>/etc/tenzir/plugin/s3.yaml` or `~/.config/tenzir/plugin/s3.yaml` exists, it is always preferred over the default AWS credentials. The configuration file must have the following format:

```yaml
access-key: your-access-key
secret-key: your-secret-key
session-token: your-session-token (optional)
```

### `uri: str`

[Section titled “uri: str”](#uri-str)

The path to the S3 object.

The syntax is `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)`.

Options can be appended to the path as query parameters, as per [Arrow](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri):

> For S3, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`.

### `anonymous = bool (optional)`

[Section titled “anonymous = bool (optional)”](#anonymous--bool-optional)

Whether to ignore any predefined credentials and try to load with anonymous credentials.

### `role = string (optional)`

[Section titled “role = string (optional)”](#role--string-optional)

A role to assume when writing to S3.

### `external_id = string (optional)`

[Section titled “external\_id = string (optional)”](#external_id--string-optional)

The external ID to use when assuming the `role`.

Defaults to no ID.

## Examples

[Section titled “Examples”](#examples)

Read CSV from an object `obj.csv` in the bucket `examplebucket`:

```tql
load_s3 "s3://examplebucket/obj.csv"
read_csv
```

Read JSON from an object `test.json` in the bucket `examplebucket`, but using a different, S3-compatible endpoint:

```tql
load_s3 "s3://examplebucket/test.json?endpoint_override=s3.us-west.mycloudservice.com"
read_json
```

## See Also

[Section titled “See Also”](#see-also)

[`save_s3`](/reference/operators/save_s3)

# load_sqs

Loads bytes from [Amazon SQS](https://docs.aws.amazon.com/sqs/) queues.

```tql
load_sqs queue:str, [poll_time=duration]
```

## Description

[Section titled “Description”](#description)

[Amazon Simple Queue Service (Amazon SQS)](https://docs.aws.amazon.com/sqs/) is a fully managed message queuing service to decouple and scale microservices, distributed systems, and serverless applications. The `load_sqs` operator reads bytes from messages of an SQS queue.

The `load_sqs` operator uses long polling, which helps reduce your cost of using SQS by reducing the number of empty responses when there are no messages available to return in reply to a message request. Use the `poll_time` option to adjust the timeout.

The operator requires the following AWS permissions:

* `sqs:GetQueueUrl`
* `sqs:ReceiveMessage`
* `sqs:DeleteMessage`

### `queue: str`

[Section titled “queue: str”](#queue-str)

The name of the queue to use.

### `poll_time = duration (optional)`

[Section titled “poll\_time = duration (optional)”](#poll_time--duration-optional)

The long polling timeout per request.

The value must be between 1 and 20 seconds.

Defaults to `3s`.

## Examples

[Section titled “Examples”](#examples)

Read JSON messages from the SQS queue `tenzir`:

```tql
load_sqs "tenzir"
```

Read JSON messages with a 20-second long poll timeout:

```tql
load_sqs "tenzir", poll_time=20s
```

## See Also

[Section titled “See Also”](#see-also)

[`save_sqs`](/reference/operators/save_sqs)

# load_stdin

Accepts bytes from standard input.

```tql
load_stdin
```

## Description

[Section titled “Description”](#description)

Accepts bytes from standard input. This is mostly useful when using the `tenzir` executable as part of a shell script.

## Examples

[Section titled “Examples”](#examples)

### Pipe text into `tenzir`

[Section titled “Pipe text into tenzir”](#pipe-text-into-tenzir)

```sh
echo "Hello World" | tenzir
```

```tql
load_stdin
read_lines
```

```tql
{
  line: "Hello World",
}
```

## See Also

[Section titled “See Also”](#see-also)

[`save_stdout`](/reference/operators/save_stdout), [`load_file`](/reference/operators/load_file)

# load_tcp

Loads bytes from a TCP or TLS connection.

```tql
load_tcp endpoint:string, [parallel=int, peer_field=field, tls=bool,
                           cacert=string, certifle=string,
                           max_buffered_chunks=int { … }]
```

## Description

[Section titled “Description”](#description)

Reads bytes from the given endpoint via TCP or TLS.

### `endpoint: string`

[Section titled “endpoint: string”](#endpoint-string)

The endpoint at which the server will listen. Must be of the form `[tcp://]<hostname>:<port>`. Use the hostname `0.0.0.0` to accept connections on all interfaces.

### `parallel = int (optional)`

[Section titled “parallel = int (optional)”](#parallel--int-optional)

Number of threads to use for reading from connections.

Defaults to 1.

### `peer_field = field (optional)`

[Section titled “peer\_field = field (optional)”](#peer_field--field-optional)

Write a record with the fields `ip`, `port`, and `hostname` resembling the peer endpoint of the respective TCP connection into the specified field at the end of the nested pipeline.

### `tls = bool (optional)`

Enables TLS.

Defaults to `false`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

### `max_buffered_chunks = int (optional)`

[Section titled “max\_buffered\_chunks = int (optional)”](#max_buffered_chunks--int-optional)

Maximum number of buffered chunks per connection.

Defaults to 10.

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

The pipeline to run for each individual TCP connection. If none is specified, no transformations are applied to the output streams. Unless you are sure that there is at most one active connection at a time, it is recommended to specify a pipeline that parses the individual connection streams into events, for instance `{ read_json }`. Otherwise, the output can be interleaved.

## Examples

[Section titled “Examples”](#examples)

### Listen for incoming Syslog over TCP

[Section titled “Listen for incoming Syslog over TCP”](#listen-for-incoming-syslog-over-tcp)

Listen on all network interfaces, parsing each individual connection as syslog:

```tql
load_tcp "0.0.0.0:8090" { read_syslog }
```

### Connect to a remote endpoint and read JSON

[Section titled “Connect to a remote endpoint and read JSON”](#connect-to-a-remote-endpoint-and-read-json)

```tql
// We know that there is only one connection, so we do not specify a pipeline.
load_tcp "example.org:8090", connect=true
read_json
```

### Listen on localhost with TLS enabled

[Section titled “Listen on localhost with TLS enabled”](#listen-on-localhost-with-tls-enabled)

Wait for connections on localhost with TLS enabled, parsing incoming JSON streams according to the schema `"my_schema"`, forwarding no more than 20 events per individual connection:

```tql
load_tcp "127.0.0.1:4000", tls=true, certfile="key_and_cert.pem", keyfile="key_and_cert.pem" {
  read_json schema="my_schema"
  head 20
}
```

This example may use a self-signed certificate that can be generated like this:

```bash
openssl req -x509 -newkey rsa:2048 -keyout key_and_cert.pem -out key_and_cert.pem -days 365 -nodes
```

You can test the endpoint locally by issuing a TLS connection:

```bash
openssl s_client 127.0.0.1:4000
```

# load_udp

Loads bytes from a UDP socket.

```tql
load_udp endpoint:str, [connect=bool, insert_newlines=bool]
```

## Description

[Section titled “Description”](#description)

Loads bytes from a UDP socket. The operator defaults to creating a socket in listening mode. Use `connect=true` if the operator should initiate the connection instead.

When you have a socket in listening mode, use `0.0.0.0` to accept connections on all interfaces. The [`nics`](/reference/operators/nics) operator lists all all available interfaces.

### `endpoint: str`

[Section titled “endpoint: str”](#endpoint-str)

The address of the remote endpoint to load bytes from. Must be of the format: `[udp://]host:port`.

### `connect = bool (optional)`

[Section titled “connect = bool (optional)”](#connect--bool-optional)

Connect to `endpoint` instead of listening at it.

Defaults to `false`.

### `insert_newlines = bool (optional)`

[Section titled “insert\_newlines = bool (optional)”](#insert_newlines--bool-optional)

Append a newline character (`\n`) at the end of every datagram.

This option comes in handy in combination with line-based parsers downstream, such as NDJSON.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Import JSON via UDP by listenting on localhost

[Section titled “Import JSON via UDP by listenting on localhost”](#import-json-via-udp-by-listenting-on-localhost)

```tql
load_udp "127.0.0.1:56789"
import
```

## See Also

[Section titled “See Also”](#see-also)

[`save_udp`](/reference/operators/save_udp)

# load_zmq

Receives ZeroMQ messages.

```tql
load_zmq [endpoint:str, filter=str, listen=bool, connect=bool,
          insert_separator=string, monitor=bool]
```

## Description

[Section titled “Description”](#description)

The `load_zmq` operator processes the bytes in a ZeroMQ message received by a `SUB` socket.

Indpendent of the socket type, the `load_zmq` operator supports specfiying the direction of connection establishment with `listen` and `connect`. This can be helpful to work around firewall restrictions and fit into broader set of existing ZeroMQ applications.

With the `monitor` option, you can activate message buffering for TCP sockets that hold off sending messages until *at least one* remote peer has connected. This can be helpful when you want to delay publishing until you have one connected subscriber, e.g., when the publisher spawns before any subscriber exists.

### `endpoint: str (optional)`

[Section titled “endpoint: str (optional)”](#endpoint-str-optional)

The endpoint for connecting to or listening on a ZeroMQ socket.

Defaults to `tcp://127.0.0.1:5555`.

### `filter = str (optional)`

[Section titled “filter = str (optional)”](#filter--str-optional)

Installs a filter for the ZeroMQ `SUB` socket at the source. Filting in ZeroMQ means performing a prefix-match on the raw bytes of the entire message.

Defaults to the empty string, which is equivalent to no filtering.

### `listen = bool (optional)`

[Section titled “listen = bool (optional)”](#listen--bool-optional)

Bind to the ZeroMQ socket.

Defaults to `false`.

### `connect = bool (optional)`

[Section titled “connect = bool (optional)”](#connect--bool-optional)

Connect to the ZeroMQ socket.

Defaults to `true`.

### `insert_separator = string (optional)`

[Section titled “insert\_separator = string (optional)”](#insert_separator--string-optional)

A separator string to append to each received ZeroMQ message.

Defaults to no separator.

### `monitor = bool (optional)`

[Section titled “monitor = bool (optional)”](#monitor--bool-optional)

Monitors a 0mq socket over TCP until the remote side establishes a connection.

## Examples

[Section titled “Examples”](#examples)

### Interpret ZeroMQ messages as JSON

[Section titled “Interpret ZeroMQ messages as JSON”](#interpret-zeromq-messages-as-json)

```tql
load_zmq "1.2.3.4:56789"
read_json
```

## See Also

[Section titled “See Also”](#see-also)

[`save_zmq`](/reference/operators/save_zmq)

# local

Forces a pipeline to run locally.

```tql
local { … }
```

## Description

[Section titled “Description”](#description)

The `local` operator takes a pipeline as an argument and forces it to run at a client process.

This operator has no effect when running a pipeline through the API or Tenzir Platform.

## Examples

[Section titled “Examples”](#examples)

### Do an expensive sort locally

[Section titled “Do an expensive sort locally”](#do-an-expensive-sort-locally)

```tql
export
where @name.starts_with("suricata")
local {
  sort timestamp
}
write_ndjson
save_file "eve.json"
```

## See Also

[Section titled “See Also”](#see-also)

[`remote`](/reference/operators/remote)

# measure

Replaces the input with metrics describing the input.

```tql
measure [real_time=bool, cumulative=bool]
```

## Description

[Section titled “Description”](#description)

The `measure` operator yields metrics for each received batch of events or bytes using the following schema, respectively:

Events Metrics

```text
type tenzir.measure.events = record{
  timestamp: time,
  events: uint64,
  schema_id: string,
  schema: string,
}
```

Bytes Metrics

```text
type tenzir.measure.bytes = record{
  timestamp: time,
  bytes: uint64,
}
```

### `real_time = bool (optional)`

[Section titled “real\_time = bool (optional)”](#real_time--bool-optional)

Whether to emit metrics immediately with every batch, rather than buffering until the upstream operator stalls, i.e., is idle or waiting for further input.

The is especially useful when `measure` should emit data without latency.

### `cumulative = bool (optional)`

[Section titled “cumulative = bool (optional)”](#cumulative--bool-optional)

Whether to emit running totals for the `events` and `bytes` fields rather than per-batch statistics.

## Examples

[Section titled “Examples”](#examples)

### Get the number of bytes read incrementally for a file

[Section titled “Get the number of bytes read incrementally for a file”](#get-the-number-of-bytes-read-incrementally-for-a-file)

```tql
load_file "input.json"
measure
```

```tql
{timestamp: 2023-04-28T10:22:10.192322, bytes: 16384}
{timestamp: 2023-04-28T10:22:10.223612, bytes: 16384}
{timestamp: 2023-04-28T10:22:10.297169, bytes: 16384}
{timestamp: 2023-04-28T10:22:10.387172, bytes: 16384}
{timestamp: 2023-04-28T10:22:10.408171, bytes: 8232}
```

### Get the number of events read incrementally from a file

[Section titled “Get the number of events read incrementally from a file”](#get-the-number-of-events-read-incrementally-from-a-file)

```tql
load_file "eve.json"
read_suricata
measure
```

```tql
{
  timestamp: 2023-04-28T10:26:45.159885,
  events: 65536,
  schema_id: "d49102998baae44a",
  schema: "suricata.dns"
}
{
  timestamp: 2023-04-28T10:26:45.812321,
  events: 412,
  schema_id: "d49102998baae44a",
  schema: "suricata.dns"
}
```

### Get the total number of events in a file, grouped by schema

[Section titled “Get the total number of events in a file, grouped by schema”](#get-the-total-number-of-events-in-a-file-grouped-by-schema)

```tql
load_file "eve.json"
read_suricata
measure
summarize schema, events=sum(events)
```

```tql
{schema: "suricata.dns", events: 65948}
```

# metrics

Retrieves metrics events from a Tenzir node.

```tql
metrics [name:string, live=bool, retro=bool]
```

## Description

[Section titled “Description”](#description)

The `metrics` operator retrieves metrics events from a Tenzir node. Metrics events are collected every second.

Retention Policy

Set the `tenzir.retention.metrics` configuration option to change how long Tenzir Nodes store metrics:

```yaml
tenzir:
  retention:
    metrics: 7d
```

### `name: string (optional)`

[Section titled “name: string (optional)”](#name-string-optional)

Show only metrics with the specified name. For example, `metrics "cpu"` only shows CPU metrics.

### `live = bool (optional)`

[Section titled “live = bool (optional)”](#live--bool-optional)

Work on all metrics events as they are generated in real-time instead of on metrics events persisted at a Tenzir node.

### `retro = bool (optional)`

[Section titled “retro = bool (optional)”](#retro--bool-optional)

Work on persisted diagnostic events (first), even when `live` is given.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir collects metrics with the following schemas.

### `tenzir.metrics.api`

[Section titled “tenzir.metrics.api”](#tenzirmetricsapi)

Contains information about all accessed API endpoints, emitted once per second.

| Field           | Type       | Description                                            |
| :-------------- | :--------- | :----------------------------------------------------- |
| `timestamp`     | `time`     | The time at which the API request was received.        |
| `request_id`    | `string`   | The unique request ID assigned by the Tenzir Platform. |
| `method`        | `double`   | The HTTP method used to access the API.                |
| `path`          | `double`   | The path of the accessed API endpoint.                 |
| `response_time` | `duration` | The time the API endpoint took to respond.             |
| `status_code`   | `uint64`   | The HTTP status code of the API response.              |
| `params`        | `record`   | The API endpoints parameters passed inused.            |

The schema of the record `params` depends on the API endpoint used. Refer to the [API documentation](/reference/node/api) to see the available parameters per endpoint.

### `tenzir.metrics.caf`

[Section titled “tenzir.metrics.caf”](#tenzirmetricscaf)

Contains metrics about the CAF (C++ Actor Framework) runtime system.

Aimed at Developers

CAF metrics primarily exist for debugging purposes. Actor names and other details contained in these metrics are documented only in source code, and we may change them without notice. Do not rely on specific actor names or metrics in production systems.

| Field       | Type           | Description                               |
| :---------- | :------------- | :---------------------------------------- |
| `system`    | `record`       | Metrics about the CAF actor system.       |
| `middleman` | `record`       | Metrics about CAF’s network layer.        |
| `actors`    | `list<record>` | Per-actor metrics for all running actors. |

The record `system` has the following schema:

| Field                    | Type     | Description                                                            |
| :----------------------- | :------- | :--------------------------------------------------------------------- |
| `running_actors`         | `int64`  | Number of currently running actors.                                    |
| `running_actors_by_name` | `list`   | Number of running actors, grouped by actor name.                       |
| `all_messages`           | `record` | Information about the total message metrics.                           |
| `messages_by_actor`      | `list`   | Information about the message metrics, grouped by receiving actor name |

The `running_actors_by_name` field is a `list` of `record`s with the following schema:

| Field   | Type     | Description                                        |
| :------ | :------- | :------------------------------------------------- |
| `name`  | `string` | Actor name.                                        |
| `count` | `int64`  | Number of actors with this name currently running. |

The `all_messages` field has the following schema:

| Field       | Type    | Description                   |
| :---------- | :------ | :---------------------------- |
| `processed` | `int64` | Number of processed messages. |
| `rejected`  | `int64` | Number of rejected messages.  |

The `messages_by_actor` field is a `list` of `record`s with the following schema:

| Field       | Type     | Description                                                                             |
| :---------- | :------- | :-------------------------------------------------------------------------------------- |
| `name`      | `string` | Name of the receiving actor. This may be null for messages without an associated actor. |
| `processed` | `int64`  | Number of processed messages.                                                           |
| `rejected`  | `int64`  | Number of rejected messages.                                                            |

The record `middleman` has the following schema:

| Field                    | Type       | Description                                           |
| :----------------------- | :--------- | :---------------------------------------------------- |
| `inbound_messages_size`  | `int64`    | Size of received messages in bytes since last metric. |
| `outbound_messages_size` | `int64`    | Size of sent messages in bytes since last metric.     |
| `serialization_time`     | `duration` | Time spent serializing messages since last metric.    |
| `deserialization_time`   | `duration` | Time spent deserializing messages since last metric.  |

Each record in the `actors` list has the following schema:

| Field             | Type       | Description                                       |
| :---------------- | :--------- | :------------------------------------------------ |
| `name`            | `string`   | Name of the actor.                                |
| `processing_time` | `duration` | Time spent processing messages since last metric. |
| `mailbox_time`    | `duration` | Time messages spent in mailbox since last metric. |
| `mailbox_size`    | `int64`    | Current number of messages in actor’s mailbox.    |

### `tenzir.metrics.buffer`

[Section titled “tenzir.metrics.buffer”](#tenzirmetricsbuffer)

Contains information about the `buffer` operator’s internal buffer.

| Field         | Type     | Description                                                                                |
| :------------ | :------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from.                              |
| `run`         | `uint64` | The number of the run, starting at 1 for the first run.                                    |
| `hidden`      | `bool`   | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `timestamp`   | `time`   | The time at which this metric was recorded.                                                |
| `operator_id` | `uint64` | The ID of the `buffer` operator in the pipeline.                                           |
| `used`        | `uint64` | The number of events stored in the buffer.                                                 |
| `free`        | `uint64` | The remaining capacity of the buffer.                                                      |
| `dropped`     | `uint64` | The number of events dropped by the buffer.                                                |

### `tenzir.metrics.cpu`

[Section titled “tenzir.metrics.cpu”](#tenzirmetricscpu)

Contains a measurement of CPU utilization.

| Field         | Type     | Description                                 |
| :------------ | :------- | :------------------------------------------ |
| `timestamp`   | `time`   | The time at which this metric was recorded. |
| `loadavg_1m`  | `double` | The load average over the last minute.      |
| `loadavg_5m`  | `double` | The load average over the last 5 minutes.   |
| `loadavg_15m` | `double` | The load average over the last 15 minutes.  |

### `tenzir.metrics.disk`

[Section titled “tenzir.metrics.disk”](#tenzirmetricsdisk)

Contains a measurement of disk space usage.

| Field         | Type     | Description                                                                        |
| :------------ | :------- | :--------------------------------------------------------------------------------- |
| `timestamp`   | `time`   | The time at which this metric was recorded.                                        |
| `path`        | `string` | The byte measurements below refer to the filesystem on which this path is located. |
| `total_bytes` | `uint64` | The total size of the volume, in bytes.                                            |
| `used_bytes`  | `uint64` | The number of bytes occupied on the volume.                                        |
| `free_bytes`  | `uint64` | The number of bytes still free on the volume.                                      |

### `tenzir.metrics.enrich`

[Section titled “tenzir.metrics.enrich”](#tenzirmetricsenrich)

Contains a measurement of the `enrich` operator, emitted once every second.

| Field         | Type     | Description                                                                                |
| :------------ | :------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from.                              |
| `run`         | `uint64` | The number of the run, starting at 1 for the first run.                                    |
| `hidden`      | `bool`   | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `timestamp`   | `time`   | The time at which this metric was recorded.                                                |
| `operator_id` | `uint64` | The ID of the `enrich` operator in the pipeline.                                           |
| `context`     | `string` | The name of the context the associated operator is using.                                  |
| `events`      | `uint64` | The amount of input events that entered the `enrich` operator since the last metric.       |
| `hits`        | `uint64` | The amount of successfully enriched events since the last metric.                          |

### `tenzir.metrics.export`

[Section titled “tenzir.metrics.export”](#tenzirmetricsexport)

Contains a measurement of the `export` operator, emitted once every second per schema. Note that internal events like metrics or diagnostics do not emit metrics themselves.

| Field           | Type     | Description                                                                                |
| :-------------- | :------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id`   | `string` | The ID of the pipeline where the associated operator is from.                              |
| `run`           | `uint64` | The number of the run, starting at 1 for the first run.                                    |
| `hidden`        | `bool`   | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `timestamp`     | `time`   | The time at which this metric was recorded.                                                |
| `operator_id`   | `uint64` | The ID of the `export` operator in the pipeline.                                           |
| `schema`        | `string` | The schema name of the batch.                                                              |
| `schema_id`     | `string` | The schema ID of the batch.                                                                |
| `events`        | `uint64` | The amount of events that were imported.                                                   |
| `queued_events` | `uint64` | The total amount of events that are enqueued in the export.                                |

### `tenzir.metrics.import`

[Section titled “tenzir.metrics.import”](#tenzirmetricsimport)

Contains a measurement the `import` operator, emitted once every second per schema. Note that internal events like metrics or diagnostics do not emit metrics themselves.

| Field         | Type     | Description                                                                                |
| :------------ | :------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from.                              |
| `run`         | `uint64` | The number of the run, starting at 1 for the first run.                                    |
| `hidden`      | `bool`   | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `timestamp`   | `time`   | The time at which this metric was recorded.                                                |
| `operator_id` | `uint64` | The ID of the `import` operator in the pipeline.                                           |
| `schema`      | `string` | The schema name of the batch.                                                              |
| `schema_id`   | `string` | The schema ID of the batch.                                                                |
| `events`      | `uint64` | The amount of events that were imported.                                                   |

### `tenzir.metrics.ingest`

[Section titled “tenzir.metrics.ingest”](#tenzirmetricsingest)

Contains a measurement of all data ingested into the database, emitted once per second and schema.

| Field       | Type     | Description                                 |
| :---------- | :------- | :------------------------------------------ |
| `timestamp` | `time`   | The time at which this metric was recorded. |
| `schema`    | `string` | The schema name of the batch.               |
| `schema_id` | `string` | The schema ID of the batch.                 |
| `events`    | `uint64` | The amount of events that were ingested.    |

### `tenzir.metrics.lookup`

[Section titled “tenzir.metrics.lookup”](#tenzirmetricslookup)

Contains a measurement of the `lookup` operator, emitted once every second.

| Field             | Type     | Description                                                                                        |
| :---------------- | :------- | :------------------------------------------------------------------------------------------------- |
| `pipeline_id`     | `string` | The ID of the pipeline where the associated operator is from.                                      |
| `run`             | `uint64` | The number of the run, starting at 1 for the first run.                                            |
| `hidden`          | `bool`   | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines.         |
| `timestamp`       | `time`   | The time at which this metric was recorded.                                                        |
| `operator_id`     | `uint64` | The ID of the `lookup` operator in the pipeline.                                                   |
| `context`         | `string` | The name of the context the associated operator is using.                                          |
| `live`            | `record` | Information about the live lookup.                                                                 |
| `retro`           | `record` | Information about the retroactive lookup.                                                          |
| `context_updates` | `uint64` | The amount of times the underlying context has been updated while the associated lookup is active. |

The record `live` has the following schema:

| Field    | Type     | Description                                                                |
| :------- | :------- | :------------------------------------------------------------------------- |
| `events` | `uint64` | The amount of input events used for the live lookup since the last metric. |
| `hits`   | `uint64` | The amount of live lookup matches since the last metric.                   |

The record `retro` has the following schema:

| Field           | Type     | Description                                                           |
| :-------------- | :------- | :-------------------------------------------------------------------- |
| `events`        | `uint64` | The amount of input events used for the lookup since the last metric. |
| `hits`          | `uint64` | The amount of lookup matches since the last metric.                   |
| `queued_events` | `uint64` | The total amount of events that were in the queue for the lookup.     |

### `tenzir.metrics.memory`

[Section titled “tenzir.metrics.memory”](#tenzirmetricsmemory)

Contains a measurement of the available memory on the host.

| Field         | Type     | Description                                 |
| :------------ | :------- | :------------------------------------------ |
| `timestamp`   | `time`   | The time at which this metric was recorded. |
| `total_bytes` | `uint64` | The total available memory, in bytes.       |
| `used_bytes`  | `uint64` | The amount of memory used, in bytes.        |
| `free_bytes`  | `uint64` | The amount of free memory, in bytes.        |

### `tenzir.metrics.operator`

[Section titled “tenzir.metrics.operator”](#tenzirmetricsoperator)

Contains input and output measurements over some amount of time for a single operator instantiation.

Deprecation Notice

Operator metrics are deprecated and will be removed in a future release. Use [pipeline metrics](#tenzirmetricspipeline) instead. While they offered great insight into the performance of operators, they were not as useful as pipeline metrics for understanding the overall performance of a pipeline, and were too expensive to collect and store.

| Field                 | Type       | Description                                                                                |
| :-------------------- | :--------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id`         | `string`   | The ID of the pipeline where the associated operator is from.                              |
| `run`                 | `uint64`   | The number of the run, starting at 1 for the first run.                                    |
| `hidden`              | `bool`     | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `timestamp`           | `time`     | The time when this event was emitted (immediately after the collection period).            |
| `operator_id`         | `uint64`   | The ID of the operator inside the pipeline referenced above.                               |
| `source`              | `bool`     | True if this is the first operator in the pipeline.                                        |
| `transformation`      | `bool`     | True if this is neither the first nor the last operator.                                   |
| `sink`                | `bool`     | True if this is the last operator in the pipeline.                                         |
| `internal`            | `bool`     | True if the data flow is considered to internal to Tenzir.                                 |
| `duration`            | `duration` | The timespan over which this data was collected.                                           |
| `starting_duration`   | `duration` | The time spent to start the operator.                                                      |
| `processing_duration` | `duration` | The time spent processing the data.                                                        |
| `scheduled_duration`  | `duration` | The time that the operator was scheduled.                                                  |
| `running_duration`    | `duration` | The time that the operator was running.                                                    |
| `paused_duration`     | `duration` | The time that the operator was paused.                                                     |
| `input`               | `record`   | Measurement of the incoming data stream.                                                   |
| `output`              | `record`   | Measurement of the outgoing data stream.                                                   |

The records `input` and `output` have the following schema:

| Field          | Type     | Description                                                     |
| :------------- | :------- | :-------------------------------------------------------------- |
| `unit`         | `string` | The type of the elements, which is `void`, `bytes` or `events`. |
| `elements`     | `uint64` | Number of elements that were seen during the collection period. |
| `approx_bytes` | `uint64` | An approximation for the number of bytes transmitted.           |
| `batches`      | `uint64` | The number of batches included in this metric.                  |

### `tenzir.metrics.pipeline`

[Section titled “tenzir.metrics.pipeline”](#tenzirmetricspipeline)

Contains measurements of data flowing through pipelines, emitted once every 10 seconds.

| Field         | Type     | Description                                     |
| :------------ | :------- | :---------------------------------------------- |
| `timestamp`   | `time`   | The time at which this metric was recorded.     |
| `pipeline_id` | `string` | The ID of the pipeline these metrics represent. |
| `ingress`     | `record` | Measurement of data entering the pipeline.      |
| `egress`      | `record` | Measurement of data exiting the pipeline.       |

The records `ingress` and `egress` have the following schema:

| Field      | Type       | Description                                              |
| :--------- | :--------- | :------------------------------------------------------- |
| `duration` | `duration` | The timespan over which this data was collected.         |
| `events`   | `uint64`   | Number of events that passed through during this period. |
| `bytes`    | `uint64`   | Approximate number of bytes that passed through.         |
| `batches`  | `uint64`   | Number of batches that passed through.                   |
| `internal` | `bool`     | True if the data flow is considered internal to Tenzir.  |

### `tenzir.metrics.platform`

[Section titled “tenzir.metrics.platform”](#tenzirmetricsplatform)

Signals whether the connection to the Tenzir Platform is working from the node’s perspective. Emitted once per second.

| Field       | Type   | Description                                 |
| :---------- | :----- | :------------------------------------------ |
| `timestamp` | `time` | The time at which this metric was recorded. |
| `connected` | `bool` | The connection status.                      |

### `tenzir.metrics.process`

[Section titled “tenzir.metrics.process”](#tenzirmetricsprocess)

Contains a measurement of the amount of memory used by the `tenzir-node` process.

| Field                  | Type     | Description                                                                       |
| :--------------------- | :------- | :-------------------------------------------------------------------------------- |
| `timestamp`            | `time`   | The time at which this metric was recorded.                                       |
| `current_memory_usage` | `uint64` | The memory currently used by this process.                                        |
| `peak_memory_usage`    | `uint64` | The peak amount of memory, in bytes.                                              |
| `swap_space_usage`     | `uint64` | The amount of swap space, in bytes. Only available on Linux systems.              |
| `open_fds`             | `uint64` | The amount of open file descriptors by the node. Only available on Linux systems. |

### `tenzir.metrics.publish`

[Section titled “tenzir.metrics.publish”](#tenzirmetricspublish)

Contains a measurement of the `publish` operator, emitted once every second per schema.

| Field         | Type     | Description                                                                                |
| :------------ | :------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from.                              |
| `run`         | `uint64` | The number of the run, starting at 1 for the first run.                                    |
| `hidden`      | `bool`   | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `timestamp`   | `time`   | The time at which this metric was recorded.                                                |
| `operator_id` | `uint64` | The ID of the `publish` operator in the pipeline.                                          |
| `topic`       | `string` | The topic name.                                                                            |
| `schema`      | `string` | The schema name of the batch.                                                              |
| `schema_id`   | `string` | The schema ID of the batch.                                                                |
| `events`      | `uint64` | The amount of events that were published to the `topic`.                                   |

### `tenzir.metrics.rebuild`

[Section titled “tenzir.metrics.rebuild”](#tenzirmetricsrebuild)

Contains a measurement of the partition rebuild process, emitted once every second.

| Field               | Type     | Description                                               |
| :------------------ | :------- | :-------------------------------------------------------- |
| `timestamp`         | `time`   | The time at which this metric was recorded.               |
| `partitions`        | `uint64` | The number of partitions currently being rebuilt.         |
| `queued_partitions` | `uint64` | The number of partitions currently queued for rebuilding. |

### `tenzir.metrics.subscribe`

[Section titled “tenzir.metrics.subscribe”](#tenzirmetricssubscribe)

Contains a measurement of the `subscribe` operator, emitted once every second per schema.

| Field         | Type     | Description                                                                                |
| :------------ | :------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from.                              |
| `run`         | `uint64` | The number of the run, starting at 1 for the first run.                                    |
| `hidden`      | `bool`   | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `timestamp`   | `time`   | The time at which this metric was recorded.                                                |
| `operator_id` | `uint64` | The ID of the `subscribe` operator in the pipeline.                                        |
| `topic`       | `string` | The topic name.                                                                            |
| `schema`      | `string` | The schema name of the batch.                                                              |
| `schema_id`   | `string` | The schema ID of the batch.                                                                |
| `events`      | `uint64` | The amount of events that were retrieved from the `topic`.                                 |

### `tenzir.metrics.tcp`

[Section titled “tenzir.metrics.tcp”](#tenzirmetricstcp)

Contains measurements about the number of read calls and the received bytes per TCP connection.

| Field           | Type     | Description                                                                                |
| :-------------- | :------- | :----------------------------------------------------------------------------------------- |
| `pipeline_id`   | `string` | The ID of the pipeline where the associated operator is from.                              |
| `run`           | `uint64` | The number of the run, starting at 1 for the first run.                                    |
| `hidden`        | `bool`   | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. |
| `timestamp`     | `time`   | The time at which this metric was recorded.                                                |
| `operator_id`   | `uint64` | The ID of the `publish` operator in the pipeline.                                          |
| `native`        | `string` | The native handle of the connection (unix: file descriptor).                               |
| `reads`         | `uint64` | The number of attempted reads since the last metric.                                       |
| `writes`        | `uint64` | The number of attempted writes since the last metric.                                      |
| `bytes_read`    | `uint64` | The number of bytes received since the last metrics.                                       |
| `bytes_written` | `uint64` | The number of bytes written since the last metrics.                                        |

## Examples

[Section titled “Examples”](#examples)

### Sort pipelines by total ingress in bytes

[Section titled “Sort pipelines by total ingress in bytes”](#sort-pipelines-by-total-ingress-in-bytes)

```tql
metrics "pipeline"
summarize pipeline_id, ingress=sum(ingress.bytes if not ingress.internal)
sort -ingress
```

```tql
{pipeline_id: "demo-node/m57-suricata", ingress: 59327586}
{pipeline_id: "demo-node/m57-zeek", ingress: 43291764}
```

### Show the CPU usage over the last hour

[Section titled “Show the CPU usage over the last hour”](#show-the-cpu-usage-over-the-last-hour)

```tql
metrics "cpu"
where timestamp > now() - 1h
select timestamp, percent=loadavg_1m
```

```tql
{timestamp: 2023-12-21T12:00:32.631102, percent: 0.40478515625}
{timestamp: 2023-12-21T11:59:32.626043, percent: 0.357421875}
{timestamp: 2023-12-21T11:58:32.620327, percent: 0.42578125}
{timestamp: 2023-12-21T11:57:32.614810, percent: 0.50390625}
{timestamp: 2023-12-21T11:56:32.609896, percent: 0.32080078125}
{timestamp: 2023-12-21T11:55:32.605871, percent: 0.5458984375}
```

### Get the current memory usage

[Section titled “Get the current memory usage”](#get-the-current-memory-usage)

```tql
metrics "memory"
sort -timestamp
tail 1
select current_memory_usage
```

```tql
{current_memory_usage: 1083031552}
```

### Show the total pipeline ingress in bytes

[Section titled “Show the total pipeline ingress in bytes”](#show-the-total-pipeline-ingress-in-bytes)

Show the inggress for every day over the last week, excluding pipelines that run in the Explorer:

```tql
metrics "operator"
where timestamp > now() - 1week
where source and not hidden
timestamp = floor(timestamp, 1day)
summarize timestamp, bytes=sum(output.approx_bytes)
```

```tql
{timestamp: 2023-11-08T00:00:00.000000, bytes: 79927223}
{timestamp: 2023-11-09T00:00:00.000000, bytes: 51788928}
{timestamp: 2023-11-10T00:00:00.000000, bytes: 80740352}
{timestamp: 2023-11-11T00:00:00.000000, bytes: 75497472}
{timestamp: 2023-11-12T00:00:00.000000, bytes: 55497472}
{timestamp: 2023-11-13T00:00:00.000000, bytes: 76546048}
{timestamp: 2023-11-14T00:00:00.000000, bytes: 68643200}
```

### Show the operators that produced the most events

[Section titled “Show the operators that produced the most events”](#show-the-operators-that-produced-the-most-events)

Show the three operator instantiations that produced the most events in total and their pipeline IDs:

```tql
metrics "operator"
where output.unit == "events"
summarize pipeline_id, operator_id, events=max(output.elements)
sort -events
head 3
```

```tql
{pipeline_id: "70a25089-b16c-448d-9492-af5566789b99", operator_id: 0, events: 391008694 }
{pipeline_id: "7842733c-06d6-4713-9b80-e20944927207", operator_id: 0, events: 246914949 }
{pipeline_id: "6df003be-0841-45ad-8be0-56ff4b7c19ef", operator_id: 1, events: 83013294 }
```

### Get the disk usage over time

[Section titled “Get the disk usage over time”](#get-the-disk-usage-over-time)

```tql
metrics "disk"
sort timestamp
select timestamp, used_bytes
```

```tql
{timestamp: 2023-12-21T12:52:32.900086, used_bytes: 461834444800}
{timestamp: 2023-12-21T12:53:32.905548, used_bytes: 461834584064}
{timestamp: 2023-12-21T12:54:32.910918, used_bytes: 461840302080}
{timestamp: 2023-12-21T12:55:32.916200, used_bytes: 461842751488}
```

### Get the memory usage over time

[Section titled “Get the memory usage over time”](#get-the-memory-usage-over-time)

```tql
metrics "memory"
sort timestamp
select timestamp, used_bytes
```

```tql
{timestamp: 2023-12-21T13:08:32.982083, used_bytes: 48572645376}
{timestamp: 2023-12-21T13:09:32.986962, used_bytes: 48380682240}
{timestamp: 2023-12-21T13:10:32.992494, used_bytes: 48438878208}
{timestamp: 2023-12-21T13:11:32.997889, used_bytes: 48491839488}
{timestamp: 2023-12-21T13:12:33.003323, used_bytes: 48529952768}
```

### Get inbound TCP traffic over time

[Section titled “Get inbound TCP traffic over time”](#get-inbound-tcp-traffic-over-time)

```tql
metrics "tcp"
sort timestamp
select timestamp, port, handle, reads, bytes
```

```tql
{
  timestamp: 2024-09-04T15:43:38.011350,
  port: 10000,
  handle: "12",
  reads: 884,
  writes: 0,
  bytes_read: 10608,
  bytes_written: 0
}
{
  timestamp: 2024-09-04T15:43:39.013575,
  port: 10000,
  handle: "12",
  reads: 428,
  writes: 0,
  bytes_read: 5136,
  bytes_written: 0
}
{
  timestamp: 2024-09-04T15:43:40.015376,
  port: 10000,
  handle: "12",
  reads: 429,
  writes: 0,
  bytes_read: 5148,
  bytes_written: 0
}
```

## See Also

[Section titled “See Also”](#see-also)

[`diagnostics`](/reference/operators/diagnostics)

# move

Moves values from one field to another, removing the original field.

```tql
move to=from, …
```

## Description

[Section titled “Description”](#description)

Moves from the field `from` to the field `to`.

### `to: field`

[Section titled “to: field”](#to-field)

The field to move into.

### `from: field`

[Section titled “from: field”](#from-field)

The field to move from.

## Examples

[Section titled “Examples”](#examples)

```tql
from {x: 1, y: 2}
move z=y, w.x=x
```

```tql
{
  z: 2,
  w: {
    x: 1,
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`set`](/reference/operators/set)

# nics

Shows a snapshot of available network interfaces.

```tql
nics
```

## Description

[Section titled “Description”](#description)

The `nics` operator shows a snapshot of all available network interfaces.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir emits network interface card information with the following schema.

### `tenzir.nic`

[Section titled “tenzir.nic”](#tenzirnic)

Contains detailed information about the network interface.

| Field         | Type     | Description                                                                  |
| :------------ | :------- | :--------------------------------------------------------------------------- |
| `name`        | `string` | The name of the network interface.                                           |
| `description` | `string` | A brief note or explanation about the network interface.                     |
| `addresses`   | `list`   | A list of IP addresses assigned to the network interface.                    |
| `loopback`    | `bool`   | Indicates if the network interface is a loopback interface.                  |
| `up`          | `bool`   | Indicates if the network interface is up and can transmit data.              |
| `running`     | `bool`   | Indicates if the network interface is running and operational.               |
| `wireless`    | `bool`   | Indicates if the network interface is a wireless interface.                  |
| `status`      | `record` | A record containing detailed status information about the network interface. |

The record `status` has the following schema:

| Field            | Type   | Description                                           |
| :--------------- | :----- | :---------------------------------------------------- |
| `unknown`        | `bool` | Indicates if the network interface status is unknown. |
| `connected`      | `bool` | Indicates if the network interface is connected.      |
| `disconnected`   | `bool` | Indicates if the network interface is disconnected.   |
| `not_applicable` | `bool` | Indicates if the network interface is not applicable. |

## Examples

[Section titled “Examples”](#examples)

### List all connected network interfaces

[Section titled “List all connected network interfaces”](#list-all-connected-network-interfaces)

```tql
nics
where status.connected
```

## See Also

[Section titled “See Also”](#see-also)

[`load_nic`](/reference/operators/load_nic)

# ocsf::apply

Casts incoming events to their OCSF type.

```tql
ocsf::apply [preserve_variants=bool]
```

## Description

[Section titled “Description”](#description)

The `ocsf::apply` operator casts incoming events to their corresponding OCSF event class type. The resulting type is determined by four fields: `metadata.version`, `metadata.profiles`, `metadata.extensions` and `class_uid`. Events sharing the same values for these fields are cast to the same type. Tenzir supports all OCSF versions (including `-dev` versions), all profiles, and all event classes. Extensions are currently limited to those versioned with OCSF, including the `win` and `linux` extensions.

To this end, the operator performs the following steps:

* Add optional fields that are not present in the original event with a `null` value
* Emit a warning for extra fields that should not be there and drop them
* Encode free-form objects (such as `unmapped`) using their JSON representation
* Assign `@name` depending on the class name, for example: `ocsf.dns_activity`

The types used for OCSF events are slightly adjusted. For example, timestamps use the native `time` type instead of an integer representing the number of milliseconds since the Unix epoch. Furthermore, some fields that would lead to infinite recursion are currently left out. We plan to support recursion up to a certain depth in the future. Furthermore, this operator will likely be extended with additional features, such as the ability to drop all optional fields, or to automatically assign OCSF enumerations based on their sibling ID.

### `preserve_variants = bool`

[Section titled “preserve\_variants = bool”](#preserve_variants--bool)

Setting this option to `true` preserves free-form objects such as `unmapped` as-is, instead of being JSON-encoded. Note that this means the resulting event schema is no longer consistent across events of the same class, as changes to these free-form objects lead to different schemas. For schema-consistency and performance reasons, we recommend keeping this option `false` and instead using `unmapped.parse_json()` to extract fields on-demand.

## Examples

[Section titled “Examples”](#examples)

### Cast a single pre-defined event

[Section titled “Cast a single pre-defined event”](#cast-a-single-pre-defined-event)

```tql
from {
  class_uid: 4001,
  class_name: "Network Activity",
  metadata: {
    version: "1.5.0",
  },
  unmapped: {
    foo: 1,
    bar: 2,
  },
  // … some more fields
}
ocsf::apply
```

```tql
{
  class_uid: 4001,
  class_name: "Network Activity",
  metadata: {
    version: "1.5.0",
    // … all other metadata fields set to `null`
  },
  unmapped: "{\"foo\": 1, \"bar\": 2}",
  // … other fields (with `null` if they didn't exist before)
}
```

### Preserve `unmapped` as a record

[Section titled “Preserve unmapped as a record”](#preserve-unmapped-as-a-record)

```tql
from {
  class_uid: 4001,
  class_name: "Network Activity",
  metadata: {
    version: "1.5.0",
  },
  unmapped: {
    foo: 1,
    bar: 2,
  },
}
ocsf::apply preserve_variants=true
select unmapped
```

```tql
{
  unmapped: {
    foo: 1,
    bar: 2,
  },
}
```

### Filter, transform and send events to ClickHouse

[Section titled “Filter, transform and send events to ClickHouse”](#filter-transform-and-send-events-to-clickhouse)

```tql
subscribe "ocsf"
where class_name == "Network Activity" and metadata.version == "1.5.0"
ocsf::apply
to_clickhouse table="network_activity"
```

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::derive`](/reference/operators/ocsf/derive), [`ocsf::trim`](/reference/operators/ocsf/trim)

# ocsf::derive

Automatically assigns enum strings from their integer counterparts and vice versa.

```tql
ocsf::derive
```

## Description

[Section titled “Description”](#description)

The `ocsf::derive` operator performs bidirectional enum derivation for OCSF events by automatically assigning enum string values based on their integer counterparts and vice versa.

In the future, this operator will also assign automatically assign computable values such as `type_uid` based on what is available in the event.

## Examples

[Section titled “Examples”](#examples)

### Integer to string

[Section titled “Integer to string”](#integer-to-string)

```tql
from {
  activity_id: 1,
  class_uid: 1001,
  metadata: {
    version: "1.5.0",
  },
}
ocsf::derive
```

```tql
{
  activity_id: 1,
  activity_name: "Create",
  class_name: "File System Activity",
  class_uid: 1001,
  metadata: {
    version: "1.5.0",
  },
}
```

### String to Integer

[Section titled “String to Integer”](#string-to-integer)

```tql
from {
  activity_name: "Read",
  class_uid: 1001,
  metadata: {
    version: "1.5.0",
  },
}
ocsf::derive
```

```tql
{
  activity_id: 2,
  activity_name: "Read",
  class_name: "File System Activity",
  class_uid: 1001,
  metadata: {
    version: "1.5.0",
  },
}
```

### Bidirectional enum validation

[Section titled “Bidirectional enum validation”](#bidirectional-enum-validation)

```tql
from {
  activity_id: 1,
  activity_name: "Delete",  // Inconsistent with activity_id=1
  class_uid: 1001,
  metadata: {
    version: "1.5.0",
  },
}
ocsf::derive
```

```tql
{
  activity_id: 1,
  activity_name: "Delete",
  class_name: "File System Activity",
  class_uid: 1001,
  metadata: {
    version: "1.5.0",
  },
}
```

This will emit a warning about inconsistent values and preserve both original values without modification, allowing the user to decide how to handle the conflict.

```plaintext
warning: found inconsistency between `activity_id` and `activity_name`
 --> <input>:9:1
  |
9 | ocsf::derive
  | ~~~~~~~~~~~~
  |
  = note: got 1 ("Create") and "Delete" (4)
```

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::apply`](/reference/operators/ocsf/apply), [`ocsf::trim`](/reference/operators/ocsf/trim)

# ocsf::trim

Drops fields from OCSF events to reduce their size.

```tql
ocsf::trim [drop_optional=bool, drop_recommended=bool]
```

## Description

[Section titled “Description”](#description)

The `ocsf::trim` operator uses intelligent analysis to determine which fields to remove from OCSF events, optimizing data size while preserving essential information.

### `drop_optional = bool`

[Section titled “drop\_optional = bool”](#drop_optional--bool)

If specified, explicitly controls whether to remove fields marked as optional in the OCSF schema. Otherwise, this decision is left to the operator itself.

### `drop_recommended = bool`

[Section titled “drop\_recommended = bool”](#drop_recommended--bool)

If specified, explicitly controls whether to remove fields marked as recommended in the OCSF schema. Otherwise, this decision is left to the operator itself.

## Examples

[Section titled “Examples”](#examples)

### Use intelligent field selection (default behavior)

[Section titled “Use intelligent field selection (default behavior)”](#use-intelligent-field-selection-default-behavior)

```tql
from {
  class_uid: 3002,
  class_name: "Authentication",  // will be removed
  metadata: {
    version: "1.5.0",
  },
  user: {
    name: "alice",
    uid: "1000",
    display_name: "Alice",  // will be removed
  },
  auth_protocol: "Kerberos",
  status: "Success",
  status_id: 1,
}
ocsf::trim
```

```tql
{
  class_uid: 3002,
  metadata: {
    version: "1.5.0",
  },
  user: {
    name: "alice",
    uid: "1000",
  },
  auth_protocol: "Kerberos",
  status: "Success",
  status_id: 1,
}
```

### Explicitly remove optional fields only

[Section titled “Explicitly remove optional fields only”](#explicitly-remove-optional-fields-only)

```tql
from {
  class_uid: 1001,
  class_name: "File System Activity",
  metadata: {version: "1.5.0"},
  file: {
    name: "document.txt",
    path: "/home/user/document.txt",
    size: 1024,  // optional: will be removed
    type: "Regular File",  // optional: also removed
  },
  activity_id: 1,
}
ocsf::trim drop_optional=true, drop_recommended=false
```

```tql
{
  class_uid: 1001,
  metadata: {
    version: "1.5.0",
  },
  file: {
    name: "document.txt",
    path: "/home/user/document.txt",
  },
  activity_id: 1,
}
```

### Only keep required fields to minimize event size

[Section titled “Only keep required fields to minimize event size”](#only-keep-required-fields-to-minimize-event-size)

```tql
from {
  class_uid: 4001,
  class_name: "Network Activity",
  metadata: {version: "1.5.0"},
  src_endpoint: {
    ip: "192.168.1.100",
    port: 443,
    hostname: "client.local",
  },
  severity: "Critical",
  severity_id: 5,
}
ocsf::trim drop_optional=true, drop_recommended=true
```

```tql
{
  class_uid: 4001,
  metadata: {
    version: "1.5.0",
  },
  severity_id: 5,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::apply`](/reference/operators/ocsf/apply), [`ocsf::derive`](/reference/operators/ocsf/derive)

# openapi

Shows the node’s OpenAPI specification.

```tql
openapi
```

## Description

[Section titled “Description”](#description)

The `openapi` operator shows the current Tenzir node’s [OpenAPI specification](/reference/node/api) for all available REST endpoint plugins.

## Examples

[Section titled “Examples”](#examples)

### Render the OpenAPI specification as YAML

[Section titled “Render the OpenAPI specification as YAML”](#render-the-openapi-specification-as-yaml)

```tql
openapi
write_yaml
```

## See Also

[Section titled “See Also”](#see-also)

[`api`](/reference/operators/api), [`serve`](/reference/operators/serve)

# package::add

Installs a package.

```tql
package::add [package_path:string, inputs=record]
```

## Description

[Section titled “Description”](#description)

The `package::add` operator installs all operators, pipelines, and contexts from a package.

### `package_path : string (optional)`

[Section titled “package\_path : string (optional)”](#package_path--string-optional)

The path to a package located on the file system.

### `inputs = record (optional)`

[Section titled “inputs = record (optional)”](#inputs--record-optional)

A record of optional package inputs that configure the package.

## Examples

[Section titled “Examples”](#examples)

### Add a package from the Community Library

[Section titled “Add a package from the Community Library”](#add-a-package-from-the-community-library)

```tql
package::add "suricata-ocsf"
```

### Add a local package with inputs

[Section titled “Add a local package with inputs”](#add-a-local-package-with-inputs)

```tql
package::add "/mnt/config/tenzir/library/zeek",
  inputs={format: "tsv", "log-directory": "/opt/tenzir/logs"}
```

## See Also

[Section titled “See Also”](#see-also)

[`list`](/reference/operators/package/list), [`remove`](/reference/operators/package/remove)

# package::list

Shows installed packages.

```tql
package::list [format=string]
```

## Description

[Section titled “Description”](#description)

The `package::list` operator returns the list of all installed packages.

### `format = string (optional)`

[Section titled “format = string (optional)”](#format--string-optional)

Controls the output format. Valid options are `compact` and `extended`.

Defaults to `compact`.

## Schemas

[Section titled “Schemas”](#schemas)

The `package::list` operator produces two output formats, controlled by the `format` option:

* `compact`: succinct output in a human-readable format
* `extended`: verbose output in a machine-readable format

The formats generate the following schemas below.

### `tenzir.package.compact`

[Section titled “tenzir.package.compact”](#tenzirpackagecompact)

The compact format prints the package information according to the following schema:

| Field         | Type     | Description                             |
| :------------ | :------- | :-------------------------------------- |
| `id`          | `string` | The unique package id.                  |
| `name`        | `string` | The name of this package.               |
| `author`      | `string` | The package author.                     |
| `description` | `string` | The description of this package.        |
| `config`      | `record` | The user-provided package configuration |

### `tenzir.package.extended`

[Section titled “tenzir.package.extended”](#tenzirpackageextended)

The `extended` format is mainly intended for use by non-human consumers, like shell scripts or frontend code. It contains all available information about a package.

| Field                | Type     | Description                                                                                                                                                         |
| :------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `package_definition` | `record` | The original package definition object asa found in the library.                                                                                                    |
| `resolved_package`   | `record` | The effective package definition that was produced by applying all inputs and overrides from the `config` section and removing all disabled pipelines and contexts. |
| `config`             | `record` | The user-provided package configuration.                                                                                                                            |
| `package_status`     | `record` | Run-time information about the package provided the package manager.                                                                                                |

The `config` object has the following schema, where all fields are optional:

| Field       | Type     | Description                                                   |
| :---------- | :------- | :------------------------------------------------------------ |
| `version`   | `string` | The package version.                                          |
| `source`    | `record` | The upstream location of the package definition.              |
| `inputs`    | `record` | User-provided values for the package inputs.                  |
| `overrides` | `record` | User-provided overrides for fields in the package definition. |
| `metadata`  | `record` | An opaque record that can be set during installation.         |

The `package_status` object has the following schema:

| Field                | Type     | Description                                                                                     |
| :------------------- | :------- | :---------------------------------------------------------------------------------------------- |
| `install_state`      | `string` | The install state of this package. One of `installing`, `installed`, `removing` or `zombie`.    |
| `from_configuration` | `bool`   | Whether the package was installed from the `package add` operator or from a configuration file. |

## Examples

[Section titled “Examples”](#examples)

### Show all installed packages

[Section titled “Show all installed packages”](#show-all-installed-packages)

```tql
package::list
```

```tql
{
  "id": "suricata-ocsf",
  "name": "Suricata OCSF Mappings",
  "author": "Tenzir",
  "description": "[Suricata](https://suricata.io) is an open-source network monitor and\nthreat detection tool.\n\nThis package converts all Suricata events published on the topic `suricata` to\nOCSF and publishes the converted events on the topic `ocsf`.\n",
  "config": {
    "inputs": {},
    "overrides": {}
  }
}
```

## See Also

[Section titled “See Also”](#see-also)

[`list`](/reference/operators/pipeline/list), [`package::add`](/reference/operators/package/add), [`package::remove`](/reference/operators/package/remove)

# package::remove

Uninstalls a package.

```tql
package::remove package_id:string
```

## Description

[Section titled “Description”](#description)

The `package::remove` operator uninstalls a previously installed package.

### `package_id : string`

[Section titled “package\_id : string”](#package_id--string)

The unique ID of the package as in the package definition.

## Examples

[Section titled “Examples”](#examples)

### Remove an installed package

[Section titled “Remove an installed package”](#remove-an-installed-package)

```tql
package::remove "suricata-ocsf"
```

## See Also

[Section titled “See Also”](#see-also)

[`package::add`](/reference/operators/package/add)

# partitions

Retrieves metadata about events stored at a node.

```tql
partitions [predicate:expr]
```

## Description

[Section titled “Description”](#description)

The `partitions` operator shows a summary of candidate partitions at a node.

### `predicate: expr (optional)`

[Section titled “predicate: expr (optional)”](#predicate-expr-optional)

Show only partitions which would be considered for pipelines of the form `export | where <expr>` instead of returning all data.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir emits partition information with the following schema:

### `tenzir.partition`

[Section titled “tenzir.partition”](#tenzirpartition)

Contains detailed information about a partition.

| Field             | Type     | Description                                                                          |
| :---------------- | :------- | :----------------------------------------------------------------------------------- |
| `uuid`            | `string` | The unique ID of the partition in the UUIDv4 format.                                 |
| `memusage`        | `uint64` | The memory usage of the partition in bytes.                                          |
| `diskusage`       | `uint64` | The disk usage of the partition in bytes.                                            |
| `events`          | `uint64` | The number of events contained in the partition.                                     |
| `min_import_time` | `time`   | The time at which the first event of the partition arrived at the `import` operator. |
| `max_import_time` | `time`   | The time at which the last event of the partition arrived at the `import` operator.  |
| `version`         | `uint64` | The version number of the internal partition storage format.                         |
| `schema`          | `string` | The schema name of the events contained in the partition.                            |
| `schema_id`       | `string` | A unique identifier for the physical layout of the partition.                        |
| `store`           | `record` | Resource information about the partition’s store.                                    |
| `indexes`         | `record` | Resource information about the partition’s indexes.                                  |
| `sketches`        | `record` | Resource information about the partition’s sketches.                                 |

The records `store`, `indexes`, and `sketches` have the following schema:

| Field  | Type     | Description               |
| :----- | :------- | :------------------------ |
| `url`  | `string` | The URL of the resource.  |
| `size` | `uint64` | The size of the resource. |

## Examples

[Section titled “Examples”](#examples)

### Get memory and disk requirements of all stored data

[Section titled “Get memory and disk requirements of all stored data”](#get-memory-and-disk-requirements-of-all-stored-data)

```tql
partitions
summarize schema,
  events=sum(events),
  diskusage=sum(diskusage),
  memusage=sum(memusage)
sort schema
```

### Get an upper bound of events that have a field `src_ip` with 127.0.0.1

[Section titled “Get an upper bound of events that have a field src\_ip with 127.0.0.1”](#get-an-upper-bound-of-events-that-have-a-field-src_ip-with-127001)

```tql
partitions src_ip == 127.0.0.1
summarize candidates=sum(events)
```

### See how many partitions contain a non-null value for the field `hostname`

[Section titled “See how many partitions contain a non-null value for the field hostname”](#see-how-many-partitions-contain-a-non-null-value-for-the-field-hostname)

```tql
partitions hostname != null
```

# pass

Does nothing with the input.

```tql
pass
```

## Description

[Section titled “Description”](#description)

The `pass` operator relays the input without any modification. Outside of testing and debugging, it is only used when an empty pipeline needs to created, as `{}` is a record, while `{ pass }` is a pipeline.

## Examples

[Section titled “Examples”](#examples)

### Forward the input without any changes

[Section titled “Forward the input without any changes”](#forward-the-input-without-any-changes)

```tql
pass
```

### Do nothing every 10s

[Section titled “Do nothing every 10s”](#do-nothing-every-10s)

```tql
every 10s {
  pass
}
```

# pipeline::activity

Summarizes the activity of pipelines.

```tql
pipeline::activity range=duration, interval=duration
```

## Description

[Section titled “Description”](#description)

Internal Operator

This operator is only for internal usage by the Tenzir Platform. It can change without notice.

### `range = duration`

[Section titled “range = duration”](#range--duration)

The range for which the activity should be fetched. Note that the individual rates returned by this operator typically represent a larger range because they are aligned with the interval.

### `interval = duration`

[Section titled “interval = duration”](#interval--duration)

The interval used to summarize the individual throughout rates. Needs to be a multiple of the built-in storage interval, which is typically `10min`. Also needs to cleanly divide `range`.

## Schemas

[Section titled “Schemas”](#schemas)

### `tenzir.activity`

[Section titled “tenzir.activity”](#tenziractivity)

| Field       | Type           | Description                                               |
| :---------- | :------------- | :-------------------------------------------------------- |
| `first`     | `time`         | The time of the first throughput rate in the lists below. |
| `last`      | `time`         | The time of the last throughput rate in the lists below.  |
| `pipelines` | `list<record>` | The activity for individual pipelines.                    |

The records in `pipelines` have the following schema:

| Field     | Type     | Description                                                        |
| :-------- | :------- | :----------------------------------------------------------------- |
| `id`      | `string` | The ID uniquely identifying the pipeline this activity belongs to. |
| `ingress` | `record` | The activity at the source of the pipeline.                        |
| `egress`  | `record` | The activity at the destination of the pipeline.                   |

The records `ingress` and `egress` have the following schema:

| Field      | Type           | Description                                              |
| :--------- | :------------- | :------------------------------------------------------- |
| `internal` | `bool`         | Whether this end of the pipeline is considered internal. |
| `bytes`    | `uint64`       | The total number of bytes over the range.                |
| `rates`    | `list<uint64>` | The throughput in bytes/second over time.                |

You can derive the time associated with a given throughput rate with the formula `first + index*interval`, except the last value, which is associated with `last`. The recommended way to chart these values is to show a sliding window over `[last - range, last]`. The value in `bytes` is an approximation for the total number of bytes inside that window.

## Examples

[Section titled “Examples”](#examples)

### Show the activity over the last 20s

[Section titled “Show the activity over the last 20s”](#show-the-activity-over-the-last-20s)

```tql
pipeline::activity range=20s, interval=20s
```

```tql
{
  first: 2025-05-07T08:33:40.000Z,
  last: 2025-05-07T08:34:10.000Z,
  pipelines: [
    {
      id: "3b43d497-5f4d-47f4-b191-5f432644d5ba",
      ingress: {
        internal: true,
        bytes: 289800,
        rates: [
          14490,
          14490,
          14490,
        ],
      },
      egress: {
        internal: true,
        bytes: 292360,
        rates: [
          14721.75,
          14514.25,
          14488.8,
        ],
      },
    },
  ],
}
```

# pipeline::detach

Starts a pipeline in the node.

```tql
pipeline::detach { … }, [id=string]
```

## Description

[Section titled “Description”](#description)

The `pipeline::detach` operator starts a hidden managed pipeline in the node, and returns as soon as the pipeline has started.

Subject to Change

This operator primarily exists for testing purposes, where it is often required to run pipelines in the background, but to be able to wait until the pipeline has started. The operator may change without further notice.

### `id = string (optional)`

[Section titled “id = string (optional)”](#id--string-optional)

Sets the pipeline’s ID explicitly, instead of assigning a random ID. This corresponds to the `id` field in the output of `pipeline::list`, and the `pipeline_id` field in the output of `metrics` and `diagnostics`.

## Examples

[Section titled “Examples”](#examples)

### Run a pipeline in the background

[Section titled “Run a pipeline in the background”](#run-a-pipeline-in-the-background)

```tql
pipeline::detach {
  every 1min {
    version
  }
  select version
  write_lines
  save_stdout
}
```

## See also

[Section titled “See also”](#see-also)

[`run`](/reference/operators/pipeline/run)

# pipeline::list

Shows managed pipelines.

```tql
pipeline::list
```

## Description

[Section titled “Description”](#description)

The `pipeline::list` operator returns the list of all managed pipelines. Managed pipelines are pipelines created through the [`/pipeline` API](/reference/node/api), which includes all pipelines run through the Tenzir Platform.

## Examples

[Section titled “Examples”](#examples)

### Count pipelines per state

[Section titled “Count pipelines per state”](#count-pipelines-per-state)

```tql
pipeline::list
top state
```

```tql
{
  "state": "running",
  "count": 31
}
{
  "state": "failed",
  "count": 4
}
{
  "state": "stopped",
  "count": 2
}
```

### Show pipelines per package

[Section titled “Show pipelines per package”](#show-pipelines-per-package)

```tql
pipeline::list
summarize package, names=collect(name)
```

```tql
{
  "package": "suricata-ocsf",
  "names": [
    "Suricata Flow to OCSF Network Activity",
    "Suricata DNS to OCSF DNS Activity",
    "Suricata SMB to OCSF SMB Activity",
    // …
  ]
}
```

## See Also

[Section titled “See Also”](#see-also)

[`list`](/reference/operators/package/list), [`run`](/reference/operators/pipeline/run)

# pipeline::run

Starts a pipeline in the node and waits for it to complete.

```tql
pipeline::run { … }, [id=string]
```

## Description

[Section titled “Description”](#description)

The `pipeline::run` operator starts a hidden managed pipeline in the node, and returns when the pipeline has finished.

Note that pipelines may emit diagnostics after they have finished.

Subject to Change

This operator primarily exists for testing purposes, where it is often required to run pipelines with an explicitly specified pipeline id.

### `{ … }`

[Section titled “{ … }”](#-)

The pipeline to execute. This pipeline runs as a separate managed pipeline within the node.

### `id = string (optional)`

[Section titled “id = string (optional)”](#id--string-optional)

Sets the pipeline’s ID explicitly, instead of assigning a random ID. This corresponds to the `id` field in the output of `pipeline::list`, and the `pipeline_id` field in the output of `metrics` and `diagnostics`.

## Examples

[Section titled “Examples”](#examples)

### Run a pipeline in the background and wait for it to complete

[Section titled “Run a pipeline in the background and wait for it to complete”](#run-a-pipeline-in-the-background-and-wait-for-it-to-complete)

```tql
pipeline::run {
  every 1min {
    version
  }
  select version
  write_lines
  save_stdout
}
```

## See Also

[Section titled “See Also”](#see-also)

[`pipeline::detach`](/reference/operators/pipeline/detach), [`pipeline::list`](/reference/operators/pipeline/list)

# plugins

Shows all available plugins and built-ins.

```tql
plugins
```

## Description

[Section titled “Description”](#description)

The `plugins` operator shows all available plugins and built-ins.

Tenzir is built on a modular monolith architecture. Most features are available as plugins and extensible by developers. Tenzir comes with a set of built-ins and bundled plugins. The former use the plugin API but are available as part of the core library, and the latter are plugins shipped with Tenzir.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir emits plugin information with the following schema.

### `tenzir.plugin`

[Section titled “tenzir.plugin”](#tenzirplugin)

Contains detailed information about the available plugins.

| Field          | Type           | Description                                                                                 |
| :------------- | :------------- | :------------------------------------------------------------------------------------------ |
| `name`         | `string`       | The unique, case-insensitive name of the plugin.                                            |
| `version`      | `string`       | The version identifier of the plugin, or `bundled` if the plugin has no version of its own. |
| `kind`         | `string`       | The kind of plugin. One of `builtin`, `static`, or `dynamic`.                               |
| `types`        | `list<string>` | The interfaces implemented by the plugin, e.g., `operator` or `function`.                   |
| `dependencies` | `list<string>` | Plugins that must be loaded for this plugin to function.                                    |

## Examples

[Section titled “Examples”](#examples)

### Show all currently available functions

[Section titled “Show all currently available functions”](#show-all-currently-available-functions)

```tql
plugins
where "function" in types
summarize functions=collect(name)
```

# processes

Shows a snapshot of running processes.

```tql
processes
```

## Description

[Section titled “Description”](#description)

The `processes` operator shows a snapshot of all currently running processes.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir emits process information with the following schema.

### `tenzir.process`

[Section titled “tenzir.process”](#tenzirprocess)

Contains detailed information about the process.

| Field          | Type           | Description                                                  |
| :------------- | :------------- | :----------------------------------------------------------- |
| `name`         | `string`       | The process name.                                            |
| `command_line` | `list<string>` | The command line of the process.                             |
| `pid`          | `uint64`       | The process identifier.                                      |
| `ppid`         | `uint64`       | The parent process identifier.                               |
| `uid`          | `uint64`       | The user identifier of the process owner.                    |
| `gid`          | `uint64`       | The group identifier of the process owner.                   |
| `ruid`         | `uint64`       | The real user identifier of the process owner.               |
| `rgid`         | `uint64`       | The real group identifier of the process owner.              |
| `priority`     | `string`       | The priority level of the process.                           |
| `startup`      | `time`         | The time when the process was started.                       |
| `vsize`        | `uint64`       | The virtual memory size of the process.                      |
| `rsize`        | `uint64`       | The resident set size (physical memory used) of the process. |
| `swap`         | `uint64`       | The amount of swap memory used by the process.               |
| `peak_mem`     | `uint64`       | Peak memory usage of the process.                            |
| `open_fds`     | `uint64`       | The number of open file descriptors by the process.          |
| `utime`        | `duration`     | The user CPU time consumed by the process.                   |
| `stime`        | `duration`     | The system CPU time consumed by the process.                 |

## Examples

[Section titled “Examples”](#examples)

### Show running processes by runtime

[Section titled “Show running processes by runtime”](#show-running-processes-by-runtime)

```tql
processes
sort -startup
```

### Show the top five running processes by name

[Section titled “Show the top five running processes by name”](#show-the-top-five-running-processes-by-name)

```tql
processes
top name
head 5
```

## See Also

[Section titled “See Also”](#see-also)

[`files`](/reference/operators/files), [`sockets`](/reference/operators/sockets)

# publish

Publishes events to a channel with a topic.

```tql
publish [topic:string]
```

## Description

[Section titled “Description”](#description)

The `publish` operator publishes events at a node in a channel with the specified topic. All [`subscribers`](/reference/operators/subscribe) of the channel operator receive the events immediately.

Note

The `publish` operator does not guarantee that events stay in their original order.

### `topic: string (optional)`

[Section titled “topic: string (optional)”](#topic-string-optional)

An optional topic for publishing events under. If unspecified, the operator publishes events to the topic `main`.

## Examples

[Section titled “Examples”](#examples)

### Publish Zeek connection logs under the fixed topic `zeek`

[Section titled “Publish Zeek connection logs under the fixed topic zeek”](#publish-zeek-connection-logs-under-the-fixed-topic-zeek)

```tql
from "conn.log.gz" {
  decompress_gzip
  read_zeek_tsv
}
publish "zeek"
```

### Publish Suricata events under a dynamic topic depending on their event type

[Section titled “Publish Suricata events under a dynamic topic depending on their event type”](#publish-suricata-events-under-a-dynamic-topic-depending-on-their-event-type)

```tql
from "eve.json" {
  read_suricata
}
publish f"suricata.{event_type}"
```

## See Also

[Section titled “See Also”](#see-also)

[`import`](/reference/operators/import), [`subscribe`](/reference/operators/subscribe)

# python

Executes Python code against each event of the input.

```tql
python code:string, [requirements=string]
python file=string, [requirements=string]
```

Requirements

A Python 3 (>=3.10) interpreter must be present in the `PATH` environment variable of the `tenzir` or `tenzir-node` process.

## Description

[Section titled “Description”](#description)

The `python` operator executes user-provided Python code against each event of the input.

By default, the Tenzir node executing the pipeline creates a virtual environment into which the `tenzir` Python package is installed. This behavior can be turned off in the node configuration using the `plugin.python.create-venvs` boolean option.

Performance

The `python` operator implementation applies the provided Python code to each input row, one by one. We use [PyArrow](https://arrow.apache.org/docs/python/index.html) to convert the input values to native Python data types and back to the Tenzir data model after the transformation. This does have a noticeable performance impact compared to native TQL operations.

### `code: string`

[Section titled “code: string”](#code-string)

The provided Python code describes an event-to-event transformation, i.e., it is executed once for each input event and produces exactly output event.

An implicitly defined `self` variable represents the event. Modify it to alter the output of the operator. Fields of the event can be accessed with the dot notation. For example, if the input event contains fields `a` and `b` then the Python code can access and modify them using `self.a` and `self.b`. Similarly, new fields are added by assigning to `self.fieldname` and existing fields can be removed by deleting them from `self`. When new fields are added, it is required that the new field has the same type for every row of the event.

### `file: string`

[Section titled “file: string”](#file-string)

Instead of providing the code inline, the `file` option allows for passing a path to a file containing the code the operator executes per event.

### `requirements = string (optional)`

[Section titled “requirements = string (optional)”](#requirements--string-optional)

The `requirements` flag can be used to pass additional package dependencies in the pip format. When it is used, the argument is passed on to `pip install` in a dedicated virtual environment.

The string is passed verbatim to `pip install`. To add multiple dependencies, separate them with a space: `requirements="foo bar"`.

## Secrets

[Section titled “Secrets”](#secrets)

By default, the `python` operator does not accept secrets. If you want to allow usage of secrets in the `code` argument, you can enable the configuration option `tenzir.allow-secrets-in-escape-hatches`.

## Examples

[Section titled “Examples”](#examples)

### Insert or modify a field

[Section titled “Insert or modify a field”](#insert-or-modify-a-field)

Set field `x` to `"hello, world"`

```tql
python "self.x = 'hello, world'"
```

### Remove all fields from an event

[Section titled “Remove all fields from an event”](#remove-all-fields-from-an-event)

Clear the contents of `self` to remove the implicit input values from the output:

```tql
python "
  self.clear()
"
```

### Insert a new field

[Section titled “Insert a new field”](#insert-a-new-field)

Define a new field `x` as the square root of the field `y`, and remove `y` from the output:

```tql
python "
  import math
  self.x = math.sqrt(self.y)
  del self.y
"
```

### Make use of third party packages

[Section titled “Make use of third party packages”](#make-use-of-third-party-packages)

```tql
python r#"
  import requests
  requests.post("http://imaginary.api/receive", data=self)
"#, requirements="requests=^2.30"
```

## See Also

[Section titled “See Also”](#see-also)

[`shell`](/reference/operators/shell)

# rare

Shows the least common values.

```tql
rare x:field
```

## Description

[Section titled “Description”](#description)

Shows the least common values for a given field. For each unique value, a new event containing its count will be produced. In general, `rare x` is equivalent to:

This operator is the dual to [`top`](/reference/operators/top).

```tql
summarize x, count=count()
sort count
```

Potentially High Memory Usage

Use caution when applying this operator to large inputs. It currently buffers all data in memory. Out-of-core processing is on our roadmap.

### `x: field`

[Section titled “x: field”](#x-field)

The name of the field to find the least common values for.

## Examples

[Section titled “Examples”](#examples)

### Find the least common values

[Section titled “Find the least common values”](#find-the-least-common-values)

```tql
from {x: "B"}, {x: "A"}, {x: "A"}, {x: "B"}, {x: "A"}, {x: "D"}, {x: "C"}, {x: "C"}
rare x
```

```tql
{x: "D", count: 1}
{x: "C", count: 2}
{x: "B", count: 2}
{x: "A", count: 3}
```

### Show the five least common values for `id.orig_h`

[Section titled “Show the five least common values for id.orig\_h”](#show-the-five-least-common-values-for-idorig_h)

```tql
rare id.orig_h
head 5
```

## See Also

[Section titled “See Also”](#see-also)

[`summarize`](/reference/operators/summarize), [`sort`](/reference/operators/sort), [`top`](/reference/operators/top)

# read_all

Parses an incoming bytes stream into a single event.

```tql
read_all [binary=bool]
```

## Description

[Section titled “Description”](#description)

The `read_all` operator takes its input bytes and produces a single event that contains everything. This is useful if the entire stream is needed for further processing at once.

The resulting events have a single field called `data`.

### `binary = bool (optional)`

[Section titled “binary = bool (optional)”](#binary--bool-optional)

Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`.

## Examples

[Section titled “Examples”](#examples)

### Read an entire text file into a single event

[Section titled “Read an entire text file into a single event”](#read-an-entire-text-file-into-a-single-event)

```tql
load_file "data.txt"
read_all
```

```tql
{data: "<file contents>"}
```

### Read an entire binary file into a single event

[Section titled “Read an entire binary file into a single event”](#read-an-entire-binary-file-into-a-single-event)

```tql
load_file "data.bin"
read_all binary=true
```

```tql
{data: b"<file contents>"}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_delimited`](/reference/operators/read_delimited), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_lines`](/reference/operators/read_lines),

# read_bitz

Parses bytes as *BITZ* format.

```tql
read_bitz
```

## Description

[Section titled “Description”](#description)

BITZ is short for **Bi**nary **T**en**z**ir and is our internal wire format.

Use BITZ when you need high-throughput structured data exchange with minimal overhead. BITZ is a thin wrapper around Arrow’s record batches. That is, BITZ lays out data in a (compressed) columnar fashion that makes it conducive for analytical workloads. Since it’s padded and byte-aligned, it is portable and doesn’t induce any deserialization cost, making it suitable for write-once-read-many use cases.

Internally, BITZ uses Arrow’s IPC format for serialization and deserialization, but prefixes each message with a 64 bit size prefix to support changing schemas between batches—something that Arrow’s IPC format does not support on its own.

## See Also

[Section titled “See Also”](#see-also)

[`read_bitz`](/reference/operators/read_bitz), [`read_feather`](/reference/operators/write_feather), [`read_parquet`](/reference/operators/write_parquet), [`write_bitz`](/reference/operators/write_bitz)

# read_cef

Parses an incoming Common Event Format (CEF) stream into events.

```tql
read_cef [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

The [Common Event Format (CEF)](https://community.microfocus.com/cfs-file/__key/communityserver-wikis-components-files/00-00-00-00-23/3731.CommonEventFormatV25.pdf) is a text-based event format that originally stems from ArcSight. It is line-based and human readable. The first 7 fields of a CEF event are always the same, and the 8th *extension* field is an optional list of key-value pairs:

```plaintext
CEF:Version|Device Vendor|Device Product|Device Version|Device Event Class ID|Name|Severity|[Extension]
```

Here is a real-world example:

```plaintext
CEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8| externalId=6 clientId=2251997 scanGroupId=3 scanGroupName=Manually Installed Agents sev=High duser=tikasrv01\\administrator cat=END-POINT Alert dhost=TikaSrv01 src=172.31.5.93 filePath=c:\\windows\\temp\\javac.exe fname=javac.exe rt=3/30/2022 10:55:34 AM fileHash=2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD rtUtc=Mar 30 2022 10:55:34.688 dtUtc=Mar 30 2022 10:55:32.458 hostLS=2022-03-30 10:55:34 GMT+00:00 osVer=Windows Server 2016 Datacenter x64 1607 epsVer=4.5.5.6845 confVer=637842168250000000 prUser=tikasrv01\\administrator pParams="C:\\Windows\\Temp\\javac.exe" sign=Not signed pct=2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609 pFileHash=1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7 pprUser=tikasrv01\\administrator ppParams=C:\\Windows\\Explorer.EXE pssdeep=49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3 pSign=Signed and has certificate info gpFileHash=CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F gpprUser=tikasrv01\\administrator gpParams=C:\\Windows\\system32\\userinit.exe gpssdeep=384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu gpSign=Signed actRem=Kill, Rename
```

The [CEF specification](https://community.microfocus.com/cfs-file/__key/communityserver-wikis-components-files/00-00-00-00-23/3731.CommonEventFormatV25.pdf) pre-defines several extension field key names and data types for the corresponding values. Tenzir’s parser does not enforce the strict definitions and instead tries to infer the type from the provided values.

Tenzir translates the `extension` field to a nested record, where the key-value pairs of the extensions map to record fields. Here is an example of the above event:

Output (shortened)

```json
{
  "cef_version": 0,
  "device_vendor": "Cynet",
  "device_product": "Cynet 360",
  "device_version": "4.5.4.22139",
  "signature_id": "0",
  "name": "Memory Pattern - Cobalt Strike Beacon ReflectiveLoader",
  "severity": "8",
  "extension": {
    "externalId": 6,
    "clientId": 2251997,
    "scanGroupId": 3,
    ...
    "gpssdeep": "384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu",
    "gpSign": "Signed",
    "actRem": "Kill, Rename"
  }
}
```

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_cef`](/reference/functions/parse_cef), [`print_cef`](/reference/functions/print_cef), [`read_leef`](/reference/operators/read_leef)

# read_csv

Read CSV (Comma-Separated Values) from a byte stream.

```tql
read_csv [list_separator=string, null_value=string, comments=bool, header=string,
          quotes=string, auto_expand=bool,
          schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

The `read_csv` operator transforms a byte stream into a event stream by parsing the bytes as [CSV](https://en.wikipedia.org/wiki/Comma-separated_values).

### `auto_expand = bool (optional)`

[Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional)

Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values.

### `comments = bool (optional)`

[Section titled “comments = bool (optional)”](#comments--bool-optional)

Treat lines beginning with ”#” as comments.

### `header = list<string>|string (optional)`

[Section titled “header = list\<string>|string (optional)”](#header--liststringstring-optional)

A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header.

### `list_separator = string (optional)`

[Section titled “list\_separator = string (optional)”](#list_separator--string-optional)

The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled.

Defaults to `;`.

### `null_value = string (optional)`

[Section titled “null\_value = string (optional)”](#null_value--string-optional)

The `string` denoting an absent value.

Defaults to empty string (`""`).

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Read a CSV file with header

[Section titled “Read a CSV file with header”](#read-a-csv-file-with-header)

input.csv

```txt
message,count,ip
some text,42,"1.1.1.1"
more text,100,"1.1.1.2"
```

```tql
load "input.csv"
read_csv
```

```tql
{message: "some text", count: 42, ip: 1.1.1.1}
{message: "more text", count: 100, ip: 1.1.1.2}
```

### Manually specify a header

[Section titled “Manually specify a header”](#manually-specify-a-header)

input\_no\_header.csv

```txt
some text,42,"1.1.1.1"
more text,100,"1.1.1.2"
```

```tql
load "input_no_header.csv"
read_csv header="message,count,ip"
```

```tql
{message: "some text", count: 42, ip: 1.1.1.1}
{message: "more text", count: 100, ip: 1.1.1.2}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_csv`](/reference/functions/parse_csv), [`print_csv`](/reference/functions/print_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv), [`write_csv`](/reference/operators/write_csv)

# read_delimited

Parses an incoming bytes stream into events using a string as delimiter.

```tql
read_delimited separator:string|blob, [binary=bool, include_separator=bool]
```

## Description

[Section titled “Description”](#description)

The `read_delimited` operator takes its input bytes and splits it using the provided string as a delimiter. This is useful for parsing data that uses simple string delimiters instead of regular expressions or standard newlines.

The resulting events have a single field called `data`.

Note

If the input ends with a separator, no additional empty event will be generated. For example, splitting `"a|b|"` with delimiter `"|"` will produce two events: `"a"` and `"b"`, not three events with an empty third one.

### `separator: string|blob (required)`

[Section titled “separator: string|blob (required)”](#separator-stringblob-required)

The string or blob to use as delimiter. The operator will split the input whenever this exact sequence is matched. When a blob literal is provided (e.g., `b"\x00\x01"`), the `binary` option defaults to `true`.

### `binary = bool (optional)`

[Section titled “binary = bool (optional)”](#binary--bool-optional)

Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`.

### `include_separator = bool (optional)`

[Section titled “include\_separator = bool (optional)”](#include_separator--bool-optional)

When enabled, includes the matched separator string in the output events. By default, the separator is excluded from the results.

## Examples

[Section titled “Examples”](#examples)

### Split on a simple delimiter

[Section titled “Split on a simple delimiter”](#split-on-a-simple-delimiter)

```tql
load_file "data.txt"
read_delimited "||"
```

### Parse CSV-like data with custom delimiter

[Section titled “Parse CSV-like data with custom delimiter”](#parse-csv-like-data-with-custom-delimiter)

```tql
load_file "custom.csv"
read_delimited ";;;"
```

### Include the separator in the output

[Section titled “Include the separator in the output”](#include-the-separator-in-the-output)

```tql
load_file "data.txt"
read_delimited "||", include_separator=true
```

### Parse binary data with blob delimiters

[Section titled “Parse binary data with blob delimiters”](#parse-binary-data-with-blob-delimiters)

```tql
load_file "binary.dat"
read_delimited b"\x00\x01"
```

### Use blob separator with include\_separator

[Section titled “Use blob separator with include\_separator”](#use-blob-separator-with-include_separator)

```tql
load_file "data.txt"
read_delimited b"||", include_separator=true
```

### Parse binary data with string delimiters

[Section titled “Parse binary data with string delimiters”](#parse-binary-data-with-string-delimiters)

```tql
load_file "binary.dat"
read_delimited "\x00\x01", binary=true
```

## See Also

[Section titled “See Also”](#see-also)

[`read_all`](/reference/operators/read_all), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_lines`](/reference/operators/read_lines), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_delimited_regex

Parses an incoming bytes stream into events using a regular expression as delimiter.

```tql
read_delimited_regex regex:string|blob, [binary=bool, include_separator=bool]
```

## Description

[Section titled “Description”](#description)

The `read_delimited_regex` operator takes its input bytes and splits it using the provided regular expression as a delimiter. This is useful for parsing data that uses custom delimiters or patterns instead of standard newlines.

The regular expression flavor is Perl compatible and documented [here](https://www.boost.org/doc/libs/1_88_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html).

The resulting events have a single field called `data`.

Note

If the input ends with a separator, no additional empty event will be generated. For example, splitting `"a|b|"` with delimiter pattern `"|"` will produce two events: `"a"` and `"b"`, not three events with an empty third one.

### `regex: string|blob (required)`

[Section titled “regex: string|blob (required)”](#regex-stringblob-required)

The regular expression pattern to use as delimiter. This can be provided as a string or blob literal. The operator will split the input whenever this pattern is matched. When a blob literal is provided (e.g., `b"\\x00\\x01"`), the `binary` option defaults to `true`.

### `binary = bool (optional)`

[Section titled “binary = bool (optional)”](#binary--bool-optional)

Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`.

### `include_separator = bool (optional)`

[Section titled “include\_separator = bool (optional)”](#include_separator--bool-optional)

When enabled, includes the matched separator pattern in the output events. By default, the separator is excluded from the results.

## Examples

[Section titled “Examples”](#examples)

### Split Syslog-like events without newline terminators from a TCP input

[Section titled “Split Syslog-like events without newline terminators from a TCP input”](#split-syslog-like-events-without-newline-terminators-from-a-tcp-input)

```tql
load_tcp "0.0.0.0:514"
read_delimited_regex "(?=<[0-9]+>)"
this = data.parse_syslog()
```

### Parse log entries separated by timestamps

[Section titled “Parse log entries separated by timestamps”](#parse-log-entries-separated-by-timestamps)

```tql
load_file "application.log"
read_delimited_regex "(?=\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})"
```

### Split on multiple possible delimiters

[Section titled “Split on multiple possible delimiters”](#split-on-multiple-possible-delimiters)

```tql
load_file "mixed_delimiters.txt"
read_delimited_regex "[;|]"
```

### Include the separator in the output

[Section titled “Include the separator in the output”](#include-the-separator-in-the-output)

```tql
load_file "data.txt"
read_delimited_regex "\\|\\|", include_separator=true
```

### Parse binary data with blob patterns

[Section titled “Parse binary data with blob patterns”](#parse-binary-data-with-blob-patterns)

```tql
load_file "binary.dat"
read_delimited_regex b"\\x00\\x01"
```

### Use blob pattern with include\_separator for binary delimiters

[Section titled “Use blob pattern with include\_separator for binary delimiters”](#use-blob-pattern-with-include_separator-for-binary-delimiters)

```tql
load_file "protocol.dat"
read_delimited_regex b"\\xFF\\xFE", include_separator=true
```

## See Also

[Section titled “See Also”](#see-also)

[`read_all`](/reference/operators/read_all), [`read_delimited`](/reference/operators/read_delimited), [`read_lines`](/reference/operators/read_lines), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_feather

Parses an incoming Feather byte stream into events.

```tql
read_feather
```

## Description

[Section titled “Description”](#description)

Transforms the input [Feather](https://arrow.apache.org/docs/python/feather.html) (a thin wrapper around [Apache Arrow’s IPC](https://arrow.apache.org/docs/python/ipc.html) wire format) byte stream to event stream.

## Examples

[Section titled “Examples”](#examples)

### Publish a feather logs file

[Section titled “Publish a feather logs file”](#publish-a-feather-logs-file)

```tql
load_file "logs.feather"
read_feather
pulish "log"
```

## See Also

[Section titled “See Also”](#see-also)

[`read_parquet`](/reference/operators/read_parquet), [`write_feather`](/reference/operators/write_feather)

# read_gelf

Parses an incoming GELF stream into events.

```tql
read_gelf [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

Parses an incoming [GELF](https://go2docs.graylog.org/current/getting_in_log_data/gelf.html) stream into events.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Read a GELF stream from a TCP socket

[Section titled “Read a GELF stream from a TCP socket”](#read-a-gelf-stream-from-a-tcp-socket)

```tql
load_tcp "0.0.0.0:54321"
read_gelf
```

# read_grok

Parses lines of input with a grok pattern.

```tql
read_grok pattern:string, [pattern_definitions=record|string, indexed_captures=bool,
          include_unnamed=bool, schema=string, selector=string,
          schema_only=bool, merge=bool, raw=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

`read_grok` uses a regular expression based parser similar to the [Logstash `grok` plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) in Elasticsearch. Tenzir ships with the same built-in patterns as Elasticsearch, found [here](https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns/ecs-v1).

In short, `pattern` consists of replacement fields, that look like `%{SYNTAX[:SEMANTIC[:CONVERSION]]}`, where:

* `SYNTAX` is a reference to a pattern, either built-in or user-defined through the `pattern_defintions` option.
* `SEMANTIC` is an identifier that names the field in the parsed record.
* `CONVERSION` is either `infer` (default), `string` (default with `raw=true`), `int`, or `float`.

The supported regular expression syntax is the one supported by [Boost.Regex](https://www.boost.org/doc/libs/1_81_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html), which is effectively Perl-compatible.

### `pattern: string`

[Section titled “pattern: string”](#pattern-string)

The `grok` pattern used for matching. Must match the input in its entirety.

### `pattern_definitions = record|string (optional)`

[Section titled “pattern\_definitions = record|string (optional)”](#pattern_definitions--recordstring-optional)

New pattern definitions to use. This may be a record of the form

```tql
{
  pattern_name: "pattern"
}
```

For example, the built-in pattern `INT` would be defined as

```tql
{ INT: "(?:[+-]?(?:[0-9]+))" }
```

Alternatively, this may be a user-defined newline-delimited list of patterns, where a line starts with the pattern name, followed by a space, and the `grok`-pattern for that pattern. For example, the built-in pattern `INT` is defined as follows:

```plaintext
INT (?:[+-]?(?:[0-9]+))
```

### `indexed_captures = bool (optional)`

[Section titled “indexed\_captures = bool (optional)”](#indexed_captures--bool-optional)

All subexpression captures are included in the output, with the `SEMANTIC` used as the field name if possible, and the capture index otherwise.

### `include_unnamed = bool (optional)`

[Section titled “include\_unnamed = bool (optional)”](#include_unnamed--bool-optional)

By default, only fields that were given a name with `SEMANTIC`, or with the regular expression named capture syntax `(?<name>...)` are included in the resulting record.

With `include_unnamed=true`, replacement fields without a `SEMANTIC` are included in the output, using their `SYNTAX` value as the record field name.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Parse a fictional HTTP request log

[Section titled “Parse a fictional HTTP request log”](#parse-a-fictional-http-request-log)

```tql
// Input: 55.3.244.1 GET /index.html 15824 0.043
let $pattern = "%{IP:client} %{WORD} %{URIPATHPARAM:req} %{NUMBER:bytes} %{NUMBER:dur}"
read_grok $pattern
```

```tql
{
  client: 55.3.244.1,
  req: "/index.html",
  bytes: 15824,
  dur: 0.043
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_grok`](/reference/functions/parse_grok)

# read_json

Tip

If you are receiving newline-delimited JSON (NDJSON), use [`read_ndjson`](/reference/operators/read_ndjson) instead.

Parses an incoming JSON stream into events.

```tql
read_json [schema=string, selector=string, schema_only=bool, merge=bool, raw=bool,
           unflatten_separator=string, arrays_of_objects=bool]
```

## Description

[Section titled “Description”](#description)

Parses an incoming JSON byte stream into events.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

### `arrays_of_objects = bool (optional)`

[Section titled “arrays\_of\_objects = bool (optional)”](#arrays_of_objects--bool-optional)

Default: `false`.

Parse arrays of objects, with every object in the outermost arrays resulting in one event each. This is particularly useful when interfacing with REST APIs, which often yield large arrays of objects instead of newline-delimited JSON objects.

## Examples

[Section titled “Examples”](#examples)

### Read a JSON file

[Section titled “Read a JSON file”](#read-a-json-file)

input.json

```json
{
  "product": "Tenzir",
  "version.major": 4,
  "version.minor": 22
}
{
  "product": "Tenzir",
  "version.major": 4,
  "version.minor": 21,
  "version.dirty": true
}
```

Pipeline

```tql
load_file "events.json"
read_json unflatten="."
```

Output

```json
{
  "product": "Tenzir",
  "version": {
    "major": 4,
    "minor": 22
  }
}
{
  "product": "Tenzir",
  "version": {
    "major": 4,
    "minor": 21,
    "dirty": true
  }
}
```

### Read a JSON array

[Section titled “Read a JSON array”](#read-a-json-array)

[JA4+](https://ja4db.com/) provides fingerprints via a REST API, which returns a single JSON array.

Example Input

```json
[
  {
    "application": "SemrushBot",
    "library": null,
    "device": null,
    "os": "Other",
    "user_agent_string": null,
    "certificate_authority": null,
    "observation_count": 449,
    "verified": false,
    "notes": null,
    "ja4_fingerprint": "t13d301000_01455d0db58d_5ac7197df9d2",
    "ja4_fingerprint_string": null,
    "ja4s_fingerprint": null,
    "ja4h_fingerprint": "ge11nn100000_c910c42e1704_e3b0c44298fc_e3b0c44298fc",
    "ja4x_fingerprint": null,
    "ja4t_fingerprint": null,
    "ja4ts_fingerprint": null,
    "ja4tscan_fingerprint": null
  },
  {
    "application": null,
    "library": null,
    "device": "Epson Printer",
    "os": null,
    "user_agent_string": null,
    "certificate_authority": null,
    "observation_count": 1,
    "verified": true,
    "notes": null,
    "ja4_fingerprint": null,
    "ja4s_fingerprint": null,
    "ja4h_fingerprint": null,
    "ja4x_fingerprint": null,
    "ja4t_fingerprint": null,
    "ja4ts_fingerprint": null,
    "ja4tscan_fingerprint": "28960_2-4-8-1-3_1460_3_1-4-8-16"
  },
  ...
]
```

You can easily ingest this into Tenzir using

Pipeline

```tql
load "https://ja4db.com/api/read/"
read_json arrays_of_objects=true
```

Example Output

```json
{
  "application": "SemrushBot",
  "library": null,
  "device": null,
  "os": "Other",
  "user_agent_string": null,
  "certificate_authority": null,
  "observation_count": 449,
  "verified": false,
  "notes": null,
  "ja4_fingerprint": "t13d301000_01455d0db58d_5ac7197df9d2",
  "ja4_fingerprint_string": null,
  "ja4s_fingerprint": null,
  "ja4h_fingerprint": "ge11nn100000_c910c42e1704_e3b0c44298fc_e3b0c44298fc",
  "ja4x_fingerprint": null,
  "ja4t_fingerprint": null,
  "ja4ts_fingerprint": null,
  "ja4tscan_fingerprint": null
},
{
  "application": null,
  "library": null,
  "device": "Epson Printer",
  "os": null,
  "user_agent_string": null,
  "certificate_authority": null,
  "observation_count": 1,
  "verified": true,
  "notes": null,
  "ja4_fingerprint": null,
  "ja4s_fingerprint": null,
  "ja4h_fingerprint": null,
  "ja4x_fingerprint": null,
  "ja4t_fingerprint": null,
  "ja4ts_fingerprint": null,
  "ja4tscan_fingerprint": "28960_2-4-8-1-3_1460_3_1-4-8-16"
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_json`](/reference/functions/parse_json), [`read_ndjson`](/reference/operators/read_ndjson)

# read_kv

Read Key-Value pairs from a byte stream.

```tql
read_kv [field_split=string, value_split=string, merge=bool, raw=bool, quotes=string,
         schema=string, selector=string, schema_only=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

The `read_kv` operator transforms a byte stream into a event stream by parsing the bytes as Key-Value pairs.

Incoming strings are first split into fields according to `field_split`. This can be a regular expression. For example, the input `foo: bar, baz: 42` can be split into `foo: bar` and `baz: 42` with the regular expression `r",\s*"` (a comma, followed by any amount of whitespace) as the field splitter. Note that the matched separators are removed when splitting a string.

Afterwards, the extracted fields are split into their key and value by `value_split`, which can again be a regular expression. In our example, `r":\s*"` could be used to split `foo: bar` into the key `foo` and its value `bar`, and similarly `baz: 42` into `baz` and `42`. The result would thus be `{"foo": "bar", "baz": 42}`. If the regex matches multiple substrings, only the first match is used. If no match is found, the “field” is considered an extension of the previous fields value.

The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `read_kv` at the moment. However, if the regular expression has a capture group, it is assumed that only the content of the capture group shall be used as the separator. This means that unsupported regular expressions such as `(?=foo)bar(?<=baz)` can be effectively expressed as `foo(bar)baz` instead.

### Quoted Values

[Section titled “Quoted Values”](#quoted-values)

The parser is aware of double-quotes (`"`). If the `field_split` or `value_split` are found within enclosing quotes, they are not considered matches. This means that both the key and the value may be enclosed in double-quotes.

For example, given `field_split` `\s*,\s*` and `value_split` `=`, the input

```plaintext
"key"="nested = value",key2="value, and more"
```

will parse as

```tql
{
  key: "nested = value",
  key2: "value, and more",
}
```

### `field_split = string (optional)`

[Section titled “field\_split = string (optional)”](#field_split--string-optional)

The regular expression used to separate individual fields.

Defaults to `r"\s"`.

### `value_split = string (optional)`

[Section titled “value\_split = string (optional)”](#value_split--string-optional)

The regular expression used to separate a key from its value.

Defaults to `"="`.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Read comma-separated key-value pairs

[Section titled “Read comma-separated key-value pairs”](#read-comma-separated-key-value-pairs)

Input

```txt
surname:"John Norman", family_name:Smith, date_of_birth: 1995-05-26
```

```tql
read_kv field_split=r"\s*,\s*",
  value_split=r"\s*:\s*"
```

```tql
{
  surname: "John Norman",
  family_name: "Smith",
  date_of_birth: 1995-05-26,
}
```

### Extract key-value pairs with more complex rules

[Section titled “Extract key-value pairs with more complex rules”](#extract-key-value-pairs-with-more-complex-rules)

Input

```txt
PATH: C:\foo INPUT_MESSAGE: hello world
PATH: D:\bar VALUE: 42 INFO: Great
```

```tql
read_kv field_split=r"(\s+)[A-Z][A-Z_]+:",
  value_split=r":\s*"
```

```tql
{
  PATH: "C:\\foo",
  INPUT_MESSAGE: "hello world",
}
{
  PATH: "D:\\bar",
  VALUE: 42,
  INFO: "Great",
}
```

This requires lookahead because not every whitespace acts as a field separator. Instead, we only want to split if the whitespace is followed by `[A-Z][A-Z_]+:`, i.e., at least two uppercase characters followed by a colon. We can express this as `"(\s+)[A-Z][A-Z_]+:"`, which yields `PATH: C:\foo` and `INPUT_MESSAGE: hello world`. We then split the key from its value with `":\s*"`. Since only the first match is used to split key and value, this leaves the path intact.

### Fields without a `value_split`

[Section titled “Fields without a value\_split”](#fields-without-a-value_split)

Input

```txt
x=1 y=2 z=3 4 5 a=6
```

```tql
read_kv
```

```tql
{
  x: 1,
  y: 2,
  z: "3 4 5",
  a: 6,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_kv`](/reference/functions/parse_kv), [`write_kv`](/reference/operators/write_kv)

# read_leef

Parses an incoming [LEEF](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) stream into events.

```tql
read_leef [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

The [Log Event Extended Format (LEEF)](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) is an event representation popularized by IBM QRadar. Many tools send LEEF over [Syslog](/reference/operators/read_syslog).

LEEF is a line-based format and every line begins with a *header* that is followed by *attributes* in the form of key-value pairs.

LEEF v1.0 defines 5 header fields and LEEF v2.0 has an additional field to customize the key-value pair separator, which can be a single character or the hex value prefixed by `0x` or `x`:

```plaintext
LEEF:1.0|Vendor|Product|Version|EventID|
LEEF:2.0|Vendor|Product|Version|EventID|DelimiterCharacter|
```

For LEEF v1.0, the tab (`\t`) character is hard-coded as attribute separator.

Here are some real-world LEEF events:

```plaintext
LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1  dst=2.10.20.20  spt=1200
LEEF:2.0|Lancope|StealthWatch|1.0|41|^|src=10.0.1.8^dst=10.0.0.5^sev=5^srcPort=81^dstPort=21
```

Tenzir translates the event attributes into a nested record, where the key-value pairs map to record fields. Here is an example of the parsed events from above:

```tql
{
  leef_version: "1.0",
  vendor: "Microsoft",
  product_name: "MSExchange",
  product_version: "2016",
  event_class_id: "15345",
  attributes: {
    src: 10.50.1.1,
    dst: 2.10.20.20,
    spt: 1200,
  }
}
{
  leef_version: "2.0",
  vendor: "Lancope",
  product_name: "StealthWatch",
  product_version: "1.0",
  event_class_id: "41",
  attributes: {
    src: 10.0.1.8,
    dst: 10.0.0.5,
    sev: 5,
    srcPort: 81,
    dstPort: 21
  }
}
```

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_leef`](/reference/functions/parse_leef), [`print_leef`](/reference/functions/print_leef), [`read_cef`](/reference/operators/read_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# read_lines

Parses an incoming bytes stream into events.

```tql
read_lines [skip_empty=bool, split_at_null=bool, split_at_regex=string]
```

## Description

[Section titled “Description”](#description)

The `read_lines` operator takes its input bytes and splits it at a newline character.

Newline characters include:

* `\n`
* `\r\n`

The resulting events have a single field called `line`.

### `skip_empty = bool (optional)`

[Section titled “skip\_empty = bool (optional)”](#skip_empty--bool-optional)

Ignores empty lines in the input.

### `split_at_null = bool (optional)`

[Section titled “split\_at\_null = bool (optional)”](#split_at_null--bool-optional)

Deprecated

This option is deprecated. Use [`read_delimited`](/reference/operators/read_delimited) instead.

Use null byte (`\0`) as the delimiter instead of newline characters.

### `split_at_regex = string (optional)`

[Section titled “split\_at\_regex = string (optional)”](#split_at_regex--string-optional)

Deprecated

This option is deprecated. Use [`read_delimited_regex`](/reference/operators/read_delimited_regex) instead.

Use the specified regex as the delimiter instead of newline characters. The regex flavor is Perl compatible and documented [here](https://www.boost.org/doc/libs/1_88_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html).

## Examples

[Section titled “Examples”](#examples)

### Reads lines from a file

[Section titled “Reads lines from a file”](#reads-lines-from-a-file)

```tql
load_file "events.log"
read_lines
is_error = line.starts_with("error:")
```

### Split Syslog-like events without newline terminators from a TCP input

[Section titled “Split Syslog-like events without newline terminators from a TCP input”](#split-syslog-like-events-without-newline-terminators-from-a-tcp-input)

Consider using [`read_delimited_regex`](/reference/operators/read_delimited_regex) for regex-based splitting:

```tql
load_tcp "0.0.0.0:514"
read_delimited_regex "(?=<[0-9]+>)"
this = line.parse_syslog()
```

```tql
load_tcp "0.0.0.0:514"
read_lines split_at_regex="(?=<[0-9]+>)"
this = line.parse_syslog()
```

## See Also

[Section titled “See Also”](#see-also)

[`read_all`](/reference/operators/read_all), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_xsv`](/reference/operators/read_xsv), [`write_lines`](/reference/operators/write_lines)

# read_ndjson

Parses an incoming NDJSON (newline-delimited JSON) stream into events.

```tql
read_ndjson [schema=string, selector=string, schema_only=bool,
             merge=bool, raw=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

Parses an incoming NDJSON byte stream into events.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Read a newline-delimited JSON file

[Section titled “Read a newline-delimited JSON file”](#read-a-newline-delimited-json-file)

versions.json

```json
{"product": "Tenzir", "version.major": 4, "version.minor": 22}
{"product": "Tenzir", "version.major": 4, "version.minor": 21}
```

```tql
load_file "versions.json"
read_ndjson unflatten="."
```

```tql
{
  product: "Tenzir",
  version: {
    major: 4,
    minor: 22,
  }
}
{
  product: "Tenzir",
  version: {
    major: 4,
    minor: 21,
  }
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_json`](/reference/operators/read_json)

# read_parquet

Reads events from a Parquet byte stream.

```tql
read_parquet
```

## Description

[Section titled “Description”](#description)

Reads events from a [Parquet](https://parquet.apache.org/) byte stream.

[Apache Parquet](https://parquet.apache.org/) is a columnar storage format that a variety of data tools support.

MMAP Parsing

When using theis with the [`load_file`](/reference/operators/load_file) operator, we recommend passing the `mmap=true` option to `load_file` to give the parser full control over the reads, which leads to better performance and memory usage.

Limitation

Tenzir currently assumes that all Parquet files use metadata recognized by Tenzir. We plan to lift this restriction in the future.

## Examples

[Section titled “Examples”](#examples)

Read a Parquet file:

```tql
load_file "/tmp/data.prq", mmap=true
read_parquet
```

## See Also

[Section titled “See Also”](#see-also)

[`read_feather`](/reference/operators/read_feather), [`to_hive`](/reference/operators/to_hive), [`write_parquet`](/reference/operators/write_parquet)

# read_pcap

Reads raw network packets in PCAP file format.

```tql
read_pcap [emit_file_headers=bool]
```

## Description

[Section titled “Description”](#description)

The `read_pcap` operator converts raw bytes representing a [PCAP](https://datatracker.ietf.org/doc/id/draft-gharris-opsawg-pcap-00.html) file into events.

PCAPNG

The current implementation does *not* support [PCAPNG](https://www.ietf.org/archive/id/draft-tuexen-opsawg-pcapng-05.html).

### `emit_file_headers = bool (optional)`

[Section titled “emit\_file\_headers = bool (optional)”](#emit_file_headers--bool-optional)

Emit a `pcap.file_header` event that represents the PCAP file header. If present, the parser injects this additional event before the subsequent stream of packets.

Emitting this extra event makes it possible to seed the [`write_pcap`](/reference/operators/write_pcap) operator with a file header from the input. This allows for controlling the timestamp formatting (microseconds vs. nanosecond granularity) and byte order in the packet headers.

When the PCAP parser processes a concatenated stream of PCAP files, specifying `emit_file_headers` will also re-emit every intermediate file header as separate event.

Use this option when you would like to reproduce the identical trace file layout of the PCAP input.

## Schemas

[Section titled “Schemas”](#schemas)

The operator emits events with the following schema.

### `pcap.packet`

[Section titled “pcap.packet”](#pcappacket)

Contains information about all accessed API endpoints, emitted once per second.

| Field                    | Type     | Description                           |
| :----------------------- | :------- | :------------------------------------ |
| `timestamp`              | `time`   | The time of capturing the packet.     |
| `linktype`               | `uint64` | The linktype of the captured packet.  |
| `original_packet_length` | `uint64` | The length of the original packet.    |
| `captured_packet_length` | `uint64` | The length of the captured packet.    |
| `data`                   | `blob`   | The captured packet’s data as a blob. |

## Examples

[Section titled “Examples”](#examples)

### Read packets from a PCAP file

[Section titled “Read packets from a PCAP file”](#read-packets-from-a-pcap-file)

```tql
load_file "/tmp/trace.pcap"
read_pcap
```

### Read packets from the [network interface](/reference/operators/load_nic) `eth0`

[Section titled “Read packets from the network interface eth0”](#read-packets-from-the-network-interface-eth0)

```tql
load_nic "eth0"
read_pcap
```

## See Also

[Section titled “See Also”](#see-also)

[`load_nic`](/reference/operators/load_nic), [`write_pcap`](/reference/operators/write_pcap)

# read_ssv

Read SSV (Space-Separated Values) from a byte stream.

```tql
read_ssv [list_separator=string, null_value=string, comments=bool, header=string,
          quotes=string, auto_expand=bool,
          schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

The `read_ssv` operator transforms a byte stream into a event stream by parsing the bytes as SSV.

### `auto_expand = bool (optional)`

[Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional)

Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values.

### `comments = bool (optional)`

[Section titled “comments = bool (optional)”](#comments--bool-optional)

Treat lines beginning with ”#” as comments.

### `header = list<string>|string (optional)`

[Section titled “header = list\<string>|string (optional)”](#header--liststringstring-optional)

A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header.

### `list_separator = string (optional)`

[Section titled “list\_separator = string (optional)”](#list_separator--string-optional)

The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled.

Defaults to `,`.

### `null_value = string (optional)`

[Section titled “null\_value = string (optional)”](#null_value--string-optional)

The `string` denoting an absent value.

Defaults to `-`.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Parse an SSV file

[Section titled “Parse an SSV file”](#parse-an-ssv-file)

input.ssv

```txt
message count ip
text 42 "1.1.1.1"
"longer string" 100 1.1.1.2
```

```tql
load "input.ssv"
read_ssv
```

```tql
{message: "text", count: 42, ip: 1.1.1.1}
{message: "longer string", count: 100, ip: 1.1.1.2}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_ssv`](/reference/functions/parse_ssv), [`read_csv`](/reference/operators/read_csv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_suricata

Parse an incoming [Suricata EVE JSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html) stream into events.

```tql
read_suricata [schema_only=bool, raw=bool]
```

## Description

[Section titled “Description”](#description)

The [Suricata](https://suricata.io) network security monitor converts network traffic into a stream of metadata events and provides a rule matching engine to generate alerts. Suricata emits events in the [EVE JSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html) format. The output is a single stream of events where the `event_type` field disambiguates the event type.

Tenzir’s [`JSON`](/reference/operators/read_json) can handle EVE JSON correctly, but for the schema names to match the value from the `event_type` field, you need to pass the option `selector=event_type:suricata`. The `suricata` parser does this by default.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

This means that JSON numbers will be parsed as numbers, but every JSON string remains a string, unless the field is in the `schema`.

## Examples

[Section titled “Examples”](#examples)

### Parse a Suricata EVE JSON log file

[Section titled “Parse a Suricata EVE JSON log file”](#parse-a-suricata-eve-json-log-file)

Here’s an `eve.log` sample:

```json
{"timestamp":"2011-08-12T14:52:57.716360+0200","flow_id":1031464864740687,"pcap_cnt":83,"event_type":"alert","src_ip":"147.32.84.165","src_port":1181,"dest_ip":"78.40.125.4","dest_port":6667,"proto":"TCP","alert":{"action":"allowed","gid":1,"signature_id":2017318,"rev":4,"signature":"ET CURRENT_EVENTS SUSPICIOUS IRC - PRIVMSG *.(exe|tar|tgz|zip)  download command","category":"Potentially Bad Traffic","severity":2},"flow":{"pkts_toserver":27,"pkts_toclient":35,"bytes_toserver":2302,"bytes_toclient":4520,"start":"2011-08-12T14:47:24.357711+0200"},"payload":"UFJJVk1TRyAjemFyYXNhNDggOiBzbXNzLmV4ZSAoMzY4KQ0K","payload_printable":"PRIVMSG #zarasa48 : smss.exe (368)\r\n","stream":0,"packet":"AB5J2xnDCAAntbcZCABFAABMGV5AAIAGLlyTIFSlTih9BASdGgvw0QvAxUWHdVAY+rCL4gAAUFJJVk1TRyAjemFyYXNhNDggOiBzbXNzLmV4ZSAoMzY4KQ0K","packet_info":{"linktype":1}}
{"timestamp":"2011-08-12T14:55:22.154618+0200","flow_id":2247896271051770,"pcap_cnt":775,"event_type":"dns","src_ip":"147.32.84.165","src_port":1141,"dest_ip":"147.32.80.9","dest_port":53,"proto":"UDP","dns":{"type":"query","id":553,"rrname":"irc.freenode.net","rrtype":"A","tx_id":0}}
{"timestamp":"2011-08-12T16:59:22.181050+0200","flow_id":472067367468746,"pcap_cnt":25767,"event_type":"fileinfo","src_ip":"74.207.254.18","src_port":80,"dest_ip":"147.32.84.165","dest_port":1046,"proto":"TCP","http":{"hostname":"www.nmap.org","url":"/","http_user_agent":"Mozilla/4.0 (compatible)","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":301,"redirect":"http://nmap.org/","length":301},"app_proto":"http","fileinfo":{"filename":"/","magic":"HTML document, ASCII text","gaps":false,"state":"CLOSED","md5":"70041821acf87389e40ddcb092004184","sha1":"10395ab3566395ca050232d2c1a0dbad69eb5fd2","sha256":"2e4c462b3424afcc04f43429d5f001e4ef9a28143bfeefb9af2254b4df3a7c1a","stored":true,"file_id":1,"size":301,"tx_id":0}}
```

Import it as follows:

```tql
read_file "eve.log"
read_suricata
import
```

### Read Suricata EVE JSON from a Unix domain socket

[Section titled “Read Suricata EVE JSON from a Unix domain socket”](#read-suricata-eve-json-from-a-unix-domain-socket)

Instead of writing to a file, Suricata can also log to a Unix domain socket that Tenzir can then read from. This saves a filesystem round-trip. This requires the following settings in your `suricata.yaml`:

```yaml
outputs:
  - eve-log:
    enabled: yes
    filetype: unix_stream
    filename: eve.sock
```

Suricata creates `eve.sock` upon startup. Thereafter, you can read from the socket:

```tql
load_file "eve.sock"
read_suricata
```

# read_syslog

Parses an incoming Syslog stream into events.

```tql
read_syslog [octet_counting=bool, merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

[Syslog](https://en.wikipedia.org/wiki/Syslog) is a standard format for message logging.

Tenzir supports reading syslog messages in both the standardized “Syslog Protocol” format ([RFC 5424](https://tools.ietf.org/html/rfc5424)), and the older “BSD syslog Protocol” format ([RFC 3164](https://tools.ietf.org/html/rfc3164)).

Depending on the syslog format, the result can be different. Here’s an example of a syslog message in RFC 5424 format:

```plaintext
<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource="Application" eventID="1011"] Event log entry
```

With this input, the parser will produce the following output, with the schema name `syslog.rfc5424`:

```tql
{
  input: "<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource=\"Application\" eventID=\"1011\"] Event log entry",
  output: {
    facility: 20,
    severity: 5,
    version: 8,
    timestamp: 2023-10-11T22:14:15.003Z,
    hostname: "mymachineexamplecom",
    app_name: "evntslog",
    process_id: "1370",
    message_id: "ID47",
    structured_data: {
      "exampleSDID@32473": {
        eventSource: "Application",
        eventID: 1011,
      },
    },
    message: "Event log entry",
  },
}
```

Here’s an example of a syslog message in RFC 3164 format:

```plaintext
<34>Nov 16 14:55:56 mymachine PROGRAM: Freeform message
```

With this input, the parser will produce the following output, with the schema name `syslog.rfc3164`:

```json
{
  "facility": 4,
  "severity": 2,
  "timestamp": "Nov 16 14:55:56",
  "hostname": "mymachine",
  "app_name": "PROGRAM",
  "process_id": null,
  "content": "Freeform message"
}
```

### `octet_counting = bool (optional)`

[Section titled “octet\_counting = bool (optional)”](#octet_counting--bool-optional)

Employs “octet counting” according to [RFC6587](https://datatracker.ietf.org/doc/html/rfc6587#section-3.4.1) to determine message boundaries instead of the parsing heuristic for multi-line messages.

Defaults to `false`.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Read in the `auth.log`

[Section titled “Read in the auth.log”](#read-in-the-authlog)

Pipeline

```tql
load_file "/var/log/auth.log"
read_syslog
```

```tql
{
  facility: null,
  severity: null,
  timestamp: 2024-10-14T07:15:01.348027,
  hostname: "tenzirs-magic-machine",
  app_name: "CRON",
  process_id: "895756",
  content: "pam_unix(cron:session): session opened for user root(uid=0) by root(uid=0)",
}
{
  facility: null,
  severity: null,
  timestamp: 2024-10-14T07:15:01.349838,
  hostname: "tenzirs-magic-machine",
  app_name: "CRON",
  process_id: "895756",
  content: "pam_unix(cron:session): session closed for user root"
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_syslog`](/reference/functions/parse_syslog), [`write_syslog`](/reference/operators/write_syslog)

# read_tsv

Read TSV (Tab-Separated Values) from a byte stream.

```tql
read_tsv [list_separator=string, null_value=string, comments=bool, header=string,
          quotes=string, auto_expand=bool,
          schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

The `read_tsv` operator transforms a byte stream into a event stream by parsing the bytes as [TSV](https://en.wikipedia.org/wiki/Tab-separated_values).

### `auto_expand = bool (optional)`

[Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional)

Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values.

### `comments = bool (optional)`

[Section titled “comments = bool (optional)”](#comments--bool-optional)

Treat lines beginning with ”#” as comments.

### `header = list<string>|string (optional)`

[Section titled “header = list\<string>|string (optional)”](#header--liststringstring-optional)

A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header.

### `list_separator = string (optional)`

[Section titled “list\_separator = string (optional)”](#list_separator--string-optional)

The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled.

Defaults to `,`.

### `null_value = string (optional)`

[Section titled “null\_value = string (optional)”](#null_value--string-optional)

The `string` denoting an absent value.

Defaults to `-`.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Parse a TSV file

[Section titled “Parse a TSV file”](#parse-a-tsv-file)

input.tsv

```txt
message  count  ip
text  42  "1.1.1.1"
"longer string"  100  "1.1.1.2"
```

```tql
load "input.tsv"
read_tsv
```

```tql
{message: "text", count: 42, ip: 1.1.1.1}
{message: "longer string", count: 100, ip: 1.1.1.2}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_tsv`](/reference/functions/parse_tsv), [`read_csv`](/reference/operators/read_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_xsv`](/reference/operators/read_xsv)

# read_xsv

Read XSV from a byte stream.

```tql
read_xsv field_separator=string, list_separator=string, null_value=string,
        [comments=bool, header=string, auto_expand=bool, quotes=string,
         selector=string, schema_only=bool, raw=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

The `read_xsv` operator transforms a byte stream into a event stream by parsing the bytes as [XSV](https://en.wikipedia.org/wiki/Delimiter-separated_values), a generalization of CSV with a more flexible separator specification.

The following table lists existing XSV configurations:

| Format                                 | Field Separator | List Separator | Null Value |
| -------------------------------------- | :-------------: | :------------: | :--------: |
| [`csv`](/reference/operators/read_csv) |       `,`       |       `;`      |    empty   |
| [`ssv`](/reference/operators/read_ssv) |    `<space>`    |       `,`      |     `-`    |
| [`tsv`](/reference/operators/read_tsv) |       `\t`      |       `,`      |     `-`    |

### `field_separator = string`

[Section titled “field\_separator = string”](#field_separator--string)

The string separating different fields.

### `list_separator = string`

[Section titled “list\_separator = string”](#list_separator--string)

The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled.

### `null_value = string`

[Section titled “null\_value = string”](#null_value--string)

The string denoting an absent value.

### `auto_expand = bool (optional)`

[Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional)

Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values.

### `comments = bool (optional)`

[Section titled “comments = bool (optional)”](#comments--bool-optional)

Treat lines beginning with `#` as comments.

### `header = list<string>|string (optional)`

[Section titled “header = list\<string>|string (optional)”](#header--liststringstring-optional)

A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header.

### `quotes = string (optional)`

[Section titled “quotes = string (optional)”](#quotes--string-optional)

A string of not escaped characters that are supposed to be considered as quotes.

Defaults to the characters `"'`.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_xsv`](/reference/functions/parse_xsv), [`read_csv`](/reference/operators/read_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv)

# read_yaml

Parses an incoming YAML stream into events.

```tql
read_yaml [merge=bool, raw=bool, schema=string, selector=string,
           schema_only=bool, unflatten_separator=string]
```

## Description

[Section titled “Description”](#description)

Parses an incoming [YAML](https://en.wikipedia.org/wiki/YAML) stream into events.

### `merge = bool (optional)`

Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution.

\*: In selector mode, only events with the same selector are merged.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

### `schema = string (optional)`

Provide the name of a schema to be used by the parser.

If a schema with a matching name is installed, the result will always have all fields from that schema.

* Fields that are specified in the schema, but did not appear in the input will be null.
* Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema.

If the given schema does not exist, this option instead assigns the output schema name only.

The `schema` option is incompatible with the `selector` option.

### `selector = string (optional)`

Designates a field value as schema name with an optional dot-separated prefix.

The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name.

For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`.

The `selector` option is incompatible with the `schema` option.

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

If the schema name is obtained via a `selector` and it does not exist, this has no effect.

This option requires either `schema` or `selector` to be set.

### `unflatten_separator = string (optional)`

A delimiter that, if present in keys, causes values to be treated as values of nested records.

A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`.

Without an unflatten separator, the data looks like this:

Without unflattening

```json
{
  "id.orig_h": "1.1.1.1",
  "id.orig_p": 10,
  "id.resp_h": "1.1.1.2",
  "id.resp_p": 5
}
```

With the unflatten separator set to `.`, Tenzir reads the events like this:

With 'unflatten'

```json
{
  "id": {
    "orig_h": "1.1.1.1",
    "orig_p": 10,
    "resp_h": "1.1.1.2",
    "resp_p": 5
  }
}
```

## Examples

[Section titled “Examples”](#examples)

### Parse a YAML file

[Section titled “Parse a YAML file”](#parse-a-yaml-file)

input.yaml

```yaml
---
name: yaml
version: bundled
kind: builtin
types:
  - parser
  - printer
dependencies:
  []
...
```

```tql
load_file "input.yaml"
read_yaml
```

```tql
{
  name: "yaml",
  version: "bundled",
  kind: "builtin",
  types: [
    "parser",
    "printer",
  ],
  dependencies: [],
}
```

***

## title: See Also

[Section titled “title: See Also”](#title-see-also)

[`parse_yaml`](/reference/functions/parse_yaml), [`print_yaml`](/reference/functions/print_yaml), [`write_yaml`](/reference/operators/write_yaml)

# read_zeek_json

Parse an incoming Zeek JSON stream into events.

```tql
read_zeek_json [schema_only=bool, raw=bool]
```

## Description

[Section titled “Description”](#description)

### `schema_only = bool (optional)`

When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema.

### `raw = bool (optional)`

Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema.

This means that JSON numbers will be parsed as numbers, but every JSON string remains a string, unless the field is in the `schema`.

## Examples

[Section titled “Examples”](#examples)

### Load a Zeek connection log

[Section titled “Load a Zeek connection log”](#load-a-zeek-connection-log)

zeek.json

```json
{"__name":"sensor_10_0_0_2","_write_ts":"2020-02-26T04:00:03.734769Z","ts":"2020-02-26T03:40:03.724911Z","uid":"Cx3bf12iVwo5m7Gkd1","id.orig_h":"193.10.255.99","id.orig_p":6667,"id.resp_h":"141.9.40.50","id.resp_p":21,"proto":"tcp","duration":1196.975041,"orig_bytes":0,"resp_bytes":0,"conn_state":"S1","local_orig":false,"local_resp":true,"missed_bytes":0,"history":"Sh","orig_pkts":194,"orig_ip_bytes":7760,"resp_pkts":191,"resp_ip_bytes":8404}
{"_path":"_0_0_2","_write_ts":"2020-02-11T03:48:57.477193Z","ts":"2020-02-11T03:48:57.477193Z","uid":"Cpk0Nl33Zb5ZWLP1tc","id.orig_h":"185.100.59.59","id.orig_p":6667,"id.resp_h":"141.9.255.157","id.resp_p":8080,"proto":"tcp","note":"LongConnection::found","msg":"185.100.59.59 -> 141.9.255.157:8080/tcp remained alive for longer than 19m55s","sub":"1194.62","src":"185.100.59.59","dst":"141.9.255.157","p":8080,"peer_descr":"worker-02","actions":["Notice::ACTION_LOG"],"suppress_for":3600}
```

```tql
load "zeek.json"
read_zeek_json
```

```tql
{
  __name: "sensor_10_0_0_2",
  _write_ts: 2020-02-26T04:00:03.734769,
  ts: 2020-02-26T03:40:03.724911,
  uid: "Cx3bf12iVwo5m7Gkd1",
  id: {
    orig_h: 193.10.255.99,
    orig_p: 6667,
    resp_h: 141.9.40.50,
    resp_p: 21,
  },
  proto: "tcp",
  duration: 1196.975041,
  orig_bytes: 0,
  resp_bytes: 0,
  conn_state: "S1",
  local_orig: false,
  local_resp: true,
  missed_bytes: 0,
  history: "Sh",
  orig_pkts: 194,
  orig_ip_bytes: 7760,
  resp_pkts: 191,
  resp_ip_bytes: 8404,
}
{
  _write_ts: 2020-02-11T03:48:57.477193,
  ts: 2020-02-11T03:48:57.477193,
  uid: "Cpk0Nl33Zb5ZWLP1tc",
  id: {
    orig_h: 185.100.59.59,
    orig_p: 6667,
    resp_h: 141.9.255.157,
    resp_p: 8080,
  },
  proto: "tcp",
  _path: "_0_0_2",
  note: "LongConnection::found",
  msg: "185.100.59.59 -> 141.9.255.157:8080/tcp remained alive for longer than 19m55s",
  sub: "1194.62",
  src: 185.100.59.59,
  dst: 141.9.255.157,
  p: 8080,
  peer_descr: "worker-02",
  actions: [
    Notice::ACTION_LOG
  ],
  suppress_for: 3600,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`read_zeek_tsv`](/reference/operators/read_zeek_tsv), [`write_zeek_tsv`](/reference/operators/write_zeek_tsv)

# read_zeek_tsv

Parses an incoming `Zeek TSV` stream into events.

```tql
read_zeek_tsv
```

## Description

[Section titled “Description”](#description)

The [Zeek](https://zeek.org) network security monitor comes with its own tab-separated value (TSV) format for representing logs. This format includes additional header fields with field names, type annotations, and additional metadata.

The `read_zeek_tsv` operator processes this metadata to extract a schema for the subsequent log entries. The Zeek types `count`, `real`, and `addr` map to the respective Tenzir types `uint64`, `double`, and `ip`.

Here’s an example of a typical Zeek `conn.log` in TSV form:

```txt
#separator \x09
#set_separator  ,
#empty_field  (empty)
#unset_field  -
#path conn
#open 2014-05-23-18-02-04
#fields ts  uid id.orig_h id.orig_p id.resp_h id.resp_p proto service duration  …orig_bytes resp_bytes  conn_state  local_orig  missed_bytes  history orig_pkts …orig_ip_bytes  resp_pkts resp_ip_bytes tunnel_parents
#types  time  string  addr  port  addr  port  enum  string  interval  count coun…t  string  bool  count string  count count count count table[string]
1258531221.486539 Pii6cUUq1v4 192.168.1.102 68  192.168.1.1 67  udp - 0.163820  …301  300 SF  - 0 Dd  1 329 1 328 (empty)
1258531680.237254 nkCxlvNN8pi 192.168.1.103 137 192.168.1.255 137 udp dns 3.7801…25 350 0 S0  - 0 D 7 546 0 0 (empty)
1258531693.816224 9VdICMMnxQ7 192.168.1.102 137 192.168.1.255 137 udp dns 3.7486…47 350 0 S0  - 0 D 7 546 0 0 (empty)
1258531635.800933 bEgBnkI31Vf 192.168.1.103 138 192.168.1.255 138 udp - 46.72538…0  560 0 S0  - 0 D 3 644 0 0 (empty)
1258531693.825212 Ol4qkvXOksc 192.168.1.102 138 192.168.1.255 138 udp - 2.248589…  348  0 S0  - 0 D 2 404 0 0 (empty)
1258531803.872834 kmnBNBtl96d 192.168.1.104 137 192.168.1.255 137 udp dns 3.7488…93 350 0 S0  - 0 D 7 546 0 0 (empty)
1258531747.077012 CFIX6YVTFp2 192.168.1.104 138 192.168.1.255 138 udp - 59.05289…8  549 0 S0  - 0 D 3 633 0 0 (empty)
1258531924.321413 KlF6tbPUSQ1 192.168.1.103 68  192.168.1.1 67  udp - 0.044779  …303  300 SF  - 0 Dd  1 331 1 328 (empty)
1258531939.613071 tP3DM6npTdj 192.168.1.102 138 192.168.1.255 138 udp - - - - S0…  -  0 D 1 229 0 0 (empty)
1258532046.693816 Jb4jIDToo77 192.168.1.104 68  192.168.1.1 67  udp - 0.002103  …311  300 SF  - 0 Dd  1 339 1 328 (empty)
1258532143.457078 xvWLhxgUmj5 192.168.1.102 1170  192.168.1.1 53  udp dns 0.0685…11 36  215 SF  - 0 Dd  1 64  1 243 (empty)
1258532203.657268 feNcvrZfDbf 192.168.1.104 1174  192.168.1.1 53  udp dns 0.1709…62 36  215 SF  - 0 Dd  1 64  1 243 (empty)
1258532331.365294 aLsTcZJHAwa 192.168.1.1 5353  224.0.0.251 5353  udp dns 0.1003…81 273 0 S0  - 0 D 2 329 0 0 (empty)
```

## Examples

[Section titled “Examples”](#examples)

### Read a Zeek connection log from a file

[Section titled “Read a Zeek connection log from a file”](#read-a-zeek-connection-log-from-a-file)

```tql
load_file "/tmp/conn.log"
read_zeek_tsv
```

## See Also

[Section titled “See Also”](#see-also)

[`read_zeek_json`](/reference/operators/read_zeek_json), [`write_zeek_tsv`](/reference/operators/write_zeek_tsv)

# remote

Forces a pipeline to run remotely at a node.

```tql
remote { … }
```

## Description

[Section titled “Description”](#description)

The `remote` operator takes a pipeline as an argument and forces it to run at a Tenzir Node.

This operator has no effect when running a pipeline through the API or Tenzir Platform.

## Examples

[Section titled “Examples”](#examples)

### Get the version of a node

[Section titled “Get the version of a node”](#get-the-version-of-a-node)

```tql
remote {
  version
}
```

## See Also

[Section titled “See Also”](#see-also)

[`local`](/reference/operators/local)

# repeat

Repeats the input a number of times.

```tql
repeat [count:int]
```

## Description

[Section titled “Description”](#description)

The `repeat` operator relays the input without any modification, and repeats its inputs a specified number of times. It is primarily used for testing and when working with generated data.

Potentially High Memory Usage

Whithout specifying `count`, the operator produces events indefinitely.

### `count: int (optional)`

[Section titled “count: int (optional)”](#count-int-optional)

The number of times to repeat the input data.

If not specified, the operator repeats its input indefinitely.

## Examples

[Section titled “Examples”](#examples)

### Repeat input indefinitely

[Section titled “Repeat input indefinitely”](#repeat-input-indefinitely)

Given the following events:

```tql
{number: 1, "text": "one"}
{number: 2, "text": "two"}
```

The `repeat` operator will repeat them indefinitely, in order:

```tql
repeat
```

```tql
{number: 1, "text": "one"}
{number: 2, "text": "two"}
{number: 1, "text": "one"}
{number: 2, "text": "two"}
{number: 1, "text": "one"}
{number: 2, "text": "two"}
// …
```

### Repeat the first event 5 times

[Section titled “Repeat the first event 5 times”](#repeat-the-first-event-5-times)

```tql
head 1
repeat 5
```

```tql
{number: 1, "text": "one"}
{number: 1, "text": "one"}
{number: 1, "text": "one"}
{number: 1, "text": "one"}
{number: 1, "text": "one"}
```

# replace

Replaces all occurrences of a value with another value.

```tql
replace [path:field...], what=any, with=any
```

## Description

[Section titled “Description”](#description)

The `replace` operator scans all fields of each input event and replaces every occurrence of a value equal to `what` with the value specified by `with`.

Note

The operator does not replace values in lists.

### `path: field... (optional)`

[Section titled “path: field... (optional)”](#path-field-optional)

An optional set of paths to restrict replacements to.

### `what: any`

[Section titled “what: any”](#what-any)

The value to search for and replace.

### `with: any`

[Section titled “with: any”](#with-any)

The value to replace in place of `what`.

## Examples

[Section titled “Examples”](#examples)

### Replace all occurrences of 42 with null

[Section titled “Replace all occurrences of 42 with null”](#replace-all-occurrences-of-42-with-null)

```tql
from {
  count: 42,
  data: {value: 42, other: 100},
  list: [42, 24, 42]
}
replace what=42, with=null
```

```tql
{
  count: null,
  data: {value: null, other: 100},
  list: [42, 24, 42]
}
```

### Replace only within specific fields

[Section titled “Replace only within specific fields”](#replace-only-within-specific-fields)

```tql
from {
  count: 42,
  data: {value: 42, other: 100},
}
replace data, what=42, with=null
```

```tql
{
  count: 42,
  data: {value: null, other: 100},
}
```

### Replace a specific IP address with a redacted value

[Section titled “Replace a specific IP address with a redacted value”](#replace-a-specific-ip-address-with-a-redacted-value)

```tql
from {
  src_ip: 192.168.1.1,
  dst_ip: 10.0.0.1,
  metadata: {source: 192.168.1.1}
}
replace what=192.168.1.1, with="REDACTED"
```

```tql
{
  src_ip: "REDACTED",
  dst_ip: 10.0.0.1,
  metadata: {
    source: "REDACTED",
  },
}
```

## See Also

[Section titled “See Also”](#see-also)

[`replace`](/reference/functions/replace)

# reverse

Reverses the event order.

```tql
reverse
```

## Description

[Section titled “Description”](#description)

`reverse` is a shorthand notation for [`slice stride=-1`](/reference/operators/slice).

Potentially High Memory Usage

Use caution when applying this operator to large inputs. It currently buffers all data in memory. Out-of-core processing is on our roadmap.

## Examples

[Section titled “Examples”](#examples)

### Reverse a stream of events

[Section titled “Reverse a stream of events”](#reverse-a-stream-of-events)

```tql
from {x: 1}, {x: 2}, {x: 3}
reverse
```

```tql
{x: 3}
{x: 2}
{x: 1}
```

## See Also

[Section titled “See Also”](#see-also)

[`sort`](/reference/operators/sort)

# sample

Dynamically samples events from a event stream.

```tql
sample [period:duration, mode=string, min_events=int, max_rate=int, max_samples=int]
```

## Description

[Section titled “Description”](#description)

Dynamically samples input data from a stream based on the frequency of receiving events for streams with varying load.

The operator counts the number of events received in the `period` and applies the specified function on the count to calculate the sampling rate for the next period.

### `period: duration (optional)`

[Section titled “period: duration (optional)”](#period-duration-optional)

The duration to count events in, i.e., how often the sample rate is computed.

The sampling rate for the first window is `1:1`.

Defaults to `30s`.

### `mode = string (optional)`

[Section titled “mode = string (optional)”](#mode--string-optional)

The function used to compute the sampling rate:

* `"ln"` (default)
* `"log2"`
* `"log10"`
* `"sqrt"`

### `min_events = int (optional)`

[Section titled “min\_events = int (optional)”](#min_events--int-optional)

The minimum number of events that must be received during the previous sampling period for the sampling mode to be applied in the current period. If the number of events in a sample group falls below this threshold, a `1:1` sample rate is used instead.

Defaults to `30`.

### `max_rate = int (optional)`

[Section titled “max\_rate = int (optional)”](#max_rate--int-optional)

The sampling rate is capped to this value if the computed rate is higher than this.

### `max_samples = int (optional)`

[Section titled “max\_samples = int (optional)”](#max_samples--int-optional)

The maximum number of events to emit per `period`.

## Examples

[Section titled “Examples”](#examples)

### Sample the input every 30s dynamically

[Section titled “Sample the input every 30s dynamically”](#sample-the-input-every-30s-dynamically)

Sample a feed `log-stream` every 30s dynamically, only changing rate when more than 50 events (`min_events`) are received. Additionally, cap the max sampling rate to `1:500`, i.e., 1 sample for every 500 events or more (`max_rate`).

```tql
subscribe "log-stream"
sample 30s, min_events=50, max_rate=500
```

### Sample metrics every hour

[Section titled “Sample metrics every hour”](#sample-metrics-every-hour)

Sample some `metrics` every hour, limiting the max samples per period to 5,000 samples (`max_samples`) and limiting the overall sample count to 100,000 samples ([`head`](/reference/operators/head)).

```tql
subscribe "metrics"
sample 1h, max_samples=5k
head 100k
```

## See Also

[Section titled “See Also”](#see-also)

[`deduplicate`](/reference/operators/deduplicate)

# save_amqp

Saves a byte stream via AMQP messages.

```tql
save_amqp [url:str, channel=int, exchange=str, routing_key=str,
           options=record, mandatory=bool, immediate=bool]
```

## Description

[Section titled “Description”](#description)

The `save_amqp` operator is an [AMQP](https://www.amqp.org/) 0-9-1 client to send messages to an exchange.

### `url: str (optional)`

[Section titled “url: str (optional)”](#url-str-optional)

A URL that specifies the AMQP server. The URL must have the following format:

```plaintext
amqp://[USERNAME[:PASSWORD]@]HOSTNAME[:PORT]/[VHOST]
```

When the URL is present, it will overwrite the corresponding values of the configuration options.

### `channel = int (optional)`

[Section titled “channel = int (optional)”](#channel--int-optional)

The channel number to use.

Defaults to `1`.

### `exchange = str (optional)`

[Section titled “exchange = str (optional)”](#exchange--str-optional)

The exchange to interact with.

Defaults to `"amq.direct"`.

### `routing_key = str (optional)`

[Section titled “routing\_key = str (optional)”](#routing_key--str-optional)

The routing key to publish messages with.

Defaults to the empty string.

### `options = record (optional)`

[Section titled “options = record (optional)”](#options--record-optional)

An option record for RabbitMQ , e.g., `{max_channels: 42, frame_size: 1024, sasl_method: "external"}`.

Available options are:

```yaml
hostname: 127.0.0.1
port: 5672
ssl: false
vhost: /
max_channels: 2047
frame_size: 131072
heartbeat: 0
sasl_method: plain
username: guest
password: guest
```

### `mandatory = bool (optional)`

[Section titled “mandatory = bool (optional)”](#mandatory--bool-optional)

This flag tells the server how to react if the message cannot be routed to a queue. If `true`, the server will return an unroutable message with a Return method. Otherwise the server silently drops the message.

Defaults to `false`.

### `immediate = bool (optional)`

[Section titled “immediate = bool (optional)”](#immediate--bool-optional)

This flag tells the server how to react if the message cannot be routed to a queue consumer immediately. If `true`, the server will return an undeliverable message with a Return method. If `false`, the server will queue the message, but with no guarantee that it will ever be consumed.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Send the list of plugins as [JSON](/reference/operators/write_json)

[Section titled “Send the list of plugins as JSON”](#send-the-list-of-plugins-as-json)

```tql
plugins
write_json
save_amqp
```

## See Also

[Section titled “See Also”](#see-also)

[`load_amqp`](/reference/operators/load_amqp)

# save_azure_blob_storage

Saves bytes to Azure Blob Storage.

```tql
save_azure_blob_storage uri:string, [account_key=string]
```

## Description

[Section titled “Description”](#description)

The `save_azure_blob_storage` operator writes bytes to a blob in an Azure Blob Store.

By default, authentication is handled by the Azure SDK’s credential chain which may read from multiple environment variables, such as:

* `AZURE_TENANT_ID`
* `AZURE_CLIENT_ID`
* `AZURE_CLIENT_SECRET`
* `AZURE_AUTHORITY_HOST`
* `AZURE_CLIENT_CERTIFICATE_PATH`
* `AZURE_FEDERATED_TOKEN_FILE`

### `uri: string`

[Section titled “uri: string”](#uri-string)

An URI identifying the blob to save to. If the blob and/or do not exist, they will be created.

Supported URI formats:

1. `abfs[s]://[:<password>@]<account>.blob.core.windows.net[/<container>[/<path>]]`
2. `abfs[s]://<container>[:<password>]@<account>.dfs.core.windows.net[/path]`
3. `abfs[s]://[<account[:<password>]@]<host[.domain]>[<:port>][/<container>[/path]]`
4. `abfs[s]://[<account[:<password>]@]<container>[/path]`

(1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs 1, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2).

Authenticate with the Azure CLI

Run `az login` on the command-line to authenticate the current user with Azure’s command-line arguments.

### `account_key = string (optional)`

[Section titled “account\_key = string (optional)”](#account_key--string-optional)

Account key to authenticate with.

## Examples

[Section titled “Examples”](#examples)

### Write JSON

[Section titled “Write JSON”](#write-json)

Write to blob `obj.json` in the blob container `container`, using the `tenzirdev` user:

```tql
write_json
save_azure_blob_storage "abfss://tenzirdev@container/obj.json"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage)

# save_email

Saves bytes through an SMTP server.

```tql
save_email recipient:str, [endpoint=str, from=str, subject=str, username=str,
      password=str, authzid=str, authorization=str, tls=bool,
      skip_peer_verification=bool, cacert=string, certfile=string, keyfile=string,
      mime=bool]
```

## Description

[Section titled “Description”](#description)

The `save_email` operator establishes a SMTP(S) connection to a mail server and sends bytes as email body.

### `recipient: str`

[Section titled “recipient: str”](#recipient-str)

The recipient of the mail.

The expected format is either `Name <user@example.org>` with the email in angle brackets, or a plain email adress, such as `user@example.org`.

### `endpoint = str (optional)`

[Section titled “endpoint = str (optional)”](#endpoint--str-optional)

The endpoint of the mail server.

To choose between SMTP and SMTPS, provide a URL with with the corresponding scheme. For example, `smtp://127.0.0.1:25` will establish an unencrypted connection, whereas `smtps://127.0.0.1:25` an encrypted one. If you specify a server without a schema, the protocol defaults to SMTPS.

Defaults to `smtp://localhost:25`.

### `from = str (optional)`

[Section titled “from = str (optional)”](#from--str-optional)

The `From` header.

If you do not specify this parameter, an empty address is sent to the SMTP server which might cause the email to be rejected.

### `subject = str (optional)`

[Section titled “subject = str (optional)”](#subject--str-optional)

The `Subject` header.

### `username = str (optional)`

[Section titled “username = str (optional)”](#username--str-optional)

The username in an authenticated SMTP connection.

### `password = str (optional)`

[Section titled “password = str (optional)”](#password--str-optional)

The password in an authenticated SMTP connection.

### `authzid = str (optional)`

[Section titled “authzid = str (optional)”](#authzid--str-optional)

The authorization identity in an authenticated SMTP connection.

This option is only applicable to the PLAIN SASL authentication mechanism where it is optional. When not specified only the authentication identity (`authcid`) as specified by the username is sent to the server, along with the password. The server derives an `authzid` from the `authcid` when not provided, which it then uses internally. When the `authzid` is specified it can be used to access another user’s inbox, that the user has been granted access to, or a shared mailbox.

### `authorization = str (optional)`

[Section titled “authorization = str (optional)”](#authorization--str-optional)

The authorization options for an authenticated SMTP connection.

This login option defines the preferred authentication mechanism, e.g., `AUTH=PLAIN`, `AUTH=LOGIN`, or `AUTH=*`.

### `tls = bool (optional)`

Enables TLS.

Defaults to `false`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

### `mime = bool (optional)`

[Section titled “mime = bool (optional)”](#mime--bool-optional)

Whether to wrap the chunk into a MIME part.

The operator uses the metadata of the byte chunk for the `Content-Type` MIME header.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

Send the Tenzir version string as CSV to `user@example.org`:

```tql
version
write_csv
save_email "user@example.org"
```

Send the email body as MIME part:

```tql
version
write_json
save_email "user@example.org", mime=true
```

This may result in the following email body:

```plaintext
--------------------------s89ecto6c12ILX7893YOEf
Content-Type: application/json
Content-Transfer-Encoding: quoted-printable


{
  "version": "4.10.4+ge0a060567b-dirty",
  "build": "ge0a060567b-dirty",
  "major": 4,
  "minor": 10,
  "patch": 4
}


--------------------------s89ecto6c12ILX7893YOEf--
```

# save_file

Writes a byte stream to a file.

```tql
save_file path:string, [append=bool, real_time=bool, uds=bool]
```

## Description

[Section titled “Description”](#description)

Writes a byte stream to a file.

### `path: string`

[Section titled “path: string”](#path-string)

The file path to write to. If intermediate directories do not exist, they will be created. When `~` is the first character, it will be substituted with the value of the `$HOME` environment variable.

### `append = bool (optional)`

[Section titled “append = bool (optional)”](#append--bool-optional)

If `true`, appends to the file instead of overwriting it.

### `real_time = bool (optional)`

[Section titled “real\_time = bool (optional)”](#real_time--bool-optional)

If `true`, immediately synchronizes the file with every chunk of bytes instead of buffering bytes to batch filesystem write operations.

### `uds = bool (optional)`

[Section titled “uds = bool (optional)”](#uds--bool-optional)

If `true`, creates a Unix Domain Socket instead of a normal file. Cannot be combined with `append=true`.

## Examples

[Section titled “Examples”](#examples)

### Save bytes to a file

[Section titled “Save bytes to a file”](#save-bytes-to-a-file)

```tql
save_file "/tmp/out.txt"
```

## See Also

[Section titled “See Also”](#see-also)

[`files`](/reference/operators/files), [`load_file`](/reference/operators/load_file), [`save_stdout`](/reference/operators/save_stdout)

# save_ftp

Saves a byte stream via FTP.

```tql
save_ftp url:str [tls=bool, cacert=string, certifle=string,
                  keyfile=string, skip_peer_verification=bool]
```

## Description

[Section titled “Description”](#description)

Saves a byte stream via FTP.

### `url: str`

[Section titled “url: str”](#url-str)

The URL to request from. The `ftp://` scheme can be omitted.

### `tls = bool (optional)`

Enables TLS.

Defaults to `true`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

```tql
save_ftp "ftp.example.org"
```

# save_gcs

Saves bytes to a Google Cloud Storage object.

```tql
save_gcs uri:string, [anonymous=bool]
```

## Description

[Section titled “Description”](#description)

The `save_gcs` operator connects to a GCS bucket to save raw bytes to a GCS object.

The connector tries to retrieve the appropriate credentials using Google’s [Application Default Credentials](https://google.aip.dev/auth/4110).

### `uri: string`

[Section titled “uri: string”](#uri-string)

The path to the GCS object.

The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist:

> For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`.

### `anonymous = bool (optional)`

[Section titled “anonymous = bool (optional)”](#anonymous--bool-optional)

Ignore any predefined credentials and try to use anonymous credentials.

## Examples

[Section titled “Examples”](#examples)

Write JSON to an object `test.json` in `bucket`, but using a different GCS-compatible endpoint:

```tql
write_json
save_gcs "gs://bucket/test.json?endpoint_override=gcs.mycloudservice.com"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_gcs`](/reference/operators/load_gcs)

# save_google_cloud_pubsub

Publishes to a Google Cloud Pub/Sub topic.

```tql
save_google_cloud_pubsub project_id=string, topic_id=string
```

Authentication

The connector tries to retrieve the appropriate credentials using Google’s [Application Default Credentials](https://google.aip.dev/auth/4110).

## Description

[Section titled “Description”](#description)

The operator publishes bytes to a Google Cloud Pub/Sub topic.

### `project_id = string`

[Section titled “project\_id = string”](#project_id--string)

The project to connect to. Note that this is the project\_id, not the display name.

### `topic_id = string`

[Section titled “topic\_id = string”](#topic_id--string)

The topic to publish to.

## URI support & integration with `from`

[Section titled “URI support & integration with from”](#uri-support--integration-with-from)

The `save_google_cloud_pubsub` operator can also be used from the [`to`](/reference/operators/to) operator. For this, the `gcps://` scheme can be used. The URI is then translated:

```tql
to "gcps://my_project/my_topic"
```

```tql
save_google_cloud_pubsub project_id="my_project", topic_id="my_topic"
```

## Examples

[Section titled “Examples”](#examples)

### Publish alerts to a given topic

[Section titled “Publish alerts to a given topic”](#publish-alerts-to-a-given-topic)

Publish `suricata.alert` events as JSON to `alerts-topic`:

```tql
export
where @name = "suricata.alert"
write_json
save_google_cloud_pubsub project_id="amazing-project-123456", topic_id="alerts-topic"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_google_cloud_pubsub`](/reference/operators/load_google_cloud_pubsub)

# save_http

Sends a byte stream via HTTP.

```tql
save_http url:string, [params=record, headers=record, method=string,
          parallel=int, tls=bool, cacert=string, certifle=string,
          keyfile=string, skip_peer_verification=bool]
```

## Description

[Section titled “Description”](#description)

The `save_http` operator performs a HTTP request with the request body being the bytes provided by the previous operator.

### `url: string`

[Section titled “url: string”](#url-string)

The URL to write to. The `http://` scheme can be omitted.

### `method = string (optional)`

[Section titled “method = string (optional)”](#method--string-optional)

The HTTP method, such as `POST` or `GET`.

The default is `"POST"`.

### `params = record (optional)`

[Section titled “params = record (optional)”](#params--record-optional)

The query parameters for the request.

### `headers = record (optional)`

[Section titled “headers = record (optional)”](#headers--record-optional)

The headers for the request.

### `tls = bool (optional)`

Enables TLS.

Defaults to `true`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Call a webhook with pipeline data

[Section titled “Call a webhook with pipeline data”](#call-a-webhook-with-pipeline-data)

```tql
save_http "example.org/api", headers={"X-API-Token": "0000-0000-0000"}
```

## See Also

[Section titled “See Also”](#see-also)

[`load_http`](/reference/operators/load_http)

# save_kafka

Saves a byte stream to a Apache Kafka topic.

```tql
save_kafka topic:string, [key=string, timestamp=time, options=record,
           aws_iam=record]
```

## Description

[Section titled “Description”](#description)

Deprecated

The `save_kafka` operator does not respect event boundaries and can combine multiple events into a single message, causing issues for consumers. Consider using `to_kafka` instead.

The `save_kafka` operator saves bytes to a Kafka topic.

The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`.

The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them:

* `bootstrap.servers`: `localhost`
* `client.id`: `tenzir`
* `group.id`: `tenzir`

### `topic: string`

[Section titled “topic: string”](#topic-string)

The Kafka topic to use.

### `key = string (optional)`

[Section titled “key = string (optional)”](#key--string-optional)

Sets a fixed key for all messages.

### `timestamp = time (optional)`

[Section titled “timestamp = time (optional)”](#timestamp--time-optional)

Sets a fixed timestamp for all messages.

### `options = record (optional)`

[Section titled “options = record (optional)”](#options--record-optional)

A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"auto.offset.reset" : "earliest", "enable.partition.eof": true}`.

The `save_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs.

We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `save_kafka` arguments.

### `aws_iam = record (optional)`

[Section titled “aws\_iam = record (optional)”](#aws_iam--record-optional)

If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified.

Available keys:

* `region`: Region of the MSK Clusters. Must be specified when using IAM.
* `assume_role`: Optional Role ARN to assume.
* `session_name`: Optional session name to use when assuming a role.
* `external_id`: Optional external id to use when assuming a role.

The operator will try to get credentials in the following order:

1. Checks your environment variables for AWS Credentials.
2. Checks your `$HOME/.aws/credentials` file for a profile and credentials
3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`.
4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that are not directly supported by AWS.
5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set.
6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON.

## Examples

[Section titled “Examples”](#examples)

### Write the Tenzir version to topic `tenzir` with timestamp from the past

[Section titled “Write the Tenzir version to topic tenzir with timestamp from the past”](#write-the-tenzir-version-to-topic-tenzir-with-timestamp-from-the-past)

```tql
version
write_json
save_kafka "tenzir", timestamp=1984-01-01
```

### Follow a CSV file and publish it to topic `data`

[Section titled “Follow a CSV file and publish it to topic data”](#follow-a-csv-file-and-publish-it-to-topic-data)

```tql
load_file "/tmp/data.csv"
read_csv
write_json
save_kafka "data"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_kafka`](/reference/operators/load_kafka)

# save_s3

Saves bytes to an Amazon S3 object.

```tql
save_s3 uri:str, [anonymous=bool, role=string, external_id=string]
```

## Description

[Section titled “Description”](#description)

The `save_s3` operator writes bytes to an S3 object in an S3 bucket.

The connector tries to retrieve the appropriate credentials using AWS’s [default credentials provider chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html).

Note

Make sure to configure AWS credentials for the same user account that runs `tenzir` and `tenzir-node`. The AWS CLI creates configuration files for the current user under `~/.aws`, which can only be read by the same user account.

The `tenzir-node` systemd unit by default creates a `tenzir` user and runs as that user, meaning that the AWS credentials must also be configured for that user. The directory `~/.aws` must be readable for the `tenzir` user.

If a config file `<prefix>/etc/tenzir/plugin/s3.yaml` or `~/.config/tenzir/plugin/s3.yaml` exists, it is always preferred over the default AWS credentials. The configuration file must have the following format:

```yaml
access-key: your-access-key
secret-key: your-secret-key
session-token: your-session-token (optional)
```

### `uri: str`

[Section titled “uri: str”](#uri-str)

The path to the S3 object.

The syntax is `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)`.

Options can be appended to the path as query parameters, as per [Arrow](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri):

> For S3, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`.

### `anonymous = bool (optional)`

[Section titled “anonymous = bool (optional)”](#anonymous--bool-optional)

Whether to ignore any predefined credentials and try to save with anonymous credentials.

### `role = string (optional)`

[Section titled “role = string (optional)”](#role--string-optional)

A role to assume when writing to S3.

### `external_id = string (optional)`

[Section titled “external\_id = string (optional)”](#external_id--string-optional)

The external ID to use when assuming the `role`.

Defaults to no ID.

## Examples

[Section titled “Examples”](#examples)

Read CSV from an object `obj.csv` in the bucket `examplebucket` and save it as YAML to another bucket `examplebucket2`:

```tql
load_s3 "s3://examplebucket/obj.csv"
read_csv
write_yaml
save_s3 "s3://examplebucket2/obj.yaml"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_s3`](/reference/operators/load_s3), [`to_amazon_security_lake`](/reference/operators/to_amazon_security_lake)

# save_sqs

Saves bytes to [Amazon SQS](https://docs.aws.amazon.com/sqs/) queues.

```tql
save_sqs queue:str, [poll_time=duration]
```

## Description

[Section titled “Description”](#description)

[Amazon Simple Queue Service (Amazon SQS)](https://docs.aws.amazon.com/sqs/) is a fully managed message queuing service to decouple and scale microservices, distributed systems, and serverless applications. The `save_sqs` operator writes bytes as messages into an SQS queue.

The `save_sqs` operator uses long polling, which helps reduce your cost of using SQS by reducing the number of empty responses when there are no messages available to return in reply to a message request. Use the `poll_time` option to adjust the timeout.

The operator requires the following AWS permissions:

* `sqs:GetQueueUrl`
* `sqs:SendMessage`

### `queue: str`

[Section titled “queue: str”](#queue-str)

The name of the queue to use.

### `poll_time = duration (optional)`

[Section titled “poll\_time = duration (optional)”](#poll_time--duration-optional)

The long polling timeout per request.

The value must be between 1 and 20 seconds.

Defaults to `3s`.

## Examples

[Section titled “Examples”](#examples)

Write JSON messages from a source feed to the SQS queue `tenzir`:

```tql
subscribe "to-sqs"
write_json
save_sqs "tenzir"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_sqs`](/reference/operators/load_sqs)

# save_stdout

Writes a byte stream to standard output.

```tql
save_stdout
```

## Description

[Section titled “Description”](#description)

Writes a byte stream to standard output. This is mostly useful when using the `tenzir` executable as part of a shell script.

## Examples

[Section titled “Examples”](#examples)

### Write colored, compact TQL-style

[Section titled “Write colored, compact TQL-style”](#write-colored-compact-tql-style)

```tql
from {x: "Hello World"}
write_tql compact=true, color=true
save_stdout
```

```tql
{x: "Hello World"}
```

## See Also

[Section titled “See Also”](#see-also)

[`load_stdin`](/reference/operators/load_stdin), [`save_file`](/reference/operators/save_file)

# save_tcp

Saves bytes to a TCP or TLS connection.

```tql
save_tcp endpoint:string, [retry_delay=duration, max_retry_count=int,
                           tls=bool, cacert=string, certifle=string,
                           keyfile=string, skip_peer_verification=bool]
```

## Description

[Section titled “Description”](#description)

Saves bytes to the given endpoint via TCP or TLS. Attempts to reconnect automatically for `max_retry_count` in case of recoverable connection errors.

Note

Due to the nature of TCP a disconnect can still lead to lost and or incomplete events on the receiving end.

### `endpoint: string`

[Section titled “endpoint: string”](#endpoint-string)

The endpoint to which the server will connect. Must be of the form `[tcp://]<hostname>:<port>`. You can also use an IANA service name instead of a numeric port.

### `retry_delay = duration (optional)`

[Section titled “retry\_delay = duration (optional)”](#retry_delay--duration-optional)

The amount of time to wait before attempting to reconnect in case a connection attempt fails and the error is deemed recoverable. Defaults to `30s`.

### \`max\_retry\_count = int (optional)

[Section titled “\`max\_retry\_count = int (optional)”](#max_retry_count--int-optional)

The number of retries to attempt in case of connection errors before transitioning into the error state. Defaults to `10`.

### `tls = bool (optional)`

Enables TLS.

Defaults to `false`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Transform incoming Syslog to BITZ and save over TCP

[Section titled “Transform incoming Syslog to BITZ and save over TCP”](#transform-incoming-syslog-to-bitz-and-save-over-tcp)

```tql
load_tcp "0.0.0.0:8090" { read_syslog }
write_bitz
save_tcp "127.0.0.1:4000"
```

### Save to localhost with TLS

[Section titled “Save to localhost with TLS”](#save-to-localhost-with-tls)

```tql
subscribe "feed"
write_json
save_tcp "127.0.0.1:4000", tls=true, skip_peer_verification=true
```

# save_udp

Saves bytes to a UDP socket.

```tql
save_udp endpoint:str
```

## Description

[Section titled “Description”](#description)

Saves bytes to a UDP socket.

### `endpoint: str`

[Section titled “endpoint: str”](#endpoint-str)

The address of the remote endpoint to load bytes from. Must be of the format: `[udp://]host:port`.

## Examples

[Section titled “Examples”](#examples)

Send the Tenzir version as CSV file to a remote endpoint via UDP:

```tql
version
write_csv
save_udp "127.0.0.1:56789"
```

Use `nc -ul 127.0.0.1 56789` to spin up a UDP server to test the above pipeline.

## See Also

[Section titled “See Also”](#see-also)

[`load_udp`](/reference/operators/load_udp)

# save_zmq

Sends bytes as ZeroMQ messages.

```tql
save_zmq [endpoint:str, listen=bool, connect=bool, monitor=bool]
```

## Description

[Section titled “Description”](#description)

The `save_zmq` operator sends bytes as a ZeroMQ message via a `PUB` socket.

Indpendent of the socket type, the `save_zmq` operator supports specfiying the direction of connection establishment with `listen` and `connect`. This can be helpful to work around firewall restrictions and fit into broader set of existing ZeroMQ applications.

With the `monitor` option, you can activate message buffering for TCP sockets that hold off sending messages until *at least one* remote peer has connected. This can be helpful when you want to delay publishing until you have one connected subscriber, e.g., when the publisher spawns before any subscriber exists.

### `endpoint: str (optional)`

[Section titled “endpoint: str (optional)”](#endpoint-str-optional)

The endpoint for connecting to or listening on a ZeroMQ socket.

Defaults to `tcp://127.0.0.1:5555`.

### `listen = bool (optional)`

[Section titled “listen = bool (optional)”](#listen--bool-optional)

Bind to the ZeroMQ socket.

Defaults to `true`.

### `connect = bool (optional)`

[Section titled “connect = bool (optional)”](#connect--bool-optional)

Connect to the ZeroMQ socket.

Defaults to `false`.

### `monitor = bool (optional)`

[Section titled “monitor = bool (optional)”](#monitor--bool-optional)

Monitors a 0mq socket over TCP until the remote side establishes a connection.

## Examples

[Section titled “Examples”](#examples)

### Publish events by connecting to a PUB socket

[Section titled “Publish events by connecting to a PUB socket”](#publish-events-by-connecting-to-a-pub-socket)

```tql
from {x: 42}
write_csv
save_zmq connect=true
```

## See Also

[Section titled “See Also”](#see-also)

[`load_zmq`](/reference/operators/load_zmq)

# schemas

Retrieves all schemas for events stored at a node.

```tql
schemas
```

## Description

[Section titled “Description”](#description)

The `schemas` operator shows all schemas of all events stored at a node.

Note that there may be multiple schema definitions with the same name, but a different set of fields, e.g., because the imported data’s schema changed over time.

## Examples

[Section titled “Examples”](#examples)

### See all available definitions for a given schema

[Section titled “See all available definitions for a given schema”](#see-all-available-definitions-for-a-given-schema)

```tql
schemas
where name == "suricata.alert"
```

# select

Selects some values and discards the rest.

```tql
select (field|assignment)...
```

## Description

[Section titled “Description”](#description)

This operator keeps only the provided fields and drops the rest.

### `field`

[Section titled “field”](#field)

The field to keep. If it does not exist, it’s given the value `null` and a warning is emitted.

### `assignment`

[Section titled “assignment”](#assignment)

An assignment of the form `<field>=<expr>`.

## Examples

[Section titled “Examples”](#examples)

### Select and create columns

[Section titled “Select and create columns”](#select-and-create-columns)

Keep `a` and introduce `y` with the value of `b`:

```tql
from {a: 1, b: 2, c: 3}
select a, y=b
```

```tql
{a: 1, y: 2}
```

A more complex example with expressions and selection through records:

```tql
from {
  name: "foo",
  pos: {
    x: 1,
    y: 2,
  },
  state: "active",
}
select id=name.to_upper(), pos.x, added=true
```

```tql
{
  id: "FOO",
  pos: {
    x: 1,
  },
  added: true,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`drop`](/reference/operators/drop), [`where`](/reference/operators/where)

# serve

Make events available under the `/serve` REST API endpoint

```tql
serve id:string, [buffer_size=int]
```

## Description

[Section titled “Description”](#description)

The `serve` operator bridges between pipelines and the corresponding `/serve` [REST API endpoint](/reference/node/api).

![Serve Operator](/_astro/serve.excalidraw.CC8LHhCr_19DKCs.svg)

Pipelines ending with the `serve` operator exit when all events have been delivered over the corresponding endpoint.

### `id: string`

[Section titled “id: string”](#id-string)

An identifier that uniquely identifies the operator. The `serve` operator errors when receiving a duplicate serve id.

### `buffer_size = int (optional)`

[Section titled “buffer\_size = int (optional)”](#buffer_size--int-optional)

The buffer size specifies the maximum number of events to keep in the `serve` operator to make them instantly available in the corresponding endpoint before throttling the pipeline execution.

Defaults to `1Ki`.

## Examples

[Section titled “Examples”](#examples)

### Make the input available as REST API

[Section titled “Make the input available as REST API”](#make-the-input-available-as-rest-api)

Read a Zeek `conn.log` and make it available as `zeek-conn-logs`:

```tql
load_file "path/to/conn.log"
read_zeek_tsv
serve "zeek-conn-logs"'
```

Then fetch the first 100 events from the `/serve` endpoint:

```bash
curl \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"serve_id": "zeek-conn-logs", "continuation_token": null, "timeout": "1s", "max_events": 100}' \
  http://localhost:5160/api/v0/serve
```

This will return up to 100 events, or less if the specified timeout of 1 second expired.

Subsequent results for further events must specify a continuation token. The token is included in the response under `next_continuation_token` if there are further events to be retrieved from the endpoint.

### Wait for the first event

[Section titled “Wait for the first event”](#wait-for-the-first-event)

This pipeline will produce 10 events after 3 seconds of doing nothing.

```tql
shell "sleep 3; jq --null-input '{foo: 1}'"
read_json
repeat 10
serve "slow-events"
```

```bash
curl \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"serve_id": "slow-events", "continuation_token": null, "timeout": "5s", "min_events": 1}' \
  http://localhost:5160/api/v0/serve
```

The call to `/serve` will wait up to 5 seconds for the first event from the pipeline arriving at the serve operator, and return immediately once the first event arrives.

## See Also

[Section titled “See Also”](#see-also)

[`api`](/reference/operators/api), [`from_http`](/reference/operators/from_http), [`openapi`](/reference/operators/openapi)

# set

Assigns a value to a field, creating it if necessary.

```tql
field = expr
set field=expr...
```

## Description

[Section titled “Description”](#description)

Assigns a value to a field, creating it if necessary. If the field does not exist, it is appended to the end. If the field name is a path such as `foo.bar.baz`, records for `foo` and `bar` will be created if they do not exist yet.

Within assignments, the `move` keyword in front of a field causes a field to be removed from the input after evaluation.

Implied operator

The `set` operator is implied whenever a direct assignment is written. We recommend to use the implicit version. For example, use `test = 42` instead of `set test=42`.

Read our [language documentation](/explanations/language/statements/#assignment) for a more detailed description.

## Examples

[Section titled “Examples”](#examples)

### Append a new field

[Section titled “Append a new field”](#append-a-new-field)

```tql
from {a: 1, b: 2}
c = a + b
```

```tql
{a: 1, b: 2, c: 3}
```

### Update an existing field

[Section titled “Update an existing field”](#update-an-existing-field)

```tql
from {a: 1, b: 2}
a = "Hello"
```

```tql
{a: "Hello", b: 2}
```

### Move a field

[Section titled “Move a field”](#move-a-field)

```tql
from {a: 1}
b = move a
```

```tql
{b: 1}
```

## See Also

[Section titled “See Also”](#see-also)

[`move`](/reference/operators/move)

# shell

Executes a system command and hooks its stdin and stdout into the pipeline.

```tql
shell cmd:string
```

## Description

[Section titled “Description”](#description)

The `shell` operator executes the provided command by spawning a new process. The input of the operator is forwarded to the child’s standard input. Similarly, the child’s standard output is forwarded to the output of the operator.

### `cmd: string`

[Section titled “cmd: string”](#cmd-string)

The command to execute and hook into the pipeline processing. It is interpreted by `/bin/sh -c`.

Lots of escaping?

Try using raw string literals: `r#"echo "i can use quotes""#`.

## Secrets

[Section titled “Secrets”](#secrets)

By default, the `shell` operator does not accept secrets. If you want to allow usage of secrets in the `cmd` argument, you can enable the configuration option `tenzir.allow-secrets-in-escape-hatches`.

## Examples

[Section titled “Examples”](#examples)

### Show a live log from the `tenzir-node` service

[Section titled “Show a live log from the tenzir-node service”](#show-a-live-log-from-the-tenzir-node-service)

```tql
shell "journalctl -u tenzir-node -f"
read_json
```

## See Also

[Section titled “See Also”](#see-also)

[`python`](/reference/operators/python)

# sigma

Filter the input with Sigma rules and output matching events.

```tql
sigma path:string, [refresh_interval=duration]
```

## Description

[Section titled “Description”](#description)

The `sigma` operator executes [Sigma rules](https://github.com/SigmaHQ/sigma) on its input. If a rule matches, the operator emits a `tenzir.sigma` event that wraps the input record into a new record along with the matching rule. The operator discards all events that do not match the provided rules.

Transpilation Process

For each rule, the operator transpiles the YAML into an [expression](/explanations/language/expressions) and instantiates a [`where`](/reference/operators/where) operator, followed by assignments to generate an output. Here’s how the transpilation works. The Sigma rule YAML format requires a `detection` attribute that includes a map of named sub-expression called *search identifiers*. In addition, `detection` must include a final `condition` that combines search identifiers using boolean algebra (AND, OR, and NOT) or syntactic sugar to reference groups of search expressions, e.g., using the `1/all of *` or plain wildcard syntax. Consider the following `detection` embedded in a rule:

```yaml
detection:
  foo:
    a: 42
    b: "evil"
  bar:
    c: 1.2.3.4
  condition: foo or not bar
```

We translate this rule piece by building a symbol table of all keys (`foo` and `bar`). Each sub-expression is a valid expression in itself:

1. `foo`: `a == 42 && b == "evil"`
2. `bar`: `c == 1.2.3.4`

Finally, we combine the expression according to `condition`:

```tql
(a == 42 && b == "evil") || ! (c == 1.2.3.4)
```

We parse the YAML string values according to Tenzir’s richer data model, e.g., the expression `c: 1.2.3.4` becomes a field named `c` and value `1.2.3.4` of type `ip`, rather than a `string`. Sigma also comes with its own [event taxonomy](https://github.com/SigmaHQ/sigma-specification/blob/main/Taxonomy_specification) to standardize field names. The `sigma` operator currently does not normalize fields according to this taxonomy but rather takes the field names verbatim from the search identifier.

Sigma uses [value modifiers](https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md#value-modifiers) to select a concrete relational operator for given search predicate. Without a modifier, Sigma uses equality comparison (`==`) of field and value. For example, the `contains` modifier changes the relational operator to substring search, and the `re` modifier switches to a regular expression match. The table below shows what modifiers the `sigma` operator supports, where ✅ means implemented, 🚧 not yet implemented but possible, and ❌ not yet supported:

| Modifier         | Use                                                      | sigmac | Tenzir |
| ---------------- | -------------------------------------------------------- | :----: | :----: |
| `contains`       | perform a substring search with the value                |    ✅   |    ✅   |
| `startswith`     | match the value as a prefix                              |    ✅   |    ✅   |
| `endswith`       | match the value as a suffix                              |    ✅   |    ✅   |
| `base64`         | encode the value with Base64                             |    ✅   |    ✅   |
| `base64offset`   | encode value as all three possible Base64 variants       |    ✅   |    ✅   |
| `utf16le`/`wide` | transform the value to UTF16 little endian               |    ✅   |   🚧   |
| `utf16be`        | transform the value to UTF16 big endian                  |    ✅   |   🚧   |
| `utf16`          | transform the value to UTF16                             |    ✅   |   🚧   |
| `re`             | interpret the value as regular expression                |    ✅   |    ✅   |
| `cidr`           | interpret the value as a IP CIDR                         |    ❌   |    ✅   |
| `all`            | changes the expression logic from OR to AND              |    ✅   |    ✅   |
| `lt`             | compare less than (`<`) the value                        |    ❌   |    ✅   |
| `lte`            | compare less than or equal to (`<=`) the value           |    ❌   |    ✅   |
| `gt`             | compare greater than (`>`) the value                     |    ❌   |    ✅   |
| `gte`            | compare greater than or equal to (`>=`) the value        |    ❌   |    ✅   |
| `expand`         | expand value to placeholder strings, e.g., `%something%` |    ❌   |    ❌   |

### `path: string`

[Section titled “path: string”](#path-string)

The rule to match.

If `path` points to a rule, the operator transpiles the rule file at the time of pipeline creation.

If this points to a directory, the operator watches it and attempts to parse each contained file as a Sigma rule. The `sigma` operator matches if *any* of the contained rules match, effectively creating a disjunction of all rules inside the directory.

### `refresh_interval = duration (optional)`

[Section titled “refresh\_interval = duration (optional)”](#refresh_interval--duration-optional)

How often the `sigma` operator looks at the specified rule or directory of rules to update its internal state.

Defaults to `5s`.

## Examples

[Section titled “Examples”](#examples)

### Apply a Sigma rule to an EVTX file

[Section titled “Apply a Sigma rule to an EVTX file”](#apply-a-sigma-rule-to-an-evtx-file)

The tool [`evtx_dump`](https://github.com/omerbenamram/evtx) turns an EVTX file into a JSON object. On the command line, use the `tenzir` binary to pipe the `evtx_dump` output to a Tenzir pipeline using the `sigma` operator:

```bash
evtx_dump -o jsonl file.evtx | tenzir 'read_json | sigma "rule.yaml"'
```

### Run a Sigma rule on historical data

[Section titled “Run a Sigma rule on historical data”](#run-a-sigma-rule-on-historical-data)

Apply a Sigma rule over historical data in a node from the last day:

```tql
export
where ts > now() - 1d
sigma "rule.yaml"
```

### Stream a file and apply a set of Sigma rules to it

[Section titled “Stream a file and apply a set of Sigma rules to it”](#stream-a-file-and-apply-a-set-of-sigma-rules-to-it)

Watch a directory of Sigma rules and apply all of them on a continuous stream of Suricata events:

```tql
load_file "eve.json", follow=true
read_suricata
sigma "/tmp/rules/"
```

When you add a new file to `/tmp/rules`, the `sigma` operator transpiles it and will match it on all subsequent inputs.

# slice

Keeps a range of events within the interval `[begin, end)` stepping by `stride`.

```tql
slice [begin=int, end=int, stride=int]
```

## Description

[Section titled “Description”](#description)

The `slice` operator selects a range of events from the input. The semantics of the operator match Python’s array slicing.

Potentially High Memory Usage

Take care when using this operator with large inputs.

### `begin = int (optional)`

[Section titled “begin = int (optional)”](#begin--int-optional)

The beginning (inclusive) of the range to keep. Use a negative number to count from the end.

### `end = int (optional)`

[Section titled “end = int (optional)”](#end--int-optional)

The end (exclusive) of the range to keep. Use a negative number to count from the end.

### `stride = int (optional)`

[Section titled “stride = int (optional)”](#stride--int-optional)

The number of elements to advance before the next element. Use a negative number to count from the end, effectively reversing the stream.

## Examples

[Section titled “Examples”](#examples)

### Get the second 100 events

[Section titled “Get the second 100 events”](#get-the-second-100-events)

```tql
slice begin=100, end=200
```

### Get the last 5 events

[Section titled “Get the last 5 events”](#get-the-last-5-events)

```tql
slice begin=-5
```

### Skip the last 10 events

[Section titled “Skip the last 10 events”](#skip-the-last-10-events)

```tql
slice end=-10
```

### Return the last 50 events, except for the last 2

[Section titled “Return the last 50 events, except for the last 2”](#return-the-last-50-events-except-for-the-last-2)

```tql
slice begin=-50, end=-2
```

### Skip the first and the last event

[Section titled “Skip the first and the last event”](#skip-the-first-and-the-last-event)

```tql
slice begin=1, end=-1
```

### Return every second event starting from the tenth

[Section titled “Return every second event starting from the tenth”](#return-every-second-event-starting-from-the-tenth)

```tql
slice begin=9, stride=2
```

### Return all but the last five events in reverse order

[Section titled “Return all but the last five events in reverse order”](#return-all-but-the-last-five-events-in-reverse-order)

```tql
slice end=-5, stride=-1
```

## See Also

[Section titled “See Also”](#see-also)

[`head`](/reference/operators/head), [`tail`](/reference/operators/tail)

# sockets

Shows a snapshot of open sockets.

```tql
sockets
```

## Description

[Section titled “Description”](#description)

The `sockets` operator shows a snapshot of all currently open sockets.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir emits socket information with the following schema.

### `tenzir.socket`

[Section titled “tenzir.socket”](#tenzirsocket)

Contains detailed information about the socket.

| Field         | Type     | Description                                        |
| :------------ | :------- | :------------------------------------------------- |
| `pid`         | `uint64` | The process identifier.                            |
| `process`     | `string` | The name of the process involved.                  |
| `protocol`    | `uint64` | The protocol used for the communication.           |
| `local_addr`  | `ip`     | The local IP address involved in the connection.   |
| `local_port`  | `port`   | The local port number involved in the connection.  |
| `remote_addr` | `ip`     | The remote IP address involved in the connection.  |
| `remote_port` | `port`   | The remote port number involved in the connection. |
| `state`       | `string` | The current state of the connection.               |

## Examples

[Section titled “Examples”](#examples)

### Show process ID, local, and remote IP address of all sockets

[Section titled “Show process ID, local, and remote IP address of all sockets”](#show-process-id-local-and-remote-ip-address-of-all-sockets)

```tql
sockets
select pid, local_addr, remote_addr
```

## See Also

[Section titled “See Also”](#see-also)

[`files`](/reference/operators/files), [`processes`](/reference/operators/processes)

# sort

Sorts events by the given expressions.

```tql
sort [-]expr...
```

## Description

[Section titled “Description”](#description)

Sorts events by the given expressions, putting all `null` values at the end.

If multiple expressions are specified, the sorting happens lexicographically, that is: Later expressions are only considered if all previous expressions evaluate to equal values.

This operator performs a stable sort (preserves relative ordering when all expressions evaluate to the same value).

Potentially High Memory Usage

Use caution when applying this operator to large inputs. It currently buffers all data in memory. Out-of-core processing is on our roadmap.

### `[-]expr`

[Section titled “\[-\]expr”](#-expr)

An expression that is evaluated for each event. Normally, events are sorted in ascending order. If the expression starts with `-`, descending order is used instead. In both cases, `null` is put last.

## Examples

[Section titled “Examples”](#examples)

### Sort by a field in ascending order

[Section titled “Sort by a field in ascending order”](#sort-by-a-field-in-ascending-order)

```tql
sort timestamp
```

### Sort by a field in descending order

[Section titled “Sort by a field in descending order”](#sort-by-a-field-in-descending-order)

```tql
sort -timestamp
```

### Sort by multiple fields

[Section titled “Sort by multiple fields”](#sort-by-multiple-fields)

Sort by a field `src_ip` and, in case of matching values, sort by `dest_ip`:

```tql
sort src_ip, dest_ip
```

Sort by the field `src_ip` in ascending order and by the field `dest_ip` in descending order.

```tql
sort src_ip, -dest_ip
```

## See Also

[Section titled “See Also”](#see-also)

[`rare`](/reference/operators/rare), [`reverse`](/reference/operators/reverse), [`top`](/reference/operators/top)

# strict

Treats all warnings as errors.

```tql
strict { … }
```

## Description

[Section titled “Description”](#description)

The `strict` operator takes a pipeline as an argument and treats all warnings emitted by the execution of the pipeline as errors. This is useful when you want to stop a pipeline on warnings or unexpected diagnostics.

## Examples

[Section titled “Examples”](#examples)

### Stop the pipeline on any warnings when sending logs

[Section titled “Stop the pipeline on any warnings when sending logs”](#stop-the-pipeline-on-any-warnings-when-sending-logs)

```tql
subscribe "log-feed"
strict {
  to_google_cloud_logging …
}
```

## See Also

[Section titled “See Also”](#see-also)

[`assert`](/reference/operators/assert)

# subscribe

Subscribes to events from a channel with a topic.

```tql
subscribe [topic:string...]
```

## Description

[Section titled “Description”](#description)

The `subscribe` operator subscribes to events from a channel with the specified topic. Multiple `subscribe` operators with the same topic receive the same events.

Subscribers propagate back pressure to publishers. If a subscribing pipeline fails to keep up, all publishers will slow down as well to a matching speed to avoid data loss. This mechanism is disabled for pipelines that are not visible on the overview page on [app.tenzir.com](https://app.tenzir.com), which drop data rather than slow down their publishers.

### `topic: string... (optional)`

[Section titled “topic: string... (optional)”](#topic-string-optional)

Optional channel names to subscribe to. If unspecified, the operator subscribes to the topic `main`.

## Examples

[Section titled “Examples”](#examples)

### Subscribe to the events under a topic

[Section titled “Subscribe to the events under a topic”](#subscribe-to-the-events-under-a-topic)

```tql
subscribe "zeek-conn"
```

### Subscribe to the multiple topics

[Section titled “Subscribe to the multiple topics”](#subscribe-to-the-multiple-topics)

```tql
subscribe "alerts", "notices", "critical"
```

## See Also

[Section titled “See Also”](#see-also)

[`export`](/reference/operators/export), [`publish`](/reference/operators/publish)

# summarize

Groups events and applies aggregate functions to each group.

```tql
summarize (group|aggregation)...
```

## Description

[Section titled “Description”](#description)

The `summarize` operator groups events according to certain fields and applies [aggregation functions](/reference/functions#aggregation) to each group. The operator consumes the entire input before producing any output, and may reorder the event stream.

The order of the output fields follows the sequence of the provided arguments. Unspecified fields are dropped.

Potentially High Memory Usage

Use caution when applying this operator to large inputs. It currently buffers all data in memory. Out-of-core processing is on our roadmap.

### `group`

[Section titled “group”](#group)

To group by a certain field, use the syntax `<field>` or `<field>=<field>`. For each unique combination of the `group` fields, a single output event will be returned.

### `aggregation`

[Section titled “aggregation”](#aggregation)

The [aggregation functions](/reference/functions#aggregation) applied to each group are specified with `f(…)` or `<field>=f(…)`, where `f` is the name of an aggregation function (see below) and `<field>` is an optional name for the result. The aggregation function will produce a single result for each group.

If no name is specified, the aggregation function call will automatically generate one. If processing continues after `summarize`, we strongly recommend to specify a custom name.

## Examples

[Section titled “Examples”](#examples)

### Compute the sum of a field over all events

[Section titled “Compute the sum of a field over all events”](#compute-the-sum-of-a-field-over-all-events)

```tql
from {x: 1}, {x: 2}
summarize x=sum(x)
```

```tql
{x: 3}
```

Group over `y` and compute the sum of `x` for each group:

```tql
from {x: 0, y: 0, z: 1},
     {x: 1, y: 1, z: 2},
     {x: 1, y: 1, z: 3}
summarize y, x=sum(x)
```

```tql
{y: 0, x: 0}
{y: 1, x: 2}
```

### Gather unique values in a list

[Section titled “Gather unique values in a list”](#gather-unique-values-in-a-list)

Group the input by `src_ip` and aggregate all unique `dest_port` values into a list:

```tql
summarize src_ip, distinct(dest_port)
```

Same as above, but produce a count of the unique number of values instead of a list:

```tql
summarize src_ip, count_distinct(dest_port)
```

### Compute min and max of a group

[Section titled “Compute min and max of a group”](#compute-min-and-max-of-a-group)

Compute minimum and maximum of the `timestamp` field per `src_ip` group:

```tql
summarize min(timestamp), max(timestamp), src_ip
```

Compute minimum and maximum of the `timestamp` field over all events:

```tql
summarize min(timestamp), max(timestamp)
```

### Check if any value of a group is true

[Section titled “Check if any value of a group is true”](#check-if-any-value-of-a-group-is-true)

Create a boolean flag `originator` that is `true` if any value in the `src_ip` group is `true`:

```tql
summarize src_ip, originator=any(is_orig)
```

### Create 1-hour time buckets

[Section titled “Create 1-hour time buckets”](#create-1-hour-time-buckets)

Create 1-hour groups and produce a summary of network traffic between host pairs:

```tql
ts = round(ts, 1h)
summarize ts, src_ip, dest_ip, sum(bytes_in), sum(bytes_out)
```

## See Also

[Section titled “See Also”](#see-also)

[`rare`](/reference/operators/rare), [`top`](/reference/operators/top)

# tail

Limits the input to the last `n` events.

```tql
tail [n:int]
```

## Description

[Section titled “Description”](#description)

Forwards the last `n` events and discards the rest.

`tail n` is a shorthand notation for [`slice begin=-n`](/reference/operators/slice).

### `n: int (optional)`

[Section titled “n: int (optional)”](#n-int-optional)

The number of events to keep.

Defaults to `10`.

## Examples

[Section titled “Examples”](#examples)

### Get the last 10 results

[Section titled “Get the last 10 results”](#get-the-last-10-results)

```tql
export
tail
```

### Get the last 5 results

[Section titled “Get the last 5 results”](#get-the-last-5-results)

```tql
export
tail 5
```

## See Also

[Section titled “See Also”](#see-also)

[`head`](/reference/operators/head), [`slice`](/reference/operators/slice)

# taste

Limits the input to `n` events per unique schema.

```tql
taste [n:int]
```

## Description

[Section titled “Description”](#description)

Forwards the first `n` events per unique schema and discards the rest.

The `taste` operator provides an exemplary overview of the “shape” of the data described by the pipeline. This helps to understand the diversity of the result, especially when interactively exploring data.

### `n: int (optional)`

[Section titled “n: int (optional)”](#n-int-optional)

The number of events to keep per schema.

Defaults to `10`.

## Examples

[Section titled “Examples”](#examples)

### Retrieve at most 10 events of each unique schema

[Section titled “Retrieve at most 10 events of each unique schema”](#retrieve-at-most-10-events-of-each-unique-schema)

```tql
export
taste
```

### Get only one sample for every unique event type

[Section titled “Get only one sample for every unique event type”](#get-only-one-sample-for-every-unique-event-type)

```tql
export
taste 1
```

# throttle

Limits the bandwidth of a pipeline.

```tql
throttle bandwidth:int, [within=duration]
```

## Description

[Section titled “Description”](#description)

The `throttle` operator limits the amount of data flowing through it to a bandwidth.

### `bandwidth: int`

[Section titled “bandwidth: int”](#bandwidth-int)

The maximum bandwidth that is enforced for this pipeline, in bytes per the specified interval.

### `within = duration (optional)`

[Section titled “within = duration (optional)”](#within--duration-optional)

The duration over which to measure the maximum bandwidth.

Defaults to `1s`.

## Examples

[Section titled “Examples”](#examples)

### Read a byte stream at 1 byte per second

[Section titled “Read a byte stream at 1 byte per second”](#read-a-byte-stream-at-1-byte-per-second)

Read a TCP stream at a rate of 1 character per second:

```tql
load_tcp "tcp://0.0.0.0:4000"
throttle 1
```

### Set a throughput limit for a given time window

[Section titled “Set a throughput limit for a given time window”](#set-a-throughput-limit-for-a-given-time-window)

Load a sample input data file at a speed of at most 1MiB every 10s and import it into the node:

```tql
load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst"
throttle 1Mi, within=10s
decompress "zstd"
read_zeek_tsv
import
```

# timeshift

Adjusts timestamps relative to a given start time, with an optional speedup.

```tql
timeshift field:time, [start=time, speed=double]
```

## Description

[Section titled “Description”](#description)

The `timeshift` operator adjusts a series of time values by anchoring them around a given `start` time.

With `speed`, you can adjust the relative speed of the time series induced by `field` with a multiplicative factor. This has the effect of making the time series “faster” for values great than 1 and “slower” for values less than 1.

![Timeshift](/_astro/timeshift.excalidraw.BVaAeL1C_19DKCs.svg)

### `field: time`

[Section titled “field: time”](#field-time)

The field containing the timestamp values.

### `start = time (optional)`

[Section titled “start = time (optional)”](#start--time-optional)

The timestamp to anchor the time values around.

Defaults to the first non-null timestamp in `field`.

### `speed = double (optional)`

[Section titled “speed = double (optional)”](#speed--double-optional)

A constant factor to be divided by the inter-arrival time. For example, 2.0 decreases the event gaps by a factor of two, resulting a twice as fast dataflow. A value of 0.1 creates dataflow that spans ten times the original time frame.

Defaults to `1.0`.

## Examples

[Section titled “Examples”](#examples)

### Reset events to begin at Jan 1, 1984

[Section titled “Reset events to begin at Jan 1, 1984”](#reset-events-to-begin-at-jan-1-1984)

```tql
load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst"
decompress "zstd"
read_zeek_tsv
timeshift ts, start=1984-01-01
```

### Scale inter-arrival times by 100x

[Section titled “Scale inter-arrival times by 100x”](#scale-inter-arrival-times-by-100x)

As above, but also make the time span of the trace 100 times longer:

```tql
load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst"
decompress "zstd"
read_zeek_tsv
timeshift ts, start=1984-01-01, speed=0.01
```

## See Also

[Section titled “See Also”](#see-also)

[`delay`](/reference/operators/delay)

# to

Saves to an URI, inferring the destination, compression and format.

```tql
to uri:string, [saver_args… { … }]
```

## Description

[Section titled “Description”](#description)

The `to` operator is an easy way to get data out of Tenzir into It will try to infer the connector, compression and format based on the given URI.

Use `to` if you can

The `to` operator is designed as an easy way to get data out of Tenzir, without having to manually write the separate steps of data formatting, compression and writing.

### `uri: string`

[Section titled “uri: string”](#uri-string)

The URI to load from.

### `saver_args… (optional)`

[Section titled “saver\_args… (optional)”](#saver_args-optional)

An optional set of arguments passed to the saver. This can be used to e.g. pass credentials to a connector:

```tql
to "https://example.org/file.json", headers={Token: "XYZ"}
```

### `{ … } (optional)`

[Section titled “{ … } (optional)”](#---optional)

The optional pipeline argument allows for explicitly specifying how `to` compresses and writes data. By default, the pipeline is inferred based on a set of [rules](#explanation).

If inference is not possible, or not sufficient, this argument can be used to control compression and writing. Providing this pipeline disables the inference.

## Explanation

[Section titled “Explanation”](#explanation)

Saving Tenzir data into some resource consists of three steps:

* [**Writing**](#writing) events as bytes according to some format
* [**compressing**](#compressing) (optional)
* [**Saving**](#saving) saving the bytes to some location

The `to` operator tries to infer all three steps from the given URI.

### Writing

[Section titled “Writing”](#writing)

The format to write inferred from the file-ending. Supported file formats are the common file endings for our [`read_*` operators](/reference/operators#parsing).

If you want to provide additional arguments to the writer, you can use the [pipeline argument](#---optional) to specify the parsing manually.

### Compressing

[Section titled “Compressing”](#compressing)

The compression, just as the format, is inferred from the “file-ending” in the URI. Under the hood, this uses the [`decompress_*` operators](/reference/operators#encode--decode). Supported compressions can be found in the [list of compression extensions](#compression).

The compression step is optional and will only happen if a compression could be inferred. If you want to write with specific compression settings, you can use the [pipeline argument](#---optional) to specify the decompression manually.

### Saving

[Section titled “Saving”](#saving)

The connector is inferred based on the URI `scheme://`. If no scheme is present, the connector attempts to save to the local filesystem.

## Supported Deductions

[Section titled “Supported Deductions”](#supported-deductions)

### URI schemes

[Section titled “URI schemes”](#uri-schemes)

| Scheme                             | Operator                                                                    | Example                                        |
| :--------------------------------- | :-------------------------------------------------------------------------- | :--------------------------------------------- |
| `abfs`,`abfss`                     | [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage)   | `to "abfs://path/to/file.json"`                |
| `amqp`                             | [`save_amqp`](/reference/operators/save_amqp)                               | `to "amqp://…`                                 |
| `elasticsearch`                    | [`to_opensearch`](/reference/operators/to_opensearch)                       | `to "elasticsearch://…`                        |
| `file`                             | [`save_file`](/reference/operators/save_file)                               | `to "file://path/to/file.json"`                |
| `fluent-bit`                       | [`to_fluent_bit`](/reference/operators/to_fluent_bit)                       | `to "fluent-bit://elasticsearch"`              |
| `ftp`, `ftps`                      | [`save_ftp`](/reference/operators/save_ftp)                                 | `to "ftp://example.com/file.json"`             |
| `gcps`                             | [`save_google_cloud_pubsub`](/reference/operators/save_google_cloud_pubsub) | `to "gcps://project_id/topic_id" { … }`        |
| `gs`                               | [`save_gcs`](/reference/operators/save_gcs)                                 | `to "gs://bucket/object.json"`                 |
| `http`, `https`                    | [`save_http`](/reference/operators/save_http)                               | `to "http://example.com/file.json"`            |
| `inproc`                           | [`save_zmq`](/reference/operators/save_zmq)                                 | `to "inproc://127.0.0.1:56789" { write_json }` |
| `kafka`                            | [`save_kafka`](/reference/operators/save_kafka)                             | `to "kafka://topic" { write_json }`            |
| `opensearch`                       | [`to_opensearch`](/reference/operators/to_opensearch)                       | `to "opensearch://…`                           |
| `s3`                               | [`save_s3`](/reference/operators/save_s3)                                   | `to "s3://bucket/file.json"`                   |
| `sqs`                              | [`save_sqs`](/reference/operators/save_sqs)                                 | `to "sqs://my-queue" { write_json }`           |
| `tcp`                              | [`save_tcp`](/reference/operators/save_tcp)                                 | `to "tcp://127.0.0.1:56789" { write_json }`    |
| `udp`                              | [`save_udp`](/reference/operators/save_udp)                                 | `to "udp://127.0.0.1:56789" { write_json }`    |
| `zmq`                              | [`save_zmq`](/reference/operators/save_zmq)                                 | `to "zmq://127.0.0.1:56789" { write_json }`    |
| `smtp`, `smtps`, `mailto`, `email` | [`save_email`](/reference/operators/save_email)                             | `to "smtp://john@example.com"`                 |

Please see the respective operator pages for details on the URI’s locator format.

### File extensions

[Section titled “File extensions”](#file-extensions)

#### Format

[Section titled “Format”](#format)

The `to` operator can deduce the file format based on these file-endings:

| Format  | File Endings         | Operator                                              |
| :------ | :------------------- | :---------------------------------------------------- |
| CSV     | `.csv`               | [`write_csv`](/reference/operators/write_csv)         |
| Feather | `.feather`, `.arrow` | [`write_feather`](/reference/operators/write_feather) |
| JSON    | `.json`              | [`write_json`](/reference/operators/write_json)       |
| NDJSON  | `.ndjson`, `.jsonl`  | [`write_ndjson`](/reference/operators/write_ndjson)   |
| Parquet | `.parquet`           | [`write_parquet`](/reference/operators/write_parquet) |
| Pcap    | `.pcap`              | [`write_pcap`](/reference/operators/write_pcap)       |
| SSV     | `.ssv`               | [`write_ssv`](/reference/operators/write_ssv)         |
| TSV     | `.tsv`               | [`write_tsv`](/reference/operators/write_tsv)         |
| YAML    | `.yaml`              | [`write_yaml`](/reference/operators/write_yaml)       |

#### Compression

[Section titled “Compression”](#compression)

The `to` operator can deduce the following compressions based on these file-endings:

| Compression | File Endings     |
| :---------- | :--------------- |
| Brotli      | `.br`, `.brotli` |
| Bzip2       | `.bz2`           |
| Gzip        | `.gz`, `.gzip`   |
| LZ4         | `.lz4`           |
| Zstd        | `.zst`, `.zstd`  |

#### Example transformation:

[Section titled “Example transformation:”](#example-transformation)

to operator

```tql
to "myfile.json.gz"
```

Effective pipeline

```tql
write_json
compress_gzip
save_file "myfile.json.gz"
```

## Examples

[Section titled “Examples”](#examples)

### Save to a local file

[Section titled “Save to a local file”](#save-to-a-local-file)

```tql
to "path/to/my/output.csv"
```

### Save to a compressed file

[Section titled “Save to a compressed file”](#save-to-a-compressed-file)

```tql
to "path/to/my/output.csv.bz2"
```

## See Also

[Section titled “See Also”](#see-also)

[from](/reference/operators/from)

# to_amazon_security_lake

Sends OCSF events to Amazon Security Lake.

```tql
to_amazon_security_lake s3_uri:string, region=string, account_id=string,
                       [timeout=duration, role=string, external_id=string]
```

## Description

[Section titled “Description”](#description)

The `to_amazon_security_lake` operator sends OCSF events to [Amazon Security Lake](https://aws.amazon.com/security-lake/), AWS’s centralized security data repository that normalizes and stores security data from multiple sources.

The operator automatically handles Amazon Security Lake’s partitioning requirements and file size constraints, but does not validate the OCSF schema of the events. Consider [`ocsf::apply`](/reference/operators/ocsf/apply) in your pipeline to ensure schema compliance.

For a list of OCSF event classes supported by Amazon Security Lake, see the [AWS documentation](https://docs.aws.amazon.com/security-lake/latest/userguide/adding-custom-sources.html#ocsf-eventclass). The operator generates random UUID (v7) file names with a `.parquet` extension.

### `s3_uri: string`

[Section titled “s3\_uri: string”](#s3_uri-string)

The base URI for the S3 storage backing the lake in the form

```plaintext
s3://<bucket>/ext/<custom-source-name>
```

Replace the placeholders as follows:

* `<bucket>`: the bucket associated with your lake
* `<custom-source-name>`: the name of your custom Amazon Security Lake source

You can copy this URI directly from the AWS Security Lake custom source interface.

### `region = string`

[Section titled “region = string”](#region--string)

The region for partitioning.

### `account_id = string`

[Section titled “account\_id = string”](#account_id--string)

The AWS account ID or external ID you chose when creating the Amazon Security Lake custom source.

Note

The user running the Tenzir Node must have permissions to write to the given partition:

```plaintext
{s3_uri}/region={region}/accountId={account_id}/
```

### `timeout = duration (optional)`

[Section titled “timeout = duration (optional)”](#timeout--duration-optional)

A duration after which the operator will write to Amazon Security Lake, regardless of file size. Amazon Security Lake requires this to be between `5min` and `1d`.

Defaults to `5min`.

### `role = string (optional)`

[Section titled “role = string (optional)”](#role--string-optional)

A role to assume when writing to S3.

When not specified, the operator automatically uses the standard Amazon Security Lake provider role based on your configuration: `arn:aws:iam::<account_id>:role/AmazonSecurityLake-Provider-<custom-source-name>-<region>`

The operator extracts the custom source name from the provided S3 URI.

For example, given:

* `account_id`: `"123456789012"`
* `s3_uri`: `"s3://aws-security-data-lake-…/ext/tnz-ocsf-4001/"`
* `region`: `"eu-west-1"`

The operator will use: `arn:aws:iam::123456789012:role/AmazonSecurityLake-Provider-tnz-ocsf-4001-eu-west-1`

When defaulted, the operator requires an `external_id` to use the role.

You can explicitly disable role authorization by setting `role=null`.

### `external_id = string (optional)`

[Section titled “external\_id = string (optional)”](#external_id--string-optional)

The external ID to use when assuming the `role`.

This is required when using the default role for the custom source.

Defaults to no ID.

## Examples

[Section titled “Examples”](#examples)

### Send OCSF Network Activity events to Amazon Security Lake

[Section titled “Send OCSF Network Activity events to Amazon Security Lake”](#send-ocsf-network-activity-events-to-amazon-security-lake)

This example shows how to send OCSF Network Activity events to an AWS Security Lake running on `eu-west-2` with a custom source called `tenzir_network_activity` and account ID `123456789012`:

```tql
let $s3_uri = "s3://aws-security-data-lake-eu-west-2-lake-abcdefghijklmnopqrstuvwxyz1234/ext/tnz-ocsf-4001/"


subscribe "ocsf"
where @name == "ocsf.network_activity"
ocsf::apply
to_amazon_security_lake $s3_uri,
  region="eu-west-2",
  account_id="123456789012"
```

## See Also

[Section titled “See Also”](#see-also)

[`ocsf::apply`](/reference/operators/ocsf/apply), [`save_s3`](/reference/operators/save_s3)

# to_azure_log_analytics

Sends events to the Microsoft Azure Logs Ingestion API.

```tql
to_azure_log_analytics tenant_id=string, client_id=string, client_secret=string,
      dce=string, dcr=string, stream=string, [batch_timeout=duration]
```

## Description

[Section titled “Description”](#description)

Sends events to the Microsoft [Azure Logs Ingestion API](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview).

The `to_azure_log_analytics` operator makes it possible to upload events to [supported tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview#supported-tables) or to [custom tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/create-custom-table?tabs=azure-portal-1%2Cazure-portal-2%2Cazure-portal-3#create-a-custom-table) in Microsoft Azure.

The operator handles access token retrievals by itself and updates that token automatically, if needed.

### `tenant_id = string`

[Section titled “tenant\_id = string”](#tenant_id--string)

The Microsoft Directory (tenant) ID, written as `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`.

### `client_id = string`

[Section titled “client\_id = string”](#client_id--string)

The Microsoft Application (client) ID, written as `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`.

### `client_secret = string`

[Section titled “client\_secret = string”](#client_secret--string)

The client secret.

### `dce = string`

[Section titled “dce = string”](#dce--string)

The data collection endpoint URL.

### `dcr = string`

[Section titled “dcr = string”](#dcr--string)

The data collection rule ID, written as `dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`.

### `stream = string`

[Section titled “stream = string”](#stream--string)

The stream to upload events to.

### `batch_timeout = duration`

[Section titled “batch\_timeout = duration”](#batch_timeout--duration)

Maximum duration to wait for new events before sending a batch.

Defaults to `5s`.

## Examples

[Section titled “Examples”](#examples)

### Upload `custom.mydata` events to the stream `Custom-MyData`

[Section titled “Upload custom.mydata events to the stream Custom-MyData”](#upload-custommydata-events-to-the-stream-custom-mydata)

```tql
export
where @name == "custom.mydata"
to_azure_log_analytics tenant_id="00a00a00-0a00-0a00-00aa-000aa0a0a000",
  client_id="000a00a0-0aa0-00a0-0000-00a000a000a0",
  client_secret="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  dce="https://my-stuff-a0a0.westeurope-1.ingest.monitor.azure.com",
  dcr="dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  stream="Custom-MyData"
```

# to_clickhouse

Sends events to a ClickHouse table.

```tql
to_clickhouse table=string, [host=string, port=int, user=string, password=string,
                             mode=string, primary=field,
                             tls=bool, cacert=string, certfile=string, keyfile=string,
                             skip_peer_verification=bool, skip_host_verification=bool]
```

## Description

[Section titled “Description”](#description)

### `table = string`

[Section titled “table = string”](#table--string)

The name of the table you want to write to. When giving a plain table name, it will use the `default` database, otherwise `database.table` can be specified.

### `host = string (optional)`

[Section titled “host = string (optional)”](#host--string-optional)

The hostname for the ClickHouse server.

Defaults to `"localhost"`.

### `port = int (optional)`

[Section titled “port = int (optional)”](#port--int-optional)

The port for the ClickHouse server.

Defaults to `9000` without TLS and `9440` with TLS.

### `user = string (optional)`

[Section titled “user = string (optional)”](#user--string-optional)

The user to use for authentication.

Defaults to `"default"`.

### `password = string (optional)`

[Section titled “password = string (optional)”](#password--string-optional)

The password for the given user.

Defaults to `""`.

### `mode = string (optional)`

[Section titled “mode = string (optional)”](#mode--string-optional)

* `"create"` if you want to create a table and fail if it already exists
* `"append"` to append to an existing table
* `"create_append"` to create a table if it does not exist and append to it otherwise.

Defaults to `"create_append"`.

### `primary = field (optional)`

[Section titled “primary = field (optional)”](#primary--field-optional)

The primary key to use when creating a table. Required for `mode = "create"` as well as for `mode = "create_append"` if the table does not yet exist.

### `tls = bool (optional)`

Enables TLS.

Defaults to `true`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

Path to the key for the client certificate.

## Types

[Section titled “Types”](#types)

Tenzir uses ClickHouse’s [clickhouse-cpp](https://github.com/ClickHouse/clickhouse-cpp) client library to communicate with ClickHouse. The below table explains the translation from Tenzir’s types to ClickHouse:

| Tenzir     | ClickHouse                     | Comment                                                                                           |
| :--------- | :----------------------------- | :------------------------------------------------------------------------------------------------ |
| `bool`     | `UInt8`                        |                                                                                                   |
| `int64`    | `Int64`                        |                                                                                                   |
| `uint64`   | `UInt64`                       |                                                                                                   |
| `double`   | `Float64`                      |                                                                                                   |
| `ip`       | `IPv6`                         |                                                                                                   |
| `subnet`   | `Tuple(ip IPv6, length UInt8)` |                                                                                                   |
| `time`     | `DateTime64(9)`                |                                                                                                   |
| `duration` | `Int64`                        | Converted as `nanoseconds(duration)`                                                              |
| `record`   | `Tuple(...)`                   | Fields in the tuple will be named with the field name. The record must have at least one element. |
| `list<T>`  | `Array(T)`                     |                                                                                                   |
| `blob`     | `Array(UInt8)`                 | Blobs that are `null` will be represented by an empty array                                       |

Tenzir also supports `Nullable` versions of the above types (or their nested types). If a `list` itself is `null`, it will be represented by an empty `Array`. If a `record` is `null`, all elements of the `Tuple` will be null, if possible. Otherwise the event will be dropped.

### Table Creation

[Section titled “Table Creation”](#table-creation)

When a ClickHouse table is created from Tenzir, all columns except the `primary` will be created as `Nullable`. For example, a column of type `ip` will be created as `Nullable(IPv6)`, while a `list<int64>` will be created as `Array(Nullable(Int64))`.

The table will be created from the first event the operator receives. Should this first event contain unsupported types/values, an error is raised.

#### Untyped nulls

[Section titled “Untyped nulls”](#untyped-nulls)

Tenzir has both typed and untyped nulls. Typed nulls have a type, but no value. They are no problem for `to_clickhouse`.

For untyped nulls, the type itself is `null`, which cannot be supported by the `to_clickhouse` operator when creating a table.

Typed and Untyped Nulls in Tenzir

```tql
from {
  typed_null: int(null),
  untyped_null: null,
}
```

Untyped nulls are usually directly caused by nulls in the input, such as in a JSON file:

```json
{
  "value": null
}
```

If your input format has untyped nulls, but you know the type, you can either define an a schema and use that when parsing the input, or you can explicitly cast the columns to their desired type:

```tql
from (
  { value: null },
  { value: 42 },
)
value = int(value) // explicit cast turns untyped into typed nulls
to_clickhouse "example_table", primary=value
```

#### Empty records

[Section titled “Empty records”](#empty-records)

Empty records cannot be send to ClickHouse. Should an empty record appear in the first event, an error is raised.

## Examples

[Section titled “Examples”](#examples)

### Send CSV file to a local ClickHouse instance, without TLS

[Section titled “Send CSV file to a local ClickHouse instance, without TLS”](#send-csv-file-to-a-local-clickhouse-instance-without-tls)

```tql
from "my_file.csv"
to_clickhouse table="my_table", tls=false
```

### Create a new table with multiple fields

[Section titled “Create a new table with multiple fields”](#create-a-new-table-with-multiple-fields)

```tql
from { i: 42, d: 10.0, b: true, l: [42], r:{ s:"string" } }
to_clickhouse table="example", primary=i
```

This creates the following table:

```plaintext
   ┌─name─┬─type────────────────────┐
1. │ i    │ Int64                   │
2. │ d    │ Nullable(Float64)       │
3. │ b    │ Nullable(UInt8)         │
4. │ l    │ Array(Nullable(Int64))  │
5. │ r    │ Tuple(                 ↴│
   │      │↳    s Nullable(String)) │
   └──────┴─────────────────────────┘
```

# to_fluent_bit

Sends events via Fluent Bit.

```tql
to_fluent_bit plugin:string, [options=record, fluent_bit_options=record,
              tls=bool, cacert=string, certfile=string, keyfile=string,
              skip_peer_verification=bool]
```

## Description

[Section titled “Description”](#description)

The `to_fluent_bit` operator acts as a bridge into the [Fluent Bit](https://docs.fluentbit.io) ecosystem, making it possible to send events to Fluent Bit [output plugin](https://docs.fluentbit.io/manual/pipeline/outputs).

An invocation of the `fluent-bit` commandline utility

```bash
fluent-bit -o plugin -p key1=value1 -p key2=value2 -p…
```

translates to our `to_fluent_bit` operator as follows:

```tql
to_fluent_bit "plugin", options={key1: value1, key2:value2, …}
```

Read from Fluent Bit

You can acquire events from Fluent Bit using the [`from_fluent_bit` operator](/reference/operators/from_fluent_bit).

### `plugin: string`

[Section titled “plugin: string”](#plugin-string)

The name of the Fluent Bit plugin.

Run `fluent-bit -h` and look under the **Outputs** section of the help text for available plugin names. The web documentation often comes with an example invocation near the bottom of the page, which also provides a good idea how you could use the operator.

### `options = record (optional)`

[Section titled “options = record (optional)”](#options--record-optional)

Sets plugin configuration properties.

The key-value pairs in this record are equivalent to `-p key=value` for the `fluent-bit` executable.

### `fluent_bit_options = record (optional)`

[Section titled “fluent\_bit\_options = record (optional)”](#fluent_bit_options--record-optional)

Sets global properties of the Fluent Bit service. E.g., `fluent_bit_options={flush:1, grace:3}`.

Consult the list of available [key-value pairs](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file#config_section) to configure Fluent Bit according to your needs.

We recommend factoring these options into the plugin-specific `fluent-bit.yaml` so that they are independent of the `fluent-bit` operator arguments.

### `tls = bool (optional)`

Enables TLS.

Defaults to `false`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

## URI support & integration with `from`

[Section titled “URI support & integration with from”](#uri-support--integration-with-from)

The `to_fluent_bit` operator can also be used from the [`to`](/reference/operators/to) operator. For this, the `fluentbit://` scheme can be used. The URI is then translated:

```tql
to "fluentbit://plugin"
```

```tql
to_fluent_bit "plugin"
```

## Examples

[Section titled “Examples”](#examples)

### Slack

[Section titled “Slack”](#slack)

Send events to [Slack](https://docs.fluentbit.io/manual/pipeline/outputs/slack):

```tql
let $slack_hook = "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
to_fluent_bit "slack", options={webhook: $slack_hook}
```

# to_google_cloud_logging

Sends events to Google Cloud Logging.

```tql
to_google_cloud_logging log_id=string, [project=string, organization=string,
          billing_account=string, folder=string,] [resource_type=string,
          resource_labels=record, payload=string, severity=string,
          timestamp=time, service_credentials=string, batch_timeout=duration,
          max_batch_size=int]
```

## Description

[Section titled “Description”](#description)

Sends events to [Google Cloud Logging](https://cloud.google.com/logging).

### `log_id = string`

[Section titled “log\_id = string”](#log_id--string)

ID to associated the ingested logs with. It must be less than 512 characters long and can only include the following characters: upper and lower case alphanumeric characters, forward-slash, underscore, hyphen, and period.

### `project = string (optional)`

[Section titled “project = string (optional)”](#project--string-optional)

A project id to associated the ingested logs with.

### `organization = string (optional)`

[Section titled “organization = string (optional)”](#organization--string-optional)

An organization id to associated the ingested logs with.

### `billing_account = string (optional)`

[Section titled “billing\_account = string (optional)”](#billing_account--string-optional)

A billing account id to associated the ingested logs with.

### `folder = string (optional)`

[Section titled “folder = string (optional)”](#folder--string-optional)

A folder id to associated the ingested logs with.

Note

At most one of `project`, `organization`, `billing_account`, and `folder` can be set. If none is set, the operator tries to fetch the project id from the metadata server.

### `resource_type = string (optional)`

[Section titled “resource\_type = string (optional)”](#resource_type--string-optional)

The type of the [monitored resource](https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource). All available types with their associated labels are listed [here](https://cloud.google.com/logging/docs/api/v2/resource-list).

Defaults to `global`.

### `resource_labels = record (optional)`

[Section titled “resource\_labels = record (optional)”](#resource_labels--record-optional)

Record of associated labels for the resource. Values of the record must be of type `string`.

Consult the [official docs](https://cloud.google.com/logging/docs/api/v2/resource-list) for available types with their associated labels.

### `payload = string (optional)`

[Section titled “payload = string (optional)”](#payload--string-optional)

The log entry payload. If unspecified, the incoming event is serialised as JSON and sent.

### `service_credentials = string (optional)`

[Section titled “service\_credentials = string (optional)”](#service_credentials--string-optional)

JSON credentials to use if using a service account.

### `severity = string (optional)`

[Section titled “severity = string (optional)”](#severity--string-optional)

Severity of the event. Consult the [official docs](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogSeverity) for available severity levels.

Defaults to `default`.

### `timestamp = time (optional)`

[Section titled “timestamp = time (optional)”](#timestamp--time-optional)

Timestamp of the event.

### `batch_timeout = duration (optional)`

[Section titled “batch\_timeout = duration (optional)”](#batch_timeout--duration-optional)

Maximum interval between sending the events.

Defaults to `5s`.

### `max_batch_size = int (optional)`

[Section titled “max\_batch\_size = int (optional)”](#max_batch_size--int-optional)

Maximum events to batch before sending.

Defaults to `1k`.

## Example

[Section titled “Example”](#example)

## Send logs, authenticating automatically via ADC

[Section titled “Send logs, authenticating automatically via ADC”](#send-logs-authenticating-automatically-via-adc)

```tql
from {
  content: "log message",
  timestamp: now(),
}
to_google_cloud_logging log_id="LOG_ID", project="PROJECT_ID"
```

## Send logs using a service account

[Section titled “Send logs using a service account”](#send-logs-using-a-service-account)

```tql
from {
  content: "totally not a made up log",
  timestamp: now(),
  resource: "global",
}
to_google_cloud_logging log_id="LOG_ID", project="PROJECT_ID"
  resource_type=resource,
  service_credentials=file_contents("/path/to/credentials.json")
```

## See Also

[Section titled “See Also”](#see-also)

[`to_google_secops`](/reference/operators/to_google_secops)

# to_google_secops

Sends unstructured events to a Google SecOps Chronicle instance.

```tql
to_google_secops customer_id=string, private_key=string, client_email=string,
                 log_type=string, log_text=string, [region=string,
                 timestamp=time, labels=record, namespace=string,
                 max_request_size=int, batch_timeout=duration]
```

## Description

[Section titled “Description”](#description)

The `to_google_secops` operator makes it possible to ingest events via the [Google SecOps Chronicle unstructured logs ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api#unstructuredlogentries).

### `customer_id = string`

[Section titled “customer\_id = string”](#customer_id--string)

The customer UUID to use.

### `private_key = string`

[Section titled “private\_key = string”](#private_key--string)

The private key to use for authentication. This corresponds to the `private_key` in the SecOps collector config.

### `client_email = string`

[Section titled “client\_email = string”](#client_email--string)

The user email to use for authentication. This corresponds to the `client_email` in the SecOps collector config.

### `log_type = string`

[Section titled “log\_type = string”](#log_type--string)

The log type of the events.

### `log_text = string`

[Section titled “log\_text = string”](#log_text--string)

The log text to send.

### `region = string (optional)`

[Section titled “region = string (optional)”](#region--string-optional)

[Regional prefix](https://cloud.google.com/chronicle/docs/reference/ingestion-api#regional_endpoints) for the Ingestion endpoint (`malachiteingestion-pa.googleapis.com`).

### `timestamp = time (optional)`

[Section titled “timestamp = time (optional)”](#timestamp--time-optional)

Optional timestamp field to attach to logs.

### `labels = record (optional)`

[Section titled “labels = record (optional)”](#labels--record-optional)

A record of labels to attach to the logs. For example, `{node: "Configured Tenzir Node"}`.

### `namespace = string (optional)`

[Section titled “namespace = string (optional)”](#namespace--string-optional)

The namespace to use when ingesting.

Defaults to `tenzir`.

### `max_request_size = int (optional)`

[Section titled “max\_request\_size = int (optional)”](#max_request_size--int-optional)

The maximum number of bytes in the request payload.

Defaults to `1M`.

### `batch_timeout = duration (optional)`

[Section titled “batch\_timeout = duration (optional)”](#batch_timeout--duration-optional)

The maximum duration to wait for new events before sending the request.

Defaults to `5s`.

## Examples

[Section titled “Examples”](#examples)

```tql
from {log: "31-Mar-2025 01:35:02.187 client 0.0.0.0#4238: query: tenzir.com IN A + (255.255.255.255)"}
to_google_secops \
  customer_id="00000000-0000-0000-00000000000000000",
  private_key=secret("my_secops_key"),
  client_email="somebody@example.com",
  log_text=log,
  log_type="BIND_DNS",
  region="europe"
```

## See Also

[Section titled “See Also”](#see-also)

[`to_google_cloud_logging`](/reference/operators/to_google_cloud_logging)

# to_hive

Writes events to a URI using hive partitioning.

```tql
to_hive uri:string, partition_by=list<field>, format=string, [timeout=duration, max_size=int]
```

## Description

[Section titled “Description”](#description)

Hive partitioning is a partitioning scheme where a set of fields is used to partition events. For each combination of these fields, a directory is derived under which all events with the same field values will be stored. For example, if the events are partitioned by the fields `year` and `month`, then the files in the directory `/year=2024/month=10` will contain all events where `year == 2024` and `month == 10`.

Files within each partition directory are named using UUIDv7 for guaranteed uniqueness and natural time-based ordering. This prevents filename conflicts when multiple processes write to the same partition simultaneously.

### `uri: string`

[Section titled “uri: string”](#uri-string)

The base URI for all partitions.

### `partition_by = list<field>`

[Section titled “partition\_by = list\<field>”](#partition_by--listfield)

A list of fields that will be used for partitioning. Note that these fields will be elided from the output, as their value is already specified by the path.

### `format = string`

[Section titled “format = string”](#format--string)

The name of the format that will be used for writing, for example `json` or `parquet`. This will also be used for the file extension.

### `timeout = duration (optional)`

[Section titled “timeout = duration (optional)”](#timeout--duration-optional)

The time after which a new file will be opened for the same partition group. Defaults to `5min`.

### `max_size = int (optional)`

[Section titled “max\_size = int (optional)”](#max_size--int-optional)

The total file size after which a new file will be opened for the same partition group. Note that files will typically be slightly larger than this limit, because it opens a new file when only after it is exceeded. Defaults to `100M`.

### `compression = string (optional)`

[Section titled “compression = string (optional)”](#compression--string-optional)

Compress the output files with the given compression algorithm. See docs for the `compress` operator for supported compression algorithms.

## Examples

[Section titled “Examples”](#examples)

### Partition by a single field into local JSON files

[Section titled “Partition by a single field into local JSON files”](#partition-by-a-single-field-into-local-json-files)

```tql
from {a: 0, b: 0}, {a: 0, b: 1}, {a: 1, b: 2}
to_hive "/tmp/out/", partition_by=[a], format="json"
// This pipeline produces two files:
// -> /tmp/out/a=0/<uuid>.json:
//    {"b": 0}
//    {"b": 1}
// -> /tmp/out/a=1/<uuid>.json:
//    {"b": 2}
```

### Write a Parquet file into Azure Blob Store

[Section titled “Write a Parquet file into Azure Blob Store”](#write-a-parquet-file-into-azure-blob-store)

Write as Parquet into the Azure Blob Filesystem, partitioned by year, month and day.

```tql
to_hive "abfs://domain/bucket", partition_by=[year, month, day], format="parquet"
// -> abfs://domain/bucket/year=<year>/month=<month>/day=<day>/<uuid>.parquet
```

### Write partitioned JSON into an S3 bucket

[Section titled “Write partitioned JSON into an S3 bucket”](#write-partitioned-json-into-an-s3-bucket)

Write JSON into S3, partitioned by year and month, opening a new file after 1 GB.

```tql
year = ts.year()
month = ts.month()
to_hive "s3://my-bucket/some/subdirectory",
  partition_by=[year, month],
  format="json",
  max_size=1G
// -> s3://my-bucket/some/subdirectory/year=<year>/month=<month>/<uuid>.json
```

## See Also

[Section titled “See Also”](#see-also)

[`read_parquet`](/reference/operators/read_parquet), [`write_bitz`](/reference/operators/write_bitz), [`write_feather`](/reference/operators/write_feather), [`write_parquet`](/reference/operators/write_parquet)

# to_kafka

Sends messages to an Apache Kafka topic.

```tql
to_kafka topic:string, [message=blob|string, key=string, timestamp=time,
         options=record, aws_iam=record]
```

## Description

[Section titled “Description”](#description)

The `to_kafka` operator sends one message per event to a Kafka topic.

The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`.

The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them:

* `bootstrap.servers`: `localhost`
* `client.id`: `tenzir`

### `topic: string`

[Section titled “topic: string”](#topic-string)

The Kafka topic to send messages to.

### `message = blob|string (optional)`

[Section titled “message = blob|string (optional)”](#message--blobstring-optional)

An expression that evaluates to the message content for each row.

Defaults to `this.print_json()` when not specified.

### `key = string (optional)`

[Section titled “key = string (optional)”](#key--string-optional)

Sets a fixed key for all messages.

### `timestamp = time (optional)`

[Section titled “timestamp = time (optional)”](#timestamp--time-optional)

Sets a fixed timestamp for all messages.

### `options = record (optional)`

[Section titled “options = record (optional)”](#options--record-optional)

A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"acks": "all", "batch.size": 16384}`.

The `to_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs.

We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `to_kafka` arguments.

### `aws_iam = record (optional)`

[Section titled “aws\_iam = record (optional)”](#aws_iam--record-optional)

If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified.

Available keys:

* `region`: Region of the MSK Clusters. Must be specified when using IAM.
* `assume_role`: Optional Role ARN to assume.
* `session_name`: Optional session name to use when assuming a role.
* `external_id`: Optional external id to use when assuming a role.

The operator will try to get credentials in the following order:

1. Checks your environment variables for AWS Credentials.
2. Checks your `$HOME/.aws/credentials` file for a profile and credentials
3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`.
4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that are not directly supported by AWS.
5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set.
6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON.

## Examples

[Section titled “Examples”](#examples)

### Send JSON-formatted events to topic `events` (using default)

[Section titled “Send JSON-formatted events to topic events (using default)”](#send-json-formatted-events-to-topic-events-using-default)

Stream security events to a Kafka topic with automatic JSON formatting:

```tql
subscribe "security-alerts"
where severity >= "high"
select timestamp, source_ip, alert_type, details
to_kafka "events"
```

This pipeline subscribes to security alerts, filters for high-severity events, selects relevant fields, and sends them to Kafka as JSON. Each event is automatically formatted using `this.print_json()`, producing messages like:

```json
{
  "timestamp": "2024-03-15T10:30:00.000000",
  "source_ip": "192.168.1.100",
  "alert_type": "brute_force",
  "details": "Multiple failed login attempts detected"
}
```

### Send JSON-formatted events with explicit message

[Section titled “Send JSON-formatted events with explicit message”](#send-json-formatted-events-with-explicit-message)

```tql
subscribe "logs"
to_kafka "events", message=this.print_json()
```

### Send specific field values with a timestamp

[Section titled “Send specific field values with a timestamp”](#send-specific-field-values-with-a-timestamp)

```tql
subscribe "logs"
to_kafka "alerts", message=alert_msg, timestamp=2024-01-01T00:00:00
```

### Send data with a fixed key for partitioning

[Section titled “Send data with a fixed key for partitioning”](#send-data-with-a-fixed-key-for-partitioning)

```tql
metrics
to_kafka "metrics", message=this.print_json(), key="server-01"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_kafka`](/reference/operators/load_kafka), [`save_kafka`](/reference/operators/save_kafka)

# to_opensearch

Sends events to an OpenSearch-compatible Bulk API.

```tql
to_opensearch url:string, action=string, [index=string, id=string, doc=record,
    user=string, passwd=string, tls=bool, skip_peer_verification=bool,
    cacert=string, certfile=string, keyfile=string, include_nulls=bool,
    max_content_length=int, buffer_timeout=duration, compress=bool]
```

## Description

[Section titled “Description”](#description)

The `to_opensearch` operator sends events to a [OpenSearch-compatible Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/) such as [ElasticSearch](https://www.elastic.co/elasticsearch).

The operator accumulates multiple events before sending them as a single request. You can control the maximum request size via the `max_content_length` and the timeout before sending all accumulated events via the `send_timeout` option.

### `url: string`

[Section titled “url: string”](#url-string)

The URL of the API endpoint.

### `action = string`

[Section titled “action = string”](#action--string)

An expression for the action that evaluates to a `string`.

Supported actions:

* `create`: Creates a document if it doesn’t already exist and returns an error otherwise.
* `delete`: Deletes a document if it exists.
* `index`: Creates a document if it doesn’t yet exist and replace the document if it already exists.
* `update`: Updates existing documents and returns an error if the document doesn’t exist.
* `upsert`: If a document exists, it is updated; if it does not exist, a new document is indexed.

### `index = string (optional)`

[Section titled “index = string (optional)”](#index--string-optional)

An optional expression for the index that evaluates to a `string`.

Must be provided if the `url` does not have an index.

### `id = string (optional)`

[Section titled “id = string (optional)”](#id--string-optional)

The `id` of the document to act on.

Must be provided when using the `delete` and `update` actions.

### `doc = record (optional)`

[Section titled “doc = record (optional)”](#doc--record-optional)

The document to serialize.

Defaults to `this`.

### `user = string (optional)`

[Section titled “user = string (optional)”](#user--string-optional)

Optional user for HTTP Basic Authentication.

### `passwd = string (optional)`

[Section titled “passwd = string (optional)”](#passwd--string-optional)

Optional password for HTTP Basic Authentication.

### `tls = bool (optional)`

Enables TLS.

Defaults to `true`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

### `include_nulls = bool (optional)`

[Section titled “include\_nulls = bool (optional)”](#include_nulls--bool-optional)

Include fields with null values in the transmitted event data. By default, the operator drops all null values to save space.

Defaults to `false`.

### `max_content_length = int (optional)`

[Section titled “max\_content\_length = int (optional)”](#max_content_length--int-optional)

The maximum size of the message uncompressed body in bytes. A message may consist of multiple events. If a single event is larger than this limit, it is dropped and a warning is emitted.

Defaults to `5Mi`.

### `buffer_timeout = duration (optional)`

[Section titled “buffer\_timeout = duration (optional)”](#buffer_timeout--duration-optional)

The maximum amount of time for which the operator accumulates messages before sending them out to the HEC endpoint as a single message.

Defaults to `5s`.

### `compress = bool (optional)`

[Section titled “compress = bool (optional)”](#compress--bool-optional)

Whether to compress the message body using standard gzip.

Defaults to `true`.

## Examples

[Section titled “Examples”](#examples)

### Send events from a JSON file

[Section titled “Send events from a JSON file”](#send-events-from-a-json-file)

```tql
from "example.json"
to_opensearch "localhost:9200", action="create", index="main"
```

## See Also

[Section titled “See Also”](#see-also)

[`from_opensearch`](/reference/operators/from_opensearch)

# to_sentinelone_data_lake

Sends security events to SentinelOne Singularity Data Lake via REST API.

```tql
to_sentinelone_data_lake url:string, token=string,
                        [session_info=record, timeout=duration]
```

## Description

[Section titled “Description”](#description)

The `to_sentinelone_data_lake` operator sends incoming events to the [SentinelOne Data Lake REST API](https://support.sentinelone.com/hc/en-us/articles/360004195934-SentinelOne-API-Guide) as structured data, using the `addEvents` endpoint.

The operator accumulates multiple events before sending them as a single request, respecting the API’s limits.

If events are OCSF events, the `time` and `severity_id` fields are automatically extracted and added to the events meta information.

The OCSF `severity_id` is mapped to the SentinelOne Data Lake `sev` property according to this table:

| OCSF `severity_id` | SentinelOne severity |
| :----------------: | :------------------: |
|     0 (Unknown)    |       3 (info)       |
|  1 (Informational) |       1 (finer)      |
|       2 (Low)      |       2 (fine)       |
|     3 (Medium)     |       3 (info)       |
|      4 (High)      |       4 (warn)       |
|    5 (Critical)    |       5 (error)      |
|      6 (Fatal)     |       6 (fatal)      |
|     99 (Other)     |       3 (info)       |

### `url: string`

[Section titled “url: string”](#url-string)

The ingest URL for the Data Lake.

Please note that using the wrong ingestion endpoint, such as an incorrect region, may silently fail, as the SentinelOne API responds with 200 OK, even for some erroneous requests.

### `token = string`

[Section titled “token = string”](#token--string)

The token to use for authorization.

### `session_info = record (optional)`

[Section titled “session\_info = record (optional)”](#session_info--record-optional)

Some additional sessionInfo to send with each batch of events, as the `sessionInfo` field in the request body. If this option is used, it is recommended that it contains a field `serverHost` to identify the Node.

This can also contain a field `parser`, which names a SentinelOne parser that will be applied to the data field `message`. This can be used to ingest unstructured data.

### `timeout = duration (optional)`

[Section titled “timeout = duration (optional)”](#timeout--duration-optional)

The delay after which events are sent, even if this results in fewer events sent per message.

Defaults to `1min`.

## Examples

[Section titled “Examples”](#examples)

### Send events to SentinelOne Data Lake

[Section titled “Send events to SentinelOne Data Lake”](#send-events-to-sentinelone-data-lake)

```tql
to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net",
  token=secret("sentinelone-token")
```

### Send additional session information

[Section titled “Send additional session information”](#send-additional-session-information)

```tql
to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net",
  token=secret("sentinelone-token"),
  session_info={
    serverHost: "Node 42",
    serverType: "Tenzir Node",
    region: "Planet Earth",
  }
```

### Send ‘unstructured’ data

[Section titled “Send ‘unstructured’ data”](#send-unstructured-data)

The operator can also be used to send unstructured data to be parsed by SentinelOne. For this, the operators input must contain a field `message` and a `parser` must be specified in the `session_info`:

```tql
select message = this.print_ndjson();         // Format the entire event as JSON
to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net",
  token=secret("sentinelone-token"),
  session_info={
    serverHost: "Node 42",
    parser: "json",                            // Have SentinelOne parse the data
  }
```

Ingest Costs

SentinelOne charges per ingested *value* byte in the events. This means that you get charged for all bytes in `message`, including the keys, structural elements and whitespace.

If you already have structured data in Tenzir, prefer sending structured data, as you will only be charged for the values and one byte per key, as opposed to the full keys and structural characters in `message`.

# to_snowflake

Sends events to a Snowflake database.

```tql
to_snowflake account_identifier=string, user_name=string, password=string,
             snowflake_database=string snowflake_schema=string table=string,
             [ingest_mode=string]
```

Note

This operator is currently only available in the amd64 Docker images.

## Description

[Section titled “Description”](#description)

The `to_snowflake` operator makes it possible to send events to a [Snowflake](https://www.snowflake.com/) database. It uploads the events via bulk-ingestion under the hood and then copies them into the target table.

The operator supports nested types as [Snowflake semi-structured types](https://docs.snowflake.com/en/sql-reference/data-types-semistructured). Alternatively, you can use the [`flatten`](/reference/functions/flatten) function operator beforehand.

### `account_identifier = string`

[Section titled “account\_identifier = string”](#account_identifier--string)

The [Snowflake account identifier](https://docs.snowflake.com/en/user-guide/admin-account-identifier) to use.

### `user_name = string`

[Section titled “user\_name = string”](#user_name--string)

The Snowflake user name. The user must have the [`CREATE STAGE`](https://docs.snowflake.com/en/sql-reference/sql/create-stage#access-control-requirements) privilege on the given schema.

### `password = string`

[Section titled “password = string”](#password--string)

The password for the user.

### `database = string`

[Section titled “database = string”](#database--string)

The [Snowflake database](https://docs.snowflake.com/en/sql-reference/ddl-database) to write to. The user must be allowed to access it.

### `schema = string`

[Section titled “schema = string”](#schema--string)

The [Snowflake schema](https://docs.snowflake.com/en/sql-reference/ddl-database) to use. The user be allowed to access it.

### `table = string`

[Section titled “table = string”](#table--string)

The name of the table that should be used/created. The user must have the required permissions to create/write to it.

Table columns that are not in the event will be null, while event fields that are not in the table will be dropped. Type mismatches between the table and events are a hard error.

### `ingest_mode = string (optional)`

[Section titled “ingest\_mode = string (optional)”](#ingest_mode--string-optional)

You can set the ingest mode to one of three options:

* `"create_append"`: Creates the table if it does not exist, otherwise appends to it.
* `"create"`: creates the table, causing an error if it already exists.
* `"append"`: appends to the table, causing an error if it does not exist.

In case the operator creates the table it will use the the first event to infer the columns.

Default to `"create_append"`.

## Examples

[Section titled “Examples”](#examples)

### Send an event to a Snowflake table

[Section titled “Send an event to a Snowflake table”](#send-an-event-to-a-snowflake-table)

Upload `suricata.alert` events to a table `TENZIR` in `MY_DB@SURICATA_ALERT`:

```tql
export
where @name == "suricata.alert"
to_snowflake \
  account_identifier="asldyuf-xgb47555",
  user_name="tenzir_user",
  password="password1234",
  database="MY_DB",
  schema="SURICATA_ALERT",
  table="TENZIR"
```

# to_splunk

Sends events to a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector).

```tql
to_splunk url:string, hec_token=string,
          [event=any, host=string, source=string, sourcetype=expr, index=expr,
          tls=bool, cacert=string, certfile=string, keyfile=string,
          skip_peer_verification=bool, print_nulls=bool, max_content_length=int,
          buffer_timeout=duration, compress=bool]
```

## Description

[Section titled “Description”](#description)

The `to_splunk` operator sends events to a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector).

The source type defaults to `_json` and the operator renders incoming events as JSON. You can specify a different source type via the `sourcetype` option.

The operator accumulates multiple events before sending them as a single message to the HEC endpoint. You can control the maximum message size via the `max_content_length` and the timeout before sending all accumulated events via the `send_timeout` option.

### `url: string`

[Section titled “url: string”](#url-string)

The address of the Splunk indexer.

### `hec_token = string`

[Section titled “hec\_token = string”](#hec_token--string)

The [HEC token](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector#Create_an_Event_Collector_token_on_Splunk_Cloud_Platform) for authentication.

### `event = any (optional)`

[Section titled “event = any (optional)”](#event--any-optional)

The event to send.

Defaults to `this`, meaning the entire event is sent.

### `host = string (optional)`

[Section titled “host = string (optional)”](#host--string-optional)

An optional value for the [Splunk `host`](https://docs.splunk.com/Splexicon:Host).

### `source = string (optional)`

[Section titled “source = string (optional)”](#source--string-optional)

An optional value for the [Splunk `source`](https://docs.splunk.com/Splexicon:Source).

### `sourcetype = expr (optional)`

[Section titled “sourcetype = expr (optional)”](#sourcetype--expr-optional)

An optional expression for [Splunk’s `sourcetype`](https://docs.splunk.com/Splexicon:Sourcetype) that evaluates to a `string`. You can use this to set the `sourcetype` per event, by providing a field instead of a string.

Regardless of the chosen `sourcetype`, the event itself is passed as a json object in `event` key of level object that is sent.

Defaults to `_json`.

### `index = expr (optional)`

[Section titled “index = expr (optional)”](#index--expr-optional)

An optional expression for the [Splunk `index`](https://docs.splunk.com/Splexicon:Index) that evaluates to a `string`.

If you do not provide this option, Splunk will use the default index.

**NB**: HEC silently drops events with an invalid `index`.

### `tls = bool (optional)`

Enables TLS.

Defaults to `true`.

### `cacert = string (optional)`

Path to the CA certificate used to verify the server’s certificate.

Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system.

### `certfile = string (optional)`

Path to the client certificate.

### `keyfile = string (optional)`

Path to the key for the client certificate.

### `skip_peer_verification = bool (optional)`

Toggles TLS certificate verification.

Defaults to `false`.

### `include_nulls = bool (optional)`

[Section titled “include\_nulls = bool (optional)”](#include_nulls--bool-optional)

Include fields with null values in the transmitted event data. By default, the operator drops all null values to save space.

### `max_content_length = int (optional)`

[Section titled “max\_content\_length = int (optional)”](#max_content_length--int-optional)

The maximum size of the message uncompressed body in bytes. A message may consist of multiple events. If a single event is larger than this limit, it is dropped and a warning is emitted.

This corresponds with Splunk’s [`max_content_length`](https://docs.splunk.com/Documentation/Splunk/9.3.1/Admin/Limitsconf#.5Bhttp_input.5D) option. Be aware that [Splunk Cloud has a default of `1MB`](https://docs.splunk.com/Documentation/SplunkCloud/9.2.2406/Service/SplunkCloudservice#Using_HTTP_Event_Collector_.28HEC.29) for `max_content_length`.

Defaults to `5Mi`.

### `buffer_timeout = duration (optional)`

[Section titled “buffer\_timeout = duration (optional)”](#buffer_timeout--duration-optional)

The maximum amount of time for which the operator accumulates messages before sending them out to the HEC endpoint as a single message.

Defaults to `5s`.

### `compress = bool (optional)`

[Section titled “compress = bool (optional)”](#compress--bool-optional)

Whether to compress the message body using standard gzip.

Defaults to `true`.

## Examples

[Section titled “Examples”](#examples)

### Send a JSON file to a HEC endpoint

[Section titled “Send a JSON file to a HEC endpoint”](#send-a-json-file-to-a-hec-endpoint)

```tql
load_file "example.json"
read_json
to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token")
```

### Data Dependent Splunk framing

[Section titled “Data Dependent Splunk framing”](#data-dependent-splunk-framing)

By default, the `to_splunk` operator sends the entire event as the `event` field to the HEC, together with any optional Splunk “frame” fields such as `host`, `source`, `sourcetype` and `index`. These special properties can be set using the operators respective arguments, with an expression that is evaluated per event.

However, this means that these special properties may be transmitted as both part of `event` and as part of the Splunk frame. This can be especially undesirable when the events are supposed to adhere to a specific schema, such as OCSF.

In this case, you can specify the additional `event` option to specify which part of the incoming event should be sent as the event.

```tql
from {
  host: "my-host",
  source: "my-source",
  a: 42,
  b: 0,
  message: "text",
  nested: { x: 0 },
}


// move the entire event into `event`
this = { event: this }


// hoist the splunk specific fields back out, so they are no longer part of the
// sent event
move host = event.host, source = event.source


to_splunk "https://localhost:8088",
  hec_token=secret("splunk-hec-token"),
  host=host,
  source=source,
  event=event
```

# top

Shows the most common values.

```tql
top x:field
```

## Description

[Section titled “Description”](#description)

Shows the most common values for a given field. For each value, a new event containing its count will be produced. In general, `top x` is equivalent to:

```tql
summarize x, count=count()
sort -count
```

This operator is the dual to [`rare`](/reference/operators/rare).

Potentially High Memory Usage

Use caution when applying this operator to large inputs. It currently buffers all data in memory. Out-of-core processing is on our roadmap.

### `x: field`

[Section titled “x: field”](#x-field)

The field to find the most common values for.

## Examples

[Section titled “Examples”](#examples)

### Find the most common values

[Section titled “Find the most common values”](#find-the-most-common-values)

```tql
from {x: "B"}, {x: "A"}, {x: "A"}, {x: "B"}, {x: "A"}, {x: "D"}, {x: "C"}, {x: "C"}
top x
```

```tql
{x: "A", count: 3}
{x: "B", count: 2}
{x: "C", count: 2}
{x: "D", count: 1}
```

### Show the 5 top-most values

[Section titled “Show the 5 top-most values”](#show-the-5-top-most-values)

```tql
top id.orig_h
head 5
```

## See Also

[Section titled “See Also”](#see-also)

[`summarize`](/reference/operators/summarize), [`rare`](/reference/operators/rare), [`sort`](/reference/operators/sort)

# unordered

Removes ordering assumptions from a pipeline.

```tql
unordered { … }
```

## Description

[Section titled “Description”](#description)

The `unordered` operator takes a pipeline as an argument and removes ordering assumptions from it. This causes some operators to run faster.

Note that some operators implicitly remove ordering assumptions. For example, `sort` tells upstream operators that ordering does not matter.

## Examples

[Section titled “Examples”](#examples)

### Parse JSON unordered

[Section titled “Parse JSON unordered”](#parse-json-unordered)

```tql
unordered {
  read_json
}
```

# unroll

Returns a new event for each member of a list or a record in an event, duplicating the surrounding event.

```tql
unroll [field:list|record]
```

## Description

[Section titled “Description”](#description)

The `unroll` returns an event for each member of a specified list or record field, leaving the surrounding event unchanged.

Drops events where the specified field is an empty record, an empty list, or null.

### `field: list|record`

[Section titled “field: list|record”](#field-listrecord)

Sets the name of the list or record field.

## Examples

[Section titled “Examples”](#examples)

### Unroll a list

[Section titled “Unroll a list”](#unroll-a-list)

```tql
from {x: "a", y: [1, 2, 3]},
     {x: "b", y: []},
     {x: "c", y: [4]}
unroll y
```

```tql
{x: "a", y: 1}
{x: "a", y: 2}
{x: "a", y: 3}
{x: "c", y: 4}
```

### Unroll a record

[Section titled “Unroll a record”](#unroll-a-record)

```tql
from {x: "a", y: {foo: 1, baz: 2}},
     {x: "b", y: {foo: null, baz: 3}},
     {x: "c", y: null}
unroll y
```

```tql
{x: "a", y: {foo: 1}}
{x: "a", y: {baz: 2}}
{x: "b", y: {foo: null}}
{x: "b", y: {baz: 3}}
```

# version

Shows the current version.

```tql
version
```

## Description

[Section titled “Description”](#description)

The `version` operator shows the current Tenzir version.

## Schemas

[Section titled “Schemas”](#schemas)

Tenzir emits version information with the following schema.

### `tenzir.version`

[Section titled “tenzir.version”](#tenzirversion)

Contains detailed information about the process version.

| Field          | Type           | Description                                                                        |
| :------------- | :------------- | :--------------------------------------------------------------------------------- |
| `version`      | `string`       | The formatted version string.                                                      |
| `tag`          | `string`       | An optional identifier of the build.                                               |
| `major`        | `uint64`       | The major release version.                                                         |
| `minor`        | `uint64`       | The minor release version.                                                         |
| `patch`        | `uint64`       | The patch release version.                                                         |
| `features`     | `list<string>` | A list of feature flags that conditionally enable features in the Tenzir Platform. |
| `build`        | `record`       | Build-time configuration options.                                                  |
| `dependencies` | `list<record>` | A list of build-time dependencies and their versions.                              |

The `build` record contains the following fields:

| Field        | Type     | Description                                                                |
| :----------- | :------- | :------------------------------------------------------------------------- |
| `type`       | `string` | The configured build type. One of `Release`, `Debug`, or `RelWithDebInfo`. |
| `tree_hash`  | `string` | A hash of all files in the source directory.                               |
| `assertions` | `bool`   | Whether potentially expensive run-time checks are enabled.                 |
| `sanitizers` | `record` | Contains information about additional run-time checks from sanitizers.     |

The `build.sanitzers` record contains the following fields:

| Field                | Type   | Description                                          |
| :------------------- | :----- | :--------------------------------------------------- |
| `address`            | `bool` | Whether the address sanitizer is enabled.            |
| `undefined_behavior` | `bool` | Whether the undefined behavior sanitizer is enabled. |

The `dependencies` record contains the following fields:

| Field     | Type     | Description                    |
| :-------- | :------- | :----------------------------- |
| `name`    | `string` | The name of the dependency.    |
| `version` | `string` | THe version of the dependency. |

## Examples

[Section titled “Examples”](#examples)

### Show the current version

[Section titled “Show the current version”](#show-the-current-version)

```tql
version
drop dependencies
```

```tql
{
  version: "5.0.1+g847fcc6334",
  tag: "g847fcc6334",
  major: 5,
  minor: 0,
  patch: 1,
  features: [
    "chart_limit",
    "modules",
    "tql2_from",
    "exact_schema",
    "tql2_only",
  ],
  build: {
    type: "Release",
    tree_hash: "ef28a81eb124cc46a646250d1fb17390",
    assertions: false,
    sanitizers: {
      address: false,
      undefined_behavior: false,
    },
  },
}
```

# where

Keeps only events for which the given predicate is true.

```tql
where predicate:bool
```

## Description

[Section titled “Description”](#description)

The `where` operator only keeps events that match the provided predicate and discards all other events. Only events for which it evaluates to `true` pass.

## Examples

[Section titled “Examples”](#examples)

### Keep only events where `src_ip` is `1.2.3.4`

[Section titled “Keep only events where src\_ip is 1.2.3.4”](#keep-only-events-where-src_ip-is-1234)

```tql
where src_ip == 1.2.3.4
```

### Use a nested field name and a temporal constraint on the `ts` field

[Section titled “Use a nested field name and a temporal constraint on the ts field”](#use-a-nested-field-name-and-a-temporal-constraint-on-the-ts-field)

```tql
where id.orig_h == 1.2.3.4 and ts > now() - 1h
```

### Combine subnet, size and duration constraints

[Section titled “Combine subnet, size and duration constraints”](#combine-subnet-size-and-duration-constraints)

```tql
where src_ip in 10.10.5.0/25 and (orig_bytes > 1Mi or duration > 30min)
```

## See Also

[Section titled “See Also”](#see-also)

[`assert`](/reference/operators/assert), [`drop`](/reference/operators/drop), [`select`](/reference/operators/select)

# write_bitz

Writes events in *BITZ* format.

```tql
write_bitz
```

## Description

[Section titled “Description”](#description)

BITZ is short for **Bi**nary **T**en**z**ir and is our internal wire format.

Use BITZ when you need high-throughput structured data exchange with minimal overhead. BITZ is a thin wrapper around Arrow’s record batches. That is, BITZ lays out data in a (compressed) columnar fashion that makes it conducive for analytical workloads. Since it’s padded and byte-aligned, it is portable and doesn’t induce any deserialization cost, making it suitable for write-once-read-many use cases.

Internally, BITZ uses Arrow’s IPC format for serialization and deserialization, but prefixes each message with a 64 bit size prefix to support changing schemas between batches—something that Arrow’s IPC format does not support on its own.

## See Also

[Section titled “See Also”](#see-also)

[`read_bitz`](/reference/operators/read_bitz), [`to_hive`](/reference/operators/to_hive), [`write_feather`](/reference/operators/write_feather), [`write_parquet`](/reference/operators/write_parquet)

# write_csv

Transforms event stream to CSV (Comma-Separated Values) byte stream.

```tql
write_csv [list_separator=str, null_value=str, no_header=bool]
```

## Description

[Section titled “Description”](#description)

The `write_csv` operator transforms an event stream into a byte stream by writing the events as CSV.

### `list_separator = str (optional)`

[Section titled “list\_separator = str (optional)”](#list_separator--str-optional)

The string separating different elements in a list within a single field.

Defaults to `";"`.

### `null_value = str (optional)`

[Section titled “null\_value = str (optional)”](#null_value--str-optional)

The string denoting an absent value.

Defaults to `" "`.

### `no_header = bool (optional)`

[Section titled “no\_header = bool (optional)”](#no_header--bool-optional)

Whether to not print a header line containing the field names.

## Examples

[Section titled “Examples”](#examples)

Write an event as CSV.

```tql
from {x:1, y:true, z: "String"}
write_csv
```

```plaintext
x,y,z
1,true,String
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_csv`](/reference/functions/parse_csv), [`print_csv`](/reference/functions/print_csv), [`read_csv`](/reference/operators/read_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_feather

Transforms the input event stream to Feather byte stream.

```tql
write_feather [compression_level=int, compression_type=str, min_space_savings=double]
```

## Description

[Section titled “Description”](#description)

Transforms the input event stream to [Feather](https://arrow.apache.org/docs/python/feather.html) (a thin wrapper around [Apache Arrow’s IPC](https://arrow.apache.org/docs/python/ipc.html) wire format) byte stream.

### `compression_level = int (optional)`

[Section titled “compression\_level = int (optional)”](#compression_level--int-optional)

An optional compression level for the corresponding compression type. This option is ignored if no compression type is specified.

Defaults to the compression type’s default compression level.

### `compression_type = str (optional)`

[Section titled “compression\_type = str (optional)”](#compression_type--str-optional)

Supported options are `zstd` for [Zstandard](http://facebook.github.io/zstd/) compression and `lz4` for [LZ4 Frame](https://android.googlesource.com/platform/external/lz4/+/HEAD/doc/lz4_Frame_format.md) compression.

Why would I use this over the `compress_*` operators?

The Feather format offers more efficient compression compared to the `compress_*` operators. This is because it compresses the data column-by-column, leaving metadata that needs to be accessed frequently uncompressed.

### `min_space_savings = double (optional)`

[Section titled “min\_space\_savings = double (optional)”](#min_space_savings--double-optional)

Minimum space savings percentage required for compression to be applied. This option is ignored if no compression is specified. The provided value must be between 0 and 1 inclusive.

Space savings are calculated as `1.0 - compressed_size / uncompressed_size`. For example, with a minimum space savings rate of 0.1, a 100-byte body buffer will not be compressed if its expected compressed size exceeds 90 bytes.

Defaults to `0`, i.e., always applying compression.

## Examples

[Section titled “Examples”](#examples)

### Convert a JSON stream into a Feather file

[Section titled “Convert a JSON stream into a Feather file”](#convert-a-json-stream-into-a-feather-file)

```tql
load_file "input.json"
read_json
write_feather
save_file "output.feather"
```

## See Also

[Section titled “See Also”](#see-also)

[`read_bitz`](/reference/operators/read_bitz), [`read_feather`](/reference/operators/read_feather), [`to_hive`](/reference/operators/to_hive), [`write_bitz`](/reference/operators/write_bitz), [`write_parquet`](/reference/operators/write_parquet)

# write_json

Transforms the input event stream to a JSON byte stream.

```tql
write_json [strip=bool, color=bool, arrays_of_objects=bool,
            strip_null_fields=bool, strip_nulls_in_lists=bool,
            strip_empty_records=bool, strip_empty_lists=bool]
```

## Description

[Section titled “Description”](#description)

Transforms the input event stream to a JSON byte stream.

Newline-Delimited JSON (NDJSON)

Use [`write_ndjson` operator](/reference/operators/write_ndjson) to write Newline-Delimited JSON.

### `strip = bool (optional)`

[Section titled “strip = bool (optional)”](#strip--bool-optional)

Enables all `strip_*` options.

Defaults to `false`.

### `color = bool (optional)`

[Section titled “color = bool (optional)”](#color--bool-optional)

Colorize the output.

Defaults to `false`.

### `arrays_of_objects = bool (optional)`

[Section titled “arrays\_of\_objects = bool (optional)”](#arrays_of_objects--bool-optional)

Prints the input as a single array of objects, instead of as separate objects.

Defaults to `false`.

### `strip_null_fields = bool (optional)`

[Section titled “strip\_null\_fields = bool (optional)”](#strip_null_fields--bool-optional)

Strips all fields with a `null` value from records.

Defaults to `false`.

### `strip_nulls_in_lists = bool (optional)`

[Section titled “strip\_nulls\_in\_lists = bool (optional)”](#strip_nulls_in_lists--bool-optional)

Strips all `null` values from lists.

Defaults to `false`.

### `strip_empty_records = bool (optional)`

[Section titled “strip\_empty\_records = bool (optional)”](#strip_empty_records--bool-optional)

Strips empty records, including those that only became empty by stripping.

Defaults to `false`.

### `strip_empty_lists = bool (optional)`

[Section titled “strip\_empty\_lists = bool (optional)”](#strip_empty_lists--bool-optional)

Strips empty lists, including those that only became empty by stripping.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Convert a YAML stream into a JSON file

[Section titled “Convert a YAML stream into a JSON file”](#convert-a-yaml-stream-into-a-json-file)

```tql
load_file "input.yaml"
read_yaml
write_json
save_file "output.json"
```

### Strip null fields

[Section titled “Strip null fields”](#strip-null-fields)

```tql
from { yes: 1, no: null}
write_json strip_null_fields=true
```

```json
{
  "yes": 1
}
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_json`](/reference/functions/parse_json), [`print_json`](/reference/functions/print_json), [`read_json`](/reference/operators/read_json), [`write_tql`](/reference/operators/write_tql)

# write_kv

Writes events in a Key-Value format.

```tql
write_kv [field_separator=str, value_separator=str, list_separator=str,
          flatten_separator=str, null_value=str]
```

## Description

[Section titled “Description”](#description)

Writes events in a Key-Value format, with one event per line. Nested data will be flattend, keys or values containing the given separators will be quoted and the special characters `\n`, `\r`, `\` and `"` will be escaped.

### `field_separator = str (optional)`

[Section titled “field\_separator = str (optional)”](#field_separator--str-optional)

A string that shall separate the key-value pairs.

Must not be an empty string.

Defaults to `" "`.

### `value_separator = str (optional)`

[Section titled “value\_separator = str (optional)”](#value_separator--str-optional)

A string that shall separate key and value within key-value pair.

Must not be an empty string.

Defaults to `"="`.

### `list_separator = str (optional)`

[Section titled “list\_separator = str (optional)”](#list_separator--str-optional)

Must not be an empty string.

Defaults to `","`.

### `flatten_separator = str (optional)`

[Section titled “flatten\_separator = str (optional)”](#flatten_separator--str-optional)

A string to join the keys of nested records with. For example, given `flatten="."`

Defaults to `"."`.

### `null_value = str (optional)`

[Section titled “null\_value = str (optional)”](#null_value--str-optional)

A string to represent null values.

Defaults to the empty string.

## Examples

[Section titled “Examples”](#examples)

### Write key-value pairs with quoted strings

[Section titled “Write key-value pairs with quoted strings”](#write-key-value-pairs-with-quoted-strings)

```tql
from {x: "hello world", y: "hello=world"}
```

```txt
x="hello world" y:"hello=world"
```

### Write key-value pairs of nested records

[Section titled “Write key-value pairs of nested records”](#write-key-value-pairs-of-nested-records)

```tql
from {x: {y: {z:0}, y2:42}, a: "string" }
write_kv
```

```txt
x.y.z=0 y.y2=42 a=string
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_kv`](/reference/functions/parse_kv), [`print_kv`](/reference/functions/print_kv), [`read_kv`](/reference/operators/read_kv), [`write_lines`](/reference/operators/write_lines)

# write_lines

Writes events as key-value pairsthe *values* of an event.

```tql
write_lines
```

Tip

Use [`write_kv`](/reference/operators/write_kv) operator if you also want to write the *key*s.

## Description

[Section titled “Description”](#description)

Each event is printed on a new line, with fields separated by spaces, and nulls skipped.

Note

The lines printer does not perform any escaping. Characters like `\n` and `"` are printed as is.

## Examples

[Section titled “Examples”](#examples)

### Write the values of an event

[Section titled “Write the values of an event”](#write-the-values-of-an-event)

```tql
from {x:1, y:true, z: "String"}
write_lines
```

```txt
1 true String
```

## See Also

[Section titled “See Also”](#see-also)

[`read_lines`](/reference/operators/read_lines), [`write_csv`](/reference/operators/write_csv), [`write_kv`](/reference/operators/write_kv), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_ndjson

Transforms the input event stream to a Newline-Delimited JSON byte stream.

```tql
write_ndjson [strip=bool, color=bool, arrays_of_objects=bool,
              strip_null_fields=bool, strip_nulls_in_lists=bool,
              strip_empty_records=bool, strip_empty_lists=bool]
```

## Description

[Section titled “Description”](#description)

Transforms the input event stream to a Newline-Delimited JSON byte stream.

### `strip = bool (optional)`

[Section titled “strip = bool (optional)”](#strip--bool-optional)

Enables all `strip_*` options.

Defaults to `false`.

### `color = bool (optional)`

[Section titled “color = bool (optional)”](#color--bool-optional)

Colorize the output.

Defaults to `false`.

### `arrays_of_objects = bool (optional)`

[Section titled “arrays\_of\_objects = bool (optional)”](#arrays_of_objects--bool-optional)

Prints the input as a single array of objects, instead of as separate objects.

Defaults to `false`.

### `strip_null_fields = bool (optional)`

[Section titled “strip\_null\_fields = bool (optional)”](#strip_null_fields--bool-optional)

Strips all fields with a `null` value from records.

Defaults to `false`.

### `strip_nulls_in_lists = bool (optional)`

[Section titled “strip\_nulls\_in\_lists = bool (optional)”](#strip_nulls_in_lists--bool-optional)

Strips all `null` values from lists.

Defaults to `false`.

### `strip_empty_records = bool (optional)`

[Section titled “strip\_empty\_records = bool (optional)”](#strip_empty_records--bool-optional)

Strips empty records, including those that only became empty by stripping.

Defaults to `false`.

### `strip_empty_lists = bool (optional)`

[Section titled “strip\_empty\_lists = bool (optional)”](#strip_empty_lists--bool-optional)

Strips empty lists, including those that only became empty by stripping.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Convert a YAML stream into a JSON file

[Section titled “Convert a YAML stream into a JSON file”](#convert-a-yaml-stream-into-a-json-file)

```tql
load_file "input.yaml"
read_yaml
write_ndjson
save_file "output.json"
```

### Strip null fields

[Section titled “Strip null fields”](#strip-null-fields)

```tql
from {
  yes: 1,
  no: null,
}
write_ndjson strip_null_fields=true
```

```json
{ "yes": 1 }
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_json`](/reference/functions/parse_json), [`print_ndjson`](/reference/functions/print_ndjson), [`print_json`](/reference/functions/print_json), [`read_json`](/reference/operators/read_json), [`read_ndjson`](/reference/operators/read_ndjson), [`write_json`](/reference/operators/write_json)

# write_parquet

Transforms event stream to a Parquet byte stream.

```tql
write_parquet [compression_level=int, compression_type=str]
```

## Description

[Section titled “Description”](#description)

[Apache Parquet](https://parquet.apache.org/) is a columnar storage format that a variety of data tools support.

### `compression_level = int (optional)`

[Section titled “compression\_level = int (optional)”](#compression_level--int-optional)

An optional compression level for the corresponding compression type. This option is ignored if no compression type is specified.

Defaults to the compression type’s default compression level.

### `compression_type = str (optional)`

[Section titled “compression\_type = str (optional)”](#compression_type--str-optional)

Specifies an optional compression type. Supported options are `zstd` for [Zstandard](http://facebook.github.io/zstd/) compression, `brotli` for [brotli](https://www.brotli.org) compression, `gzip` for [gzip](https://www.gzip.org) compression, and `snappy` for [snappy](https://google.github.io/snappy/) compression.

Why would I use this over the `compress` operator?

The Parquet format offers more efficient compression compared to the [`compress`](/reference/operators/compress) operator. This is because it compresses the data column-by-column, leaving metadata that needs to be accessed frequently uncompressed.

## Examples

[Section titled “Examples”](#examples)

Write a Parquet file:

```tql
load_file "/tmp/data.json"
read_json
write_parquet
```

## See Also

[Section titled “See Also”](#see-also)

[`read_bitz`](/reference/operators/read_bitz), [`read_parquet`](/reference/operators/read_parquet), [`to_hive`](/reference/operators/to_hive), [`write_bitz`](/reference/operators/write_bitz), [`write_feather`](/reference/operators/write_feather)

# write_pcap

Transforms event stream to PCAP byte stream.

```tql
write_pcap
```

## Description

[Section titled “Description”](#description)

Transforms event stream to [PCAP](https://datatracker.ietf.org/doc/id/draft-gharris-opsawg-pcap-00.html) byte stream.

The structured representation of packets has the `pcap.packet` schema:

```yaml
pcap.packet:
  record:
    - linktype: uint64
    - time:
        timestamp: time
    - captured_packet_length: uint64
    - original_packet_length: uint64
    - data: string
```

PCAPNG

The current implementation does *not* support [PCAPNG](https://www.ietf.org/archive/id/draft-tuexen-opsawg-pcapng-05.html).

## Examples

[Section titled “Examples”](#examples)

### Write packet events as a PCAP file

[Section titled “Write packet events as a PCAP file”](#write-packet-events-as-a-pcap-file)

```tql
subscribe "packets"
write_pcap
save_file "/logs/packets.pcap"
```

## See Also

[Section titled “See Also”](#see-also)

[`load_nic`](/reference/operators/load_nic), [`read_pcap`](/reference/operators/read_pcap)

# write_ssv

Transforms event stream to SSV (Space-Separated Values) byte stream.

```tql
write_ssv [list_separator=str, null_value=str, no_header=bool]
```

## Description

[Section titled “Description”](#description)

The `write_ssv` operator transforms an event stream into a byte stream by writing the events as SSV.

### `list_separator = str (optional)`

[Section titled “list\_separator = str (optional)”](#list_separator--str-optional)

The string separating different elements in a list within a single field.

Defaults to `","`.

### `null_value = str (optional)`

[Section titled “null\_value = str (optional)”](#null_value--str-optional)

The string denoting an absent value.

Defaults to `"-"`.

### `no_header = bool (optional)`

[Section titled “no\_header = bool (optional)”](#no_header--bool-optional)

Whether to not print a header line containing the field names.

## Examples

[Section titled “Examples”](#examples)

Write an event as SSV.

```tql
from {x:1, y:true, z: "String"}
write_ssv
```

```plaintext
x y z
1 true String
```

## See Also

[Section titled “See Also”](#see-also)

[`print_ssv`](/reference/functions/print_ssv), [`read_ssv`](/reference/operators/read_ssv), [`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_syslog

Writes events as syslog.

```tql
write_syslog [facility=int, severity=int, timestamp=time, hostname=string,
              app_name=string, process_id=string, message_id=string,
              structured_data=record, message=string]
```

## Description

[Section titled “Description”](#description)

Writes events as [RFC 5424](https://datatracker.ietf.org/doc/html/rfc5424) Syslog messages.

All options to the operator try to get values for the respective fields from the same-named fields in the input events if unspecified.

### `facility = int (optional)`

[Section titled “facility = int (optional)”](#facility--int-optional)

Set the facility of the syslog.

Defaults to `1` if `null`.

### `severity = int (optional)`

[Section titled “severity = int (optional)”](#severity--int-optional)

Set the severity of the syslog.

Defaults to `6` if `null`.

### `timestamp = time (optional)`

[Section titled “timestamp = time (optional)”](#timestamp--time-optional)

Set the timestamp of the syslog.

### `hostname = string (optional)`

[Section titled “hostname = string (optional)”](#hostname--string-optional)

Set the hostname of the syslog.

### `app_name = string (optional)`

[Section titled “app\_name = string (optional)”](#app_name--string-optional)

Set the application name of the syslog.

### `process_id = string (optional)`

[Section titled “process\_id = string (optional)”](#process_id--string-optional)

Set the process id of the syslog.

### `message_id = string (optional)`

[Section titled “message\_id = string (optional)”](#message_id--string-optional)

Set the message id of the syslog.

### `structured_data = record (optional)`

[Section titled “structured\_data = record (optional)”](#structured_data--record-optional)

Set the structured data of the syslog.

### `message = string (optional)`

[Section titled “message = string (optional)”](#message--string-optional)

Set the message of the syslog.

## Examples

[Section titled “Examples”](#examples)

### Create a syslog manually

[Section titled “Create a syslog manually”](#create-a-syslog-manually)

```tql
from {
  facility: 1,
  severity: 1,
  timestamp: now(),
  hostname: "localhost",
  structured_data: {
    origin: {
      key: "value",
    },
  },
  message: "Tenzir",
}
write_syslog
```

```log
<9>1 2025-03-31T13:28:55.971210Z localhost - - - [origin key="value"] Tenzir
```

## See Also

[Section titled “See Also”](#see-also)

[`parse_syslog`](/reference/functions/parse_syslog), [`read_syslog`](/reference/operators/read_syslog)

# write_tql

Transforms the input event stream to a TQL notation byte stream.

```tql
write_tql [strip=bool, color=bool, compact=bool,
           strip_null_fields=bool, strip_nulls_in_lists=bool,
           strip_empty_records=bool, strip_empty_lists=bool]
```

## Description

[Section titled “Description”](#description)

Transforms the input event stream to a TQL notation byte stream.

Tip

`write_tql color=true` is the default sink for terminal output.

### `strip = bool (optional)`

[Section titled “strip = bool (optional)”](#strip--bool-optional)

Enables all `strip_*` options.

Defaults to `false`.

### `compact = bool (optional)`

[Section titled “compact = bool (optional)”](#compact--bool-optional)

Write one event per line, omitting linebreaks and indentation of records.

Defaults to `false`.

### `color = bool (optional)`

[Section titled “color = bool (optional)”](#color--bool-optional)

Colorize the output.

Defaults to `false`.

### `strip_null_fields = bool (optional)`

[Section titled “strip\_null\_fields = bool (optional)”](#strip_null_fields--bool-optional)

Strips all fields with a `null` value from records.

Defaults to `false`.

### `strip_nulls_in_lists = bool (optional)`

[Section titled “strip\_nulls\_in\_lists = bool (optional)”](#strip_nulls_in_lists--bool-optional)

Strips all `null` values from lists.

Defaults to `false`.

### `strip_empty_records = bool (optional)`

[Section titled “strip\_empty\_records = bool (optional)”](#strip_empty_records--bool-optional)

Strips empty records, including those that only became empty by stripping.

Defaults to `false`.

### `strip_empty_lists = bool (optional)`

[Section titled “strip\_empty\_lists = bool (optional)”](#strip_empty_lists--bool-optional)

Strips empty lists, including those that only became empty by stripping.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Print an event as TQL

[Section titled “Print an event as TQL”](#print-an-event-as-tql)

```tql
from {activity_id: 16, activity_name: "Query", rdata: 31.3.245.133, dst_endpoint: {ip: 192.168.4.1, port: 53}}
write_tql
```

```tql
{
  activity_id: 16,
  activity_name: "Query",
  rdata: 31.3.245.133,
  dst_endpoint: {
    ip: 192.168.4.1,
    port: 53,
  },
}
```

### Strip null fields

[Section titled “Strip null fields”](#strip-null-fields)

```tql
from {yes: 1, no: null}
write_tql strip_null_fields=true
```

```tql
{
  yes: 1,
}
```

## See Also

[Section titled “See Also”](#see-also)

[`write_json`](/reference/operators/write_json)

# write_tsv

Transforms event stream to TSV (Tab-Separated Values) byte stream.

```tql
write_tsv [list_separator=str, null_value=str, no_header=bool]
```

## Description

[Section titled “Description”](#description)

The `write_tsv` operator transforms an event stream into a byte stream by writing the events as TSV.

### `list_separator = str (optional)`

[Section titled “list\_separator = str (optional)”](#list_separator--str-optional)

The string separating different elements in a list within a single field.

Defaults to `","`.

### `null_value = str (optional)`

[Section titled “null\_value = str (optional)”](#null_value--str-optional)

The string denoting an absent value.

Defaults to `"-"`.

### `no_header = bool (optional)`

[Section titled “no\_header = bool (optional)”](#no_header--bool-optional)

Whether to not print a header line containing the field names.

## Examples

[Section titled “Examples”](#examples)

Write an event as TSV.

```tql
from {x:1, y:true, z: "String"}
write_tsv
```

```plaintext
x  y  z
1  true  String
```

## See Also

[Section titled “See Also”](#see-also)

[`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_xsv`](/reference/operators/write_xsv)

# write_xsv

Transforms event stream to XSV byte stream.

```tql
write_xsv field_separator=str, list_separator=str, null_value=str, [no_header=bool]
```

## Description

[Section titled “Description”](#description)

The [`xsv`](https://en.wikipedia.org/wiki/Delimiter-separated_values) format is a generalization of [comma-separated values (CSV)](https://en.wikipedia.org/wiki/Comma-separated_values) data in tabular form with a more flexible separator specification supporting tabs, commas, and spaces. The first line in an XSV file is the header that describes the field names. The remaining lines contain concrete values. One line corresponds to one event, minus the header.

The following table lists existing XSV configurations:

| Format                                  | Field Separator | List Separator | Null Value |
| --------------------------------------- | :-------------: | :------------: | :--------: |
| [`csv`](/reference/operators/write_csv) |       `,`       |       `;`      |    empty   |
| [`ssv`](/reference/operators/write_ssv) |    `<space>`    |       `,`      |     `-`    |
| [`tsv`](/reference/operators/write_tsv) |       `\t`      |       `,`      |     `-`    |

Note that nested records have dot-separated field names.

### `field_separator = str`

[Section titled “field\_separator = str”](#field_separator--str)

The string separating different fields.

### `list_separator = str`

[Section titled “list\_separator = str”](#list_separator--str)

The string separating different elements in a list within a single field.

### `null_value = str`

[Section titled “null\_value = str”](#null_value--str)

The string denoting an absent value.

### `no_header=bool (optional)`

[Section titled “no\_header=bool (optional)”](#no_headerbool-optional)

Whether to not print a header line containing the field names.

## Examples

[Section titled “Examples”](#examples)

```tql
from {x:1, y:true, z: "String"}
write_xsv field_separator="/", list_separator=";", null_value=""
```

```plaintext
x/y/z
1/true/String
```

## See Also

[Section titled “See Also”](#see-also)

[`print_xsv`](/reference/functions/print_xsv), [`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv)

# write_yaml

Transforms the input event stream to YAML byte stream.

```tql
write_yaml
```

## Description

[Section titled “Description”](#description)

Transforms the input event stream to YAML byte stream.

## Examples

[Section titled “Examples”](#examples)

### Convert a JSON file into a YAML file

[Section titled “Convert a JSON file into a YAML file”](#convert-a-json-file-into-a-yaml-file)

```tql
load_file "input.json"
read_json
write_yaml
save_file "output.yaml"
```

## See Also

[Section titled “See Also”](#see-also)

[`read_yaml`](/reference/operators/read_yaml), [`parse_yaml`](/reference/functions/parse_yaml), [`print_yaml`](/reference/functions/print_yaml)

# write_zeek_tsv

Transforms event stream into Zeek Tab-Separated Value byte stream.

```tql
write_zeek_tsv [set_separator=str, empty_field=str, unset_field=str, disable_timestamp_tags=bool]
```

## Description

[Section titled “Description”](#description)

The [Zeek](https://zeek.org) network security monitor comes with its own tab-separated value (TSV) format for representing logs. This format includes additional header fields with field names, type annotations, and additional metadata.

The `write_zeek_tsv` operator (re)generates the TSV metadata based on Tenzir’s internal schema. Tenzir’s data model is a superset of Zeek’s, so the conversion into Zeek TSV may be lossy. The Zeek types `count`, `real`, and `addr` map to the respective Tenzir types `uint64`, `double`, and `ip`.

### `set_separator = str (optional)`

[Section titled “set\_separator = str (optional)”](#set_separator--str-optional)

Specifies the set separator.

Defaults to `\x09`.

### `empty_field = str (optional)`

[Section titled “empty\_field = str (optional)”](#empty_field--str-optional)

Specifies the separator for empty fields.

Defaults to `(empty)`.

### `unset_field = str (optional)`

[Section titled “unset\_field = str (optional)”](#unset_field--str-optional)

Specifies the separator for unset “null” fields.

Defaults to `-`.

### `disable_timestamp_tags = bool (optional)`

[Section titled “disable\_timestamp\_tags = bool (optional)”](#disable_timestamp_tags--bool-optional)

Disables the `#open` and `#close` timestamp tags.

Defaults to `false`.

## Examples

[Section titled “Examples”](#examples)

### Write pipelines results in Zeek TSV format

[Section titled “Write pipelines results in Zeek TSV format”](#write-pipelines-results-in-zeek-tsv-format)

```tql
subscribe "zeek-logs"
where duration > 2s and id.orig_p != 80
write_zeek_tsv
save_file "filtered_conn.log"
```

## See Also

[Section titled “See Also”](#see-also)

[`read_zeek_json`](/reference/operators/read_zeek_json), [`read_zeek_tsv`](/reference/operators/read_zeek_tsv)

# yara

Executes YARA rules on byte streams.

```tql
yara rule:list<string>, [blockwise=bool, compiled_rules=bool, fast_scan=bool]
```

## Description

[Section titled “Description”](#description)

The `yara` operator applies [YARA](https://virustotal.github.io/yara/) rules to an input of bytes, emitting rule context upon a match.

![YARA Operator](/_astro/yara-operator.excalidraw.CVYOkp4d_19DKCs.svg)

We modeled the operator after the official [`yara` command-line utility](https://yara.readthedocs.io/en/stable/commandline.html) to enable a familiar experience for the command users. Similar to the official `yara` command, the operator compiles the rules by default, unless you provide the option `compiled_rules=true`. To quote from the above link:

> This is a security measure to prevent users from inadvertently using compiled rules coming from a third-party. Using compiled rules from untrusted sources can lead to the execution of malicious code in your computer.

The operator uses a YARA *scanner* under the hood that buffers blocks of bytes incrementally. Even though the input arrives in non-contiguous blocks of memories, the YARA scanner engine support matching across block boundaries. For continuously running pipelines, use the `blockwise=true` option that considers each block as a separate unit. Otherwise the scanner engine would simply accumulate blocks but never trigger a scan.

### `rule: list<string>`

[Section titled “rule: list\<string>”](#rule-liststring)

The path to the YARA rule(s).

If the path is a directory, the operator attempts to recursively add all contained files as YARA rules.

### `blockwise = bool (optional)`

[Section titled “blockwise = bool (optional)”](#blockwise--bool-optional)

Whether to match on every byte chunk instead of triggering a scan when the input exhausted.

This option makes sense for never-ending dataflows where each chunk of bytes constitutes a self-contained unit, such as a single file.

### `compiled_rules = bool (optional)`

[Section titled “compiled\_rules = bool (optional)”](#compiled_rules--bool-optional)

Whether to interpret the rules as compiled.

When providing this flag, you must exactly provide one rule path as positional argument.

### `fast_scan = bool (optional)`

[Section titled “fast\_scan = bool (optional)”](#fast_scan--bool-optional)

Enable fast matching mode.

## Examples

[Section titled “Examples”](#examples)

The examples below show how you can scan a single file and how you can create a simple rule scanning service.

### Perform one-shot scanning of files

[Section titled “Perform one-shot scanning of files”](#perform-one-shot-scanning-of-files)

Scan a file with a set of YARA rules:

```tql
load_file "evil.exe", mmap=true
yara "rule.yara"
```

Memory Mapping Optimization

The `mmap` flag is merely an optimization that constructs a single chunk of bytes instead of a contiguous stream. Without `mmap=true`, [`load_file`](/reference/operators/load_file) generates a stream of byte chunks and feeds them incrementally to the `yara` operator. This also works, but performance is better due to memory locality when using `mmap`.

Let’s unpack a concrete example:

```plaintext
rule test {
  meta:
    string = "string meta data"
    integer = 42
    boolean = true


  strings:
    $foo = "foo"
    $bar = "bar"
    $baz = "baz"


  condition:
    ($foo and $bar) or $baz
}
```

You can produce test matches by feeding bytes into the `yara` operator. You will get one `yara.match` per matching rule:

```tql
{
  rule: {
    identifier: "test",
    namespace: "default",
    tags: [],
    meta: {
      string: "string meta data",
      integer: 42,
      boolean: true
    },
    strings: {
      "$foo": "foo",
      "$bar": "bar",
      "$baz": "baz"
    }
  },
  matches: {
    "$foo": [
      {
        data: "Zm9v",
        base: 0,
        offset: 0,
        match_length: 3
      }
    ],
    "$bar": [
      {
        data: "YmFy",
        base: 0,
        offset: 4,
        match_length: 3
      }
    ]
  }
}
```

Each match has a `rule` field describing the rule and a `matches` record indexed by string identifier to report a list of matches per rule string.

# Platform command line interface

The Tenzir Platform command-line interface (CLI) allows you to interact with the Tenzir Platform from the command line to manage workspaces and nodes.

## Installation

[Section titled “Installation”](#installation)

Install the [`tenzir-platform`](https://pypi.org/project/tenzir-platform/) package from PyPI.

```text
pip install tenzir-platform
```

## Global Options

[Section titled “Global Options”](#global-options)

The following options are available for all `tenzir-platform` commands:

* `-v, --verbose`: Enable verbose logging for additional error information and debugging output.

## Environment Variable Configuration

[Section titled “Environment Variable Configuration”](#environment-variable-configuration)

The Tenzir Platform CLI supports several environment variables to configure authentication and platform connection settings. All configuration variables for the CLI share the prefix `TENZIR_PLATFORM_CLI_`.

### Supported Settings

[Section titled “Supported Settings”](#supported-settings)

* `TENZIR_PLATFORM_CLI_API_ENDPOINT`: The URL of the Tenzir Platform API instance to connect to. Defaults to `https://rest.tenzir.app/production-v1`.

* `TENZIR_PLATFORM_CLI_EXTRA_HEADERS`: Additional headers that the CLI should send with any request to the Tenzir Platform. Must provide as a JSON object.

* `TENZIR_PLATFORM_CLI_ISSUER_URL`: The OIDC issuer URL for authentication. Defaults to the issuer URL that the public Tenzir Platform instance uses at <https://app.tenzir.com>.

* `TENZIR_PLATFORM_CLI_CLIENT_ID`: The client ID for the CLI client. Defaults to the client id that the public Tenzir Platform instance uses at <https://app.tenzir.com>.

* `TENZIR_PLATFORM_CLI_CLIENT_SECRET`: The client secret for the CLI client. When set, the CLI automatically attempts to use the client credentials flow instead of the device code flow for authentication.

* `TENZIR_PLATFORM_CLI_CLIENT_SECRET_FILE`: Path to a file containing the client secret. Alternative to providing the secret directly via `TENZIR_PLATFORM_CLI_CLIENT_SECRET`.

* `TENZIR_PLATFORM_CLI_ID_TOKEN`: If provided, skip the login workflow completely and use this token for authentication.

* `TENZIR_PLATFORM_CLI_AUDIENCE`: Override the OIDC audience parameter. Defaults to the client ID. Required for non-interactive logins with some identity providers like Microsoft Entra.

* `TENZIR_PLATFORM_CLI_SCOPE`: Override the default OIDC scopes. Defaults to `openid email` for device code flow and `openid` for client credentials flow. Use this to customize the requested permissions during authentication.

## Authentication

[Section titled “Authentication”](#authentication)

### Synopsis

[Section titled “Synopsis”](#synopsis)

```text
tenzir-platform auth login
tenzir-platform workspace list
tenzir-platform workspace select <workspace_id>
```

### Description

[Section titled “Description”](#description)

The `tenzir-platform auth login` command authenticates you with the platform.

The `tenzir-platform workspace list` command shows all workspaces available to you. The `tenzir-platform workspace select` command selects a workspace for subsequent operations.

#### `<workspace_id>`

[Section titled “\<workspace\_id>”](#workspace_id)

The unique ID of the workspace, as shown in `tenzir-platform workspace list`.

## Manage Nodes

[Section titled “Manage Nodes”](#manage-nodes)

### Synopsis

[Section titled “Synopsis”](#synopsis-1)

```text
tenzir-platform node list
tenzir-platform node ping <node_id>
tenzir-platform node create [--name <node_name>]
tenzir-platform node delete <node_id>
tenzir-platform node run [--name <node_name>] [--image <container_image>]
```

### Description

[Section titled “Description”](#description-1)

The following commands interact with the selected workspace:

* `tenzir-platform node list` lists all nodes in the selected workspace, including their ID, name, and connection status.
* `tenzir-platform node ping` pings the specified node.
* `tenzir-platform node create` registers a new node at the platform. This command creates a new API key that a node can use to connect to the platform. It does not start or configure a node.
* `tenzir-platform node delete` removes a node from the platform. This command does not stop the node; it only removes it from the platform.
* `tenzir-platform node run` creates and registers an ad-hoc node, then starts it on the local host. This command requires Docker Compose. The platform deletes the temporary node when you stop the `run` command.

#### `<node_id>`

[Section titled “\<node\_id>”](#node_id)

The unique ID of the node, as shown in `tenzir-platform node list`.

#### `<node_name>`

[Section titled “\<node\_name>”](#node_name)

The name of the node as shown in the app.

#### `<container_image>`

[Section titled “\<container\_image>”](#container_image)

The Docker image to use for the ad-hoc created node. We recommend using one of the following images:

* `tenzir/tenzir:latest` to use the latest release.
* `tenzir/tenzir:main` to use the current development version.
* `tenzir/tenzir:v5` to pin to a major release.
* `tenzir/tenzir:v5.1` to pin to a minor release.
* `tenzir/tenzir:v5.1.3` to use a specific release.

## Manage Secrets

[Section titled “Manage Secrets”](#manage-secrets)

The Tenzir Platform provides secret storage that pipelines running on a Tenzir node can access.

### Synopsis

[Section titled “Synopsis”](#synopsis-2)

```text
tenzir-platform secret add <name> [--file=<file>] [--value=<value>] [--env]
tenzir-platform secret update <secret_id> [--file=<file>] [--value=<value>] [--env]
tenzir-platform secret delete <secret_id>
tenzir-platform secret list [--json]
```

### Description

[Section titled “Description”](#description-2)

The following commands manage secrets in the Tenzir Platform:

* `tenzir-platform secret add` adds a new secret to the platform. You can provide the secret value directly via the `--value` option, read it from a file using the `--file` option, or source it from an environment variable using the `--env` flag. The platform identifies the secret by the provided `<name>`.

* `tenzir-platform secret update` updates an existing secret identified by `<secret_id>`. Like adding a secret, you can provide the new value via `--value`, `--file`, or `--env`.

* `tenzir-platform secret delete` removes a secret from the platform. The command identifies the secret to delete by its `<secret_id>`.

* `tenzir-platform secret list` lists all secrets available in the platform. Use the `--json` flag to output the list in JSON format for easier integration with other tools.

#### `<name>`

[Section titled “\<name>”](#name)

The unique name to identify the secret. Pipelines use this name when they access the secret.

#### `<secret_id>`

[Section titled “\<secret\_id>”](#secret_id)

The unique identifier of the secret, as shown in the output of `tenzir-platform secret list`.

#### `--file=<file>`

[Section titled “--file=\<file>”](#--filefile)

Specifies a file containing the secret value. The platform securely stores the file’s contents as the secret value.

#### `--value=<value>`

[Section titled “--value=\<value>”](#--valuevalue)

Specifies the secret value directly as a command-line argument.

#### `--env`

[Section titled “--env”](#--env)

Indicates that the command should source the secret value from an environment variable. You must set the environment variable before you run the command.

#### `--json`

[Section titled “--json”](#--json)

Outputs the list of secrets in JSON format when used with the `tenzir-platform secret list` command.

## Manage external secret stores

[Section titled “Manage external secret stores”](#manage-external-secret-stores)

You can configure workspaces to use external secret stores instead of the Tenzir Platform’s built-in secret store.

Currently, the platform only supports [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) as an external store.

By default, the platform mounts external secret stores as read-only. You can’t add or update secrets from the CLI or web interface. Some external secret store implementations may offer write access options.

### Synopsis

[Section titled “Synopsis”](#synopsis-3)

```text
tenzir-platform secret store add aws
    --region=<region>
    --assumed-role-arn=<assumed_role_arn>
    [--name=<name>]
    [--prefix=<prefix>]
    [--access-key-id=<key_id>]
    [--secret-access-key=<key>]
tenzir-platform secret store set-default <store_id>
tenzir-platform secret store delete <store_id>
tenzir-platform secret store list [--json]
```

### Description

[Section titled “Description”](#description-3)

When a node accesses a secret using the `secret("foo")` function in a pipeline, the platform looks up the secret named `foo` in the workspace’s default secret store and returns the value to the node.

When using an external secret store, the platform needs the necessary permissions to read secret values from that store.

For AWS Secrets Manager, the Tenzir Platform uses [AWS Security Token Service (STS)](https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html) to assume a role with the necessary permissions. You must create this role and configure one of the following options:

1. Create an IAM user. Download access keys and pass them via the `--access-key-id` and `--secret-access-key` arguments. This approach is the easiest to set up but only acceptable for self-hosted testing or development instances of the Tenzir Platform because it stores long-term credentials in the Tenzir Platform.

2. Assign the permissions to the task role of the websocket gateway. We recommend this option if you’re running the Sovereign Edition of the Platform and deploying it to your own AWS account.

3. Assign the permission to assume the configured role to Tenzir’s AWS account with the ID `660178929208`. This option works only with our publicly hosted platform instance on `https://app.tenzir.com`.

We plan to add an OIDC-based option as an alternative to options 1 and 3.

## Manage Alerts

[Section titled “Manage Alerts”](#manage-alerts)

### Synopsis

[Section titled “Synopsis”](#synopsis-4)

```text
tenzir-platform alert add <node> <duration> <webhook_url> [<webhook_body>]
tenzir-platform alert delete <alert_id>
tenzir-platform alert list
```

### Description

[Section titled “Description”](#description-4)

The following commands set up alerts for specific nodes. When a node remains disconnected for the configured duration, the alert triggers by sending a POST request to the configured webhook URL.

#### `<node>`

[Section titled “\<node>”](#node)

The node to monitor. You can provide either a node ID or a node name, as long as the name is unambiguous.

#### `<duration>`

[Section titled “\<duration>”](#duration)

The amount of time to wait between the node disconnect and triggering the alert.

#### `<webhook_url>`

[Section titled “\<webhook\_url>”](#webhook_url)

The platform sends a POST request to this URL when the alert triggers.

#### `<webhook_body>`

[Section titled “\<webhook\_body>”](#webhook_body)

The body to send with the webhook. Must be valid JSON. The body may contain `$NODE_NAME`, which the platform replaces with the name of the node that triggered the alert.

Defaults to `{"text": "Node $NODE_NAME disconnected for more than {duration}s"}`, where the platform sets `$NODE_NAME` and `{duration}` dynamically from the CLI parameters.

### Example

[Section titled “Example”](#example)

Consider nodes like this:

```text
$ tenzir-platform node list
🟢 Node-1 (n-w2tjezz3)
🟢 Node-2 (n-kzw21299)
🔴 Node-3 (n-ie2tdgca)
```

We want to receive a Slack notification whenever Node-3 is offline for more than three minutes. First, we create a webhook as described in the [Slack docs](https://api.slack.com/messaging/webhooks). Next, we configure the alert in the Tenzir Platform:

```text
tenzir-platform alert add Node-3 3m "https://hooks.slack.com/services/XXXXX/YYYYY/ZZZZZ" '{"text": "Alert! Look after node $NODE_NAME"}'
```

If Node-3 doesn’t reconnect within three minutes, a message appears in the configured Slack channel.

## Manage workspaces

[Section titled “Manage workspaces”](#manage-workspaces)

On-premise setup required

This CLI functionality requires an on-premise platform deployment, available with the [Sovereign Edition](https://tenzir.com/pricing).

These CLI commands are available only to local platform administrators. The [`TENZIR_PLATFORM_OIDC_ADMIN_RULES` variable](/guides/platform-setup/configure-identity-provider) defines who’s an administrator in your platform deployment.

### Synopsis

[Section titled “Synopsis”](#synopsis-5)

```text
tenzir-platform admin list-global-workspaces
tenzir-platform admin create-workspace <owner_namespace> <owner_id> [--name <workspace_name>]
tenzir-platform admin delete-workspace <workspace_id>
```

### Description

[Section titled “Description”](#description-5)

The `tenzir-platform admin list-global-workspaces`, `tenzir-platform admin create-workspace`, and `tenzir-platform admin delete-workspace` commands list, create, or delete workspaces, respectively.

#### `<owner_namespace>`

[Section titled “\<owner\_namespace>”](#owner_namespace)

Either `user` or `organization`, depending on whether the workspace associates with a user or an organization.

The selected namespace determines the *default* access rules for the workspace:

* For a user workspace, the platform creates a single access rule that allows access to the user whose user ID matches the given `owner_id`.
* For an organization workspace, the platform creates no rules by default. You must manually add them using the `add-auth-rule` subcommand described below.

#### `<owner_id>`

[Section titled “\<owner\_id>”](#owner_id)

The unique ID of the workspace owner:

* If `<owner_namespace>` is `user`, this matches the user’s `sub` claim in the OIDC token.
* If `<owner_namespace>` is `organization`, this is an arbitrary string that uniquely identifies the organization the workspace belongs to.

#### `--name <workspace_name>`

[Section titled “--name \<workspace\_name>”](#--name-workspace_name)

The name of the workspace as shown in the app.

#### `<workspace_id>`

[Section titled “\<workspace\_id>”](#workspace_id-1)

The unique ID of the workspace, as shown in `tenzir-platform workspace list` or `tenzir-platform admin list-global-workspaces`.

## Configure access rules

[Section titled “Configure access rules”](#configure-access-rules)

On-premise setup required

You can use this CLI functionality only with an on-premise platform deployment, which is available to users of the [Sovereign Edition](https://tenzir.com/pricing).

These CLI commands are available only to local platform administrators. The [`TENZIR_PLATFORM_OIDC_ADMIN_RULES` variable](/guides/platform-setup/configure-identity-provider) defines who’s an administrator in your platform deployment.

### Synopsis

[Section titled “Synopsis”](#synopsis-6)

```text
tenzir-platform admin list-auth-rules <workspace_id>


tenzir-platform admin add-auth-rule allow-all <workspace_id>
tenzir-platform admin add-auth-rule user <workspace_id> <user_id>
tenzir-platform admin add-auth-rule
    email-domain <workspace_id> <connection> <domain>
tenzir-platform admin add-auth-rule
    organization-membership <workspace_id> <connection> <organization_claim> <organization>
tenzir-platform admin add-auth-rule
    organization-role <workspace_id> <connection> <roles_claim> <role> <organization_claim> <organization>


tenzir-platform admin delete-auth-rule <workspace_id> <auth_rule_index>
```

### Description

[Section titled “Description”](#description-6)

You can use the `tenzir-platform admin list-auth-rules`, `tenzir-platform admin add-auth-rule`, and `tenzir-platform admin delete-auth-rule` commands to list, create, or delete authentication rules for all users, respectively, if you have admin permissions.

Authentication rules allow you to access the workspace with the provided `<workspace_id>` if your `id_token` matches the configured rule. You gain access to a workspace if any configured rule allows access. The following rules exist:

* **Email Suffix Rule**: `tenzir-platform admin add-auth-rule email-domain` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>` and an `email` field that ends with the configured `<domain>`.

* **Organization Membership**: `tenzir-platform admin add-auth-rule organization-membership` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>` and an `<organization_claim>` field that exactly matches the provided `<organization>`.

  Note that you can freely choose the `<organization_claim>` and `<organization>`, so you can also repurpose this rule for generic claims that are not necessarily related to organizations.

* **Organization Role Rule**: `tenzir-platform admin add-auth-rule organization-role` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>`, an `<organization_claim>` field that exactly matches the provided `<organization>`, and a `<roles_claim>` field that must be a list containing a value exactly matching `<role>`.

  We recommend using organization role rules to check if you have a specific role with an organization.

* **User Rule**: `tenzir-platform admin add-auth-rule user` allows access if the `id_token` contains a `sub` field that exactly matches the provided `<user_id>`.

* **Allow all rule**: `tenzir-platform admin add-auth-rule allow-all` allows access to every user. Use this rule to set up a workspace that all users of a platform instance can access.

# Platform Configuration

This page lists configuration settings for the Tenzir Platform.

## Environment variables

[Section titled “Environment variables”](#environment-variables)

The Tenzir Platform runs as a set of containers in a Docker Compose stack. Our [example files](https://github.com/tenzir/platform/tree/main/examples) pick up configuration parameters from environment variables.

To configure the platform, create a `.env` file in the same directory as your `docker-compose.yaml` file and set the environment variables described below.

### General Settings

[Section titled “General Settings”](#general-settings)

You must configure these settings for every platform instance.

```sh
# The docker image tag that you use for platform deployment.
TENZIR_PLATFORM_VERSION=latest


# By default, the Tenzir UI Frontend communicates directly with the Tenzir
# Gateway to get the current status of all connected nodes. When you set this
# to true, the UI backend proxies this communication instead.
TENZIR_PLATFORM_USE_INTERNAL_WS_PROXY=false


# When enabled, this setting allows users to spawn demo nodes that run inside
# the same Docker Compose stack as the platform.
TENZIR_PLATFORM_DISABLE_LOCAL_DEMO_NODES=true


# The Docker image for running demo nodes.
TENZIR_PLATFORM_DEMO_NODE_IMAGE=tenzir/tenzir-node:latest


# Optional file defining the static workspace configuration for this platform.
TENZIR_PLATFORM_CONFIG_FILE=


# To configure administrators, provide a list of authentication rules. Every
# user matching any of the provided rules becomes an administrator of this
# platform instance and can use the `tenzir-platform admin` CLI commands. Use
# the `tenzir-platform tools print-auth-rule` CLI command to get valid rules.
TENZIR_PLATFORM_ADMIN_RULES=[]


# A random string used to encrypt frontend cookies.
# Generate with `openssl rand -hex 32`.
TENZIR_PLATFORM_INTERNAL_AUTH_SECRET=


# A random string used to generate user keys.
# Generate with `openssl rand 32 | base64`.
TENZIR_PLATFORM_INTERNAL_TENANT_TOKEN_ENCRYPTION_KEY=


# A random string the app uses to access restricted API endpoints.
# Generate with `openssl rand -hex 32`.
TENZIR_PLATFORM_INTERNAL_APP_API_KEY=
```

### External Connectivity

[Section titled “External Connectivity”](#external-connectivity)

These settings define the outward-facing interface of the Tenzir Platform.

```sh
# The domain where users can reach the Tenzir UI, e.g.,
# `https://app.tenzir.example`. Route this to the `app` service through your
# external HTTPS proxy.
TENZIR_PLATFORM_UI_ENDPOINT=


# The domain where the API is reachable, e.g., `https://api.tenzir.example`.
# Route this to the `platform` service through your external HTTPS proxy.
TENZIR_PLATFORM_API_ENDPOINT=


# The endpoint where Tenzir nodes connect. Use a URL with `ws://` or `wss://`
# scheme, e.g., `wss://nodes.tenzir.example`. Route this to the
# `websocket-gateway` service through your external HTTPS proxy.
TENZIR_PLATFORM_NODES_ENDPOINT=


# The URL where the platform exposes blob storage, e.g.,
# `https://downloads.tenzir.example`. If you use the bundled blob storage,
# route this to the `seaweed` service through your external HTTPS proxy.
TENZIR_PLATFORM_DOWNLOADS_ENDPOINT=
```

### Identity Provider

[Section titled “Identity Provider”](#identity-provider)

Create OAuth clients for the Tenzir Platform in your identity provider and fill in the values below to enable platform connectivity.

```sh
# A short identifier for the OIDC provider (e.g., 'auth0', 'keycloak')
TENZIR_PLATFORM_OIDC_PROVIDER_NAME=


# The OIDC provider for platform authentication.
TENZIR_PLATFORM_OIDC_PROVIDER_ISSUER_URL=


# A JSON object (or array of objects) containing the OIDC issuer and audiences that the platform
# accepts. Example: '{"issuer": "keycloak.example.org", "audiences": ["tenzir_platform"]}'
TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES=


# The client ID for the Tenzir Platform CLI.
TENZIR_PLATFORM_OIDC_CLI_CLIENT_ID=


# The client ID and client secret for the Tenzir UI.
TENZIR_PLATFORM_OIDC_UI_CLIENT_ID=
TENZIR_PLATFORM_OIDC_UI_CLIENT_SECRET=
```

### Database

[Section titled “Database”](#database)

You need to specify the following environment variables so the Tenzir Platform can connect to a postgres instance.

```sh
TENZIR_PLATFORM_POSTGRES_USER=
TENZIR_PLATFORM_POSTGRES_PASSWORD=
TENZIR_PLATFORM_POSTGRES_DB=
TENZIR_PLATFORM_POSTGRES_HOSTNAME=
```

### Blob Storage

[Section titled “Blob Storage”](#blob-storage)

```sh
# When using S3 or another external blob storage, create the bucket and provide
# a valid access key with read and write permissions. When using the bundled
# Seaweed instance, set these values to arbitrary strings.
TENZIR_PLATFORM_INTERNAL_BUCKET_NAME=
TENZIR_PLATFORM_INTERNAL_ACCESS_KEY_ID=
TENZIR_PLATFORM_INTERNAL_SECRET_ACCESS_KEY=
```

## Configuration File

[Section titled “Configuration File”](#configuration-file)

Currently, the configuration file supports only static workspace configuration.

```yaml
---
workspaces:
  static0:
    # The name of this workspace
    name: Tenzir


    # The category for this workspace in the workspace switcher.
    category: Statically Configured Workspaces


    # The icon to use for this workspace.
    icon-url: https://storage.googleapis.com/tenzir-public-data/icons/tenzir-logo-square.svg


    # Nodes use this token to connect to the workspace as ephemeral nodes.
    token: wsk_e9ee76d4faf4b213745dd5c99a9be11f501d7009ded63f2d5NmDS38vXR
    #  - or -
    # token-file: /path/to/token


    # All users can access this workspace.
    auth-rules:
      - { "auth_fn": "auth_allow_all" }


    # Example dashboard definition.
    dashboards:
      dashboard1:
        name: Example Dashboard
        cells:
          - name: Dashboard 1
            definition: |
              partitions
              where not internal
              summarize events=sum(events), schema
              sort -events
            type: table
            x: 0
            y: 0
            w: 12
            h: 12
```

# Test Framework

The [`tenzir-test`](https://github.com/tenzir/test) harness discovers and runs integration tests for pipelines, fixtures, and custom runners. Use this page as a reference for concepts, configuration, and CLI details. For step-by-step walkthroughs, see the guides for [writing tests](/guides/testing/write-tests), [creating fixtures](/guides/testing/create-fixtures), and [adding custom runners](/guides/testing/add-custom-runners).

## Install

[Section titled “Install”](#install)

`tenzir-test` ships as a Python package that requires Python 3.12 or later. Install it with [`uv`](https://docs.astral.sh/uv/) (or `pip`) and verify the console script:

```sh
uv add tenzir-test
uvx tenzir-test --help
```

## Core concepts

[Section titled “Core concepts”](#core-concepts)

* **Project root** – Directory passed to `--root`; typically contains `fixtures/`, `inputs/`, `runners/`, and `tests/`.
* **Mode** – Auto-detected as *project* or *package*. A `package.yaml` in the current directory (or its parent when you run from `<package>/tests`) switches to package mode.
* **Test** – Any supported file under `tests/`; frontmatter controls execution.
* **Runner** – Named strategy that executes a test (`tenzir`, `python`, custom entries).
* **Fixture** – Reusable environment provider registered under `fixtures/` and requested via frontmatter.
* **Suite** – Directory-owned group of tests that share fixtures and run sequentially. Declare it with `suite:` in a `test.yaml`; all descendants join automatically.
* **Input** – Data accessed with `TENZIR_INPUTS`; defaults to `<root>/inputs` but you can override it per directory or per test with an `inputs:` setting.
* **Scratch directory** – Ephemeral workspace exposed as `TENZIR_TMP_DIR` during each test run.
* **Artifact / Baseline** – Runner output persisted next to the test; regenerate with `--update`.
* **Configuration sources** – Frontmatter plus inherited `test.yaml` files; `tenzir.yaml` still configures the Tenzir binary.

A typical project layout looks like this:

```text
project-root/
├── fixtures/
│   └── __init__.py
├── inputs/
│   └── sample.ndjson
├── runners/
│   └── __init__.py
└── tests/
    ├── alerts/
    │   ├── sample.tql
    │   └── sample.txt
    └── python/
        └── quick-check.py
```

For a package layout (with `package.yaml`), the structure may look like:

```text
my-package/
├── package.yaml
├── operators/
│   └── custom-op.tql
├── pipelines/
│   └── smoke.tql
└── tests/
    ├── inputs/
    │   └── sample.ndjson
    ├── fixtures/
    │   └── __init__.py
    ├── runners/
    │   └── __init__.py
    └── pipelines/
        ├── custom-op.tql
        └── custom-op.txt
```

## Execution modes and packages

[Section titled “Execution modes and packages”](#execution-modes-and-packages)

* The harness treats `--root` as the project root. If that directory (or its parent when named `tests`) contains `package.yaml`, `tenzir-test` switches to **package mode** and exposes:

  * `TENZIR_PACKAGE_ROOT` – Absolute package directory.
  * `TENZIR_INPUTS` – `<package>/tests/inputs/` unless a directory `test.yaml` or the test frontmatter overrides it.
  * `--package-dirs=<package>` – Passed automatically to the `tenzir` binary.

* Without a manifest the harness stays in **project mode**, recursively discovers tests under `tests/`, and applies global fixtures, runners, and inputs.

## CLI reference

[Section titled “CLI reference”](#cli-reference)

Run the tests from the project root:

```sh
uvx tenzir-test
```

Useful options:

* `--tenzir-binary /path/to/tenzir`: Override binary lookup.
* `--tenzir-node-binary /path/to/tenzir-node`: Override node binary path.
* `--update`: Rewrite reference artifacts next to each test.
* `--purge`: Remove generated artifacts (diffs, text outputs) from previous runs.
* `--jobs N`: Control concurrency (`4 * CPU cores` by default).
* `--coverage` and `--coverage-source-dir`: Enable LLVM coverage.
* `-k`, `--keep`: Preserve per-test scratch directories instead of deleting them (same as setting `TENZIR_KEEP_TMP_DIRS=1`).
* `--debug`: Emit framework-level diagnostics (fixture lifecycle, discovery notes, comparison targets, etc.). The same mode is available via `TENZIR_TEST_DEBUG=1`.
* `--summary`: Print the tabular breakdown and failure tree after each project.
* `--diff/--no-diff`: Toggle unified diff output for failed comparisons. Diffs are shown by default; disable them when you only need aggregated statistics.
* `--diff-stat/--no-diff-stat`: Show (or suppress) the per-file change counter, which summarises additions and deletions even when the diff body is hidden.
* `-p`, `--passthrough`: Stream raw stdout/stderr to the terminal instead of comparing against reference artifacts. The harness forces single-job execution (overriding `--jobs` when necessary) and ignores `--update` while passthrough is active.
* `-a`, `--all-projects`: Run the root project together with any satellites provided on the command line.

Set `TENZIR_TEST_DEBUG=1` in CI when you want the same diagnostics without passing `--debug` on the command line.

## Selections

[Section titled “Selections”](#selections)

A *selection* is the ordered list of positional paths you pass after the CLI flags. Each element can point to a single test file, a directory, or an entire project. The harness resolves every element relative to the current working directory first and then relative to the root project. How you shape the selection determines which projects run:

* No positional arguments → run every test in the root project.
* Paths inside the root project → run only those targets (plus any explicitly named satellites).
* Paths that resolve to satellite projects → run those satellites, skipping the root unless you also request it.

Use `--all-projects` when you want the root project to execute alongside a selection that only names satellites. This keeps the CLI predictable: the selection lists the exact satellites you care about, and the flag opts the root back in without duplicating its path on the command line.

## Suites

[Section titled “Suites”](#suites)

Suites let you run several tests under one shared fixture lifecycle. Declare a suite in a directory-level `test.yaml`; the definition applies to every test under that directory, including nested subdirectories.

tests/http/test.yaml

```yaml
suite: smoke-http
fixtures: [http]
timeout: 45
```

Key rules:

* Suites are directory-owned. Once a `test.yaml` sets `suite`, all descendants belong to that suite. Put tests that should remain independent outside the suite directory or in a sibling directory with a different suite.
* Per-test frontmatter may not declare `suite`.
* Suite members inherit the directory defaults and can still override most keys on a per-file basis. The exceptions are `fixtures` and `retry`, which must be defined at the directory level once a suite is active so every member agrees on the shared lifecycle. Outside suites you can still set those keys directly in frontmatter.
* The harness runs suite members sequentially in lexicographic order of their relative paths. Each suite occupies a single worker, but different suites can run in parallel when `--jobs` allows it.
* The CLI executes all suites before any remaining standalone tests so shared fixtures start and stop predictably.
* Run the directory that defines the suite (for example `tenzir-test tests/http`) when you want to focus on it. Selecting an individual member now raises an error so every run exercises the full lifecycle and shared fixtures stay in sync.

## Run a subset of tests

[Section titled “Run a subset of tests”](#run-a-subset-of-tests)

```sh
uvx tenzir-test tests/alerts/high-severity.tql
```

You can list multiple paths in a single invocation. `tenzir-test` wires every argument into the same runner and fixture registry, so you can mix scenarios from the project and external checkouts:

```sh
uvx tenzir-test tests/alerts ../contrib/plugins/*/tests
```

### Run multiple projects with one command

[Section titled “Run multiple projects with one command”](#run-multiple-projects-with-one-command)

Pass additional project directories after `--root` to execute several projects in one go. Include `--all-projects` so the root executes next to its satellites. The directory given to `--root` acts as the **root project**; all other directories are treated as **satellites**:

```sh
uvx tenzir-test --root example-project --all-projects example-satellite
```

Key rules:

* The root project provides the baseline configuration (fixtures, runners, `test.yaml` defaults, inputs). Satellites layer their own fixtures and runners on top; duplicate names raise an error so conflicts surface early.
* Paths printed in the CLI summary are relative to the working directory. The harness announces each project before running it and lists the runner mix per project for quick insight.
* You can target subsets inside each project with additional positional arguments (`tenzir-test --root main --all-projects secondary tests/smoke`). When you skip `--root` entirely and only list satellite directories, the harness runs those satellites in isolation.
* Satellites keep their own `tests/`, `inputs/`, `fixtures/`, and `runners/` folders. A root project can host shared assets that satellites reuse without duplication—for example, the example repository includes an `example-satellite/` directory that consumes the `xxd` runner exported by the root project while defining a satellite-specific fixture.

To regenerate baselines while targeting a specific binary and project root:

```sh
TENZIR_BINARY=/opt/tenzir/bin/tenzir \
TENZIR_NODE_BINARY=/opt/tenzir/bin/tenzir-node \
uvx tenzir-test --root tests --update
```

## Runners

[Section titled “Runners”](#runners)

| Runner   | Command/behavior                       | Input extension | Artifact |
| -------- | -------------------------------------- | --------------- | -------- |
| `tenzir` | `tenzir -f <test>`                     | `.tql`          | `.txt`   |
| `python` | Execute with the active Python runtime | `.py`           | `.txt`   |
| `shell`  | `sh -eu <test>` via the harness helper | `.sh`           | varies   |

Selection flow:

1. The harness chooses the first registered runner that claimed the file extension.
2. Default suffix mapping applies when no runner explicitly claims an extension: `.tql → tenzir`, `.py → python`, `.sh → shell`.
3. A `runner: <name>` frontmatter entry overrides the automatic choice.
4. If no runner claims the extension and none is specified in frontmatter, the harness fails with an error instead of guessing.

### Shell runner

[Section titled “Shell runner”](#shell-runner)

Place scripts (for example under `tests/shell/`) with the `.sh` suffix to run them under `bash -eu` via the `shell` runner. The harness also prepends `<root>/_shell` to `PATH` so project-specific helper binaries become discoverable. The runner captures stdout and stderr (like `2>&1`) and compares the combined output with `<test>.txt`; run `tenzir-test --update path/to/test.sh` when you need to refresh the baseline.

Register custom runners in `runners/__init__.py` via `tenzir_test.runners.register()` or the `@tenzir_test.runners.startup()` decorator. Use `replace=True` to override a bundled runner or `register_alias()` to publish alternate names.

The [runner guide](/guides/testing/add-custom-runners) contains a full example (`XxdRunner`).

### Passthrough-aware subprocesses

[Section titled “Passthrough-aware subprocesses”](#passthrough-aware-subprocesses)

When passthrough mode is active the harness streams stdout/stderr directly to the terminal and skips reference comparisons. Runner implementations can respect this automatically by spawning processes through `tenzir_test.run.run_subprocess(...)`. The helper captures output when the harness needs it and inherits the parent descriptors otherwise. Pass `force_capture=True` when your runner must collect stdout even in passthrough mode. If you need to branch on the current behavior, call `tenzir_test.run.get_harness_mode()` or `tenzir_test.run.is_passthrough_enabled()`.

The harness cycles between three internal modes:

* `HarnessMode.COMPARE` – default behavior; compare actual output with stored baselines.
* `HarnessMode.UPDATE` – engaged when you pass `--update`; runners should overwrite reference files.
* `HarnessMode.PASSTHROUGH` – enabled via `-p/--passthrough`; stream output directly without touching baselines.

`get_harness_mode()` returns the current enum value so custom runners can adapt logic if needed.

## Configuration and frontmatter

[Section titled “Configuration and frontmatter”](#configuration-and-frontmatter)

`tenzir-test` merges configuration sources in this order (later wins):

1. Project defaults (`test.yaml` files, applied per directory).
2. Per-test frontmatter (YAML for `.tql`/`.xxd`, `# key: value` comments for Python and shell scripts).

Common frontmatter keys:

| Key        | Type            | Default   | Description                                            |
| ---------- | --------------- | --------- | ------------------------------------------------------ |
| `runner`   | string          | by suffix | Runner name (`tenzir`, `python`, `shell`, custom).     |
| `fixtures` | list of strings | `[]`      | Requested fixtures; use `fixture` for a single value.  |
| `timeout`  | integer (s)     | `30`      | Command timeout. (`--coverage` multiplies it by five.) |
| `error`    | boolean         | `false`   | Expect a non-zero exit code.                           |
| `skip`     | string          | unset     | Mark the test as skipped (reason required).            |
| `inputs`   | string          | project   | Override `TENZIR_INPUTS` for this directory or test.   |
| `retry`    | integer         | `1`       | Total attempt budget for flaky tests (see below).      |

`test.yaml` files accept the same keys and apply recursively to child directories. A relative `inputs:` value resolves against the file that defines it, so `inputs: ../data` inside `tests/alerts/test.yaml` points at `tests/data/`. Frontmatter values follow the same rule and win over directory defaults. Adjacent `tenzir.yaml` files still configure the Tenzir binary; the harness appends `--config=<file>` automatically. The lookup keeps working even when you point the CLI at extra directories on the command line.

`retry` represents the **total number of attempts** the harness should make before declaring the test failed. Intermediate attempts stay quiet; the final outcome line includes `attempts=N/M` whenever the budget exceeds one. Keep the value small and treat it as a temporary guardrail while you fix the underlying flakiness.

### Tenzir configuration files

[Section titled “Tenzir configuration files”](#tenzir-configuration-files)

* The harness inspects the directory that owns each test. If it finds `tenzir.yaml`, it appends `--config=<path>` to every invocation of the bundled `tenzir`/`tql`/`diff` runners. The path also seeds `TENZIR_CONFIG` unless you set that variable yourself. Custom runners that call the Tenzir binary should either use `run.get_test_env_and_config_args(test)` or honour the exported environment variables explicitly.
* The built-in `node` fixture uses the same discovery process and starts `tenzir-node` from the directory that owns the test file, so relative paths inside `tenzir-node.yaml` resolve against the test location. See the [built-in node fixture](#built-in-node-fixture) section for precedence rules.
* This lets you keep one config for CLI-driven scenarios while passing a different config to the embedded node, for example to tweak endpoints or data directories independently.

## Fixtures

[Section titled “Fixtures”](#fixtures)

### Declaring fixtures

[Section titled “Declaring fixtures”](#declaring-fixtures)

* List fixture names in frontmatter (`fixtures: [node, http]`). Importing the project `fixtures` package is enough to register custom fixtures thanks to the side effects in `fixtures/__init__.py`.

* The harness encodes requests in `TENZIR_TEST_FIXTURES` and exposes helper APIs in `tenzir_test.fixtures`:

  * `fixtures()` – Read-only view of active fixtures. Attribute access is supported, e.g. `fixtures().node` returns `True` if the fixture was requested and raises `AttributeError` otherwise.
  * `acquire_fixture("name")` – Manual controller for the named fixture. Use it as a context manager for automatic `start()`/`stop()` or call those methods explicitly to interleave lifecycle steps and optional hooks (for example `kill()` or `restart()`).
  * `require("name")` – Assert that a fixture was requested.
  * `Executor()` – Convenience wrapper that runs Tenzir commands with resolved binaries and timeout budget.

Example use from a Python helper:

```python
from tenzir_test.fixtures import Executor


executor = Executor()
result = executor.run("from_file 'inputs/events.ndjson' | where severity >= 5\n")
assert result.returncode == 0
```

### Built-in node fixture

[Section titled “Built-in node fixture”](#built-in-node-fixture)

* Request the fixture with `fixtures: [node]`; the harness will start `tenzir-node` with the binaries discovered for the current test.

* Configuration precedence:

  1. `TENZIR_NODE_CONFIG` in the environment.
  2. A `tenzir-node.yaml` placed next to the test file (exported automatically).
  3. The Tenzir defaults (no config file).

* The node process inherits the test directory as its current working directory, letting `tenzir-node.yaml` reference files with relative paths (for example `state/` or `schemas/`).

* Each controller reuses its state and cache directories across `start()`/`stop()` cycles. By default they live under the per-test scratch directory (`TENZIR_TMP_DIR/tenzir-node-*`) and are removed once the fixture context ends. Starting a fresh controller (for example in another test run) yields a brand-new workspace.

* The fixture reuses other inherited arguments (for example `--package-dirs=…`) but replaces any existing `--config=` flag so the node process always honours the chosen configuration file.

* Tests can read `TENZIR_NODE_CLIENT_ENDPOINT`, `TENZIR_NODE_CLIENT_BINARY`, `TENZIR_NODE_CLIENT_TIMEOUT`, `TENZIR_NODE_STATE_DIRECTORY`, and `TENZIR_NODE_CACHE_DIRECTORY` from the environment to connect to the spawned node and inspect its working tree.

* Pipelines launched by the bundled Tenzir runners automatically receive `--endpoint=<value>` when this fixture is active, so they talk to the transient node without additional wiring.

* CLI and node configuration are independent: configure the CLI with `tenzir.yaml` and drop a `tenzir-node.yaml` (or set `TENZIR_NODE_CONFIG`) only when the node needs custom settings.

### Registering fixtures

[Section titled “Registering fixtures”](#registering-fixtures)

Implement fixtures in `fixtures/` and register them with `@tenzir_test.fixture()`. Decorate a generator function, yield the environment mapping, and handle cleanup in a `finally` block:

```python
from tenzir_test import fixture




@fixture()
def http():
    server = _start_server()
    try:
        yield {"HTTP_FIXTURE_URL": server.url}
    finally:
        server.stop()
```

`@fixture` also accepts regular callables returning dictionaries, context managers, or `FixtureHandle` instances for advanced scenarios.

The [fixture guide](/guides/testing/create-fixtures) demonstrates an HTTP echo server that exposes `HTTP_FIXTURE_URL` and tears down cleanly.

## Environment variables

[Section titled “Environment variables”](#environment-variables)

`tenzir-test` recognises the following environment variables:

* `TENZIR_TEST_ROOT` – Default test root when `--root` is omitted.
* `TENZIR_BINARY` / `TENZIR_NODE_BINARY` – Override binary discovery.
* `TENZIR_INPUTS` – Preferred data directory. Defaults to the project inputs folder but reflects any `inputs:` override from `test.yaml` or frontmatter.
* `TENZIR_KEEP_TMP_DIRS` – Keep per-test scratch directories (equivalent to `--keep`).
* `TENZIR_TEST_DEBUG` – Enable debug logging (equivalent to `--debug`).

Fixtures often publish additional variables (for example `TENZIR_NODE_CLIENT_*`, `TENZIR_NODE_STATE_DIRECTORY`, `TENZIR_NODE_CACHE_DIRECTORY`, `HTTP_FIXTURE_URL`).

During execution the harness also adds transient variables such as `TENZIR_TMP_DIR` so tests and fixtures can create temporary artifacts without polluting the repository. Combine it with `--keep` (or `TENZIR_KEEP_TMP_DIRS=1`) when you need to inspect the generated files after a run.

## Baselines and artifacts

[Section titled “Baselines and artifacts”](#baselines-and-artifacts)

Regenerate reference output whenever behavior changes intentionally:

```sh
uvx tenzir-test --update
```

`--purge` removes stale artifacts (diffs, temporary files). Keep generated `.txt` files under version control so future runs can diff against them.

## Troubleshooting

[Section titled “Troubleshooting”](#troubleshooting)

* **Missing binaries** – Ensure `tenzir` and `tenzir-node` are on `PATH` or set `TENZIR_BINARY` / `TENZIR_NODE_BINARY` explicitly.
* **Unexpected exits** – Set `error: true` in frontmatter when a non-zero exit is expected.
* **Skipped tests** – Use `skip: reason` to document temporary skips; baseline files can stay empty.
* **Noisy output** – Use `--jobs 1` to serialize worker logs, and enable `--debug` (or set `TENZIR_TEST_DEBUG=1`) when you need to trace comparisons and fixture activity.

## Further reading

[Section titled “Further reading”](#further-reading)

* [Write tests](/guides/testing/write-tests)
* [Create fixtures](/guides/testing/create-fixtures)
* [Add custom runners](/guides/testing/add-custom-runners)

# Tutorials

**Tutorials** are learning-oriented lessons that take you through a series of steps to complete a project. They are most useful when you want to get started with Tenzir.

<!-- The SVG image -->

![Documentation structure](/tutorials.svg)

<!-- Clickable overlay areas -->

<!-- Tutorials (top-left) -->

<!-- Guides (top-right) -->

[](/guides/)

<!-- Explanations (bottom-left) -->

[](/explanations/)

<!-- Reference (bottom-right) -->

[](/reference/)

# Learn idiomatic TQL

This tutorial teaches you how to write TQL that is clear, efficient, and maintainable. It assumes you already know basic TQL syntax and operators. You’ll learn the patterns and practices that make TQL code idiomatic—the way experienced TQL developers write it.

Understanding TQL First

If you’re new to TQL or want to understand its design philosophy and architecture, read the [TQL language explanation](/explanations/language) first. This tutorial focuses on practical patterns, while the explanation covers the conceptual foundations.

## What makes TQL idiomatic?

[Section titled “What makes TQL idiomatic?”](#what-makes-tql-idiomatic)

Idiomatic TQL follows consistent patterns that leverage the language’s strengths:

* **Vertical clarity**: Pipelines flow top-to-bottom for readability
* **Explicit data contracts**: Clear about what data should vs. might exist
* **Domain-aware types**: Uses IP addresses, not strings; durations, not integers
* **Composition over complexity**: Small, focused operators that combine well
* **Performance-conscious**: Filters early, aggregates late

This tutorial shows you these patterns through concrete examples, comparing idiomatic approaches with common pitfalls.

## Pipeline structure

[Section titled “Pipeline structure”](#pipeline-structure)

### Vertical vs horizontal: choosing the right style

[Section titled “Vertical vs horizontal: choosing the right style”](#vertical-vs-horizontal-choosing-the-right-style)

TQL offers two ways to chain statements: a newline `\n` (vertical) or pipe `|` (horizontal). While both are valid, each has its place.

#### Always use vertical structure in files

[Section titled “Always use vertical structure in files”](#always-use-vertical-structure-in-files)

✅ Idiomatic vertical structure:

```tql
let $ports = [22, 443]


from "/tmp/logs.json"
where port in $ports
select src_ip, dst_ip, bytes
summarize src_ip, total=sum(bytes)
```

Benefits of vertical structure:

* **Readability**: Easy to scan and understand data flow
* **Debugging**: Simple to comment out individual operators
* **Modification**: Easy to insert or remove pipeline stages
* **Version Control**: Clear diffs when pipelines change

#### Use horizontal structure only for command-line

[Section titled “Use horizontal structure only for command-line”](#use-horizontal-structure-only-for-command-line)

✅ Appropriate for one-liners:

```tql
tenzir 'from "logs.json" | where severity == "high" | summarize count()'
```

The horizontal approach is ideal for:

* **Command-line usage**: Quick ad-hoc queries in the terminal
* **API requests**: Single-line strings in JSON payloads
* **Shell scripts**: Embedding TQL in bash scripts
* **Interactive exploration**: Building pipelines in a REPL

#### Never mix styles

[Section titled “Never mix styles”](#never-mix-styles)

❌ Avoid hybrid approaches:

```tql
let $ports = [22, 443]


from "/tmp/logs.json"
| where port in $ports
| select src_ip, dst_ip, bytes
| summarize src_ip, total=sum(bytes)
```

This Kusto-like style makes code harder to read and maintain, especially with nested pipelines that increase indentation.

Quick rule

* **In files**: Always use newlines (vertical)
* **On command-line**: Use pipes (horizontal)
* **Never**: Mix both styles

### Trailing commas in vertical structures

[Section titled “Trailing commas in vertical structures”](#trailing-commas-in-vertical-structures)

When writing vertical structures, use trailing commas consistently to improve maintainability.

#### Lists and records

[Section titled “Lists and records”](#lists-and-records)

✅ Vertical structures with trailing commas:

```tql
let $ports = [
  22,
  80,
  443,
  3306,
]


let $config = {
  threshold: 100,
  timeout: 30s,
  enabled: true,
}
```

Benefits:

* Add new items without modifying existing lines
* Reorder items without worrying about comma placement
* Get cleaner diffs in version control
* Avoid syntax errors when adding/removing items

✅ Horizontal structures without trailing commas:

```tql
let $ports = [22, 80, 443, 3306]
let $config = {threshold: 100, timeout: 30s}
```

❌ Never use trailing commas horizontally:

```tql
let $ports = [22, 80, 443, 3306,]  // Wrong!
let $config = {threshold: 100, timeout: 30s,}  // Wrong!
```

❌ No trailing comma after an operator argument sequence:

```tql
from {status: 200, path: "/api"},
     {status: 404, path: "/missing"}  // No comma here
```

Quick rule

Trailing commas are only allowed in contexts with parentheses (’()’), brackets (’\[]’), or braces ('').

## Field management

[Section titled “Field management”](#field-management)

### Use `move` expressions to prevent field duplication

[Section titled “Use move expressions to prevent field duplication”](#use-move-expressions-to-prevent-field-duplication)

✅ Clean field transfers with move:

```tql
// Moving fields during transformation
normalized.src_ip = move raw.source_address
normalized.dst_port = move raw.destination.port
normalized.severity = move alert.level
```

❌ Avoid copy-then-drop pattern:

```tql
normalized.src_ip = raw.source_address
normalized.dst_port = raw.destination.port
normalized.severity = alert.level
drop raw.source_address, raw.destination.port, alert.level
```

### Be intentional about field preservation

[Section titled “Be intentional about field preservation”](#be-intentional-about-field-preservation)

✅ Move fields that are transformed:

```tql
ocsf.activity_id = move http_methods[method]? else 99
```

✅ Drop only static metadata fields:

```tql
drop event_kind  // Same across all events
```

❌ Don’t leave transformed data in original location:

```tql
ocsf.src_ip = original.ip  // Bad: original.ip still exists
```

When normalizing data (e.g., to OCSF format):

* Use `move` for fields being transformed to prevent duplication
* Ensure transformed values don’t appear in both old and new locations
* Only drop fields you’re certain are constant across events
* Verify no critical data ends up in `unmapped` fields
* Treat all input-derived values as dynamic, not constants
* Don’t hardcode field values based on example data

### Use meaningful names for computed fields

[Section titled “Use meaningful names for computed fields”](#use-meaningful-names-for-computed-fields)

✅ Clear intent:

```tql
summarize \
  src_ip,
  total_traffic=sum(bytes),
  avg_response=mean(response_time),
  error_rate=count(status >= 400) / count()
```

❌ Unclear:

```tql
summarize src_ip, sum(bytes), mean(response_time)
```

## Type awareness

[Section titled “Type awareness”](#type-awareness)

### Leverage TQL’s domain-specific types

[Section titled “Leverage TQL’s domain-specific types”](#leverage-tqls-domain-specific-types)

✅ Use native types:

```tql
let $timestamp = now()
let $weekend = $timestamp.day_of_week() in ["Saturday", "Sunday"]


where src_ip in 10.0.0.0/8
where duration > 5min
```

⚠️ Less expressive and error-prone:

```tql
let $timestamp = now()
let $weekend = $timestamp_day in [0, 6]  // What do 0 and 6 mean?


where src_ip.string().starts_with("10.")
where duration_ms > 300000
```

## Performance considerations

[Section titled “Performance considerations”](#performance-considerations)

### Place filters early in pipelines

[Section titled “Place filters early in pipelines”](#place-filters-early-in-pipelines)

✅ Filter first, reduce data volume:

```tql
from "large_dataset.json"
where severity == "critical"     // Reduce early
where timestamp > now() - 1h     // Further reduction
select relevant_fields           // Drop unnecessary data
summarize ...                    // Aggregate reduced dataset
```

❌ Process everything, then filter:

```tql
from "large_dataset.json"
select all_fields
summarize ...
where result > threshold        // Filter after expensive operation
```

## Composition patterns

[Section titled “Composition patterns”](#composition-patterns)

### Use constants for reusable values

[Section titled “Use constants for reusable values”](#use-constants-for-reusable-values)

✅ Maintainable and self-documenting:

```tql
let $internal_net = 10.0.0.0/8
let $critical_ports = [22, 3389, 5432]  // SSH, RDP, PostgreSQL
let $high_risk_threshold = 0.8


where src_ip in $internal_net
where dst_port in $critical_ports
where risk_score > $high_risk_threshold
```

❌ Magic numbers scattered throughout:

```tql
where src_ip in 10.0.0.0/8
where dst_port in [22, 3389, 5432]
where risk_score > 0.8          // What does 0.8 mean?
```

## Record constants and mappings

[Section titled “Record constants and mappings”](#record-constants-and-mappings)

### Use record constants for mappings instead of if-else chains

[Section titled “Use record constants for mappings instead of if-else chains”](#use-record-constants-for-mappings-instead-of-if-else-chains)

✅ Clean record-based mappings with else fallback:

```tql
let $http_methods = {
  CONNECT: 1,
  DELETE: 2,
  GET: 3,
  HEAD: 4,
  OPTIONS: 5,
  POST: 6,
  PUT: 7,
  TRACE: 8,
  PATCH: 9,
}


let $activity_names = [
  "Unknown",
  "Connect",
  "Delete",
  "Get",
  "Head",
  "Options",
  "Post",
  "Put",
  "Trace",
  "Patch",
]


let $dispositions = {
  OBSERVED: {id: 15, name: "Detected"},
  LOGGED: {id: 17, name: "Logged"},
  ALLOWED: {id: 1, name: "Allowed"},
  BLOCKED: {id: 2, name: "Blocked"},
  DENIED: {id: 2, name: "Blocked"},
}


// Use record indexing with else for fallback values
activity_id = $http_methods[method]? else 99
activity_name = $activity_names[activity_id]? else "Other"
disposition = $dispositions[action]? else {id: 0, name: "Unknown"}
```

❌ Complex if-else chains:

```tql
// Hard to maintain and extend
if method == "GET" {
  activity_id = 3
} else if method == "POST" {
  activity_id = 6
} else if method == "PUT" {
  activity_id = 7
} else if method == "DELETE" {
  activity_id = 2
} else {
  activity_id = 99
}


// Error-prone string building
if activity_id == 1 {
  activity_name = "Connect"
} else if activity_id == 2 {
  activity_name = "Delete"
} else if activity_id == 3 {
  activity_name = "Get"
} // ... many more conditions
```

❌ Inline nested if-else expressions are especially problematic:

```tql
// TERRIBLE: Unreadable chain of inline conditionals
activity_id = 3 if method == "GET" else 6 if method == "POST" else 7 if method == "PUT" else 2 if method == "DELETE" else 99


// AWFUL: Complex nested logic impossible to follow
severity_level = "critical" if score > 90 else "high" if score > 70 else "medium" if score > 50 else "low" if score > 30 else "info"


// UNMAINTAINABLE: Mixed logic with nested conditions
result = "blocked" if is_malicious else "allowed" if is_trusted else "quarantine" if risk > 0.8 else "review" if risk > 0.5 else "log"
```

These inline chains are a serious anti-pattern because:

* **Unreadable**: Eyes can’t parse the logic flow easily
* **Error-prone**: Easy to mix up conditions and values
* **Unmaintainable**: Adding/removing conditions requires rewriting the entire expression
* **Debugging nightmare**: Can’t set breakpoints or log intermediate values
* **Performance issues**: Every condition is evaluated sequentially

✅ Always use record constants instead:

```tql
// Clean, maintainable lookups with record indexing
activity_id = $http_methods[method]? else 99
activity_name = $activity_names[activity_id]? else "Other"
disposition = $dispositions[action]? else {id: 0, name: "Unknown"}
```

The `else` keyword provides a fallback value when:

* A field doesn’t exist (`field? else default`)
* An array index is out of bounds (`array[index]? else default`)
* A record key doesn’t exist (`record[key]? else default`)

This pattern is particularly powerful for:

* Normalizing data to standard formats (like OCSF)
* Mapping between different naming conventions
* Providing sensible defaults for missing data
* Creating reusable transformation logic

## Writing comments

[Section titled “Writing comments”](#writing-comments)

Good comments explain the reasoning, business logic, or non-obvious decisions behind code. The code itself should show what it does; comments should explain why it does it.

❌ Bad; explains what (redundant):

```tql
// Increment counter by 1
set counter = counter + 1
```

✅ Good; Explains why:

```tql
/*
 * Binary field curves are deprecated due to:
 * 1. Weak reduction polynomials in some cases
 * 2. Complex implementation leading to side-channel vulnerabilities
 * 3. Patent concerns that historically limited adoption
 * 4. Generally slower performance compared to prime field curves
 * 5. Less scrutiny from cryptographic community
 * RFC 8422 deprecates these for TLS 1.3.
 */
let $weak_prime_curves = [
  "secp160k1", // 160-bit curves
  "secp160r1",
  "secp160r2",
  "secp192k1", // 192-bit curves
  "secp224k1", // 224-bit curves
  "secp224r1", // NIST P-224
]
```

Why you should comment your code

Don’t be skimpy with your comments—here’s why:

* **Prevent “improvements” that break things**: Future developers won’t accidentally “fix” your intentional design choices.
* **Save investigation time**: Document your research so others don’t repeat the same Stack Overflow searches.
* **Explain business constraints**: Code reviews can’t capture why you chose the “worse” technical solution for valid business reasons.
* **Help your future self**: You’ll forget your own reasoning faster than you think.
* **Speed up onboarding**: New team members understand decisions instead of guessing at intent.

Remember: well-named fields and clear structure communicate *what* your code does—comments explain *why* you chose to do it that way.

## Data quality

[Section titled “Data quality”](#data-quality)

TQL’s diagnostic system helps you maintain data quality by distinguishing between expected variations and genuine problems. Understanding how to work with warnings intentionally is key to building robust pipelines.

In TQL, warnings are not annoyances to suppress—they’re signals about your data’s health. The language provides tools to express your expectations clearly:

* **No `?`**: Field should exist; warning indicates a problem
* **With `?`**: Field naturally varies; absence is normal
* **`assert`**: Enforce invariants with warnings
* **`strict`**: Escalate warnings to errors when quality matters

### Be deliberate about optional field access

[Section titled “Be deliberate about optional field access”](#be-deliberate-about-optional-field-access)

The `?` operator controls whether missing fields trigger warnings. Use it to express your data contract clearly.

✅ Clear data expectations in transformations:

```tql
// Required field - warning if missing
result = {id: event_id, severity: severity}


// Optional field - no warning if missing
result = {id: event_id, customer: customer_id?}
```

✅ Express expectations in selections:

```tql
// Required field - warning if missing
select event_id, timestamp


// Mix required and optional fields
select event_id, customer_id?
```

❌ Suppressing warnings on required fields:

```tql
// Bad: This hides data quality problems
select event_id?  // Should warn if missing!
```

Tip

The `?` operator expresses your data contract. Use it to distinguish between “this field should exist” and “this field naturally varies.”

### Enforce invariants with `assert`

[Section titled “Enforce invariants with assert”](#enforce-invariants-with-assert)

Use `assert` when specific conditions must be true for your pipeline to work correctly. Unlike `where`, which silently filters, `assert` emits warnings when invariants are violated.

✅ Use `assert` for data quality checks:

```tql
// Ensure critical field has valid values
assert severity in ["low", "medium", "high", "critical"]


// Verify schema expectations
subscribe "ocsf"
assert @name == "ocsf.network_activity"  // Wrong event type = warning
```

✅ Combine `assert` with filtering:

```tql
// First assert invariant (with warning)
assert src_ip != null


// Then filter normally (silent)
where src_ip.is_private()
```

❌ Don’t use `assert` for normal filtering:

```tql
// Wrong: This creates unnecessary warnings
assert severity == "critical"


// Right: Use where for filtering
where severity == "critical"
```

Performance

`where` is faster than `assert` due to optimizations like predicate pushdown. Only use `assert` when you specifically want warnings for violated conditions.

### Treat warnings as errors with `strict`

[Section titled “Treat warnings as errors with strict”](#treat-warnings-as-errors-with-strict)

The [`strict`](/reference/operators/strict) operator escalates all warnings to errors within its scope, stopping the pipeline when data quality issues occur.

✅ Use `strict` for critical data processing:

```tql
// Stop pipeline if any required field is missing
strict {
  select transaction_id  // Warning → Error if missing
}
```

✅ Combine with `assert` for comprehensive checks:

```tql
strict {
  // Assertion becomes fatal if violated
  assert amount > 0


  // Missing field also becomes fatal
  select customer_id
}
```

Quick rule

By default, trust the operator to accurately distinguish between warnings and errors. Only use `strict` when warnings should halt the pipeline, and when data loss is less important than data correctness.

### Choose the right quality control

[Section titled “Choose the right quality control”](#choose-the-right-quality-control)

| Tool         | Use When             | Behavior           |
| ------------ | -------------------- | ------------------ |
| `field`      | Field must exist     | Warning if missing |
| `field?`     | Field is optional    | Silent if missing  |
| `where`      | Filtering data       | Silent filter      |
| `assert`     | Enforcing invariants | Warning + filter   |
| `strict { }` | Zero tolerance       | Warnings → Errors  |

✅ Production pipeline with layered quality control:

```tql
// Constants for validation
let $valid_severities = ["low", "medium", "high", "critical"]
let $required_fields = ["event_id", "timestamp", "source"]


// Strict mode for critical path
strict {
  subscribe "prod"


  // Assertions for data integrity
  assert severity in $valid_severities
  assert timestamp > 2024-01-01


  // Required field access (warnings → errors)
  where event_id != null and source != null


  // Normal processing
  context::enrich "geo", key=source
}


// Optional enrichment (outside strict)
where geo?.country? == "US"  // No warning if geo missing
```

This layered approach ensures critical data meets requirements while allowing flexibility for optional enrichments.

# Map data to OCSF

In this tutorial you’ll learn how to **map events to [Open Cybersecurity Schema Framework (OCSF)](https://schema.ocsf.io)**. We walk you through an example of events from a network monitor and show how you can use Tenzir pipelines to transform them into OCSF-compliant events.

![OCSF Pipeline](/_astro/ocsf-pipeline.DpXOPlHZ_19DKCs.svg)

The diagram above illustrates the data lifecycle and shows where the OCSF mapping takes place: you collect data from various data sources, each of which has a different shape, and then convert them to a standardized representation. Normalization decouples data acquisition from downstream analytics, so you can scale each process independently.

## OCSF Primer

[Section titled “OCSF Primer”](#ocsf-primer)

OCSF is a vendor-agnostic event schema (aka. “taxonomy”) that defines structure and semantics for security events. Here are some key terms you need to know to map events:

* **Attribute**: a unique identifier for a specific type, e.g., `parent_folder` of type `String` or `observables` of type `Observable Array`.
* **Event Class**: a description of an event that uses specific attributes, e.g., `HTTP Activity` and `Detection Finding`.
* **Category**: a group of event classes, e.g., `System Activity` or `Findings`.

The diagram below illustrates how subsets of attributes form an event class:

![OCSF Event Classes](/_astro/ocsf-event-classes.BlDtprEG_19DKCs.svg)

The **Base Event Class** is a special event class that appears in every event class. Think of it as a mixin of attributes that OCSF automatically includes:

![OCSF Base Event Class](/_astro/ocsf-base-event-class.Cn0E_XRl_19DKCs.svg)

For this tutorial, we look at OCSF from the perspective of the *mapper* persona, i.e., as someone who converts existing events into the OCSF schema. OCSF also defines three other personas, author, producer, and analyst. This tutorial doesn’t cover them. Our mission as mapper is to study the event semantics of the data source we want to map, and translate the event to the appropriate OCSF event class.

## Case Study: Zeek Logs

[Section titled “Case Study: Zeek Logs”](#case-study-zeek-logs)

Let’s map some [Zeek](https://zeek.org) logs to OCSF!

What is Zeek?

The [Zeek](https://zeek.org) network monitor turns raw network traffic into detailed, structured logs. The logs range across the OSI stack from link layer activity to application-specific messages. In addition, Zeek provides a powerful scripting language to act on network events, making it a versatile tool for writing network-based detections to raise alerts.

Zeek generates logs in tab-separated values (TSV) or JSON format. Here’s an example of a connection log in TSV format:

conn.log (TSV)

```text
#separator \x09
#set_separator  ,
#empty_field  (empty)
#unset_field  -
#path  conn
#open  2023-03-07-10-23-46
#fields  ts  uid  id.orig_h  id.orig_p  id.resp_h  id.resp_p  id.vlan  id.vlan_inner  proto  service  duration  orig_bytes  resp_bytes  conn_state  local_orig  local_resp  missed_bytes  history  orig_pkts  orig_ip_bytes  resp_pkts  resp_ip_bytes  tunnel_parents  vlan  inner_vlan  orig_l2_addr  resp_l2_addr  geo.orig.country_code  geo.orig.region  geo.orig.city  geo.orig.latitude  geo.orig.longitude  geo.resp.country_code  geo.resp.region  geo.resp.city  geo.resp.latitude  geo.resp.longitude  community_id
#types  time  string  addr  port  addr  port  int  int  enum  string  interval  count  count  string  bool  bool  count  string  count  count  count  count  set[string]  int  int  string  string  string  string  string  double  double  string  string  string  double  double  string
1637155963.237882  CZwqhx3td8eTfCSwJb  128.14.134.170  57468  198.71.247.91  80  -  -  tcp  http  5.162805  205  278  SF  -  -  0  ShADadfF  6  525  5  546  -  -  -  64:9e:f3:be:db:66  00:16:3c:f1:fd:6d  US  CA  Los Angeles  34.0544  -118.2441  US  VA  Ashburn  39.0469  -77.4903  1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=
1637157758.165570  CnrwFesjfOhI3fuu1  45.137.23.27  47958  198.71.247.91  53  -  -  udp  dns  -  -  -  S0  -  -  0  D  1  58  0  0  -  -  -  64:9e:f3:be:db:66  00:16:3c:f1:fd:6d  BD  -  -  23.7018  90.3742  US  VA  Ashburn  39.0469  -77.4903  1:0nZC/6S/pr+IceCZ04RjDZbX+KI=
1637229399.549141  CBTne9tomX1ktuCQa  10.4.21.101  53824  107.23.103.216  587  -  -  tcp  smtp  606.747526  975904  11950  SF  -  -  0  ShAdDaTtTfF  1786  1069118  1070  55168  -  -  -  00:08:02:1c:47:ae  20:e5:2a:b6:93:f1  -  -  -  -  -  US  VA  Ashburn  39.0469  -77.4903  1:I6VoTvbCqaKvPrlFnNbRRbjlMsc=
```

You can also [download this sample](/packages/zeek/tests/inputs/conn.log) to avoid dealing with tabs and spaces in the snippet above.

### Step 1: Parse the input

[Section titled “Step 1: Parse the input”](#step-1-parse-the-input)

We first parse the log file into a structured form so that we can work with the individual fields. The [`read_zeek_tsv`](/reference/operators/read_zeek_tsv) operator parses the above structure out of the box:

```sh
tenzir 'read_zeek_tsv' < conn.log
```

Output

```tql
{
  ts: 2021-11-17T13:32:43.237881856Z,
  uid: "CZwqhx3td8eTfCSwJb",
  id: {
    orig_h: 128.14.134.170,
    orig_p: 57468,
    resp_h: 198.71.247.91,
    resp_p: 80,
  },
  proto: "tcp",
  service: "http",
  duration: 5.162805s,
  orig_bytes: 205,
  resp_bytes: 278,
  conn_state: "SF",
  local_orig: null,
  local_resp: null,
  missed_bytes: 0,
  history: "ShADadfF",
  orig_pkts: 6,
  orig_ip_bytes: 525,
  resp_pkts: 5,
  resp_ip_bytes: 546,
  tunnel_parents: null,
  community_id: "1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=",
  _write_ts: null,
}
{
  ts: 2021-11-17T14:02:38.165570048Z,
  uid: "CnrwFesjfOhI3fuu1",
  id: {
    orig_h: 45.137.23.27,
    orig_p: 47958,
    resp_h: 198.71.247.91,
    resp_p: 53,
  },
  proto: "udp",
  service: "dns",
  duration: null,
  orig_bytes: null,
  resp_bytes: null,
  conn_state: "S0",
  local_orig: null,
  local_resp: null,
  missed_bytes: 0,
  history: "D",
  orig_pkts: 1,
  orig_ip_bytes: 58,
  resp_pkts: 0,
  resp_ip_bytes: 0,
  tunnel_parents: null,
  community_id: "1:0nZC/6S/pr+IceCZ04RjDZbX+KI=",
  _write_ts: null,
}
{
  ts: 2021-11-18T09:56:39.549140992Z,
  uid: "CBTne9tomX1ktuCQa",
  id: {
    orig_h: 10.4.21.101,
    orig_p: 53824,
    resp_h: 107.23.103.216,
    resp_p: 587,
  },
  proto: "tcp",
  service: "smtp",
  duration: 10.112458766666666min,
  orig_bytes: 975904,
  resp_bytes: 11950,
  conn_state: "SF",
  local_orig: null,
  local_resp: null,
  missed_bytes: 0,
  history: "ShAdDaTtTfF",
  orig_pkts: 1786,
  orig_ip_bytes: 1069118,
  resp_pkts: 1070,
  resp_ip_bytes: 55168,
  tunnel_parents: null,
  community_id: "1:I6VoTvbCqaKvPrlFnNbRRbjlMsc=",
  _write_ts: null,
}
```

Now that we have decomposed the data into its atomic values, we can map them to the corresponding OCSF fields.

### Step 2: Map to OCSF

[Section titled “Step 2: Map to OCSF”](#step-2-map-to-ocsf)

To map fields, you must first identify the appropriate OCSF event class. In our example, the corresponding event class in OCSF is [Network Activity](https://schema.ocsf.io/1.6.0/classes/network_activity). We use OCSF v1.6.0 throughout this tutorial.

To make the mapping process more organized, we map per *attribute group*. The schema has four groups:

1. **Classification**: Important for the taxonomy and schema itself
2. **Occurrence**: Temporal characteristics about when the event happened
3. **Context**: Auxiliary information about the event
4. **Primary**: Defines the key semantics of the given event

Here’s a template for the mapping pipeline:

```tql
// --- Preamble ---------------------------------


// Move the original event into a dedicated field that we pull our values from.
// We recommend naming the field so that it represents the respective data
// source.
this = { zeek: this }


// (2) Populate the OCSF event. Style-wise, we recommend using one coherent
// block of TQL per OCSF attribute group to provide a bit of structure for the
// reader.


// --- OCSF: classification attributes ----------


ocsf.activity_id = 6
ocsf.class_uid = 4001
// ...fill out remaining classification attributes.


// --- OCSF: occurence attributes ---------------


ocsf.time = move zeek.ts // 👈 remove source field while mapping
ocsf.duration = move zeek.duration
ocsf.end_time = ocsf.time + ocsf.duration
ocsf.start_time = ocsf.time
// ...fill out remaining occurence attributes.


// --- OCSF: context attributes -----------------


ocsf.metadata = {
  product: {
    name: "Zeek",
  },
  uid: move zeek.uid,
  version: "1.6.0",
}
// ...fill out remaining context attributes.


// --- OCSF: primary attributes -----------------


ocsf.src_endpoint = {
  ip: zeek.id.orig_h,
  port: zeek.id.orig_p,
}
// ...fill out remaining primary attributes.
drop zeek.id // 👈 remove source field after mapping


// --- Finalize ---------------------------------


// (3) Hoist all `ocsf` attributes into the root and declare the remaining fields in
// `zeek` as `unmapped`.
this = {...ocsf, unmapped: zeek}


// (4) Assign metadata, such as a name for easier filtering downstream.
@name = "ocsf.network_activity"
```

Let’s unpack this:

1. With `this = { zeek: this }` we move the original event into the field `zeek`. This approach also avoids name clashes when we create new fields in the next steps. Because we are mapping Zeek logs, we chose `zeek` as a name to make the subsequent mappings almost self-explanatory.
2. The main work takes place here. Our approach is structured: for every field in the source event, (1) map it, and (2) remove it. Ideally, use the `move` keyword to perform (1) and (2) together, e.g., `ocsf.x = move source.y`. If a field needs to be used multiple times in the same expression, use the [`drop`](/reference/operators/drop) afterwards.
3. The assignment `this = {...ocsf, unmapped: zeek}` means that we move all fields from the `ocsf` record into the top-level record (`this`), and at the same time add a new field `unmapped` that contains everything that we didn’t map. This is why it’s important to remove the fields from the source as we perform the mapping. If you forget to map a field, it simply lands in the `unmapped` catch-all record and you can tweak your mapping later.
4. We give the event a new schema name so that we can easily filter by its shape in further pipelines.

Now that we have a template, let’s get our hands dirty and go deep into the actual mapping.

#### Classification Attributes

[Section titled “Classification Attributes”](#classification-attributes)

The classification attributes are important for the schema. Mapping them is mechanical and mostly involves reviewing the schema docs.

```tql
ocsf.activity_id = 6
ocsf.activity_name = "Traffic"
ocsf.category_uid = 4
ocsf.category_name = "Network Activity"
ocsf.class_uid = 4001
ocsf.class_name = "Network Activity"
ocsf.severity_id = 1
ocsf.severity = "Informational"
ocsf.type_uid = ocsf.class_uid * 100 + ocsf.activity_id
```

Note that computing the field `type_uid` requires simple arithmetic. You can also rely on [`ocsf::derive`](/reference/operators/ocsf/derive) to populate sibling fields—an optimization to write more concise mappings.

#### Occurrence Attributes

[Section titled “Occurrence Attributes”](#occurrence-attributes)

Let’s tackle the occurrence group. These attributes are all about time.

```tql
ocsf.time = move zeek.ts
ocsf.duration = move zeek.duration
ocsf.end_time = ocsf.time + ocsf.duration
ocsf.start_time = ocsf.time
```

Using `+` with a value of type `time` and `duration` yields a new `time` value, just as you’d expect.

#### Context Attributes

[Section titled “Context Attributes”](#context-attributes)

The context attributes provide auxiliary information. Most notably, the `metadata` attribute holds data-source specific information. Even though `unmapped` belongs to this group, we deal with it at the very end.

```tql
ocsf.metadata = {
  log_name: "conn.log",
  logged_time: move zeek._write_ts?,
  product: {
    name: "Zeek",
    vendor_name: "Zeek",
    cpe_name: "cpe:2.3:a:zeek:zeek",
  },
  uid: move zeek.uid,
  version: "1.6.0",
}
drop zeek._path? // implied in metadata.log_name
ocsf.app_name = move zeek.service
```

We use `?` when accessing fields that are not always present, e.g., `zeek._write_ts?`. If you omit the `?`, the pipeline emits a warning when `_write_ts` is missing.

#### Primary Attributes

[Section titled “Primary Attributes”](#primary-attributes)

The primary attributes define the semantics of the event class itself. This is where the core value of the data is, as we are mapping the most event-specific information.

```tql
ocsf.src_endpoint = {
  ip: zeek.id.orig_h,
  port: zeek.id.orig_p,
}
ocsf.dst_endpoint = {
  ip: zeek.id.resp_h,
  port: zeek.id.resp_p,
}
// Here, we use `drop` because we simply want to get rid of the intermediate
// `id` record that we already mapped above.
drop zeek.id
// Locality of reference: we define the protocol numbers close where they are
// used for easier readability.
let $proto_nums = {
  tcp: 6,
  udp: 17,
  icmp: 1,
  icmpv6: 58,
  ipv6: 41,
}
ocsf.connection_info = {
  community_uid: move zeek.community_id?,
  protocol_name: move zeek.proto,
  protocol_num: $proto_nums[zeek.proto]? else -1
}
// If we cannot use static records, branch with if/else statements.
if ocsf.src_endpoint.ip.is_v6() or ocsf.dst_endpoint.ip.is_v6() {
  ocsf.connection_info.protocol_ver_id = 6
} else {
  ocsf.connection_info.protocol_ver_id = 4
}
if zeek.local_orig and zeek.local_resp {
  ocsf.connection_info.direction = "Lateral"
  ocsf.connection_info.direction_id = 3
} else if zeek.local_orig {
  ocsf.connection_info.direction = "Outbound"
  ocsf.connection_info.direction_id = 2
} else if zeek.local_resp {
  ocsf.connection_info.direction = "Inbound"
  ocsf.connection_info.direction_id = 1
} else {
  ocsf.connection_info.direction = "Unknown"
  ocsf.connection_info.direction_id = 0
}
drop zeek.local_orig, zeek.local_resp
// The `status` attribute in OCSF is a success indicator. While we could use
// `zeek.conn_state` to extract success/failure, this would go beyond the
// tutorial.
ocsf.status_id = 99
ocsf.status = "Other"
ocsf.status_code = move zeek.conn_state
ocsf.traffic = {
  bytes_in: zeek.resp_bytes,
  bytes_out: zeek.orig_bytes,
  packets_in: zeek.resp_pkts,
  packets_out: zeek.orig_pkts,
  total_bytes: zeek.orig_bytes + zeek.resp_bytes,
  total_packets: zeek.orig_pkts + zeek.resp_pkts,
}
drop zeek.resp_bytes, zeek.orig_bytes, zeek.resp_pkts, zeek.orig_pkts
```

Here’s what happens here:

* The expression `$proto_nums[zeek.proto]` takes the value of Zeek’s `proto` field (e.g., `tcp`) and uses it as an index into a static record `$proto_nums`. Add a `?` at the end to avoid warnings when the lookup returns `null`, and use the inline `else` expression for the fallback value.
* To check whether we have an IPv4 or an IPv6 connection, we call [`is_v6()`](/reference/functions/is_v6) on the IPs of the connection record. TQL comes with numerous other domain-specific [functions](/reference/functions) that make mapping security data a breeze.

#### Putting it together

[Section titled “Putting it together”](#putting-it-together)

When we combine all TQL snippets from above, we get the following output:

```tql
{
  activity_id: 6,
  activity_name: "Traffic",
  category_uid: 4,
  category_name: "Network Activity",
  class_uid: 4001,
  class_name: "Network Activity",
  severity_id: 1,
  severity: "Informational",
  type_uid: 400106,
  time: 2021-11-17T13:32:43.237881856Z,
  duration: 5.162805s,
  end_time: 2021-11-17T13:32:48.400686856Z,
  start_time: 2021-11-17T13:32:43.237881856Z,
  metadata: {
    log_name: "conn.log",
    logged_time: null,
    product: {
      name: "Zeek",
      vendor_name: "Zeek",
      cpe_name: "cpe:2.3:a:zeek:zeek",
    },
    uid: "CZwqhx3td8eTfCSwJb",
    version: "1.6.0",
  },
  app_name: "http",
  src_endpoint: {
    ip: 128.14.134.170,
    port: 57468,
  },
  dst_endpoint: {
    ip: 198.71.247.91,
    port: 80,
  },
  connection_info: {
    community_uid: "1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=",
    flag_history: "ShADadfF",
    protocol_name: "tcp",
    protocol_num: 6,
    protocol_ver_id: 4,
  },
  status: "Other",
  status_code: "SF",
  status_id: 99,
  traffic: {
    bytes_in: 278,
    bytes_out: 205,
    packets_in: 5,
    packets_out: 6,
    total_bytes: 483,
    total_packets: 11,
  },
  unmapped: {
    missed_bytes: 0,
    orig_ip_bytes: 525,
    resp_ip_bytes: 546,
    tunnel_parents: null,
    vlan: null,
    inner_vlan: null,
    orig_l2_addr: "64:9e:f3:be:db:66",
    resp_l2_addr: "00:16:3c:f1:fd:6d",
    geo: {
      orig: {
        country_code: "US",
        region: "CA",
        city: "Los Angeles",
        latitude: 34.0544,
        longitude: -118.2441,
      },
      resp: {
        country_code: "US",
        region: "VA",
        city: "Ashburn",
        latitude: 39.0469,
        longitude: -77.4903,
      },
    },
  },
}
```

There are still several fields that we can map to the schema, but we’ll leave this as an exercise for the reader.

#### Recap: Understand the OCSF pipeline architecture

[Section titled “Recap: Understand the OCSF pipeline architecture”](#recap-understand-the-ocsf-pipeline-architecture)

Most pipelines (1) onboard data from a source, (2) transform it, and (3) send it somewhere. This tutorial focuses on the middle piece, with transformation being the mapping to OCSF.

![Zeek OCSF Pipeline](/_astro/ocsf-zeek-pipeline.tOw32iXh_19DKCs.svg)

We’ve addressed data *onboarding* by reading a log file and decomposing the unstructured Zeek TSV contents into a structured record. The built-in [`read_zeek_tsv`](/reference/operators/read_zeek_tsv) operator made this trivial. But it often requires a lot more elbow grease to get there. Check out our extensive [guide on extracting structured data from text](/guides/data-shaping/extract-structured-data-from-text/) for more details. We haven’t yet addressed the other end of the pipeline: data *offboarding*. Our examples run `tenzir` on the command line, relying on an implicit output operator that writes the result of the last transformation to the terminal.

In other words, we have a sandwich structure in our pipeline. To make it explicit:

```tql
// (1) Onboard data (explicit)
load_stdin
read_zeek_tsv


// (2) Map to OCSF
// ...
// Lots of TQL here!
// ...


// (3) Offboard data (implicit)
write_tql
save_stdout
```

Such a pipeline is impractical because data may arrive via multiple channels: log files, Kafka messages, or via Syslog over the wire. Even the encoding may vary. Zeek TSV is one way you can configure Zeek, but JSON output is another format. Similarly, users may want to consume the data in various ways.

In theory, we’re done now. We have a working mapping. It’s just not yet very (re)usable. For maximum flexibility, we want to split the pipeline into independently usable snippets. The next section describes how to achieve this.

### Step 3: Package the mapping

[Section titled “Step 3: Package the mapping”](#step-3-package-the-mapping)

To make our OCSF mapping more reusable, we extract it as **user-defined operator** and put it into a [package](/explanations/packages). There is an entire [tutorial on writing packages](/tutorials/write-a-package), but all you need to know right now that packages are one-click installable bundles that you can flexibly deploy. After installation, you can call the newly introduced mapping operators from any pipeline.

#### Break down complexity with user-defined operators

[Section titled “Break down complexity with user-defined operators”](#break-down-complexity-with-user-defined-operators)

Let’s work towards a package that comes with a user-defined operator called `zeek::ocsf::map` that maps Zeek connection logs to OCSF:

```tql
load_stdin
read_zeek_tsv
zeek::ocsf::map // 👈 Maps Zeek logs to OCSF with a user-defined operator.
write_tql
save_stdout
```

All you have to do to get there is create a package with following directory structure:

* zeek/

  * examples/

    * …

  * operators/ 👈 user-defined operators go here

    * …

  * pipelines/

    * …

  * tests/

    * …

  * package.yaml

Notice how `zeek::ocsf::map` has two modules that are colon-separated: `zeek` and `ocsf`. The directory structure in the package determines the module hierarchy:

* zeek/

  * operators/

    * ocsf/

      * map.tql 👈 Exposes `zeek::ocsf::map` as operator

Since operators fully compose, you can implement `zeek::ocsf::map` as a combination of other operators, e.g., one operator per log type:

zeek/operators/ocsf/map.tql

```tql
// Dispatch mappings based on schema name. This assumes that you've taken apart
// the input logs and added the appropriate @name based on the log type. The
// read_zeek_tsv operator does this automatically. You could also dispatch based
// on any other field, such as _path, event_type, etc.
if @name == "zeek.conn" {
  // Map "conn.log" events
  zeek::ocsf::logs::conn
} else if @name == "zeek.dns" {
    // Map "dns.log" events
  zeek::ocsf::logs::dns
} else {
  // In the future, raise a warning here or put the event into a DLQ.
  pass
}
```

In this layout, you’d put the mapping operators in the following directories:

* zeek/

  * operators/

    * ocsf/

      * logs/ 👈 Exposes `zeek::ocsf::log::*` operators

        * conn.tql
        * dns.tql
        * …

      * map.tql

#### Write tests for prod-grade reliability

[Section titled “Write tests for prod-grade reliability”](#write-tests-for-prod-grade-reliability)

To achieve production-grade quality of your mappings, you must ensure that they do what they promise. In practice, this means shipping tests along the mappings: given a mapping, test whether a provided input produces a valid output.

Package writing tutorial

We have an [in-depth tutorial on how to write packages](/tutorials/write-a-package). This is just a small excerpt to showcase you you can test your OCSF mappings.

This is where our [test framework](/reference/test-framework) comes into play. Put your test scenarios in `tests/` and sample data into `tests/inputs`:

* zeek/

  * tests/

    * inputs/ 👈 raw log samples

      * conn.log
      * …

    * map\_one.tql 👈 test scenario go anywhere else

    * map\_one.txt 👈 expected output for test scneario

    * …

Here’s an example of the test:

zeek/tests/map\_one.tql

```tql
from_file f"{env("TENZIR_INPUTS")}/conn.log" {
  read_zeek_tsv
}
zeek::ocsf::map
head 1
```

Now run the test framework in the package directory:

```sh
uvx tenzir-test
```

```txt
i executing project: zeek (.)
i running 1 tests (44 jobs) in project .
i   1× tenzir (v5.16.0+gc0a0c3ba49)
✘ tests/map_one.tql
└─▶ Failed to find ref file: "../docs/public/packages/zeek/tests/map_one.txt"
i ran 1 test: 0 passed (0%) / 1 failed (100%)
 passed (100%) / 0 failed (0%)
```

There is no baseline for the test yet, let’s generate it via `--update`

```sh
uvx tenzir-test --update
```

Now there’s a \*.txt file next to the test scenario. Verify it that it has the expected output. Alternatively, use `uvx tenzir-test --passthrough` to print the output to the terminal for inline inspection.

Generated \*.txt test scenario baseline

zeek/tests/map\_one.txt

```tql
{
  activity_id: 6,
  activity_name: "Traffic",
  category_uid: 4,
  category_name: "Network Activity",
  class_uid: 4001,
  class_name: "Network Activity",
  severity_id: 1,
  severity: "Informational",
  type_uid: 400106,
  time: 2021-11-17T13:32:43.237881856Z,
  duration: 5.162805s,
  end_time: 2021-11-17T13:32:48.400686856Z,
  start_time: 2021-11-17T13:32:43.237881856Z,
  metadata: {
    log_name: "conn.log",
    logged_time: null,
    product: {
      name: "Zeek",
      vendor_name: "Zeek",
      cpe_name: "cpe:2.3:a:zeek:zeek",
    },
    uid: "CZwqhx3td8eTfCSwJb",
    version: "1.6.0",
  },
  app_name: "http",
  src_endpoint: {
    ip: 128.14.134.170,
    port: 57468,
  },
  dst_endpoint: {
    ip: 198.71.247.91,
    port: 80,
  },
  connection_info: {
    community_uid: "1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=",
    flag_history: "ShADadfF",
    protocol_name: "tcp",
    protocol_num: 6,
    protocol_ver_id: 4,
  },
  status: "Other",
  status_code: "SF",
  status_id: 99,
  traffic: {
    bytes_in: 278,
    bytes_out: 205,
    packets_in: 5,
    packets_out: 6,
    total_bytes: 483,
    total_packets: 11,
  },
  unmapped: {
    missed_bytes: 0,
    orig_ip_bytes: 525,
    resp_ip_bytes: 546,
    tunnel_parents: null,
    vlan: null,
    inner_vlan: null,
    orig_l2_addr: "64:9e:f3:be:db:66",
    resp_l2_addr: "00:16:3c:f1:fd:6d",
    geo: {
      orig: {
        country_code: "US",
        region: "CA",
        city: "Los Angeles",
        latitude: 34.0544,
        longitude: -118.2441,
      },
      resp: {
        country_code: "US",
        region: "VA",
        city: "Ashburn",
        latitude: 39.0469,
        longitude: -77.4903,
      },
    },
  },
}
```

After running `uvx tenzir-test` again without any flags, you get the following output:

```txt
i executing project: zeek (.)
i running 1 tests (44 jobs) in project .; update
i   1× tenzir (v5.16.0+gc0a0c3ba49)
✔ tests/map_one.tql
i ran 1 test: 1 passed (100%) / 0 failed (0%)
```

Perfect. Now proceed with all log types and you have production-grade package.

### Step 4: Install and use the package

[Section titled “Step 4: Install and use the package”](#step-4-install-and-use-the-package)

After you have fleshed out the complete package, [install it](/guides/basic-usage/install-a-package), either interactively via [`package::add`](/reference/operators/package/add), or IaC-style by putting it into a git repo and pointing the config option `tenzir.package-dirs` to it.

Tenzir Community Library

Wrote a package that anyone could benefit from? Contribute it to the [Tenzir Community Library](https://github.com/tenzir/library)! This GitHub repository hosts a collection of packages with pre-packaged OCSF mappings and other use cases.

## Summary

[Section titled “Summary”](#summary)

This tutorial showed you how to map security data to OCSF using TQL pipelines. You learned:

* How OCSF events look like in high-level terms
* How to structure your mapping pipeline using OCSF attribute groups (classification, occurrence, context, and primary)
* How to use TQL operators and expressions to transform raw events into OCSF-compliant records
* How to package your mappings as reusable operators in a Tenzir package

The key to successful OCSF mapping is systematic organization: move your source event into a dedicated field, map attributes group by group while removing source fields as you go, and collect any unmapped fields in a catch-all record. This approach ensures you don’t miss any fields and makes your mappings easy to maintain and extend.

For more examples and ready-to-use OCSF mappings, check out the [Tenzir Community Library](https://github.com/tenzir/library).

# Plot data with charts

In this tutorial, you will learn how to **use pipelines to plot data as charts**.

The Tenzir Query Language (TQL) excels at slicing and dicing even the most complex shapes of data. But turning tabular results into actionable insights often calls for visualization. This is where charts come into play.

![Charts](/_astro/charts.yAPl35Lg_Z28NVUp.svg)

## Available chart types

[Section titled “Available chart types”](#available-chart-types)

Tenzir supports four types of charts, each with a dedicated operator:

1. **Pie**: [`chart_pie`](/reference/operators/chart_pie)
2. **Bar**: [`chart_bar`](/reference/operators/chart_bar)
3. **Line**: [`chart_line`](/reference/operators/chart_line)
4. **Area**: [`chart_area`](/reference/operators/chart_area)

## How to plot data

[Section titled “How to plot data”](#how-to-plot-data)

Plotting data in the Explorer involves three steps:

1. [Run a pipeline](/guides/usage/basics/run-pipelines) to prepare the data.
2. Add a `chart_*` operator to render the plot.
3. View the chart below the Editor.

![Pipeline to Chart](/_astro/pipeline-to-chart.0tOnE2lJ_ZFDCk6.svg)

After generating a chart, you can **download it** or **add it to a dashboard** to make it permanent refresh it periodically.

**Example**: Here's how you generate a pie chart that shows the application breakdown from Zeek connection logs:

```tql
export
where @name == "zeek.conn"
where service != null
top service
head
chart_pie label=service, value=count
```

### Add a chart to a dashboard

[Section titled “Add a chart to a dashboard”](#add-a-chart-to-a-dashboard)

To make a chart permanent:

1. Click the **Dashboard** button.

   ![Add Chart to Dashboard](/_astro/add-chart-to-dashboard-1.CKWKUeyV_Z21fAfD.webp)

2. Enter a title for the chart, then click **Add to Dashboard**.

   ![Add Chart Title](/_astro/add-chart-to-dashboard-2.CH4qklWh_Z14uwlt.webp)

3. View the chart in your dashboard.

   ![Chart in Dashboard](/_astro/add-chart-to-dashboard-3.CbiY8Gpy_2pk3KS.webp)

🎉 Congratulations! Your chart is now saved and will automatically reload when you open the dashboard.

### Download a chart

[Section titled “Download a chart”](#download-a-chart)

Download a chart in the Explorer as follows:

1. Click the download button in the top-right corner.

   ![Download Chart](/_astro/download-chart-explorer.BaT4xIUq_yQMFO.webp)

2. Choose **PNG** or **SVG** to save the chart as an image.

You can also download a chart on a dashboard:

1. Click the three-dot menu in the top-right corner of the chart.

2. Click **Download**

   ![Download Chart](/_astro/download-chart-dashboard.CDK7X36c_Z1Ailmu.webp)

3. Choose **PNG** or **SVG** to save the chart as an image.

⬇️ You have now successfully save the chart to your computer. Enjoy.

## Master essential charting techniques

[Section titled “Master essential charting techniques”](#master-essential-charting-techniques)

Now that you know how to create charts, let us explore some common techniques to enhance your charting skills.

### Plot counters as bar chart

[Section titled “Plot counters as bar chart”](#plot-counters-as-bar-chart)

A good use case for bar charts is visualization of counters of categorical values, because comparing bar heights is an effective way to gain a relative understanding of the data at hand.

1. **Shape your data**: Suppose you want to create a bar chart showing the outcomes of coin flips. First, generate a few observations:

   ```tql
   from {}
   repeat 20
   set outcome = "heads" if random().round() == 1 else "tails"
   summarize outcome, n=count()
   ```

   Sample output:

   ```tql
   {outcome: "tails", n: 9}
   {outcome: "heads", n: 11}
   ```

2. **Plot the data**: Add the [`chart_bar`](/reference/operators/chart_bar) operator to visualize the counts.

   Map the outcome and count fields to the x-axis and y-axis:

   ```tql
   from {outcome: "tails", n: 9},
        {outcome: "heads", n: 11}
   chart_bar x=outcome, y=n
   ```

   ![Bar chart](/_astro/chart-bar.DEYyg_Q2_Z1AsRFd.webp)

##### Group and stack bars

[Section titled “Group and stack bars”](#group-and-stack-bars)

Sometimes, your data has a third dimension. You can **group** multiple series into a single plot.

Example with a `time` dimension:

```tql
from (
  {outcome: "tails", n: 9, time: "Morning"},
  {outcome: "heads", n: 11, time: "Morning"},
  {outcome: "tails", n: 14, time: "Afternoon"},
  {outcome: "heads", n: 15, time: "Afternoon"},
  {outcome: "tails", n: 4, time: "Evening"},
  {outcome: "heads", n: 12, time: "Evening"},
)
chart_bar x=outcome, y=n, group=time
```

![Grouped bar chart](/_astro/chart-bar-grouped.CFNoOv9R_1wr5sN.webp)

To **stack** the grouped bars, add `position="stacked"`:

```tql
from (
  {outcome: "tails", n: 9, time: "Morning"},
  {outcome: "heads", n: 11, time: "Morning"},
  {outcome: "tails", n: 14, time: "Afternoon"},
  {outcome: "heads", n: 15, time: "Afternoon"},
  {outcome: "tails", n: 4, time: "Evening"},
  {outcome: "heads", n: 12, time: "Evening"},
)
chart_bar x=outcome, y=n, group=time, position="stacked"
```

![Stacked bar chart](/_astro/chart-bar-stacked.D14MtCVs_kxnVL.webp)

#### Scale the y-axis logarithmically

[Section titled “Scale the y-axis logarithmically”](#scale-the-y-axis-logarithmically)

If your data spans several orders of magnitude, **log scaling** can make smaller values visible.

Example without log scaling:

```tql
from (
  {outcome: "A", n: 3},
  {outcome: "B", n: 5},
  {outcome: "C", n: 10},
  {outcome: "D", n: 21},
  {outcome: "E", n: 10000},
)
chart_bar x=outcome, y=n
```

![Unscaled bar chart](/_astro/chart-bar-nolog.Bbkxdwf4_Z21683g.webp)

The large value (`E`) dominates the chart, hiding the smaller categories.

Enable log scaling via `y_log=true` to reveal them:

```tql
from (
  {outcome: "A", n: 3},
  {outcome: "B", n: 5},
  {outcome: "C", n: 10},
  {outcome: "D", n: 21},
  {outcome: "E", n: 10000},
)
chart_bar x=outcome, y=n, y_log=true
```

![Log-scaled bar chart](/_astro/chart-bar-log.DkLhynLc_2vI7Kf.webp)

Now, you can clearly see all the values! 👀

Interpreting Log-Scaled Plots

Log scaling removes linearity. Comparing bar heights no longer reflects a simple numeric ratio. Stacked values are not additive anymore.

### Plot compositions as pie chart

[Section titled “Plot compositions as pie chart”](#plot-compositions-as-pie-chart)

Pie charts are well-understood and frequently occur in management dashboards. Let's plot some synthetic data with the [`chart_pie`](/reference/operators/chart_pie) operator:

```tql
from (
  {category: "A", percentage: 40},
  {category: "B", percentage: 25},
  {category: "C", percentage: 20},
  {category: "D", percentage: 10},
  {category: "E", percentage: 5},
)
chart_pie label=category, value=percentage
```

![Pie chart](/_astro/chart-pie.DZN_Mgb1_23nSAM.webp)

To provide a consistent user experience across all chart types, `chart_pie` treats `label` and `x` as interchangeable, as well as `value` and `y`. This mapping makes intuitive sense when you consider a pie chart as a bar chart rendered in a radial coordinate system.

Bar Charts > Pie Charts

**Use bar charts when you can and pie charts when you must**. Why? Pie charts are often considered inferior to bar charts because they rely on human perception of angles, which is less accurate than judging lengths, making comparisons between categories harder. Bar charts allow for quick, precise comparisons, handle many categories cleanly, and often do not require labels, while pie charts become cluttered and confusing with numerous slices. Bar charts also allow easy sorting, are more space-efficient, and tend to stay cleaner without unnecessary visual distractions like 3D effects. Overall, bar charts communicate data more clearly, accurately, and efficiently than pie charts.

### Plot metrics as line chart

[Section titled “Plot metrics as line chart”](#plot-metrics-as-line-chart)

Line charts come in handy when visualizing data trends over a continuous scale, such as time series data.

1. **Shape your data**: For our line chart demo, we'll use some internal node metrics provided by the [`metrics`](/reference/operators/metrics) operator. Let's look at the RAM usage of the node:

   ```tql
   metrics "process"
   drop swap_space, open_fds
   head 3
   ```

   ```plaintext
   {timestamp: 2025-04-27T18:16:17.692Z, current_memory_usage: 2363461632, peak_memory_usage: 4021136}
   {timestamp: 2025-04-27T18:16:18.693Z, current_memory_usage: 2366595072, peak_memory_usage: 4021136}
   {timestamp: 2025-04-27T18:16:19.694Z, current_memory_usage: 2385154048, peak_memory_usage: 4021136}
   ```

2. **Plot the data**: Add the [`chart_line`](/reference/operators/chart_line) operator to visualize the time series. We are going to plot the memory usage within the last day:

   ```tql
   metrics "process"
   where timestamp > now() - 1d
   chart_line x=timestamp, y=current_memory_usage
   ```

   ![Line chart](/_astro/chart-line.BGCcmM4p_Z1Uiv90.webp)

3. **Aggregate to reduce the resolution**: Plotting metrics with a 1-second granularity over the course of a full day can make a line chart very noisy. In fact, we have a total of 86,400 samples in our plot. This can make a line chart quickly illegible. Let's reduce the noise by aggregating the samples into 15-min buckets:

   ```tql
   metrics "process"
   where timestamp > now() - 1d
   set timestamp = timestamp.round(15min)
   summarize timestamp, mem=mean(current_memory_usage)
   chart_line x=timestamp, y=mem
   ```

   ![Line chart with summarize](/_astro/chart-line-summarize.C-kldtdX_Z1tWhKG.webp)

   This looks a lot smoother! **Pro tip**: you can even further [optimize the above pipeline](#optimize-plotting-with-inline-expressions) by using additional operator arguments.

#### Compare multiple series

[Section titled “Compare multiple series”](#compare-multiple-series)

Our metrics data not only includes the current memory usage but also peak usage. Comparing the these two in the same chart helps us understand potentially dangerous spikes. Let's add that second series to the y-axis by upgrading from a single value to a record that represents the series.

```tql
metrics "process"
where timestamp > now() - 1d
chart_line (
  x=timestamp,
  y={current: mean(current_memory_usage), peak: max(peak_memory_usage * 1Ki)},
  resolution=15min
)
```

![Line chart with multiple series](/_astro/chart-line-multiple.Dp7WqfNy_1T4thc.webp)

Because `current_memory_usage` comes in gigabytes and `peak_memory_usage` in megabytes, we cannot compare them directly. Hence we normalized the peak usage to gigabytes to make them comparable in a single plot.

If you cannot enumerate the series to plot statically in a record, use the `group` option to specify a field that contains a unique identifier per series. Here's an example that plots the number of events per pipeline:

```tql
metrics "publish"
chart_line (
  x=timestamp,
  y=sum(events),
  x_min=now()-1d,
  group=pipeline_id,
  resolution=30min,
)
```

### Plot distributions as area chart

[Section titled “Plot distributions as area chart”](#plot-distributions-as-area-chart)

Area charts are fantastic for visualizing quantities that accumulate over a continuous variable, such as time or value ranges. They are similar to line charts but emphasize the volume underneath the line.

In the above section about line charts, you can exchange every call to [`chart_line`](/reference/operators/chart_line) with [`chart_area`](/reference/operators/chart_area) and will get a working plot.

```tql
from (
  {time: 1, a: 10, b: 20},
  {time: 2, a: 8, b: 25},
  {time: 3, a: 14, b: 30},
  {time: 4, a: 10, b: 25},
  {time: 5, a: 18, b: 40},
)
chart_area x=time, y={a: a, b: b}
```

![Area chart](/_astro/chart-area.qpx_UOJY_ZQkjSA.webp)

The area under the curve gives you a strong visual impression of the total event volume over time.

#### Stack multiple series

[Section titled “Stack multiple series”](#stack-multiple-series)

Like bar charts, area charts can display **stacked series**. This means that the values of the series add up, helping you *compare contributions* from different groups while still highlighting the overall cumulative shape.

Pass `position="stacked" to see the difference`:

```tql
from (
  {time: 1, a: 10, b: 20},
  {time: 2, a: 8, b: 25},
  {time: 3, a: 14, b: 30},
  {time: 4, a: 10, b: 25},
  {time: 5, a: 18, b: 40},
)
chart_area x=time, y={a: a, b: b}, position="stacked"
```

![Stacked area chart](/_astro/chart-area-stacked.D1LdGZR3_Z1uSgEu.webp)

Notice the difference in the y-axis interpretation:

* Without stacking, the areas *overlap* each other.
* With stacking, the areas become *disjoint* and *cumulatively add up* to the total height.

## Optimize Plotting with Inline Expressions

[Section titled “Optimize Plotting with Inline Expressions”](#optimize-plotting-with-inline-expressions)

While it's intuitive to *first* prepare your data and *then* start thinking about how to parameterize your chart operator, this leaves some opportunities for optimization and better results on the table. That's why the `chart_*` operators offer additional options for inline filtering, rounding, and summarization. These options allow you often to immediately jump to charting without having to think too much about data prepping.

To appreciate these optimization, let's start with our metrics pipeline from the above line chart example:

```tql
metrics "process"
where timestamp > now() - 1d                         // filtering
set timestamp = timestamp.round(15min)               // rounding
summarize timestamp, mem=mean(current_memory_usage)  // aggregation
chart_line x=timestamp, y=mem
```

![Line chart with summarize](/_astro/chart-line-summarize.C-kldtdX_Z1tWhKG.webp)

You can push the filtering, rounding, and aggregation into the chart operator:

```tql
metrics "process"
chart_line (
  x=timestamp,
  y=mean(current_memory_usage), // aggregation
  resolution=15min,             // rounding (= flooring)
  x_min=now() - 1d,             // filtering
)
```

![Line chart with resolution](/_astro/chart-line-resolution.CA1-QTst_Z1GGY5O.webp)

Note how this make the pipeline more succinct by removing the extra `where`, `set`, and `summarize` operators.

Here are some important details to remember:

* The `x_min`/`x_max` (also `y_min`/`y_max`) options set the visible axis domain to fixed interval. Use these options to crop or expand the viewport.
* When using the `x_min` or `x_max` options, the chart operator implicitly filters your data for the correct time range, just as if you had specified `where x >= floor(x_min, resolution) and x < ceil(x_max, resolution)`. This avoids needing to specify the ranges twice, and makes sure that the resolution is taken into account correctly.
* Specifying a duration with `resolution` option creates nicer buckets than naïve rounding, as it uses a dynamic floor for the minimum and a ceiling for the maximum value in the respective bucket. This results in nicer axis ticks that align on the bucket boundary, e.g., hourly or daily.
* The combination of `resolution` with an [aggregation function](/reference/functions#aggregation) for the `y` series is equivalent to the manual `summarize`.
* When you use `resolution`, you can additionally use the `fill` option to patch up missing values with a provided value, e.g., `fill=0` replaces otherwise empty buckets with a data point in the plot.

# Write a package

This tutorial walks you through building a package for an SSL blacklist. Packages bundle pipelines, operators, contexts, and examples. You can [install packages](/guides/basic-usage/install-a-package) from the [Tenzir Library](https://app.tenzir.com/library) or deploy them as code.

## Map the use case

[Section titled “Map the use case”](#map-the-use-case)

We’ll pick an example from the SecOps space: detect malicious certificates listed on the [SSLBL blocklist](https://sslbl.abuse.ch/).

This involves three primary actions:

1. Build a lookup table of SHA-1 hashes that mirror SSLBL data.
2. Extract SHA1 hashes of certificates in OCSF Network Activity events and compare them against the lookup table.
3. Tag matching events with the OCSF OSINT profile, so that downstream tools can escalate the match into an alert or detection finding.

We’ll begin with the managing the lookup table. But first we need to get the package scaffolding in place.

## Create the package scaffold

[Section titled “Create the package scaffold”](#create-the-package-scaffold)

Create a directory named `sslbl` and add the standard package layout:

* sslbl/

  * examples/ Runnable snippets for users

    * …

  * operators/ Reusable building blocks for pipelines

    * …

  * pipelines/ Deployable pipelines

    * …

  * tests/ Integration tests

    * …

  * package.yaml Manifest: metadata, contexts, and inputs

## Add the package manifest

[Section titled “Add the package manifest”](#add-the-package-manifest)

The [`package.yaml`](/packages/sslbl/package.yaml) is the **package manifest**. I contains descriptive metadata, but also the definitions of contexts and inputs, as we shall see below.

### Add descriptive metadata

[Section titled “Add descriptive metadata”](#add-descriptive-metadata)

sslbl/package.yaml

```yaml
name: SSLBL
author: Tenzir
author_icon: https://github.com/tenzir.png
package_icon: |
  https://raw.githubusercontent.com/tenzir/library/main/sslbl/package.svg
description: |
  The [SSLBL](https://sslbl.abuse.ch/) package provides a lookup table with
  SHA1 hashes of blacklisted certificates for TLS monitoring use cases.
```

### Define the lookup table context

[Section titled “Define the lookup table context”](#define-the-lookup-table-context)

Next, add a lookup table context to the manifest. The node creates the context when you install the package.

sslbl/package.yaml

```yaml
contexts:
  sslbl:
    type: lookup-table
    description: |
      A table keyed by SHA1 hashes of SSL certificates on the SSL blocklist.
```

## Add user-defined operators

[Section titled “Add user-defined operators”](#add-user-defined-operators)

Packages give you the ability to implement **user-defined operators** that live right next to Tenzir’s [built-in operators](/reference/operators). These custom operators are an essential capability to scale your data processing, as you can break down complex operations into smaller, testable building blocks.

### Create the user-defined operators

[Section titled “Create the user-defined operators”](#create-the-user-defined-operators)

First, we create an operator that fetches the latest SSL blocklist from the [SSLBL](https://sslbl.abuse.ch/) website. The operator in (/packages/sslbl/operators/fetch.tql) looks as follows:

operators/fetch.tql

```tql
from_http "https://sslbl.abuse.ch/blacklist/sslblacklist.csv" {
  read_csv comments=true, header="timestamp,SHA1,reason"
}
```

The relative path in the packagage defines the operator name. After installing the package, you can call this operator via `sslbl::fetch`. It will produce events of this shape:

```tql
{
  timestamp: 2014-05-04T08:09:56Z,
  SHA1: "b08a4939fb88f375a2757eaddc47b1fb8b554439",
  reason: "Shylock C&C",
}
```

Let’s create another operator to map this data to [OSINT objects](https://schema.ocsf.io/1.6.0/objects/osint)—the standardized representation of an indicators of compromise (IOCs) in OCSF.

operators/ocsf/to\_osint.tql

```tql
confidence = "High"
confidence_id = 3
created_time = move timestamp
malware = [{
  classification_ids: [3],
  classifications: ["Bot"],
  // The source lists the name as "$NAME C&C" and we drop the C&C suffix.
  name: (move reason).split(" ")[0],
}]
value = move SHA1
type = "Hash"
type_id = 4
```

This pipeline translates the original feed into this shape:

```tql
{
  confidence: "High",
  confidence_id: 3,
  created_time: 2014-05-04T08:09:56Z,
  malware: [
    {
      classification_ids: [
        3,
      ],
      classifications: [
        "Bot",
      ],
      name: "Shylock",
    },
  ],
  value: "b08a4939fb88f375a2757eaddc47b1fb8b554439",
  type: "Hash",
  type_id: 4,
}
```

OCSF Verbosity

You may notice that this shape is a lot more verbose than the original event. Don’t worry, it is absolutely normal when upgrading your raw data to a semantically richer representation like OCSF. You can always trim the feed down again later, either automatically with our [`ocsf::trim`](/reference/operators/ocsf/trim) operator or manually by [`drop`](/reference/operators/drop)ping fields. But while the data is in motion, the additional semantics unlock generic analytics when the context of the original source is long gone.

We’re not done yet. Let’s create one final operator that wraps a single fetch into an OCSF event that describes a single collection of IoCs: the [OSINT Inventory Info](https://schema.ocsf.io/1.6.0/classes/osint_inventory_info) event.

operators/ocsf/to\_osint\_inventory\_info.tql

```tql
// Collect a single fetch into an array of OSINT objects.
summarize osint=collect(this)
// Categorization
activity_id = 2
activity_name = "Collect"
category_uid = 5
category_name = "Discovery"
class_uid = 5021
class_name = "OSINT Inventory Info"
severity_id = 1
severity = "Informational"
type_uid = class_uid * 100 + activity_id
// Additional context attributes
actor = {
  app_name: "Tenzir"
}
metadata = {
  product: {
    name: "SSLBL SSL Certificate Blacklist",
    vendor_name: "abuse.ch",
  },
  version: "1.6.0",
}
// Occurence attributes
time = now()
// Apply Tenzir event metadata
@name = "ocsf.osint_inventory_info"
```

We can now call all three operators in one shot to construct an OCSF event:

```tql
sslbl::fetch
sslbl::ocsf::to_osint
sslbl::ocsf::to_osint_inventory_info
```

Now that we have building blocks, let’s combine them into something meaningful.

OCSF Mapping Tutorial

Mapping data to OCSF can feel like a daunting task. Check out our [dedicated tutorial OCSF mapping](/tutorials/map-data-to-ocsf) where we cover OCSF at great length.

## Add deployable pipelines

[Section titled “Add deployable pipelines”](#add-deployable-pipelines)

With our user-defined operators, we get building blocks, but not yet entire pipelines.

We now create a pipeline that downloads SSLBL data and updates the context periodically so that we always have the latest version of the SSLBL data for enrichment.

The `sslbl::fetch` operator just downloads the blacklist entries once. But the remote data source changes periodically, and we want to always work with the latest version. So we turn the one-shot download into a continuous data feed using the [`every`](/reference/operators/every) operator:

sslbl/pipelines/publish-as-ocsf.tql

```tql
every 1h {
  sslbl::fetch
}
sslbl::ocsf::to_osint
sslbl::ocsf::to_osint_inventory_info
publish "ocsf"
```

This is a closed pipeline, meaning, it has an input operator ([`every`](/reference/operators/every)) and an output operator ([`publish`](/reference/operators/publish)). The pipeline produces a new OCSF Inventory Info event every hour and publishes it to the `ocsf` topic so that other pipelines in the same node can consume it. This is a best-practice design pattern to expose data that you may reuse multiple times.

But instead of publishing the data as OCSF events and subscribing to it afterwards, we can directly update the lookup table from the plain OSINT objects:

sslbl/pipelines/update-lookup-table.tql

```tql
every 1h {
  sslbl::fetch
}
sslbl::ocsf::to_osint
context::update "sslbl", key=value
```

Thanks to our user-defined operators, implementing these two different pipelines doesn’t take much effort.

## Add examples

[Section titled “Add examples”](#add-examples)

To illustrate how others can use the package, we encourage package authors to add a few TQL snippets to the `examples` directory in the package.

### Example 1: One-shot lookup table update

[Section titled “Example 1: One-shot lookup table update”](#example-1-one-shot-lookup-table-update)

Here’s a snippet that perform a single fetch followed by an update of the lookup table:

examples/one-shot-update.tql

```tql
---
description: |
  This example demonstrates how to fetch SSLBL data, convert it to OSINT format,
  and update the context with the new data.
---


sslbl::fetch
sslbl::ocsf::to_osint
context::update "sslbl", key=value
```

### Example 2: Enrich with the context

[Section titled “Example 2: Enrich with the context”](#example-2-enrich-with-the-context)

What do we do with feed of SHA1 hashes that correspond to bad certificates? One natural use case is to look at TLS traffic and compare these values with the SHA1 hashes in the feed.

Here’s a pipeline for this:

examples/enrich-network-activity.tql

```tql
---
description: |
  Subscribe to all OCSF events and extract those network events containing SHA-1
  certificate hashes. Correlate those events with the SSLBL database and attach
  the OSINT profile to matching events.
---


subscribe "ocsf"
where category_uid == 4
// Filter out network events that have SHA1 certificate hashes.
where not tls?.certificate?.fingerprints?.where(x => x.algorithm_id == 2).is_empty()
// Convert the list of SHA1 hashes into a record for enrichment. In the future,
// we'd want to enrich also within arrays. When we unroll we unfortunately lose
// all other certificate hash values, so this is sub-optimal.
unroll tls.certificate.fingerprints
enrich "sslbl",
  key=tls.certificate.fingerprints.value,
  into=_tmp
// Slap OSINT profile onto the event on match.
if _tmp != null {
  osint.add(move _tmp)
  metadata.profiles?.add("osint")
} else {
  drop _tmp
}
publish "ocsf-osint"
```

This pipelines hones in on OCSF Network Activity events (`category_uid == 4`) that come with a SHA1 TLS certificate fingerprint (`algorithm_id == 2`). If we have a matche, we add the `osint` profile to the event and publish it to separate topic `ocsf-osint` for further processing.

### Example 3: Show a summary of the dataset

[Section titled “Example 3: Show a summary of the dataset”](#example-3-show-a-summary-of-the-dataset)

examples/top-malware.tql

```tql
---
description: |
  Inspect all data in the context and count the different malware types,
  rendering the result as a pie chart.
---


context::inspect "sslbl"
select malware = value.malware[0].name
top malware
chart_pie x=malware, y=count
```

## Make your package configurable

[Section titled “Make your package configurable”](#make-your-package-configurable)

To make a package more reusable, **inputs** offer a simple templating mechanism to replace variables with user-provided values.

For example, to replace the hard-coded 1-hour refresh cadence with an input, replace the value with `{{ inputs.refresh_interval }}` in the above pipeline:

sslbl/pipelines/update-as-ocsf.tql

```tql
every {{ inputs.refresh_interval }} {
  sslbl::fetch
}
sslbl::ocsf::to_osint
context::update "sslbl", key=value
```

Then add the input to your `package.yaml`:

sslbl/package.yaml

```yaml
inputs:
  refresh_interval:
    name: Time between context updates
    description: |
      How often the pipeline refreshes the SSLBL lookup table.
    default: 1h
```

Users can accept the default or override the value [during installation](/guides/basic-usage/install-a-package), e.g., when using [`package::add`](/reference/operators/package/add):

```tql
package::add "/path/to/sslbl", inputs={refresh_interval: 24h}
```

## Test your package

[Section titled “Test your package”](#test-your-package)

Testing ensures that you always have a working package during development. The earlier you start, the better!

### Add tests for your operators

[Section titled “Add tests for your operators”](#add-tests-for-your-operators)

Since our package ships with user-defined operators, we highly recommend to write tests for them, for the following reasons:

1. You help users gain confidence in the functionality.
2. You provide illustrative input-output pairs.
3. You evolve faster with less regressions.

Yes, writing tests is often boring and cumbersome. But it doesn’t have to be that way! With our purpose-built [test framework](/reference/test-framework) for the Tenzir ecosystem, it is actually fun. 🕺

Let’s bootstrap the `tests` directory:

```sh
mkdir tests
mkdir tests/inputs # files we reference from tests
```

We’ll put a trimmed version of <https://sslbl.abuse.ch/blacklist/sslblacklist.csv> into `tests/inputs/`:

tests/inputs/sslblacklist.csv

```csv
################################################################
# abuse.ch SSLBL SSL Certificate Blacklist (SHA1 Fingerprints) #
# Last updated: 2025-10-08 06:32:12 UTC                        #
#                                                              #
# Terms Of Use: https://sslbl.abuse.ch/blacklist/              #
# For questions please contact sslbl [at] abuse.ch             #
################################################################
#
# Listingdate,SHA1,Listingreason
2025-10-08 06:32:12,e8f4490420d0b0fc554d1296a8e9d5c35eb2b36e,Vidar C&C
2025-10-08 06:12:48,d9b07483491c0748a479308b29c5c754b92d6e06,ACRStealer C&C
2025-10-08 06:10:17,31740cfc82d05c82280fd6cce503543a150e861f,Rhadamanthys C&C
2025-10-08 06:09:30,302ed4eeb28e1e8ca560c645b8eb342498300134,Rhadamanthys C&C
2025-10-08 06:08:57,eaf3a17e7f86a626afcfce9f4a85ac20a7f62a67,Rhadamanthys C&C
2025-10-07 18:25:40,5b7f9db9187f5ccfaf1bcdb49f9d1db0396dabde,ACRStealer C&C
2025-10-07 18:24:31,70cf9d9812b38101361bd8855274bf1766840837,OffLoader C&C
2025-10-07 18:24:30,6e4ea638522aa0f5f6e7f14571f5ff89826f6e07,OffLoader C&C
2025-10-07 06:15:35,fc0afaa2e30b121c2d929827afd69e4c63669ac3,ACRStealer C&C
2025-10-07 06:14:59,7948950f56e714a3ecf853664ffa65ae86a70051,QuasarRAT C&C
2025-10-07 06:14:51,1fcb051a4dfa5999fee4877bbd4141f22b3a4074,AsyncRAT C&C
2025-10-07 05:53:50,e5632523b028288923c241251c9f1b6275e3db61,OffLoader C&C
2025-10-07 05:53:49,8b6d220268c0c6206f4164fb8422bac2653924b4,OffLoader C&C
2025-10-07 05:53:07,9d686b89970266a03647f2118f143ecb47f59188,Rhadamanthys C&C
2025-10-07 05:52:18,89e31fdaeefdaddcf0554e3b3e2d48c1b683abbd,Rhadamanthys C&C
2025-10-07 05:51:35,79a805627bc3b86aa42272728209fc28019de02f,Rhadamanthys C&C
2025-10-06 11:01:21,fc55fdafe2f70b2f409bbf7f89613ca2ca915181,QuasarRAT C&C
2025-10-06 06:44:36,6f932e3a0bf05164eb2bf02cfb5a29c1b210ebb2,Mythic C&C
2025-10-06 06:44:31,8f442060bb10d59eca5d8c9b7cd6d8653a3c3ac8,Vidar C&C
2025-10-06 06:44:27,7cd14588ba5bbeb56ce42f05023bd2f5159d8f19,Vidar C&C
2025-10-06 06:42:52,b36f4c106bf048b68b43b448835f5dab1c982083,QuasarRAT C&C
2025-10-04 10:46:00,bf0b77de0046e53200e5287be0916f6921e86336,ACRStealer C&C
2025-10-04 10:44:54,e33db1f571fdb786ecc8e5a4b43131fdcbe2c0d1,ACRStealer C&C
2025-10-04 10:44:38,c75f347253ebf8a4c26053d2db7ce5bf3e1417f5,QuasarRAT C&C
2025-10-04 10:43:27,fc216972dbec1903de5622107dc97b6c33535074,QuasarRAT C&C
2025-10-04 10:43:21,2e14c326f940b962e2ecefe69a5ee4fc12d26cab,AsyncRAT C&C
```

Let’s test the operator that maps our input to OCSF OSINT objects:

tests/ocsf/to\_osint.tql

```tql
from_file f"{env("TENZIR_INPUTS")}/sslblacklist.csv" {
  read_csv comments=true, header="timestamp,SHA1,reason"
}
sslbl::ocsf::to_osint
head 1
```

We first watch the terminal output it in passthrough mode:

```sh
uvx tenzir-test --passthrough
```

tests/ocsf/to\_osint.txt

```tql
{
  confidence: "High",
  confidence_id: 3,
  created_time: 2025-10-08T06:32:12Z,
  malware: [
    {
      classification_ids: [
        3,
      ],
      classifications: [
        "Bot",
      ],
      name: "Vidar",
    },
  ],
  value: "e8f4490420d0b0fc554d1296a8e9d5c35eb2b36e",
  type: "Hash",
  type_id: 4,
}
```

As expected, a valid OCSF OSINT object. Let’s make confirm this as our new baseline:

```sh
uvx tenzir-test --update
```

This created a [`to_osint.txt`](/packages/sslbl/tests/ocsf/to_osint.txt) file next to the [`to_osint.tql`](/packages/sslbl/tests/ocsf/to_osint.tql) file. Future runs will use this baseline for comparisons.

Continue to test the remaining operators, or add additional tests for some examples.

### Test contexts and node interaction

[Section titled “Test contexts and node interaction”](#test-contexts-and-node-interaction)

Our package defines a context that lives in a node. Writing tests with nodes is a bit more involved than writing tests for stateless operators that we can simply run through the `tenzir` binary.

To test node interactions, we need to use the **suite** concept from the test framework, which spins up a single fixture and then executes a series of tests sequentially against that fixture. With that capability, we can finally test the multi-stage process of updating the context, inspecting it, and using it for an enrichment.

Defining a suite doesn’t take much, just add a `test.yaml` file to a sub-directory that represents the suite of tests. We’ll do this:

* sslbl/tests/context/

  * 01-context-list.tql
  * 02-context-update.tql
  * 03-context-inspect.tql
  * test.yaml

Here’s the test suite definition:

tests/context/test.yaml

```yaml
suite: context
fixtures: [node]
timeout: 30
```

Let’s take a look at our tests:

tests/context/01-context-list.tql

```tql
// Ensure the package installed the context properly.
context::list
```

Then load our input into the node’s lookup table:

tests/context/02-context-update.tql

```tql
from_file f"{env("TENZIR_INPUTS")}/sslblacklist.csv" {
  read_csv comments=true, header="timestamp,SHA1,reason"
}
context::update "sslbl",
  key=SHA1,
  value={time: timestamp, malware: reason}
```

Finally ensure that the lookup table has the expected values:

tests/context/03-context-inspect.tql

```tql
// Summarize the context.
context::inspect "sslbl"
select malware = value.malware
top malware
```

Automatic package installation

When the test harness invokes the `tenzir` (as runner) and `tenzir-node` binary (as fixture), it passes the `--package-dirs` option in both cases. This ensures that user-defined operators are available for testing with the runner and contexts in the fixture. When writing tests, you can simply assume that the package is installed at the node.

## Share and contribute

[Section titled “Share and contribute”](#share-and-contribute)

Phew, you made it! You now have a reusable package. 🎉

Now that you have a package, what’s next?

1. Join our [Discord server](/discord) and showcase the package in the `show-and-tell` channel to gather feedback.
2. If you deem it useful for everyone, open a pull request in our [Community Library on GitHub](https://github.com/tenzir/library). Packages from this library appear automatically in the [Tenzir Library](https://app.tenzir.com/library).
3. Spread the word on social media and tag us so we can amplify it.

AI-based package creation

It’s quite a bit of work to manually create a package. In the age of AI and modern agentic tooling, you have powertools available to fully automate this task. Stay tuned for updates on our [MCP Server](/reference/mcp-server), as we have plans to make package creation a 100% hands-off-keyboard activity.

# Tenzir Node Changelog

This page lists the changelog for Tenzir Node.

## Versions

[Section titled “Versions”](#versions)

* [Version 5.18.0](/changelog/node/v5-18-0)
* [Version 5.17.0](/changelog/node/v5-17-0)
* [Version 5.16.0](/changelog/node/v5-16-0)
* [Version 5.15.0](/changelog/node/v5-15-0)
* [Version 5.14.0](/changelog/node/v5-14-0)
* [Version 5.13.2](/changelog/node/v5-13-2)
* [Version 5.13.1](/changelog/node/v5-13-1)
* [Version 5.13.0](/changelog/node/v5-13-0)
* [Version 5.12.1](/changelog/node/v5-12-1)
* [Version 5.12.0](/changelog/node/v5-12-0)
* [Version 5.11.1](/changelog/node/v5-11-1)
* [Version 5.11.0](/changelog/node/v5-11-0)
* [Version 5.10.0](/changelog/node/v5-10-0)
* [Version 5.9.0](/changelog/node/v5-9-0)
* [Version 5.8.0](/changelog/node/v5-8-0)
* [Version 5.7.0](/changelog/node/v5-7-0)
* [Version 5.6.1](/changelog/node/v5-6-1)
* [Version 5.6.0](/changelog/node/v5-6-0)
* [Version 5.5.0](/changelog/node/v5-5-0)
* [Version 5.4.1](/changelog/node/v5-4-1)
* [Version 5.4.0](/changelog/node/v5-4-0)
* [Version 5.3.4](/changelog/node/v5-3-4)
* [Version 5.3.3](/changelog/node/v5-3-3)
* [Version 5.3.2](/changelog/node/v5-3-2)
* [Version 5.3.0](/changelog/node/v5-3-0)
* [Version 5.2.0](/changelog/node/v5-2-0)
* [Version 5.1.8](/changelog/node/v5-1-8)
* [Version 5.1.7](/changelog/node/v5-1-7)
* [Version 5.1.6](/changelog/node/v5-1-6)
* [Version 5.1.5](/changelog/node/v5-1-5)
* [Version 5.1.4](/changelog/node/v5-1-4)
* [Version 5.1.3](/changelog/node/v5-1-3)
* [Version 5.1.2](/changelog/node/v5-1-2)
* [Version 5.1.1](/changelog/node/v5-1-1)
* [Version 5.1.0](/changelog/node/v5-1-0)
* [Version 5.0.1](/changelog/node/v5-0-1)
* [Version 5.0.0](/changelog/node/v5-0-0)
* [Version 4.32.1](/changelog/node/v4-32-1)
* [Version 4.32.0](/changelog/node/v4-32-0)
* [Version 4.31.2](/changelog/node/v4-31-2)
* [Version 4.31.0](/changelog/node/v4-31-0)
* [Version 4.30.3](/changelog/node/v4-30-3)
* [Version 4.30.2](/changelog/node/v4-30-2)
* [Version 4.30.1](/changelog/node/v4-30-1)
* [Version 4.30.0](/changelog/node/v4-30-0)
* [Version 4.29.2](/changelog/node/v4-29-2)
* [Version 4.29.1](/changelog/node/v4-29-1)
* [Version 4.29.0](/changelog/node/v4-29-0)
* [Version 4.28.2](/changelog/node/v4-28-2)
* [Version 4.28.0](/changelog/node/v4-28-0)
* [Version 4.27.0](/changelog/node/v4-27-0)
* [Version 4.26.0](/changelog/node/v4-26-0)
* [Version 4.25.0](/changelog/node/v4-25-0)
* [Version 4.24.1](/changelog/node/v4-24-1)
* [Version 4.24.0](/changelog/node/v4-24-0)
* [Version 4.23.1](/changelog/node/v4-23-1)
* [Version 4.23.0](/changelog/node/v4-23-0)
* [Version 4.22.2](/changelog/node/v4-22-2)
* [Version 4.22.1](/changelog/node/v4-22-1)
* [Version 4.22.0](/changelog/node/v4-22-0)
* [Version 4.21.1](/changelog/node/v4-21-1)
* [Version 4.21.0](/changelog/node/v4-21-0)
* [Version 4.20.3](/changelog/node/v4-20-3)
* [Version 4.20.2](/changelog/node/v4-20-2)
* [Version 4.20.1](/changelog/node/v4-20-1)
* [Version 4.20.0](/changelog/node/v4-20-0)
* [Version 4.19.6](/changelog/node/v4-19-6)
* [Version 4.19.5](/changelog/node/v4-19-5)
* [Version 4.19.4](/changelog/node/v4-19-4)
* [Version 4.19.3](/changelog/node/v4-19-3)
* [Version 4.19.2](/changelog/node/v4-19-2)
* [Version 4.19.1](/changelog/node/v4-19-1)
* [Version 4.19.0](/changelog/node/v4-19-0)
* [Version 4.18.5](/changelog/node/v4-18-5)
* [Version 4.18.4](/changelog/node/v4-18-4)
* [Version 4.18.3](/changelog/node/v4-18-3)
* [Version 4.18.2](/changelog/node/v4-18-2)
* [Version 4.18.1](/changelog/node/v4-18-1)
* [Version 4.18.0](/changelog/node/v4-18-0)
* [Version 4.17.4](/changelog/node/v4-17-4)
* [Version 4.17.3](/changelog/node/v4-17-3)
* [Version 4.17.2](/changelog/node/v4-17-2)
* [Version 4.17.1](/changelog/node/v4-17-1)
* [Version 4.17.0](/changelog/node/v4-17-0)
* [Version 4.16.0](/changelog/node/v4-16-0)
* [Version 4.15.2](/changelog/node/v4-15-2)
* [Version 4.15.1](/changelog/node/v4-15-1)
* [Version 4.15.0](/changelog/node/v4-15-0)
* [Version 4.14.0](/changelog/node/v4-14-0)
* [Version 4.13.1](/changelog/node/v4-13-1)
* [Version 4.13.0](/changelog/node/v4-13-0)
* [Version 4.12.2](/changelog/node/v4-12-2)
* [Version 4.12.1](/changelog/node/v4-12-1)
* [Version 4.12.0](/changelog/node/v4-12-0)
* [Version 4.11.2](/changelog/node/v4-11-2)
* [Version 4.11.0](/changelog/node/v4-11-0)
* [Version 4.10.4](/changelog/node/v4-10-4)
* [Version 4.10.3](/changelog/node/v4-10-3)
* [Version 4.10.1](/changelog/node/v4-10-1)
* [Version 4.10.0](/changelog/node/v4-10-0)
* [Version 4.9.0](/changelog/node/v4-9-0)
* [Version 4.8.2](/changelog/node/v4-8-2)
* [Version 4.8.1](/changelog/node/v4-8-1)
* [Version 4.8.0](/changelog/node/v4-8-0)
* [Version 4.7.1](/changelog/node/v4-7-1)
* [Version 4.7.0](/changelog/node/v4-7-0)
* [Version 4.6.4](/changelog/node/v4-6-4)
* [Version 4.6.3](/changelog/node/v4-6-3)
* [Version 4.6.0](/changelog/node/v4-6-0)
* [Version 4.5.0](/changelog/node/v4-5-0)
* [Version 4.4.0](/changelog/node/v4-4-0)
* [Version 4.3.0](/changelog/node/v4-3-0)
* [Version 4.2.0](/changelog/node/v4-2-0)
* [Version 4.1.0](/changelog/node/v4-1-0)
* [Version 4.0.1](/changelog/node/v4-0-1)
* [Version 4.0.0](/changelog/node/v4-0-0)
* [Version 3.1.0](/changelog/node/v3-1-0)
* [Version 3.0.4](/changelog/node/v3-0-4)
* [Version 3.0.3](/changelog/node/v3-0-3)
* [Version 3.0.2](/changelog/node/v3-0-2)
* [Version 3.0.1](/changelog/node/v3-0-1)
* [Version 3.0.0](/changelog/node/v3-0-0)
* [Version 2.4.2](/changelog/node/v2-4-2)
* [Version 2.4.1](/changelog/node/v2-4-1)
* [Version 2.4.0](/changelog/node/v2-4-0)
* [Version 2.3.1](/changelog/node/v2-3-1)
* [Version 2.3.0](/changelog/node/v2-3-0)
* [Version 2.2.0](/changelog/node/v2-2-0)
* [Version 2.1.0](/changelog/node/v2-1-0)
* [Version 2.0.0](/changelog/node/v2-0-0)
* [Version 1.1.2](/changelog/node/v1-1-2)
* [Version 1.1.1](/changelog/node/v1-1-1)
* [Version 1.1.0](/changelog/node/v1-1-0)
* [Version 1.0.0](/changelog/node/v1-0-0)
* [Version 2021.12.16](/changelog/node/2021-12-16)
* [Version 2021.11.18](/changelog/node/2021-11-18)
* [Version 2021.09.30](/changelog/node/2021-09-30)
* [Version 2021.08.26](/changelog/node/2021-08-26)
* [Version 2021.07.29](/changelog/node/2021-07-29)
* [Version 2021.06.24](/changelog/node/2021-06-24)
* [Version 2021.05.27](/changelog/node/2021-05-27)
* [Version 2021.04.29](/changelog/node/2021-04-29)
* [Version 2021.03.25](/changelog/node/2021-03-25)
* [Version 2021.02.24](/changelog/node/2021-02-24)
* [Version 2021.01.28](/changelog/node/2021-01-28)
* [Version 2020.12.16](/changelog/node/2020-12-16)
* [Version 2020.10.29](/changelog/node/2020-10-29)
* [Version 2020.09.30](/changelog/node/2020-09-30)
* [Version 2020.08.28](/changelog/node/2020-08-28)
* [Version 2020.07.28](/changelog/node/2020-07-28)
* [Version 2020.06.25](/changelog/node/2020-06-25)
* [Version 2020.05.28](/changelog/node/2020-05-28)
* [Version 2020.04.29](/changelog/node/2020-04-29)
* [Version 2020.03.26](/changelog/node/2020-03-26)
* [Version 2020.02.27](/changelog/node/2020-02-27)
* [Version 2020.01.31](/changelog/node/2020-01-31)

# VAST 2020.01.31

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.01.31).

### Features

[Section titled “Features”](#features)

#### Add -c as shorthand for —config

[Section titled “Add -c as shorthand for —config”](#add--c-as-shorthand-for-config)

The long option `--config`, which sets an explicit path to the VAST configuration file, now also has the short option `-c`.

By [@mavam](https://github.com/mavam) in [#689](https://github.com/tenzir/tenzir/pull/689).

#### PRs 632-726

[Section titled “PRs 632-726”](#prs-632-726)

When a record field has the `#index=hash` attribute, VAST will choose an optimized index implementation. This new index type only supports (in)equality queries and is therefore intended to be used with opaque types, such as unique identifiers or random strings.

By [@mavam](https://github.com/mavam) in [#632](https://github.com/tenzir/tenzir/pull/632).

#### Add support for Apache Arrow

[Section titled “Add support for Apache Arrow”](#add-support-for-apache-arrow)

Added *Apache Arrow* as new export format. This allows users to export query results as Apache Arrow record batches for processing the results downstream, e.g., in Python or Spark.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#633](https://github.com/tenzir/tenzir/pull/633).

#### Allow configuring pcap snapshot length

[Section titled “Allow configuring pcap snapshot length”](#allow-configuring-pcap-snapshot-length)

The `import pcap` command now takes an optional snapshot length via `--snaplen`. If the snapshot length is set to snaplen, and snaplen is less than the size of a packet that is captured, only the first snaplen bytes of that packet will be captured and provided as packet data.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#642](https://github.com/tenzir/tenzir/pull/642).

#### Add FreeBSD rc.d script

[Section titled “Add FreeBSD rc.d script”](#add-freebsd-rcd-script)

On FreeBSD, a VAST installation now includes an rc.d script that simpliefies spinning up a VAST node. CMake installs the script at `PREFIX/etc/rc.d/vast`.

By [@mavam](https://github.com/mavam) in [#693](https://github.com/tenzir/tenzir/pull/693).

#### Add Python module for submitting queries to VAST

[Section titled “Add Python module for submitting queries to VAST”](#add-python-module-for-submitting-queries-to-vast)

An experimental new Python module enables querying VAST and processing results as [pyarrow](https://arrow.apache.org/docs/python/) tables.

By [@tobim](https://github.com/tobim) in [#685](https://github.com/tenzir/tenzir/pull/685).

### Changes

[Section titled “Changes”](#changes)

#### Add Python module for submitting queries to VAST

[Section titled “Add Python module for submitting queries to VAST”](#add-python-module-for-submitting-queries-to-vast-1)

Record field names can now be entered as quoted strings in the schema and expression languages. This lifts a restriction where JSON fields with whitespaces or special characters could not be ingested.

By [@tobim](https://github.com/tobim) in [#685](https://github.com/tenzir/tenzir/pull/685).

#### Minor parser changes

[Section titled “Minor parser changes”](#minor-parser-changes)

Two minor modifications were done in the parsing framework: (i) the parsers for enums and records now allow trailing separators, and (ii) the dash (`-`) was removed from the allowed characters of schema type names.

By [@tobim](https://github.com/tobim) in [#706](https://github.com/tenzir/tenzir/pull/706).

#### Add separate interface option for import pcap

[Section titled “Add separate interface option for import pcap”](#add-separate-interface-option-for-import-pcap)

The `import pcap` command no longer takes interface names via `--read,-r`, but instead from a separate option named `--interface,-i`. This change has been made for consistency with other tools.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#641](https://github.com/tenzir/tenzir/pull/641).

#### Switch to a calendar-based versioning scheme

[Section titled “Switch to a calendar-based versioning scheme”](#switch-to-a-calendar-based-versioning-scheme)

VAST is switching to a calendar-based versioning scheme starting with this release.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#739](https://github.com/tenzir/tenzir/pull/739).

#### Perform pass over build config defaults

[Section titled “Perform pass over build config defaults”](#perform-pass-over-build-config-defaults)

Build configuration defaults have been adapated for a better user experience. Installations are now relocatable by default, which can be reverted by configuring with `--without-relocatable`. Additionally, new sets of defaults named `--release` and `--debug` (renamed from `--dev-mode`) have been added.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#695](https://github.com/tenzir/tenzir/pull/695).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix configuration file option parsing

[Section titled “Fix configuration file option parsing”](#fix-configuration-file-option-parsing)

The example configuration file contained an invalid section `vast`. This has been changed to the correct name `system`.

By [@tobim](https://github.com/tobim) in [#705](https://github.com/tenzir/tenzir/pull/705).

#### Fix datagram source actor not running heartbeat

[Section titled “Fix datagram source actor not running heartbeat”](#fix-datagram-source-actor-not-running-heartbeat)

The import process did not print statistics when importing events over UDP. Additionally, warnings about dropped UDP packets are no longer shown per packet, but rather periodically reported in a readable format.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#662](https://github.com/tenzir/tenzir/pull/662).

#### Fix race in index lookup

[Section titled “Fix race in index lookup”](#fix-race-in-index-lookup)

A race condition in the index logic was able to lead to incomplete or empty result sets for `vast export`.

By [@tobim](https://github.com/tobim) in [#703](https://github.com/tenzir/tenzir/pull/703).

#### Refactor importer initialization

[Section titled “Refactor importer initialization”](#refactor-importer-initialization)

In some cases it was possible that a source would connect to a node before it was fully initialized, resulting in a hanging `vast import` process.

By [@tobim](https://github.com/tobim) in [#647](https://github.com/tenzir/tenzir/pull/647).

#### Ignore VLAN tags in PCAP import

[Section titled “Ignore VLAN tags in PCAP import”](#ignore-vlan-tags-in-pcap-import)

PCAP ingestion failed for traces containing VLAN tags. VAST now strips [IEEE 802.1Q](https://en.wikipedia.org/wiki/IEEE_802.1Q) headers instead of skipping VLAN-tagged packets.

By [@mavam](https://github.com/mavam) in [#650](https://github.com/tenzir/tenzir/pull/650).

#### Register the accountant for datagram sources

[Section titled “Register the accountant for datagram sources”](#register-the-accountant-for-datagram-sources)

Importing events over UDP with `vast import <format> --listen :<port>/udp` failed to register the accountant component. This caused an unexpected message warning to be printed on startup and resulted in losing import statistics. VAST now correctly registers the accountant.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#655](https://github.com/tenzir/tenzir/pull/655).

#### Add Python module for submitting queries to VAST

[Section titled “Add Python module for submitting queries to VAST”](#add-python-module-for-submitting-queries-to-vast-2)

A bug in the quoted string parser caused a parsing failure if an escape character occurred in the last position.

By [@tobim](https://github.com/tobim) in [#685](https://github.com/tenzir/tenzir/pull/685).

# VAST 2020.02.27

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.02.27).

### Features

[Section titled “Features”](#features)

#### Add a nix package expression for VAST

[Section titled “Add a nix package expression for VAST”](#add-a-nix-package-expression-for-vast)

For users of the [Nix](https://nixos.org/nix/) package manager, expressions have been added to generate reproducible development environments with `nix-shell`.

By [@tobim](https://github.com/tobim) in [#740](https://github.com/tenzir/tenzir/pull/740).

### Changes

[Section titled “Changes”](#changes)

#### Revert “Annotate schemas with

[Section titled “Revert “Annotate schemas with”](#revert-annotate-schemas-with)

Hash indices have been disabled again due to a performance regression.

By [@lava](https://github.com/lava) in [#765](https://github.com/tenzir/tenzir/pull/765).

#### Remove default option for historical queries

[Section titled “Remove default option for historical queries”](#remove-default-option-for-historical-queries)

The option `--historical` for export commands has been removed, as it was the default already.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#754](https://github.com/tenzir/tenzir/pull/754).

#### Update VAST for Apache Arrow 0.16

[Section titled “Update VAST for Apache Arrow 0.16”](#update-vast-for-apache-arrow-016)

VAST now supports (and requires) Apache Arrow >= 0.16.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#751](https://github.com/tenzir/tenzir/pull/751).

#### Add a nix package expression for VAST

[Section titled “Add a nix package expression for VAST”](#add-a-nix-package-expression-for-vast-1)

The build system will from now on try use the CAF library from the system, if one is provided. If it is not found, the CAF submodule will be used as a fallback.

By [@tobim](https://github.com/tobim) in [#740](https://github.com/tenzir/tenzir/pull/740).

#### Allow for separating persistent state and log directories

[Section titled “Allow for separating persistent state and log directories”](#allow-for-separating-persistent-state-and-log-directories)

The option `--directory` has been replaced by `--db-directory` and `log-directory`, which set directories for persistent state and log files respectively. The default log file path has changed from `vast.db/log` to `vast.log`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#758](https://github.com/tenzir/tenzir/pull/758).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Continuous import of Zeek logs in VAST is broken

[Section titled “Continuous import of Zeek logs in VAST is broken”](#continuous-import-of-zeek-logs-in-vast-is-broken)

Continuously importing events from a Zeek process with a low rate of emitted events resulted in a long delay until the data would be included in the result set of queries. This is because the import process would buffer up to 10,000 events before sending them to the server as a batch. The algorithm has been tuned to flush its buffers if no data is available for more than 500 milliseconds.

By [@tobim](https://github.com/tobim) in [#750](https://github.com/tenzir/tenzir/pull/750).

# VAST 2020.03.26

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.03.26).

### Features

[Section titled “Features”](#features)

#### Use heterogenous lookup for hash index

[Section titled “Use heterogenous lookup for hash index”](#use-heterogenous-lookup-for-hash-index)

The hash index has been re-enabled after it was outfitted with a new [high-performance hash map](https://github.com/Tessil/robin-map/) implementation that increased performance to the point where it is on par with the regular index.

By [@lava](https://github.com/lava) in [#796](https://github.com/tenzir/tenzir/pull/796).

#### Require end-of-input to be reached for range-based parser invocations

[Section titled “Require end-of-input to be reached for range-based parser invocations”](#require-end-of-input-to-be-reached-for-range-based-parser-invocations)

An under-the-hood change to our parser-combinator framework makes sure that we do not discard possibly invalid input data up the the end of input. This uncovered a bug in our MRT/bgpdump integrations, which have thus been disabled (for now), and will be fixed at a later point in time.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#808](https://github.com/tenzir/tenzir/pull/808).

#### Fix user shutdown handling for continuous exports

[Section titled “Fix user shutdown handling for continuous exports”](#fix-user-shutdown-handling-for-continuous-exports)

Continuous export processes can now be stopped correctly. Before this change, the node showed an error message and the exporting process exited with a non-zero exit code.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#779](https://github.com/tenzir/tenzir/pull/779).

#### Implement reader for Syslog RFC5424

[Section titled “Implement reader for Syslog RFC5424”](#implement-reader-for-syslog-rfc5424)

The new `vast import syslog` command allows importing Syslog messages as defined in [RFC5424](https://tools.ietf.org/html/rfc5424).

By [@knapperzbusch](https://github.com/knapperzbusch) in [#770](https://github.com/tenzir/tenzir/pull/770).

#### Remove -c short option for setting config file

[Section titled “Remove -c short option for setting config file”](#remove--c-short-option-for-setting-config-file)

The short option `-c` for setting the configuration file has been removed. The long option `--config` must now be used instead. This fixed a bug that did not allow for `-c` to be used for continuous exports.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#781](https://github.com/tenzir/tenzir/pull/781).

#### Allow disabling Community ID computation for PCAPs

[Section titled “Allow disabling Community ID computation for PCAPs”](#allow-disabling-community-id-computation-for-pcaps)

The option `--disable-community-id` has been added to the `vast import pcap` command for disabling the automatic computation of Community IDs.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#777](https://github.com/tenzir/tenzir/pull/777).

#### Require expressions to be parsed to end-of-input

[Section titled “Require expressions to be parsed to end-of-input”](#require-expressions-to-be-parsed-to-end-of-input)

Expressions must now be parsed to the end of input. This fixes a bug that caused malformed queries to be evaluated until the parser failed. For example, the query `#type == "suricata.http" && .dest_port == 80` was erroneously evaluated as `#type == "suricata.http"` instead.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#791](https://github.com/tenzir/tenzir/pull/791).

### Changes

[Section titled “Changes”](#changes)

#### Rename vast.account event type to vast.statistics

[Section titled “Rename vast.account event type to vast.statistics”](#rename-vastaccount-event-type-to-vaststatistics)

The internal statistics event type `vast.account` has been renamed to `vast.statistics` for clarity.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#789](https://github.com/tenzir/tenzir/pull/789).

#### Add timestamp attribute to statistics ts field

[Section titled “Add timestamp attribute to statistics ts field”](#add-timestamp-attribute-to-statistics-ts-field)

The config option `system.log-directory` was deprecated and replaced by the new option `system.log-file`. All logs will now be written to a single file.

By [@tobim](https://github.com/tobim) in [#806](https://github.com/tenzir/tenzir/pull/806).

#### Restrict log file creation to ‘vast start’

[Section titled “Restrict log file creation to ‘vast start’”](#restrict-log-file-creation-to-vast-start)

The log folder `vast.log/` in the current directory will not be created by default any more. Users must explicitly set the `system.file-verbosity` option if they wish to keep the old behavior.

By [@lava](https://github.com/lava) in [#803](https://github.com/tenzir/tenzir/pull/803).

#### Introduce the VERBOSE log level

[Section titled “Introduce the VERBOSE log level”](#introduce-the-verbose-log-level)

The VERBOSE log level has been added between INFO and DEBUG. This level is enabled at build time for all build types, making it possible to get more detailed logging output from release builds.

By [@tobim](https://github.com/tobim) in [#787](https://github.com/tenzir/tenzir/pull/787).

#### Replace ‘caf#’ prefix for CAF options with ‘caf.’

[Section titled “Replace ‘caf#’ prefix for CAF options with ‘caf.’”](#replace-caf-prefix-for-caf-options-with-caf)

The command line options prefix for changing CAF options was changed from `--caf#` to `--caf.`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#797](https://github.com/tenzir/tenzir/pull/797).

# VAST 2020.04.29

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.04.29).

### Features

[Section titled “Features”](#features)

#### PRs 827-844

[Section titled “PRs 827-844”](#prs-827-844)

Packet drop and discard statistics are now reported to the accountant for PCAP import, and are available using the keys `pcap-reader.recv`, `pcap-reader.drop`, `pcap-reader.ifdrop`, `pcap-reader.discard`, and `pcap-reader.discard-rate` in the `vast.statistics` event. If the number of dropped packets exceeds a configurable threshold, VAST additionally warns about packet drops on the command line.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#827](https://github.com/tenzir/tenzir/pull/827).

#### Add Bash autocompletion for VAST

[Section titled “Add Bash autocompletion for VAST”](#add-bash-autocompletion-for-vast)

Bash autocompletion for `vast` is now available via the autocomplete script located at `scripts/vast-completions.bash` in the VAST source tree.

By [@lava](https://github.com/lava) in [#833](https://github.com/tenzir/tenzir/pull/833).

### Changes

[Section titled “Changes”](#changes)

#### Rename count.skip-candidate-checks to count.estimate

[Section titled “Rename count.skip-candidate-checks to count.estimate”](#rename-countskip-candidate-checks-to-countestimate)

The option `--skip-candidate-checks` / `-s` for the `count` command was renamed to `--estimate` / `-e`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#843](https://github.com/tenzir/tenzir/pull/843).

#### Change default listen address to ‘localhost’

[Section titled “Change default listen address to ‘localhost’”](#change-default-listen-address-to-localhost)

The default bind address has been changed from `::` to `localhost`.

By [@lava](https://github.com/lava) in [#828](https://github.com/tenzir/tenzir/pull/828).

#### Simplify partition structure

[Section titled “Simplify partition structure”](#simplify-partition-structure)

The index specific options `max-partition-size`, `max-resident-partitions`, `max-taste-partitions`, and `max-queries` can now be specified on the command line when starting a node.

By [@tobim](https://github.com/tobim) in [#728](https://github.com/tenzir/tenzir/pull/728).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix meta index nondeterminism

[Section titled “Fix meta index nondeterminism”](#fix-meta-index-nondeterminism)

For some queries, the index evaluated only a subset of all relevant partitions in a non-deterministic manner. Fixing a violated evaluation invariant now guarantees deterministic execution.

By [@tobim](https://github.com/tobim) in [#842](https://github.com/tenzir/tenzir/pull/842).

#### Use line reader timeout

[Section titled “Use line reader timeout”](#use-line-reader-timeout)

Fixed a bug that could cause stalled input streams not to forward events to the index and archive components for the JSON, CSV, and Syslog readers, when the input stopped arriving but no EOF was sent. This is a follow-up to [#750](https://github.com/tenzir/vast/pull/750). A timeout now ensures that that the readers continue when some events were already handled, but the input appears to be stalled.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#835](https://github.com/tenzir/tenzir/pull/835).

#### Fix inequality port lookups

[Section titled “Fix inequality port lookups”](#fix-inequality-port-lookups)

Queries of the form `x != 80/tcp` were falsely evaluated as `x != 80/? && x != ?/tcp`. (The syntax in the second predicate does not yet exist; it only illustrates the bug.) Port inequality queries now correctly evaluate `x != 80/? || x != ?/tcp`. E.g., the result now contains values like `80/udp` and `80/?`, but also `8080/tcp`.

By [@mavam](https://github.com/mavam) in [#834](https://github.com/tenzir/tenzir/pull/834).

#### Make archive session extraction interruptible

[Section titled “Make archive session extraction interruptible”](#make-archive-session-extraction-interruptible)

Archive lookups are now interruptible. This change fixes an issue that caused consecutive exports to slow down the node, which improves the overall performance for larger databases considerably.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#825](https://github.com/tenzir/tenzir/pull/825).

#### Remove assertion from expression tailoring

[Section titled “Remove assertion from expression tailoring”](#remove-assertion-from-expression-tailoring)

Fixed a crash when importing data while a continuous export was running for unrelated events.

By [@lava](https://github.com/lava) in [#830](https://github.com/tenzir/tenzir/pull/830).

#### Make stop command blocking and return properly

[Section titled “Make stop command blocking and return properly”](#make-stop-command-blocking-and-return-properly)

The `stop` command always returned immediately, regardless of whether it succeeded. It now blocks until the remote node shut down properly or returns an error exit code upon failure.

By [@mavam](https://github.com/mavam) in [#849](https://github.com/tenzir/tenzir/pull/849).

# VAST 2020.05.28

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.05.28).

### Features

[Section titled “Features”](#features)

#### Support \`

[Section titled “Support \`”](#support)

\`-style newlines in VAST

All input parsers now support mixed `\n` and `\r\n` line endings.

By [@lava](https://github.com/lava) in [#865](https://github.com/tenzir/tenzir/pull/865).

#### Deduce types for heterogenous JSONL import

[Section titled “Deduce types for heterogenous JSONL import”](#deduce-types-for-heterogenous-jsonl-import)

When importing events of a new or updated type, VAST now only requires the type to be specified once (e.g., in a schema file). For consecutive imports, the event type does not need to be specified again. A list of registered types can now be viewed using `vast status` under the key `node.type-registry.types`.

When importing JSON data without knowing the type of the imported events a priori, VAST now supports automatic event type deduction based on the JSON object keys in the data. VAST selects a type *iff* the set of fields match a known type. The `--type` / `-t` option to the `import` command restricts the matching to the set of types that share the provided prefix. Omitting `-t` attempts to match JSON against all known types. If only a single variant of a type is matched, the import falls back to the old behavior and fills in `nil` for mismatched keys.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#875](https://github.com/tenzir/tenzir/pull/875).

#### UX improvements for `read_query()`

[Section titled “UX improvements for read\_query()”](#ux-improvements-for-read_query)

VAST now prints a message when it is waiting for user input to read a query from a terminal.

By [@lava](https://github.com/lava) in [#878](https://github.com/tenzir/tenzir/pull/878).

#### Publish Sysmon schema

[Section titled “Publish Sysmon schema”](#publish-sysmon-schema)

VAST now ships with a schema suitable for Sysmon import.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#886](https://github.com/tenzir/tenzir/pull/886).

#### PRs 873-877

[Section titled “PRs 873-877”](#prs-873-877)

Added a new `explore` command to VAST that can be used to show data records within a certain time from the results of a query.

By [@lava](https://github.com/lava) in [#873](https://github.com/tenzir/tenzir/pull/873).

### Changes

[Section titled “Changes”](#changes)

#### Rename statistics event to metrics

[Section titled “Rename statistics event to metrics”](#rename-statistics-event-to-metrics)

The command line flag for disabling the accountant has been renamed to `--disable-metrics` to more accurately reflect its intended purpose. The internal `vast.statistics` event has been renamed to `vast.metrics`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#870](https://github.com/tenzir/tenzir/pull/870).

#### UX improvements for `read_query()`

[Section titled “UX improvements for read\_query()”](#ux-improvements-for-read_query-1)

Spreading a query over multiple command line arguments in commands like explore/export/pivot/etc. has been deprecated.

By [@lava](https://github.com/lava) in [#878](https://github.com/tenzir/tenzir/pull/878).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Correct check for user schema in zeek reader

[Section titled “Correct check for user schema in zeek reader”](#correct-check-for-user-schema-in-zeek-reader)

The parser for Zeek tsv data used to ignore attributes that were defined for the Zeek-specific types in the schema files. It has been modified to respect and prefer the specified attributes for the fields that are present in the input data.

By [@tobim](https://github.com/tobim) in [#847](https://github.com/tenzir/tenzir/pull/847).

#### Various config and default setting fixes

[Section titled “Various config and default setting fixes”](#various-config-and-default-setting-fixes)

Fixed a bug that caused `vast import` processes to produce `'default'` table slices, despite having the `'arrow'` type as the default.

Fixed a bug where setting the `logger.file-verbosity` in the config file would not have an effect.

By [@tobim](https://github.com/tobim) in [#866](https://github.com/tenzir/tenzir/pull/866).

# VAST 2020.06.25

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.06.25).

### Features

[Section titled “Features”](#features)

#### Allow output format selection for the pivot/explore command

[Section titled “Allow output format selection for the pivot/explore command”](#allow-output-format-selection-for-the-pivotexplore-command)

The output format for the `explore` and `pivot` commands can now be set using the `explore.format` and `pivot.format` options respectively. Both default to JSON.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#921](https://github.com/tenzir/tenzir/pull/921).

#### Support type relaxation for JSON import

[Section titled “Support type relaxation for JSON import”](#support-type-relaxation-for-json-import)

The `import json` command’s type restrictions are more relaxed now, and can additionally convert from JSON strings to VAST internal data types.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#891](https://github.com/tenzir/tenzir/pull/891).

#### Support /etc/vast/vast.conf as global config

[Section titled “Support /etc/vast/vast.conf as global config”](#support-etcvastvastconf-as-global-config)

VAST now supports `/etc/vast/vast.conf` as an additional fallback for the configuration file. The following file locations are looked at in order: Path specified on the command line via `--config=path/to/vast.conf`, `vast.conf` in current working directory, `${INSTALL_PREFIX}/etc/vast/vast.conf`, and `/etc/vast/vast.conf`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#898](https://github.com/tenzir/tenzir/pull/898).

#### Support aging out data based on a query

[Section titled “Support aging out data based on a query”](#support-aging-out-data-based-on-a-query)

VAST now supports aging out existing data. This feature currently only concerns data in the archive. The options `system.aging-frequency` and `system.aging-query` configure a query that runs on a regular schedule to determine which events to delete. It is also possible to trigger an aging cycle manually.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#929](https://github.com/tenzir/tenzir/pull/929).

#### Add limit options for vast explore

[Section titled “Add limit options for vast explore”](#add-limit-options-for-vast-explore)

VAST now has options to limit the amount of results produced by an invocation of `vast explore`.

By [@lava](https://github.com/lava) in [#882](https://github.com/tenzir/tenzir/pull/882).

#### Forcefully emit batches on input timeout error

[Section titled “Forcefully emit batches on input timeout error”](#forcefully-emit-batches-on-input-timeout-error)

The `import` command gained a new `--read-timeout` option that forces data to be forwarded to the importer regardless of the internal batching parameters and table slices being unfinished. This allows for reducing the latency between the `import` command and the node. The default timeout is 10 seconds.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#916](https://github.com/tenzir/tenzir/pull/916).

#### Add Bloom filter meta index

[Section titled “Add Bloom filter meta index”](#add-bloom-filter-meta-index)

The meta index now uses Bloom filters for equality queries involving IP addresses. This especially accelerates queries where the user wants to know whether a certain IP address exists in the entire database.

By [@mavam](https://github.com/mavam) in [#931](https://github.com/tenzir/tenzir/pull/931).

### Changes

[Section titled “Changes”](#changes)

#### Rename the ‘default’ table slice type to ‘caf’

[Section titled “Rename the ‘default’ table slice type to ‘caf’”](#rename-the-default-table-slice-type-to-caf)

The `default` table slice type has been renamed to `caf`. It has not been the default when built with Apache Arrow support for a while now, and the new name more accurately reflects what it is doing.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#948](https://github.com/tenzir/tenzir/pull/948).

#### Print timestamps with full precision for JSON

[Section titled “Print timestamps with full precision for JSON”](#print-timestamps-with-full-precision-for-json)

The JSON export format now renders timestamps using strings instead of numbers in order to avoid possible loss of precision.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#909](https://github.com/tenzir/tenzir/pull/909).

#### PRs 908-951

[Section titled “PRs 908-951”](#prs-908-951)

The options `system.table-slice-type` and `system.table-slice-size` have been removed, as they duplicated `import.table-slice-type` and `import.table-slice-size` respectively.

By [@tobim](https://github.com/tobim) in [#908](https://github.com/tenzir/tenzir/pull/908).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Improve handling of UTF-8 input

[Section titled “Improve handling of UTF-8 input”](#improve-handling-of-utf-8-input)

The `export json` command now correctly unescapes its output.

VAST now correctly checks for control characters in inputs.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#910](https://github.com/tenzir/tenzir/pull/910).

#### Simplify id space management

[Section titled “Simplify id space management”](#simplify-id-space-management)

A bogus import process that assembled table slices with a greater number of events than expected by the node was able to lead to wrong query results.

By [@tobim](https://github.com/tobim) in [#908](https://github.com/tenzir/tenzir/pull/908).

#### Fix use-after-free bug in indexer state

[Section titled “Fix use-after-free bug in indexer state”](#fix-use-after-free-bug-in-indexer-state)

A use after free bug would sometimes crash the node while it was shutting down.

By [@lava](https://github.com/lava) in [#896](https://github.com/tenzir/tenzir/pull/896).

# VAST 2020.07.28

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.07.28).

### Features

[Section titled “Features”](#features)

#### Add a static binary workflow

[Section titled “Add a static binary workflow”](#add-a-static-binary-workflow)

Starting with this release, installing VAST on any Linux becomes significantly easier: A static binary will be provided with each release on the GitHub releases page.

By [@tobim](https://github.com/tobim) in [#966](https://github.com/tenzir/tenzir/pull/966).

#### Add MsgPack-based Table Slice implementation

[Section titled “Add MsgPack-based Table Slice implementation”](#add-msgpack-based-table-slice-implementation)

We open-sourced our [MessagePack](http://msgpack.org)-based table slice implementation, which provides a compact row-oriented encoding of data. This encoding works well for binary formats (e.g., PCAP) and access patterns that involve materializing entire rows. The MessagePack table slice is the new default when Apache Arrow is unavailable. To enable parsing into MessagePack, you can pass `--table-slice-type=msgpack` to the `import` command, or set the configuration option `import.table-slice-type` to `'msgpack'`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#975](https://github.com/tenzir/tenzir/pull/975).

### Changes

[Section titled “Changes”](#changes)

#### Use Flatbuffers for Persistent State of Segment Store and Meta Index

[Section titled “Use Flatbuffers for Persistent State of Segment Store and Meta Index”](#use-flatbuffers-for-persistent-state-of-segment-store-and-meta-index)

[FlatBuffers](https://google.github.io/flatbuffers/) is now a required dependency for VAST. The archive and the segment store use FlatBuffers to store and version their on-disk persistent state.

By [@lava](https://github.com/lava) in [#972](https://github.com/tenzir/tenzir/pull/972).

#### Improve handling of the default schema paths

[Section titled “Improve handling of the default schema paths”](#improve-handling-of-the-default-schema-paths)

VAST now recognizes `/etc/vast/schema` as an additional default directory for schema files.

By [@tobim](https://github.com/tobim) in [#980](https://github.com/tenzir/tenzir/pull/980).

#### PRs 954-986

[Section titled “PRs 954-986”](#prs-954-986)

The suricata schema file contains new type definitions for the stats, krb5, smb, and ssh events.

By [@tobim](https://github.com/tobim) in [#954](https://github.com/tenzir/tenzir/pull/954).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix spawn source subcommand

[Section titled “Fix spawn source subcommand”](#fix-spawn-source-subcommand)

The PCAP reader now correctly shows the amount of generated events.

By [@tobim](https://github.com/tobim) in [#954](https://github.com/tenzir/tenzir/pull/954).

# VAST 2020.08.28

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.08.28).

### Features

[Section titled “Features”](#features)

#### Always convert JSON null to VAST nil

[Section titled “Always convert JSON null to VAST nil”](#always-convert-json-null-to-vast-nil)

The default schema for Suricata has been updated to support the `suricata.ftp` and `suricata.ftp_data` event types.

VAST now prints the location of the configuration file that is used.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1009](https://github.com/tenzir/tenzir/pull/1009).

#### Protect DB directory with PID lock

[Section titled “Protect DB directory with PID lock”](#protect-db-directory-with-pid-lock)

VAST now writes a PID lock file on startup to prevent multiple server processes from accessing the same persistent state. The `pid.lock` file resides in the `vast.db` directory.

By [@mavam](https://github.com/mavam) in [#1001](https://github.com/tenzir/tenzir/pull/1001).

### Changes

[Section titled “Changes”](#changes)

#### Rename vector to list

[Section titled “Rename vector to list”](#rename-vector-to-list)

The `vector` type has been renamed to `list`. In an effort to streamline the type system vocabulary, we favor `list` over `vector` because it’s closer to existing terminology (e.g., Apache Arrow). This change requires updating existing schemas by changing `vector<T>` to `list<T>`.

By [@mavam](https://github.com/mavam) in [#1016](https://github.com/tenzir/tenzir/pull/1016).

#### Allow the ’-’ in the expression key parser

[Section titled “Allow the ’-’ in the expression key parser”](#allow-the---in-the-expression-key-parser)

The expression field parser now allows the ’-’ character.

By [@tobim](https://github.com/tobim) in [#999](https://github.com/tenzir/tenzir/pull/999).

#### Remove set data type

[Section titled “Remove set data type”](#remove-set-data-type)

The `set` type has been removed. Experience with the data model showed that there is no strong use case to separate sets from vectors in the core. While this may be useful in programming languages, VAST deals with immutable data where set constraints have been enforced upstream. This change requires updating existing schemas by changing `set<T>` to `vector<T>`. In the query language, the new symbol for the empty `map` changed from `{-}` to `{}`, as it now unambiguously identifies `map` instances.

By [@mavam](https://github.com/mavam) in [#1010](https://github.com/tenzir/tenzir/pull/1010).

#### Expand CAF stream slot ids to 32 bits

[Section titled “Expand CAF stream slot ids to 32 bits”](#expand-caf-stream-slot-ids-to-32-bits)

We now bundle a patched version of CAF, with a changed ABI. This means that if you’re linking against the bundled CAF library, you also need to distribute that library so that VAST can use it at runtime. The versions are API compatible so linking against a system version of CAF is still possible and supported.

By [@lava](https://github.com/lava) in [#1020](https://github.com/tenzir/tenzir/pull/1020).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix bug in decoding multi-object MsgPack types

[Section titled “Fix bug in decoding multi-object MsgPack types”](#fix-bug-in-decoding-multi-object-msgpack-types)

MessagePack-encoded table slices now work correctly for nested container types.

By [@mavam](https://github.com/mavam) in [#984](https://github.com/tenzir/tenzir/pull/984).

#### Expand CAF stream slot ids to 32 bits

[Section titled “Expand CAF stream slot ids to 32 bits”](#expand-caf-stream-slot-ids-to-32-bits-1)

When running VAST under heavy load, CAF stream slot ids could wrap around after a few days and deadlock the system. As a workaround, we extended the slot id bit width to make the time until this happens unrealistically large.

By [@lava](https://github.com/lava) in [#1020](https://github.com/tenzir/tenzir/pull/1020).

#### Allow the ’-’ in the expression key parser

[Section titled “Allow the ’-’ in the expression key parser”](#allow-the---in-the-expression-key-parser-1)

A bug in the expression parser prevented the correct parsing of fields starting with either ‘F’ or ‘T’.

By [@tobim](https://github.com/tobim) in [#999](https://github.com/tenzir/tenzir/pull/999).

#### Terminate exporters when sinks die

[Section titled “Terminate exporters when sinks die”](#terminate-exporters-when-sinks-die)

When continuous query in a client process terminated, the node did not clean up the corresponding server-side state. This memory leak no longer exists.

By [@mavam](https://github.com/mavam) in [#1006](https://github.com/tenzir/tenzir/pull/1006).

#### Support hard-kill for unresponsive actors

[Section titled “Support hard-kill for unresponsive actors”](#support-hard-kill-for-unresponsive-actors)

The shutdown process of the server process could potentially hang forever. VAST now uses a 2-step procedure that first attempts to terminate all components cleanly. If that fails, it will attempt a hard kill afterwards, and if that fails after another timeout, the process will call `abort(3)`.

By [@mavam](https://github.com/mavam) in [#1005](https://github.com/tenzir/tenzir/pull/1005).

#### Make port-encoding for Arrow host-independent

[Section titled “Make port-encoding for Arrow host-independent”](#make-port-encoding-for-arrow-host-independent)

The port encoding for Arrow-encoded table slices is now host-independent and always uses network-byte order.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1007](https://github.com/tenzir/tenzir/pull/1007).

#### Close file descriptor by default in ‘vast::file’

[Section titled “Close file descriptor by default in ‘vast::file’”](#close-file-descriptor-by-default-in-vastfile)

Some file descriptors remained open when they weren’t needed any more. This descriptor leak has been fixed.

By [@lava](https://github.com/lava) in [#1018](https://github.com/tenzir/tenzir/pull/1018).

#### Always convert JSON null to VAST nil

[Section titled “Always convert JSON null to VAST nil”](#always-convert-json-null-to-vast-nil-1)

Importing JSON no longer fails for JSON fields containing `null` when the corresponding VAST type in the schema is a non-trivial type like `vector<string>`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1009](https://github.com/tenzir/tenzir/pull/1009).

#### Correct the use of ::read()

[Section titled “Correct the use of ::read()”](#correct-the-use-of-read)

Incomplete reads have not been handled properly, which manifested for files larger than 2GB. On macOS, writing files larger than 2GB may have failed previously. VAST now respects OS-specific constraints on the maximum block size.

By [@tobim](https://github.com/tobim) in [#1025](https://github.com/tenzir/tenzir/pull/1025).

#### Shutdown node when component startup fails

[Section titled “Shutdown node when component startup fails”](#shutdown-node-when-component-startup-fails)

VAST did not terminate when a critical component failed during startup. VAST now binds the lifetime of the node to all critical components.

By [@mavam](https://github.com/mavam) in [#1028](https://github.com/tenzir/tenzir/pull/1028).

#### Don’t overwrite index state after startup error

[Section titled “Don’t overwrite index state after startup error”](#dont-overwrite-index-state-after-startup-error)

VAST would overwrite existing on-disk state data when encountering a partial read during startup. This state-corrupting behavior no longer exists.

By [@lava](https://github.com/lava) in [#1026](https://github.com/tenzir/tenzir/pull/1026).

# VAST 2020.09.30

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.09.30).

### Features

[Section titled “Features”](#features)

#### Restructure vast status

[Section titled “Restructure vast status”](#restructure-vast-status)

The output of the `status` command was restructured with a strong focus on usability. The new flags `--detailed` and `--debug` add additional content to the output.

By [@tobim](https://github.com/tobim) in [#995](https://github.com/tenzir/tenzir/pull/995).

#### Flatbufferize index

[Section titled “Flatbufferize index”](#flatbufferize-index)

VAST now ships with a new tool `lsvast` to display information about the contents of a VAST database directory. See `lsvast --help` for usage instructions.

By [@mavam](https://github.com/mavam) in [#863](https://github.com/tenzir/tenzir/pull/863).

#### Merge contents of all configuration files

[Section titled “Merge contents of all configuration files”](#merge-contents-of-all-configuration-files)

VAST now merges the contents of all used configuration files instead of using only the most user-specific file. The file specified using `--config` takes the highest precedence, followed by the user-specific path `${XDG_CONFIG_HOME:-${HOME}/.config}/vast/vast.conf`, and the compile-time path `<sysconfdir>/vast/vast.conf`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1040](https://github.com/tenzir/tenzir/pull/1040).

#### Add a get subcommand to retrieve events from the archive directly

[Section titled “Add a get subcommand to retrieve events from the archive directly”](#add-a-get-subcommand-to-retrieve-events-from-the-archive-directly)

The `vast get` command has been added. It retrieves events from the database directly by their ids.

By [@tobim](https://github.com/tobim) in [#938](https://github.com/tenzir/tenzir/pull/938).

#### Make vast.conf lookup on Linux systems more intuitive

[Section titled “Make vast.conf lookup on Linux systems more intuitive”](#make-vastconf-lookup-on-linux-systems-more-intuitive)

VAST now supports the XDG base directory specification: The `vast.conf` is now found at `${XDG_CONFIG_HOME:-${HOME}/.config}/vast/vast.conf`, and schema files at `${XDG_DATA_HOME:-${HOME}/.local/share}/vast/schema/`. The user-specific configuration file takes precedence over the global configuration file in `<sysconfdir>/vast/vast.conf`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1036](https://github.com/tenzir/tenzir/pull/1036).

### Changes

[Section titled “Changes”](#changes)

#### PRs 1045-1055-1059-1062

[Section titled “PRs 1045-1055-1059-1062”](#prs-1045-1055-1059-1062)

The proprietary VAST configuration file has changed to the more ops-friendly industry standard YAML. This change introduced also a new dependency: [yaml-cpp](https://github.com/jbeder/yaml-cpp) version 0.6.2 or greater. The top-level `vast.yaml.example` illustrates how the new YAML config looks like. Please rename existing configuration files from `vast.conf` to `vast.yaml`. VAST still reads `vast.conf` but will soon only look for `vast.yaml` or `vast.yml` files in available configuration file paths.

By [@mavam](https://github.com/mavam) in [#1045](https://github.com/tenzir/tenzir/pull/1045).

#### Add event type name to the record batch metadata

[Section titled “Add event type name to the record batch metadata”](#add-event-type-name-to-the-record-batch-metadata)

Data exported in the Apache Arrow format now contains the name of the payload record type in the metadata section of the schema.

By [@tobim](https://github.com/tobim) in [#1072](https://github.com/tenzir/tenzir/pull/1072).

#### Render duration and port as JSON strings

[Section titled “Render duration and port as JSON strings”](#render-duration-and-port-as-json-strings)

The JSON export format now renders `duration` and `port` fields using strings as opposed to numbers. This avoids a possible loss of information and enables users to re-use the output in follow-up queries directly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1034](https://github.com/tenzir/tenzir/pull/1034).

#### Make periodic logging more sensible

[Section titled “Make periodic logging more sensible”](#make-periodic-logging-more-sensible)

The delay between the periodic log messages for reporting the current event rates has been increased to 10 seconds.

By [@tobim](https://github.com/tobim) in [#1035](https://github.com/tenzir/tenzir/pull/1035).

#### Make vast.conf lookup on Linux systems more intuitive

[Section titled “Make vast.conf lookup on Linux systems more intuitive”](#make-vastconf-lookup-on-linux-systems-more-intuitive-1)

The global VAST configuration now always resides in `<sysconfdir>/vast/vast.conf`, and bundled schemas always in `<datadir>/vast/schema/`. VAST no longer supports reading a `vast.conf` file in the current working directory.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1036](https://github.com/tenzir/tenzir/pull/1036).

#### Flatbufferize index

[Section titled “Flatbufferize index”](#flatbufferize-index-1)

The persistent storage format of the index now uses FlatBuffers.

By [@mavam](https://github.com/mavam) in [#863](https://github.com/tenzir/tenzir/pull/863).

#### Improve import batching options

[Section titled “Improve import batching options”](#improve-import-batching-options)

The options that affect batches in the `import` command received new, more user-facing names: `import.table-slice-type`, `import.table-slice-size`, and `import.read-timeout` are now called `import.batch-encoding`, `import.batch-size`, and `import.read-timeout` respectively.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1058](https://github.com/tenzir/tenzir/pull/1058).

#### Flatbufferize index

[Section titled “Flatbufferize index”](#flatbufferize-index-2)

We refactored the index architecture to improve stability and responsiveness. This includes fixes for several shutdown issues.

By [@mavam](https://github.com/mavam) in [#863](https://github.com/tenzir/tenzir/pull/863).

#### Restructure configuration file hierarchy

[Section titled “Restructure configuration file hierarchy”](#restructure-configuration-file-hierarchy)

All configuration options are now grouped into `vast` and `caf` sections, depending on whether they affect VAST itself or are handed through to the underlying actor framework CAF directly. Take a look at the bundled `vast.yaml.example` file for an explanation of the new layout.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1073](https://github.com/tenzir/tenzir/pull/1073).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Improve import batching options

[Section titled “Improve import batching options”](#improve-import-batching-options-1)

Stalled sources that were unable to generate new events no longer stop import processes from shutting down under rare circumstances.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1058](https://github.com/tenzir/tenzir/pull/1058).

# VAST 2020.10.29

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.10.29).

### Features

[Section titled “Features”](#features)

#### Age rotation for old data

[Section titled “Age rotation for old data”](#age-rotation-for-old-data)

The new options `vast.segments` and `vast.max-segment-size` control how the archive generates segments.

By [@lava](https://github.com/lava) in [#1103](https://github.com/tenzir/tenzir/pull/1103).

#### Enable loading of concepts from disk

[Section titled “Enable loading of concepts from disk”](#enable-loading-of-concepts-from-disk)

The query language now comes with support for concepts, the first part of taxonomies. Concepts is a mechanism to unify the various naming schemes of different data formats into a single, coherent nomenclature.

By [@tobim](https://github.com/tobim) in [#1102](https://github.com/tenzir/tenzir/pull/1102).

#### Allow tuple-style syntax for parsing records

[Section titled “Allow tuple-style syntax for parsing records”](#allow-tuple-style-syntax-for-parsing-records)

The expression language now accepts records without field names. For example,`id == <192.168.0.1, 41824, 143.51.53.13, 25, "tcp">` is now valid syntax and instantiates a record with 5 fields. Note: expressions with records currently do not execute.

By [@tobim](https://github.com/tobim) in [#1129](https://github.com/tenzir/tenzir/pull/1129).

#### Add script to convert CIM to VAST taxonomy

[Section titled “Add script to convert CIM to VAST taxonomy”](#add-script-to-convert-cim-to-vast-taxonomy)

The new script `splunk-to-vast` converts a splunk CIM model file in JSON to a VAST taxonomy. For example, `splunk-to-vast < Network_Traffic.json` renders the concept definitions for the *Network Traffic* datamodel. The generated taxonomy does not include field definitions, which users should add separately according to their data formats.

By [@mavam](https://github.com/mavam) in [#1121](https://github.com/tenzir/tenzir/pull/1121).

#### Support native systemd startup notification from VAST

[Section titled “Support native systemd startup notification from VAST”](#support-native-systemd-startup-notification-from-vast)

When running VAST under systemd supervision, it is now possible to use the `Type=notify` directive in the unit file to let VAST notify the service manager when it becomes ready.

By [@lava](https://github.com/lava) in [#1091](https://github.com/tenzir/tenzir/pull/1091).

#### Age rotation for old data

[Section titled “Age rotation for old data”](#age-rotation-for-old-data-1)

A new *disk monitor* component can now monitor the database size and delete data that exceeds a specified threshold. Once VAST reaches the maximum amount of disk space, the disk monitor deletes the oldest data. The command-line options `--disk-quota-high`, `--disk-quota-low`, and `--disk-quota-check-interval` control the rotation behavior.

By [@lava](https://github.com/lava) in [#1103](https://github.com/tenzir/tenzir/pull/1103).

### Changes

[Section titled “Changes”](#changes)

#### Make default log format less verbose

[Section titled “Make default log format less verbose”](#make-default-log-format-less-verbose)

Log files are now less verbose because class and function names are not printed on every line.

By [@lava](https://github.com/lava) in [#1107](https://github.com/tenzir/tenzir/pull/1107).

#### Make the source actor more responsive

[Section titled “Make the source actor more responsive”](#make-the-source-actor-more-responsive)

The new option `import.read-timeout` allows for setting an input timeout for low volume sources. Reaching the timeout causes the current batch to be forwarded immediately. This behavior was previously controlled by `import.batch-timeout`, which now only controls the maximum buffer time before the source forwards batches to the server.

By [@tobim](https://github.com/tobim) in [#1096](https://github.com/tenzir/tenzir/pull/1096).

#### Change /var/db to /var/lib on Linux deployments

[Section titled “Change /var/db to /var/lib on Linux deployments”](#change-vardb-to-varlib-on-linux-deployments)

The default database directory moved to `/var/lib/vast` for Linux deployments.

By [@0snap](https://github.com/0snap) in [#1116](https://github.com/tenzir/tenzir/pull/1116).

#### Warn on client-server version mismatch

[Section titled “Warn on client-server version mismatch”](#warn-on-client-server-version-mismatch)

VAST will now warn if a client command connects to a server that runs on a different version of the vast binary.

By [@tobim](https://github.com/tobim) in [#1098](https://github.com/tenzir/tenzir/pull/1098).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix merging of source status objects

[Section titled “Fix merging of source status objects”](#fix-merging-of-source-status-objects)

The `vast status --detailed` command now correctly shows the status of all sources, i.e., `vast import` or `vast spawn source` commands.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1109](https://github.com/tenzir/tenzir/pull/1109).

#### Fix file identifier check in lsvast

[Section titled “Fix file identifier check in lsvast”](#fix-file-identifier-check-in-lsvast)

The `lsvast` tool failed to print FlatBuffers schemas correctly. The output now renders correctly.

By [@lava](https://github.com/lava) in [#1123](https://github.com/tenzir/tenzir/pull/1123).

#### Use a stable set for schema directories

[Section titled “Use a stable set for schema directories”](#use-a-stable-set-for-schema-directories)

The lookup for schema directories now happens in a fixed order.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1086](https://github.com/tenzir/tenzir/pull/1086).

#### Don’t enable automatic connections by default

[Section titled “Don’t enable automatic connections by default”](#dont-enable-automatic-connections-by-default)

VAST no longer opens a random public port, which used to be enabled in the experimental VAST cluster mode in order to transparently establish a full mesh.

By [@lava](https://github.com/lava) in [#1110](https://github.com/tenzir/tenzir/pull/1110).

#### Make the source actor more responsive

[Section titled “Make the source actor more responsive”](#make-the-source-actor-more-responsive-1)

Sources that receive no or very little input do not block `vast status` any longer.

By [@tobim](https://github.com/tobim) in [#1096](https://github.com/tenzir/tenzir/pull/1096).

# VAST 2020.12.16

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2020.12.16).

### Features

[Section titled “Features”](#features)

#### PRs 1196-1233

[Section titled “PRs 1196-1233”](#prs-1196-1233)

The new `dump` command prints configuration and schema-related information. The implementation allows for printing all registered concepts and models, via `vast dump concepts` and `vast dump models`. The flag to `--yaml` to `dump` switches from JSON to YAML output, such that it confirms to the taxonomy configuration syntax.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1196](https://github.com/tenzir/tenzir/pull/1196).

#### Add support for type-level synopses and a string synopsis

[Section titled “Add support for type-level synopses and a string synopsis”](#add-support-for-type-level-synopses-and-a-string-synopsis)

Low-selectivity queries of string (in)equality queries now run up to 30x faster, thanks to more intelligent selection of relevant index partitions.

By [@tobim](https://github.com/tobim) in [#1214](https://github.com/tenzir/tenzir/pull/1214).

#### Add support for USDT tracepoints in VAST

[Section titled “Add support for USDT tracepoints in VAST”](#add-support-for-usdt-tracepoints-in-vast)

On Linux, VAST now contains a set of built-in USDT tracepoints that can be used by tools like `perf` or `bpftrace` when debugging. Initially, we provide the two tracepoints `chunk_make` and `chunk_destroy`, which trigger every time a `vast::chunk` is created or destroyed.

By [@lava](https://github.com/lava) in [#1206](https://github.com/tenzir/tenzir/pull/1206).

#### PRs 1135-1150

[Section titled “PRs 1135-1150”](#prs-1135-1150)

VAST now ships with its own taxonomy and basic concept definitions for Suricata, Zeek, and Sysmon.

By [@mavam](https://github.com/mavam) in [#1135](https://github.com/tenzir/tenzir/pull/1135).

#### Show file size information in lsvast

[Section titled “Show file size information in lsvast”](#show-file-size-information-in-lsvast)

The new option `--print-bytesizes` of `lsvast` prints information about the size of certain fields of the flatbuffers inside a VAST database directory.

By [@lava](https://github.com/lava) in [#1149](https://github.com/tenzir/tenzir/pull/1149).

#### Introduce the #field meta extractor

[Section titled “Introduce the #field meta extractor”](#introduce-the-field-meta-extractor)

The expression language gained support for the `#field` meta extractor. It is the complement for `#type` and uses suffix matching for field names at the layout level.

By [@tobim](https://github.com/tobim) in [#1228](https://github.com/tenzir/tenzir/pull/1228).

#### PRs 1172-1200-1216

[Section titled “PRs 1172-1200-1216”](#prs-1172-1200-1216)

The storage required for index IP addresses has been optimized. This should result in significantly reduced memory usage over time, as well as faster restart times and reduced disk space requirements.

By [@lava](https://github.com/lava) in [#1172](https://github.com/tenzir/tenzir/pull/1172).

#### Allow for enabling client file logging

[Section titled “Allow for enabling client file logging”](#allow-for-enabling-client-file-logging)

The new option `vast.client-log-file` enables client-side logging. By default, VAST only writes log files for the server process.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1132](https://github.com/tenzir/tenzir/pull/1132).

#### Show meta index size in vast status

[Section titled “Show meta index size in vast status”](#show-meta-index-size-in-vast-status)

A new key ‘meta-index-bytes’ appears in the status output generated by `vast status --detailed`.

By [@lava](https://github.com/lava) in [#1193](https://github.com/tenzir/tenzir/pull/1193).

#### PRs 1185-1228

[Section titled “PRs 1185-1228”](#prs-1185-1228)

The query language now supports models. Models combine a list of concepts into a semantic unit that can be fulfiled by an event. If the type of an event contains a field for every concept in a model. Turn to [the documentation](https://vast.io/docs/understand/data-model/taxonomies#models) for more information.

By [@tobim](https://github.com/tobim) in [#1185](https://github.com/tenzir/tenzir/pull/1185).

### Changes

[Section titled “Changes”](#changes)

#### Make Zeek writer work with all data types

[Section titled “Make Zeek writer work with all data types”](#make-zeek-writer-work-with-all-data-types)

The `zeek` export format now strips off the prefix `zeek.` to ensure full compatibility with regular Zeek output. For all non-Zeek types, the prefix remains intact.

By [@mavam](https://github.com/mavam) in [#1205](https://github.com/tenzir/tenzir/pull/1205).

#### Process schema directories recursively

[Section titled “Process schema directories recursively”](#process-schema-directories-recursively)

VAST now processes the schema directory recursively, as opposed to stopping at nested directories.

By [@mavam](https://github.com/mavam) in [#1154](https://github.com/tenzir/tenzir/pull/1154).

#### Make metrics opt-in

[Section titled “Make metrics opt-in”](#make-metrics-opt-in)

VAST does not produce metrics by default any more. The option `--disable-metrics` has been renamed to `--enable-metrics` accordingly.

By [@tobim](https://github.com/tobim) in [#1137](https://github.com/tenzir/tenzir/pull/1137).

#### PRs 1176-1180-1186-1237-satta

[Section titled “PRs 1176-1180-1186-1237-satta”](#prs-1176-1180-1186-1237-satta)

The Suricata schemas received an overhaul: there now exist `vlan` and `in_iface` fields in all types. In addition, VAST ships with new types for `ikev2`, `nfs`, `snmp`, `tftp`, `rdp`, `sip` and `dcerpc`. The `tls` type gets support for the additional `sni` and `session_resumed` fields.

By [@satta](https://github.com/satta) in [#1176](https://github.com/tenzir/tenzir/pull/1176).

#### Move schema definitions into subdirectory

[Section titled “Move schema definitions into subdirectory”](#move-schema-definitions-into-subdirectory)

Installed schema definitions now reside in `<datadir>/vast/schema/types`, taxonomy definitions in `<datadir>/vast/schema/taxonomy`, and concept definitions in `<datadir/vast/schema/concepts`, as opposed to them all being in the schema directory directly. When overriding an existing installation, you *may* have to delete the old schema definitions by hand.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1194](https://github.com/tenzir/tenzir/pull/1194).

#### Set fallback port for underspecified endpoints

[Section titled “Set fallback port for underspecified endpoints”](#set-fallback-port-for-underspecified-endpoints)

VAST now listens on port 42000 instead of letting the operating system choose the port if the option `vast.endpoint` specifies an endpoint without a port. To restore the old behavior, set the port to 0 explicitly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1170](https://github.com/tenzir/tenzir/pull/1170).

#### Fulfill Deployment Requirements

[Section titled “Fulfill Deployment Requirements”](#fulfill-deployment-requirements)

The build configuration of VAST received a major overhaul. Inclusion of libvast in other procects via `add_subdirectory(path/to/vast)` is now easily possible. The names of all build options were aligned, and the new build summary shows all available options.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1175](https://github.com/tenzir/tenzir/pull/1175).

#### Replace PID file if process does not exist

[Section titled “Replace PID file if process does not exist”](#replace-pid-file-if-process-does-not-exist)

VAST no longer requires you to manually remove a stale PID file from a no-longer running `vast` process. Instead, VAST prints a warning and overwrites the old PID file.

By [@tobim](https://github.com/tobim) in [#1128](https://github.com/tenzir/tenzir/pull/1128).

#### PRs 1143-1157-1160-1165

[Section titled “PRs 1143-1157-1160-1165”](#prs-1143-1157-1160-1165)

The on-disk format for table slices now supports versioning of table slice encodings. This breaking change makes it so that adding further encodings or adding new versions of existing encodings is possible without breaking again in the future.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1143](https://github.com/tenzir/tenzir/pull/1143).

#### Generalize splunk-to-vast

[Section titled “Generalize splunk-to-vast”](#generalize-splunk-to-vast)

The `splunk-to-vast` script has a new name: `taxonomize`. The script now also generates taxonomy declarations for Azure Sentinel.

By [@mavam](https://github.com/mavam) in [#1134](https://github.com/tenzir/tenzir/pull/1134).

#### Remove port type

[Section titled “Remove port type”](#remove-port-type)

The `port` type is no longer a first-class type. The new way to represent transport-layer ports relies on `count` instead. In the schema, VAST ships with a new alias `type port = count` to keep existing schema definitions in tact. However, this is a breaking change because the on-disk format and Arrow data representation changed. Queries with `:port` type extractors no longer work. Similarly, the syntax `53/udp` no longer exists; use `count` syntax `53` instead. Since most `port` occurrences do not carry a known transport-layer type, and the type information exists typically in a separate field, removing `port` as native type streamlines the data model.

By [@mavam](https://github.com/mavam) in [#1187](https://github.com/tenzir/tenzir/pull/1187).

#### Remove Version FlatBuffers table

[Section titled “Remove Version FlatBuffers table”](#remove-version-flatbuffers-table)

Archive segments no longer include an additional, unnecessary version identifier. We took the opportunity to clean this up bundled with the other recent breaking changes.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1168](https://github.com/tenzir/tenzir/pull/1168).

#### Remove CAF-encoded table slices

[Section titled “Remove CAF-encoded table slices”](#remove-caf-encoded-table-slices)

CAF-encoded table slices no longer exist. As such, the option `vast.import.batch-encoding` now only supports `arrow` and `msgpack` as arguments.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1142](https://github.com/tenzir/tenzir/pull/1142).

#### Increase the default segment size to 1 GiB

[Section titled “Increase the default segment size to 1 GiB”](#increase-the-default-segment-size-to-1-gib)

The default segment size in the archive is now 1 GiB. This reduces fragmentation of the archive meta data and speeds up VAST startup time.

By [@mavam](https://github.com/mavam) in [#1166](https://github.com/tenzir/tenzir/pull/1166).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Reply to status requests in sinks

[Section titled “Reply to status requests in sinks”](#reply-to-status-requests-in-sinks)

The output of `vast status --detailed` now contains informations about runnings sinks, e.g., `vast export <format> <query>` processes.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1155](https://github.com/tenzir/tenzir/pull/1155).

#### Send correct message to index when dropping further results

[Section titled “Send correct message to index when dropping further results”](#send-correct-message-to-index-when-dropping-further-results)

The index now correctly drops further results when queries finish early, thus improving the performance of queries for a limited number of events.

By [@lava](https://github.com/lava) in [#1209](https://github.com/tenzir/tenzir/pull/1209).

#### Error when specified config file does not exist

[Section titled “Error when specified config file does not exist”](#error-when-specified-config-file-does-not-exist)

VAST no longer starts if the specified config file does not exist.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1147](https://github.com/tenzir/tenzir/pull/1147).

#### Introduce the #field meta extractor

[Section titled “Introduce the #field meta extractor”](#introduce-the-field-meta-extractor-1)

The summary log message of `vast export` now contains the correct number of candidate events.

By [@tobim](https://github.com/tobim) in [#1228](https://github.com/tenzir/tenzir/pull/1228).

#### Fix index worker depletion

[Section titled “Fix index worker depletion”](#fix-index-worker-depletion)

The index no longer causes exporters to deadlock when the meta index produces false positives.

By [@tobim](https://github.com/tobim) in [#1225](https://github.com/tenzir/tenzir/pull/1225).

#### Fix YAML syntax errors in example config

[Section titled “Fix YAML syntax errors in example config”](#fix-yaml-syntax-errors-in-example-config)

The `vast.yaml.example` contained syntax errors. The example config file now works again.

By [@mavam](https://github.com/mavam) in [#1145](https://github.com/tenzir/tenzir/pull/1145).

#### Fix loading and dumping of composed concepts

[Section titled “Fix loading and dumping of composed concepts”](#fix-loading-and-dumping-of-composed-concepts)

Concepts that reference other concepts are now loaded correctly from their definition.

By [@tobim](https://github.com/tobim) in [#1236](https://github.com/tenzir/tenzir/pull/1236).

#### Detect and handle breaking changes in schemas

[Section titled “Detect and handle breaking changes in schemas”](#detect-and-handle-breaking-changes-in-schemas)

The type registry now detects and handles breaking changes in schemas, e.g., when a field type changes or a field is dropped from record.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1195](https://github.com/tenzir/tenzir/pull/1195).

#### Fix a hang when trying to process an invalid query

[Section titled “Fix a hang when trying to process an invalid query”](#fix-a-hang-when-trying-to-process-an-invalid-query)

VAST no longer blocks when an invalid query operation is issued.

By [@tobim](https://github.com/tobim) in [#1189](https://github.com/tenzir/tenzir/pull/1189).

#### Fix a bug that causes sources to stall

[Section titled “Fix a bug that causes sources to stall”](#fix-a-bug-that-causes-sources-to-stall)

`vast import` no longer stalls when it doesn’t receive any data for more than 10 seconds.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1136](https://github.com/tenzir/tenzir/pull/1136).

#### Don’t collect status from sources and sinks

[Section titled “Don’t collect status from sources and sinks”](#dont-collect-status-from-sources-and-sinks)

The `vast status` command does not collect status information from sources and sinks any longer. They were often too busy to respond, leading to a long delay before the command completed.

By [@tobim](https://github.com/tobim) in [#1234](https://github.com/tenzir/tenzir/pull/1234).

#### Switch index behavior when running out of workers

[Section titled “Switch index behavior when running out of workers”](#switch-index-behavior-when-running-out-of-workers)

The index no longer crashes when too many parallel queries are running.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1210](https://github.com/tenzir/tenzir/pull/1210).

# VAST 2021.01.28

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.01.28).

### Features

[Section titled “Features”](#features)

#### Add the import zeek-json command

[Section titled “Add the import zeek-json command”](#add-the-import-zeek-json-command)

The new `import zeek-json` command allows for importing line-delimited Zeek JSON logs as produced by the [json-streaming-logs](https://github.com/corelight/json-streaming-logs) package. Unlike stock Zeek JSON logs, where one file contains exactly one log type, the streaming format contains different log event types in a single stream and uses an additional `_path` field to disambiguate the log type. For stock Zeek JSON logs, use the existing `import json` with the `-t` flag to specify the log type.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1259](https://github.com/tenzir/tenzir/pull/1259).

#### Add per partition memory usage information to vast status

[Section titled “Add per partition memory usage information to vast status”](#add-per-partition-memory-usage-information-to-vast-status)

The output of `vast status` contains detailed memory usage information about active and cached partitions.

By [@tobim](https://github.com/tobim) in [#1297](https://github.com/tenzir/tenzir/pull/1297).

#### PRs 1230-1246-1281-1314-1315-ngrodzitski

[Section titled “PRs 1230-1246-1281-1314-1315-ngrodzitski”](#prs-1230-1246-1281-1314-1315-ngrodzitski)

VAST relies on [simdjson](https://github.com/simdjson/simdjson) for JSON parsing. The substantial gains in throughput shift the bottleneck of the ingest path from parsing input to indexing at the node. To use the (yet experimental) feature, use `vast import json|suricata|zeek-json --simdjson`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1230](https://github.com/tenzir/tenzir/pull/1230).

#### Make duration units abbreviations consistent

[Section titled “Make duration units abbreviations consistent”](#make-duration-units-abbreviations-consistent)

VAST queries also accept `nanoseconds`, `microseconds`, `milliseconds` `seconds` and `minutes` as units for a duration.

By [@rolandpeelen](https://github.com/rolandpeelen) in [#1265](https://github.com/tenzir/tenzir/pull/1265).

#### Add third-party licenses for embedded dependencies

[Section titled “Add third-party licenses for embedded dependencies”](#add-third-party-licenses-for-embedded-dependencies)

VAST installations bundle a LICENSE.3rdparty file alongside the regular LICENSE file that lists all embedded code that is under a separate license.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1306](https://github.com/tenzir/tenzir/pull/1306).

#### PRs 1208-1264-1275-1282-1285-1287-1302-1307-1316

[Section titled “PRs 1208-1264-1275-1282-1285-1287-1302-1307-1316”](#prs-1208-1264-1275-1282-1285-1287-1302-1307-1316)

VAST features a new plugin framework to support efficient customization points at various places of the data processing pipeline. There exist several base classes that define an interface, e.g., for adding new commands or spawning a new actor that processes the incoming stream of data. The directory `examples/plugins/example` contains an example plugin.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1208](https://github.com/tenzir/tenzir/pull/1208).

### Changes

[Section titled “Changes”](#changes)

#### Consolidate Dockerfiles

[Section titled “Consolidate Dockerfiles”](#consolidate-dockerfiles)

The GitHub CI changed to Debian Buster and produces Debian artifacts instead of Ubuntu artifacts. Similarly, the Docker images we provide on [Docker Hub](https://hub.docker.com/r/tenzir/vast) use Debian Buster as base image. To build Docker images locally, users must set `DOCKER_BUILDKIT=1` in the build environment.

By [@0snap](https://github.com/0snap) in [#1294](https://github.com/tenzir/tenzir/pull/1294).

#### Rename `*-paths` to `*-dirs` options

[Section titled “Rename \*-paths to \*-dirs options”](#rename--paths-to--dirs-options)

The option `vast.schema-paths` is renamed to `vast.schema-dirs`. The old option is deprecated and will be removed in a future release.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1287](https://github.com/tenzir/tenzir/pull/1287).

#### Support -v, -vv, -vvv, -q, -qq, -qqq for verbosity

[Section titled “Support -v, -vv, -vvv, -q, -qq, -qqq for verbosity”](#support--v--vv--vvv--q--qq--qqq-for-verbosity)

The new short options `-v`, `-vv`, `-vvv`, `-q`, `-qq`, and `-qqq` map onto the existing verbosity levels. The existing short syntax, e.g., `-v debug`, no longer works.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1244](https://github.com/tenzir/tenzir/pull/1244).

#### PRs 1257-1289

[Section titled “PRs 1257-1289”](#prs-1257-1289)

VAST preserves nested JSON objects in events instead of formatting them in a flattened form when exporting data with `vast export json`. The old behavior can be enabled with `vast export json --flatten`.

By [@tobim](https://github.com/tobim) in [#1257](https://github.com/tenzir/tenzir/pull/1257).

#### Make the start command print the endpoint on stdout

[Section titled “Make the start command print the endpoint on stdout”](#make-the-start-command-print-the-endpoint-on-stdout)

`vast start` prints the endpoint it is listening on when providing the option `--print-endpoint`.

By [@tobim](https://github.com/tobim) in [#1271](https://github.com/tenzir/tenzir/pull/1271).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Handle Arrow decoder errors gracefully

[Section titled “Handle Arrow decoder errors gracefully”](#handle-arrow-decoder-errors-gracefully)

Invalid Arrow table slices read from disk no longer trigger a segmentation fault. Instead, the invalid on-disk state is ignored.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1247](https://github.com/tenzir/tenzir/pull/1247).

#### Check that disk budget was not specified as non-string

[Section titled “Check that disk budget was not specified as non-string”](#check-that-disk-budget-was-not-specified-as-non-string)

Disk monitor quota settings not ending in a ‘B’ are no longer silently discarded.

By [@lava](https://github.com/lava) in [#1278](https://github.com/tenzir/tenzir/pull/1278).

#### Follow up for the CMake refactoring

[Section titled “Follow up for the CMake refactoring”](#follow-up-for-the-cmake-refactoring)

For relocatable installations, the list of schema loading paths does not include a build-time configured path any more.

By [@tobim](https://github.com/tobim) in [#1249](https://github.com/tenzir/tenzir/pull/1249).

#### Gracefully deal with JSON to data conversion errors

[Section titled “Gracefully deal with JSON to data conversion errors”](#gracefully-deal-with-json-to-data-conversion-errors)

Values in JSON fields that can’t be converted to the type that is specified in the schema won’t cause the containing event to be dropped any longer.

By [@tobim](https://github.com/tobim) in [#1250](https://github.com/tenzir/tenzir/pull/1250).

#### Remove check whether config file is a regular file

[Section titled “Remove check whether config file is a regular file”](#remove-check-whether-config-file-is-a-regular-file)

Manually specified configuration files may reside in the default location directories. Configuration files can be symlinked.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1248](https://github.com/tenzir/tenzir/pull/1248).

#### Fix potential race condition between evaluator and partition

[Section titled “Fix potential race condition between evaluator and partition”](#fix-potential-race-condition-between-evaluator-and-partition)

A potential race condition that could lead to a hanging export if a partition was persisted just as it was scanned no longer exists.

By [@lava](https://github.com/lava) in [#1295](https://github.com/tenzir/tenzir/pull/1295).

#### Don’t overwrite line content after a read timeout

[Section titled “Don’t overwrite line content after a read timeout”](#dont-overwrite-line-content-after-a-read-timeout)

Line based imports correctly handle read timeouts that occur in the middle of a line.

By [@tobim](https://github.com/tobim) in [#1276](https://github.com/tenzir/tenzir/pull/1276).

# VAST 2021.02.24

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.02.24).

### Features

[Section titled “Features”](#features)

#### PRs 1330-1376

[Section titled “PRs 1330-1376”](#prs-1330-1376)

The meta index now stores partition synopses in separate files. This will decrease restart times for systems with large databases, slow disks and aggressive `readahead` settings. A new config setting `vast.meta-index-dir` allows storing the meta index information in a separate directory.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1330](https://github.com/tenzir/tenzir/pull/1330).

#### Enable real-time metrics reporting

[Section titled “Enable real-time metrics reporting”](#enable-real-time-metrics-reporting)

The new options `vast.metrics.file-sink.real-time` and `vast.metrics.uds-sink.real-time` enable real-time metrics reporting for the file sink and UDS sink respectively.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1368](https://github.com/tenzir/tenzir/pull/1368).

#### PRs 1343-1356-ngrodzitski

[Section titled “PRs 1343-1356-ngrodzitski”](#prs-1343-1356-ngrodzitski)

The JSON import now always relies upon [simdjson](https://simdjson.org). The previously experimental `--simdjson` option to the `vast import json|suricata|zeek-json` commands no longer exist as the feature is considered stable.

By [@ngrodzitski](https://github.com/ngrodzitski) in [#1343](https://github.com/tenzir/tenzir/pull/1343).

#### PRs 1223-1362

[Section titled “PRs 1223-1362”](#prs-1223-1362)

VAST rotates server logs by default. The new config options `vast.disable-log-rotation` and `vast.log-rotation-threshold` can be used to control this behaviour.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1223](https://github.com/tenzir/tenzir/pull/1223).

#### Enable type extractors to support user defined types

[Section titled “Enable type extractors to support user defined types”](#enable-type-extractors-to-support-user-defined-types)

The type extractor in the expression language now works with user defined types. For example the type `port` is defined as `type port = count` in the base schema. This type can now be queried with an expression like `:port == 80`.

By [@tobim](https://github.com/tobim) in [#1382](https://github.com/tenzir/tenzir/pull/1382).

#### Add native Sigma support

[Section titled “Add native Sigma support”](#add-native-sigma-support)

[Sigma](https://github.com/Neo23x0/sigma) rules are now a valid format to represent query expression. VAST parses the `detection` attribute of a rule and translates it into a native query expression. To run a query using a Sigma rule, pass it on standard input, e.g., `vast export json < rule.yaml`.

By [@mavam](https://github.com/mavam) in [#1379](https://github.com/tenzir/tenzir/pull/1379).

### Changes

[Section titled “Changes”](#changes)

#### Render help and documentation on stdout

[Section titled “Render help and documentation on stdout”](#render-help-and-documentation-on-stdout)

The output of `vast help` and `vast documentation` now goes to *stdout* instead of to stderr. Erroneous invocations of `vast` also print the helptext, but in this case the output still goes to stderr to avoid interference with downstream tooling.

By [@mavam](https://github.com/mavam) in [#1385](https://github.com/tenzir/tenzir/pull/1385).

#### PRs 1343-1356-ngrodzitski

[Section titled “PRs 1343-1356-ngrodzitski”](#prs-1343-1356-ngrodzitski-1)

The `infer` command has an improved heuristic for the number types `int`, `count`, and `real`.

By [@ngrodzitski](https://github.com/ngrodzitski) in [#1343](https://github.com/tenzir/tenzir/pull/1343).

#### Enable real-time metrics reporting

[Section titled “Enable real-time metrics reporting”](#enable-real-time-metrics-reporting-1)

All options in `vast.metrics.*` had underscores in their names replaced with dashes to align with other options. For example, `vast.metrics.file_sink` is now `vast.metrics.file-sink`. The old options no longer work.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1368](https://github.com/tenzir/tenzir/pull/1368).

#### PRs 1223-1328-1334-1390-a4z

[Section titled “PRs 1223-1328-1334-1390-a4z”](#prs-1223-1328-1334-1390-a4z)

VAST switched to [spdlog >= 1.5.0](https://github.com/gabime/spdlog) for logging. For users, this means: The `vast.console-format` and `vast.file-format` now must be specified using the spdlog pattern syntax as described [here](https://github.com/gabime/spdlog/wiki/3.-Custom-formatting#pattern-flags). All settings under `caf.logger.*` are now ignored by VAST, and only the `vast.*` counterparts are used for logger configuration.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1223](https://github.com/tenzir/tenzir/pull/1223).

#### Move options from format to the import subcommand

[Section titled “Move options from format to the import subcommand”](#move-options-from-format-to-the-import-subcommand)

The options `listen`, `read`, `schema`, `schema-file`, `type`, and `uds` can from now on be supplied to the `import` command directly. Similarly, the options `write` and `uds` can be supplied to the `export` command. All options can still be used after the format subcommand, but that usage is deprecated.

By [@tobim](https://github.com/tobim) in [#1354](https://github.com/tenzir/tenzir/pull/1354).

#### Expand subnet value predicates

[Section titled “Expand subnet value predicates”](#expand-subnet-value-predicates)

The query normalizer interprets value predicates of type `subnet` more broadly: given a subnet `S`, the parser expands this to the expression `:subnet == S || :addr in S`. This change makes it easier to search for IP addresses belonging to a specific subnet.

By [@mavam](https://github.com/mavam) in [#1373](https://github.com/tenzir/tenzir/pull/1373).

#### Read user-supplied schema files from config dirs

[Section titled “Read user-supplied schema files from config dirs”](#read-user-supplied-schema-files-from-config-dirs)

User-supplied schema files are now picked up from `<SYSCONFDIR>/vast/schema` and `<XDG_CONFIG_HOME>/vast/schema` instead of `<XDG_DATA_HOME>/vast/schema`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1372](https://github.com/tenzir/tenzir/pull/1372).

#### Remove long-deprecated code

[Section titled “Remove long-deprecated code”](#remove-long-deprecated-code)

The previously deprecated options `vast.spawn.importer.ids` and `vast.schema-paths` no longer work. Furthermore, queries spread over multiple arguments are now disallowed instead of triggering a deprecation warning.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1374](https://github.com/tenzir/tenzir/pull/1374).

#### Require fmt to be installed separately from spdlog

[Section titled “Require fmt to be installed separately from spdlog”](#require-fmt-to-be-installed-separately-from-spdlog)

VAST now requires [fmt >= 5.2.1](https://fmt.dev) to be installed.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1330](https://github.com/tenzir/tenzir/pull/1330).

#### Introduce and make use of the timestamp type

[Section titled “Introduce and make use of the timestamp type”](#introduce-and-make-use-of-the-timestamp-type)

The special meaning of the `#timestamp` attribute has been removed from the schema language. Timestamps can from now on be marked as such by using the `timestamp` type instead. Queries of the form `#timestamp <op> value` remain operational but are deprecated in favor of `:timestamp`. Note that this change also affects `:time` queries, which aren’t supersets of `#timestamp` queries any longer.

By [@tobim](https://github.com/tobim) in [#1388](https://github.com/tenzir/tenzir/pull/1388).

#### Make it easier to reference user defined types in the schema language

[Section titled “Make it easier to reference user defined types in the schema language”](#make-it-easier-to-reference-user-defined-types-in-the-schema-language)

Schema parsing now uses a 2-pass loading phase so that type aliases can reference other types that are later defined in the same directory. Additionally, type definitions from already parsed schema dirs can be referenced from schema types that are parsed later. Types can also be redefined in later directories, but a type can not be defined twice in the same directory.

By [@tobim](https://github.com/tobim) in [#1331](https://github.com/tenzir/tenzir/pull/1331).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Revert “Fix potential race condition between evaluator and partition”

[Section titled “Revert “Fix potential race condition between evaluator and partition””](#revert-fix-potential-race-condition-between-evaluator-and-partition)

An ordering issue introduced in [#1295](https://github.com/tenzir/vast/pull/1295) that could lead to a segfault with long-running queries was reverted.

By [@lava](https://github.com/lava) in [#1381](https://github.com/tenzir/tenzir/pull/1381).

#### Let the JSON reader recover from unexpected inputs

[Section titled “Let the JSON reader recover from unexpected inputs”](#let-the-json-reader-recover-from-unexpected-inputs)

A bug in the new simdjson based JSON reader introduced in [#1356](https://github.com/tenzir/vast/pull/1356) could trigger an assertion in the `vast import` process if an input field could not be converted to the field type in the target layout. This is no longer the case.

By [@tobim](https://github.com/tobim) in [#1386](https://github.com/tenzir/tenzir/pull/1386).

# VAST 2021.03.25

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.03.25).

### Features

[Section titled “Features”](#features)

#### PRs 1407-1487-1490

[Section titled “PRs 1407-1487-1490”](#prs-1407-1487-1490)

The schema language now supports 4 operations on record types: `+` combines the fields of 2 records into a new record. `<+` and `+>` are variations of `+` that give precedence to the left and right operand respectively. `-` creates a record with the field specified as its right operand removed.

By [@tobim](https://github.com/tobim) in [#1407](https://github.com/tenzir/tenzir/pull/1407).

#### Support nested records in the Arrow Builder

[Section titled “Support nested records in the Arrow Builder”](#support-nested-records-in-the-arrow-builder)

VAST now supports nested records in Arrow table slices and in the JSON import, e.g., data of type `list<record<name: string, age: count>`. While nested record fields are not yet queryable, ingesting such data will no longer cause VAST to crash. MessagePack table slices don’t support records in lists yet.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1429](https://github.com/tenzir/tenzir/pull/1429).

### Changes

[Section titled “Changes”](#changes)

#### Move zeek-to-vast from tenzir/vast to tenzir/zeek-vast

[Section titled “Move zeek-to-vast from tenzir/vast to tenzir/zeek-vast”](#move-zeek-to-vast-from-tenzirvast-to-tenzirzeek-vast)

The zeek-to-vast utility was moved to the [tenzir/zeek-vast](https://github.com/tenzir/zeek-vast) repository. All options related to zeek-to-vast and the bundled Broker submodule were removed.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1435](https://github.com/tenzir/tenzir/pull/1435).

#### PRs 1408-satta

[Section titled “PRs 1408-satta”](#prs-1408-satta)

VAST now ships with schema record types for Suricata’s `mqtt` and `anomaly` event types.

By [@satta](https://github.com/satta) in [#1408](https://github.com/tenzir/tenzir/pull/1408).

#### Deprecate the vast.no-default-schema option

[Section titled “Deprecate the vast.no-default-schema option”](#deprecate-the-vastno-default-schema-option)

The option `vast.no-default-schema` is deprecated, as it is no longer needed to override types from bundled schemas.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1409](https://github.com/tenzir/tenzir/pull/1409).

#### PRs 1445-1452

[Section titled “PRs 1445-1452”](#prs-1445-1452)

Plugins can now be linked statically against VAST. A new `VASTRegisterPlugin` CMake function enables easy setup of the build scaffolding required for plugins. Configure with `--with-static-plugins` or build a static binary to link all plugins built alongside VAST statically. All plugin build scaffoldings must be adapted, older plugins do no longer work.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1445](https://github.com/tenzir/tenzir/pull/1445).

#### Rename the attribute\_extractor to meta\_extractor

[Section titled “Rename the attribute\_extractor to meta\_extractor”](#rename-the-attribute_extractor-to-meta_extractor)

The previously deprecated `#timestamp` extractor has been removed from the query language entirely. Use `:timestamp` instead.

By [@tobim](https://github.com/tobim) in [#1399](https://github.com/tenzir/tenzir/pull/1399).

#### Establish subtyping relationships for type extractors

[Section titled “Establish subtyping relationships for type extractors”](#establish-subtyping-relationships-for-type-extractors)

The type extractor in the expression language now works with type aliases. For example, given the type definition for port from the base schema `type port = count`, a search for `:count` will also consider fields of type `port`.

By [@tobim](https://github.com/tobim) in [#1446](https://github.com/tenzir/tenzir/pull/1446).

#### Change the default batch size to 1024

[Section titled “Change the default batch size to 1024”](#change-the-default-batch-size-to-1024)

The default size of table slices (event batches) that is created from `vast import` processes has been changed from 1,000 to 1,024.

By [@tobim](https://github.com/tobim) in [#1396](https://github.com/tenzir/tenzir/pull/1396).

#### Prune expressions for the meta index lookup

[Section titled “Prune expressions for the meta index lookup”](#prune-expressions-for-the-meta-index-lookup)

Query latency for expressions that contain concept names has improved substantially. For DB sizes in the TB region, and with a large variety of event types, queries with a high selectivity experience speedups of up to 5x.

By [@tobim](https://github.com/tobim) in [#1433](https://github.com/tenzir/tenzir/pull/1433).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Remove erased partitions from the meta index

[Section titled “Remove erased partitions from the meta index”](#remove-erased-partitions-from-the-meta-index)

The disk monitor now correctly erases partition synopses from the meta index.

By [@lava](https://github.com/lava) in [#1450](https://github.com/tenzir/tenzir/pull/1450).

#### Fix printing of non-null intrusive pointers

[Section titled “Fix printing of non-null intrusive pointers”](#fix-printing-of-non-null-intrusive-pointers)

Some non-null pointers were incorrectly rendered as `*nullptr` in log messages.

By [@lava](https://github.com/lava) in [#1430](https://github.com/tenzir/tenzir/pull/1430).

#### Don’t allow field extractors to match field name suffixes

[Section titled “Don’t allow field extractors to match field name suffixes”](#dont-allow-field-extractors-to-match-field-name-suffixes)

A query for a field or field name suffix that matches multiple fields of different types would erroneously return no results.

By [@lava](https://github.com/lava) in [#1447](https://github.com/tenzir/tenzir/pull/1447).

#### Fix possibly unhandled exception in disk monitor

[Section titled “Fix possibly unhandled exception in disk monitor”](#fix-possibly-unhandled-exception-in-disk-monitor)

VAST no longer crashes when the disk monitor tries to calculate the size of the database while files are being deleted. Instead, it will retry after the configured scan interval.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1458](https://github.com/tenzir/tenzir/pull/1458).

#### Make the disk-monitor a singleton component

[Section titled “Make the disk-monitor a singleton component”](#make-the-disk-monitor-a-singleton-component)

Enabling the disk budget feature no longer prevents the server process from exiting after it was stopped.

By [@tobim](https://github.com/tobim) in [#1495](https://github.com/tenzir/tenzir/pull/1495).

#### Use non-throwing std::filesystem functions in the type registry

[Section titled “Use non-throwing std::filesystem functions in the type registry”](#use-non-throwing-stdfilesystem-functions-in-the-type-registry)

Insufficient permissions for one of the paths in the `schema-dirs` option would lead to a crash in `vast start`.

By [@tobim](https://github.com/tobim) in [#1472](https://github.com/tenzir/tenzir/pull/1472).

#### PRs 1473-1485

[Section titled “PRs 1473-1485”](#prs-1473-1485)

A race condition during server shutdown could lead to an invariant violation, resulting in a firing assertion. Streamlining the shutdown logic resolved the issue.

By [@mavam](https://github.com/mavam) in [#1473](https://github.com/tenzir/tenzir/pull/1473).

#### Report metrics while idle

[Section titled “Report metrics while idle”](#report-metrics-while-idle)

The archive, index, source, and sink components now report metrics when idle instead of omitting them entirely. This allows for distinguishing between idle and not running components from the metrics.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1451](https://github.com/tenzir/tenzir/pull/1451).

#### Fix :timestamp queries for old data

[Section titled “Fix :timestamp queries for old data”](#fix-timestamp-queries-for-old-data)

Data that was ingested before the deprecation of the `#timestamp` attribute wasn’t exported correctly with newer versions. This is now corrected.

By [@tobim](https://github.com/tobim) in [#1432](https://github.com/tenzir/tenzir/pull/1432).

#### Accept numbers in place of strings in JSON

[Section titled “Accept numbers in place of strings in JSON”](#accept-numbers-in-place-of-strings-in-json)

The JSON parser now accepts data with numerical or boolean values in fields that expect strings according to the schema. VAST converts these values into string representations.

By [@tobim](https://github.com/tobim) in [#1439](https://github.com/tenzir/tenzir/pull/1439).

# VAST 2021.04.29

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.04.29).

### Features

[Section titled “Features”](#features)

#### Move PCAP import/export into a plugin

[Section titled “Move PCAP import/export into a plugin”](#move-pcap-importexport-into-a-plugin)

*Reader Plugins* and *Writer Plugins* are a new family of plugins that add import/export formats. The previously optional PCAP format moved into a dedicated plugin. Configure with `--with-pcap-plugin` and add `pcap` to `vast.plugins` to enable the PCAP plugin.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1549](https://github.com/tenzir/tenzir/pull/1549).

#### PRs 1532-1541

[Section titled “PRs 1532-1541”](#prs-1532-1541)

The `VAST_PLUGIN_DIRS` and `VAST_SCHEMA_DIRS` environment variables allow for setting additional plugin and schema directories separated with `:` with higher precedence than other plugin and schema directories.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1532](https://github.com/tenzir/tenzir/pull/1532).

#### Allow for building plugins separately from VAST

[Section titled “Allow for building plugins separately from VAST”](#allow-for-building-plugins-separately-from-vast)

It is now possible to build plugins against an installed VAST. This requires a slight adaptation to every plugin’s build scaffolding. The example plugin was updated accordingly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1532](https://github.com/tenzir/tenzir/pull/1532).

#### Add ‘—disk-budget-check-binary’ option to disk monitor

[Section titled “Add ‘—disk-budget-check-binary’ option to disk monitor”](#add-disk-budget-check-binary-option-to-disk-monitor)

The disk monitor gained a new `vast.start.disk-budget-check-binary` option that can be used to specify an external binary to determine the size of the database directory. This can be useful in cases where `stat()` does not give the correct answer, e.g. on compressed filesystems.

By [@lava](https://github.com/lava) in [#1453](https://github.com/tenzir/tenzir/pull/1453).

#### PRs 1544-1547-1588

[Section titled “PRs 1544-1547-1588”](#prs-1544-1547-1588)

*Component Plugins* are a new category of plugins that execute code within the VAST server process. *Analyzer Plugins* are now a specialization of *Component Plugins*, and their API remains unchanged.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1544](https://github.com/tenzir/tenzir/pull/1544).

### Changes

[Section titled “Changes”](#changes)

#### Make the source a regular class

[Section titled “Make the source a regular class”](#make-the-source-a-regular-class)

The metrics for Suricata Eve JSON and Zeek Streaming JSON imports are now under the categories `suricata-reader` and `zeek-reader` respectively so they can be distinguished from the regular JSON import, which is still under `json-reader`.

By [@tobim](https://github.com/tobim) in [#1498](https://github.com/tenzir/tenzir/pull/1498).

#### Small fixes for projections

[Section titled “Small fixes for projections”](#small-fixes-for-projections)

The Suricata `dns` schema type now defines the `dns.grouped.A` field containing a list of all returned addresses.

By [@tobim](https://github.com/tobim) in [#1531](https://github.com/tenzir/tenzir/pull/1531).

#### Upstream Debian patches

[Section titled “Upstream Debian patches”](#upstream-debian-patches)

We upstreamed the Debian patches provided by [@satta](https://github.com/satta). VAST now prefers an installed `tsl-robin-map>=0.6.2` to the bundled one unless configured with `--with-bundled-robin-map`, and we provide a manpage for `lsvast` if `pandoc` is installed.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1515](https://github.com/tenzir/tenzir/pull/1515).

#### Remove deprecated no-default-schema option

[Section titled “Remove deprecated no-default-schema option”](#remove-deprecated-no-default-schema-option)

The previously deprecated ([#1409](https://github.com/tenzir/vast/pull/1409)) option `vast.no-default-schema` no longer exists.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1507](https://github.com/tenzir/tenzir/pull/1507).

#### Use individual files for changelog entries

[Section titled “Use individual files for changelog entries”](#use-individual-files-for-changelog-entries)

Building VAST now requires CMake >= 3.15.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1559](https://github.com/tenzir/tenzir/pull/1559).

#### Align plugin and library output names

[Section titled “Align plugin and library output names”](#align-plugin-and-library-output-names)

Plugins configured via `vast.plugins` in the configuration file can now be specified using either the plugin name or the full path to the shared plugin library. We no longer allow omitting the extension from specified plugin files, and recommend using the plugin name as a more portable solution, e.g., `example` over `libexample` and `/path/to/libexample.so` over `/path/to/libexample`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1527](https://github.com/tenzir/tenzir/pull/1527).

#### Prefix plugin library output names with vast-plugin-

[Section titled “Prefix plugin library output names with vast-plugin-”](#prefix-plugin-library-output-names-with-vast-plugin)

To avoid confusion between the PCAP plugin and libpcap, which both have a library file named `libpcap.so`, we now generally prefix the plugin library output names with `vast-plugin-`. E.g., The PCAP plugin library file is now named `libvast-plugin-pcap.so`. Plugins specified with a full path in the configuration under `vast.plugins` must be adapted accordingly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1593](https://github.com/tenzir/tenzir/pull/1593).

#### Make it possible to run VAST without user configs

[Section titled “Make it possible to run VAST without user configs”](#make-it-possible-to-run-vast-without-user-configs)

The new option `--disable-default-config-dirs` disables the loading of user and system configuration, schema, and plugin directories. We use this option internally when running integration tests.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1557](https://github.com/tenzir/tenzir/pull/1557).

#### Update chat URL in README

[Section titled “Update chat URL in README”](#update-chat-url-in-readme)

The VAST community chat moved from Element to Gitter. Join us at [gitter.im/tenzir/vast](https://gitter.im/tenzir/vast) or via Matrix at `#tenzir_vast:gitter.im`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1591](https://github.com/tenzir/tenzir/pull/1591).

#### PRs 1514-1574

[Section titled “PRs 1514-1574”](#prs-1514-1574)

The `exporter.hits` metric has been removed.

By [@tobim](https://github.com/tobim) in [#1514](https://github.com/tenzir/tenzir/pull/1514).

#### PRs 1499-satta

[Section titled “PRs 1499-satta”](#prs-1499-satta)

VAST now ships with a schema record type for Suricata’s `rfb` event type.

By [@satta](https://github.com/satta) in [#1499](https://github.com/tenzir/tenzir/pull/1499).

#### Remove deprecated format-specific options

[Section titled “Remove deprecated format-specific options”](#remove-deprecated-format-specific-options)

The previously deprecated usage ([#1354](https://github.com/tenzir/vast/pull/1354)) of format-independent options after the format in commands is now no longer possible. This affects the options `listen`, `read`, `schema`, `schema-file`, `type`, and `uds` for import commands and the `write` and `uds` options for export commands.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1529](https://github.com/tenzir/tenzir/pull/1529).

#### Factor common functionality in component plugin

[Section titled “Factor common functionality in component plugin”](#factor-common-functionality-in-component-plugin)

The status output of *Analyzer Plugins* moved from the `importer.analyzers` key into the top-level record.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1544](https://github.com/tenzir/tenzir/pull/1544).

#### Move PCAP import/export into a plugin

[Section titled “Move PCAP import/export into a plugin”](#move-pcap-importexport-into-a-plugin-1)

Plugins must define a separate entrypoint in their build scaffolding using the argument `ENTRYPOINT` to the CMake function `VASTRegisterPlugin`. If only a single value is given to the argument `SOURCES`, it is interpreted as the `ENTRYPOINT` automatically.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1549](https://github.com/tenzir/tenzir/pull/1549).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Handle alias types properly for CSV

[Section titled “Handle alias types properly for CSV”](#handle-alias-types-properly-for-csv)

The CSV reader no longer crashes when encountering nested type aliases.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1534](https://github.com/tenzir/tenzir/pull/1534).

#### Move PCAP import/export into a plugin

[Section titled “Move PCAP import/export into a plugin”](#move-pcap-importexport-into-a-plugin-2)

Plugin unit tests now correctly load and initialize their respective plugins.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1549](https://github.com/tenzir/tenzir/pull/1549).

#### Fix start command detection for spdlog

[Section titled “Fix start command detection for spdlog”](#fix-start-command-detection-for-spdlog)

Custom commands from plugins ending in `start` no longer try to write to the server instead of the client log file.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1530](https://github.com/tenzir/tenzir/pull/1530).

#### Ignore spaces before SI prefixes

[Section titled “Ignore spaces before SI prefixes”](#ignore-spaces-before-si-prefixes)

Spaces before SI prefixes in command line arguments and configuration options are now generally ignored, e.g., it is now possible to set the disk monitor budgets to `2 GiB` rather than `2GiB`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1590](https://github.com/tenzir/tenzir/pull/1590).

#### Multiple node shutdown fixes

[Section titled “Multiple node shutdown fixes”](#multiple-node-shutdown-fixes)

The shutdown logic contained a bug that would make the node fail to terminate in case a plugin actor is registered at said node.

A race condition in the shutdown logic that caused an assertion was fixed.

By [@tobim](https://github.com/tobim) in [#1563](https://github.com/tenzir/tenzir/pull/1563).

#### Fix exporter.selectivity for idle periods

[Section titled “Fix exporter.selectivity for idle periods”](#fix-exporterselectivity-for-idle-periods)

The `exporter.selectivity` metric is now 1.0 instead of NaN for idle periods.

VAST no longer renders JSON numbers with non-finite numbers as `NaN`, `-NaN`, `inf`, or `-inf`, resulting in invalid JSON output. Instead, such numbers are now rendered as `null`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1574](https://github.com/tenzir/tenzir/pull/1574).

#### Deduplicate plugin entrypoint in sources

[Section titled “Deduplicate plugin entrypoint in sources”](#deduplicate-plugin-entrypoint-in-sources)

We fixed a regression that made it impossible to build static binaries from outside of the repository root directory.

The `VASTRegisterPlugin` CMake function now correctly removes the `ENTRYPOINT` from the given `SOURCES`, allowing for plugin developers to easily glob for sources again.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1573](https://github.com/tenzir/tenzir/pull/1573).

#### Use proper full install dirs for system config

[Section titled “Use proper full install dirs for system config”](#use-proper-full-install-dirs-for-system-config)

Specifying relative `CMAKE_INSTALL_*DIR` in the build configuration no longer causes VAST not to pick up system-wide installed configuration files, schemas, and plugins. The configured install prefix is now used correctly. The defunct `VAST_SYSCONFDIR`, `VAST_DATADIR`, and `VAST_LIBDIR` CMake options no longer exist. Use a combination of `CMAKE_INSTALL_PREFIX` and `CMAKE_INSTALL_*DIR` instead.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1580](https://github.com/tenzir/tenzir/pull/1580).

#### Fix building VAST within a shallow git tree

[Section titled “Fix building VAST within a shallow git tree”](#fix-building-vast-within-a-shallow-git-tree)

VAST now correctly builds within shallow clones of the repository. If the build system is unable to determine the correct version from `git-describe`, it now always falls back to the version of the last release.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1570](https://github.com/tenzir/tenzir/pull/1570).

#### Avoid shutdown when config dirs are not readable

[Section titled “Avoid shutdown when config dirs are not readable”](#avoid-shutdown-when-config-dirs-are-not-readable)

VAST no longer refuses to start when any of the configuration file directories is unreadable, e.g., because VAST is running in a sandbox.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1533](https://github.com/tenzir/tenzir/pull/1533).

#### Ignore static plugins when specified in config

[Section titled “Ignore static plugins when specified in config”](#ignore-static-plugins-when-specified-in-config)

VAST no longer erroneously tries to load explicitly specified plugins dynamically that are linked statically.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1528](https://github.com/tenzir/tenzir/pull/1528).

#### Fix the timezone shift to UTC for ISO8601 dates

[Section titled “Fix the timezone shift to UTC for ISO8601 dates”](#fix-the-timezone-shift-to-utc-for-iso8601-dates)

A bug in the parsing of ISO8601 formatted dates that incorrectly adjusted the time to the UTC timezone has been fixed.

By [@tobim](https://github.com/tobim) in [#1537](https://github.com/tenzir/tenzir/pull/1537).

#### Allow for building plugins separately from VAST

[Section titled “Allow for building plugins separately from VAST”](#allow-for-building-plugins-separately-from-vast-1)

Linking against an installed VAST via CMake now correctly resolves VAST’s dependencies.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1532](https://github.com/tenzir/tenzir/pull/1532).

#### Fix out-of-bounds access in command-line parser

[Section titled “Fix out-of-bounds access in command-line parser”](#fix-out-of-bounds-access-in-command-line-parser)

The command-line parser no longer crashes when encountering a flag with missing value in the last position of a command invocation.

By [@lava](https://github.com/lava) in [#1536](https://github.com/tenzir/tenzir/pull/1536).

# VAST 2021.05.27

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.05.27).

### Features

[Section titled “Features”](#features)

#### Load plugin schemas after built-in schemas

[Section titled “Load plugin schemas after built-in schemas”](#load-plugin-schemas-after-built-in-schemas)

Plugin schemas are now installed to `<datadir>/vast/plugin/<plugin>/schema`, while VAST’s built-in schemas reside in `<datadir>/vast/schema`. The load order guarantees that plugins are able to reliably override the schemas bundled with VAST.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1608](https://github.com/tenzir/tenzir/pull/1608).

#### Support optional numeric duration output for JSON

[Section titled “Support optional numeric duration output for JSON”](#support-optional-numeric-duration-output-for-json)

To enable easier post-processing, the new option `vast.export.json.numeric-durations` switches JSON output of `duration` types from human-readable strings (e.g., `"4.2m"`) to numeric (e.g., `252.15`) in fractional seconds.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1628](https://github.com/tenzir/tenzir/pull/1628).

#### Add a timeout option to the export command

[Section titled “Add a timeout option to the export command”](#add-a-timeout-option-to-the-export-command)

The new option `vast export --timeout=<duration>` allows for setting a timeout for VAST queries. Cancelled exports result in a non-zero exit code.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1611](https://github.com/tenzir/tenzir/pull/1611).

#### PRs 1517-1656

[Section titled “PRs 1517-1656”](#prs-1517-1656)

The new *transforms* feature allows VAST to apply transformations to incoming and outgoing data. A transform consists of a sequence of steps that execute sequentially, e.g., to remove, overwrite, hash, encrypt data. A new plugin type makes it easy to write custom transforms.

By [@lava](https://github.com/lava) in [#1517](https://github.com/tenzir/tenzir/pull/1517).

#### Add step size to disk monitor

[Section titled “Add step size to disk monitor”](#add-step-size-to-disk-monitor)

The new setting `vast.disk-monitor-step-size` enables the disk monitor to remove *N* partitions at once before re-checking if the new size of the database directory is now small enough. This is useful when checking the size of a directory is an expensive operation itself, e.g., on compressed filesystems.

By [@lava](https://github.com/lava) in [#1655](https://github.com/tenzir/tenzir/pull/1655).

#### Print the remote-version in the status command

[Section titled “Print the remote-version in the status command”](#print-the-remote-version-in-the-status-command)

The `status` command now prints the VAST server version information under the `version` key.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1652](https://github.com/tenzir/tenzir/pull/1652).

### Changes

[Section titled “Changes”](#changes)

#### Don’t send dynamic type information to connecting sources

[Section titled “Don’t send dynamic type information to connecting sources”](#dont-send-dynamic-type-information-to-connecting-sources)

Schemas are no longer implicitly shared between sources, i.e., an `import` process importing data with a custom schema will no longer affect other sources started at a later point in time. Schemas known to the VAST server process are still available to all `import` processes. We do not expect this change to have a real-world impact, but it could break setups where some sources have been installed on hosts without their own schema files, the VAST server did not have up-to-date schema files, and other sources were (ab)used to provide the latest type information.

By [@lava](https://github.com/lava) in [#1656](https://github.com/tenzir/tenzir/pull/1656).

#### Deprecate builds without Apache Arrow

[Section titled “Deprecate builds without Apache Arrow”](#deprecate-builds-without-apache-arrow)

Building VAST without Apache Arrow via `-DVAST_ENABLE_ARROW=OFF` is now deprecated, and support for the option will be removed in a future release. As the Arrow ecosystem and libraries matured, we feel confident in making it a required dependency and plan to build upon it more in the future.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1682](https://github.com/tenzir/tenzir/pull/1682).

#### Remove configure script

[Section titled “Remove configure script”](#remove-configure-script)

The `configure` script was removed. This was a custom script that mimicked the functionality of an autotools-based `configure` script by writing directly to the cmake cache. Instead, users now must use the `cmake` and/or `ccmake` binaries directly to configure VAST.

By [@lava](https://github.com/lava) in [#1657](https://github.com/tenzir/tenzir/pull/1657).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Sort loaded plugins by name

[Section titled “Sort loaded plugins by name”](#sort-loaded-plugins-by-name)

VAST no longer erroneously warns about a version mismatch between client and server when their plugin load order differs.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1679](https://github.com/tenzir/tenzir/pull/1679).

#### Print the remote-version in the status command

[Section titled “Print the remote-version in the status command”](#print-the-remote-version-in-the-status-command-1)

VAST no longer erroneously skips the version mismatch detection between client and server. The check now additionally compares running plugins.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1652](https://github.com/tenzir/tenzir/pull/1652).

#### Error when initializing a plugin fails

[Section titled “Error when initializing a plugin fails”](#error-when-initializing-a-plugin-fails)

VAST now correctly refuses to run when loaded plugins fail their initialization, i.e., are in a state that cannot be reasoned about.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1618](https://github.com/tenzir/tenzir/pull/1618).

#### Allow for running unit tests in parallel

[Section titled “Allow for running unit tests in parallel”](#allow-for-running-unit-tests-in-parallel)

Executing VAST’s unit test suite in parallel no longer fails.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1659](https://github.com/tenzir/tenzir/pull/1659).

#### Fix install dirs wrt binary relocatability

[Section titled “Fix install dirs wrt binary relocatability”](#fix-install-dirs-wrt-binary-relocatability)

Non-relocatable VAST binaries no longer look for configuration, schemas, and plugins in directories relative to the binary location. Vice versa, relocatable VAST binaries no longer look for configuration, schemas, and plugins in their original install directory, and instead always use paths relative to their binary location. On macOS, we now always build relocatable binaries. Relocatable binaries now work correctly on systems where the libary install directory is `lib64` instead of `lib`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1624](https://github.com/tenzir/tenzir/pull/1624).

#### Fix the datagram source

[Section titled “Fix the datagram source”](#fix-the-datagram-source)

A recent change caused imports over UDP not to forward its events to the VAST server process. Running `vast import -l :<port>/udp <format>` now works as expected again.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1622](https://github.com/tenzir/tenzir/pull/1622).

#### Fix build without Arrow

[Section titled “Fix build without Arrow”](#fix-build-without-arrow)

VAST and transform plugins now build without Arrow support again.

The `delete` transform step correctly deletes fields from the layout when running VAST with Arrow disabled.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1673](https://github.com/tenzir/tenzir/pull/1673).

# VAST 2021.06.24

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.06.24).

### Features

[Section titled “Features”](#features)

#### Add support for per-plugin configuration files

[Section titled “Add support for per-plugin configuration files”](#add-support-for-per-plugin-configuration-files)

Plugins load their respective configuration from `<configdir>/vast/plugin/<plugin-name>.yaml` in addition to the regular configuration file at `<configdir>/vast/vast.yaml`. The new plugin-specific file does not require putting configuration under the key `plugins.<plugin-name>`. This allows for deploying plugins without needing to touch the `<configdir>/vast/vast.yaml` configuration file.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1724](https://github.com/tenzir/tenzir/pull/1724).

#### Optionally read environment variable for VAST endpoint

[Section titled “Optionally read environment variable for VAST endpoint”](#optionally-read-environment-variable-for-vast-endpoint)

It’s now possible to configure the VAST endpoint as an environment variable by setting `VAST_ENDPOINT`. This has higher precedence than setting `vast.endpoint` in configuration files, but lower precedence than passing `--endpoint=` on the command-line.

By [@rolandpeelen](https://github.com/rolandpeelen) in [#1714](https://github.com/tenzir/tenzir/pull/1714).

#### Rework the plugin loading logic

[Section titled “Rework the plugin loading logic”](#rework-the-plugin-loading-logic)

The options `vast.plugins` and `vast.plugin-dirs` may now be specified on the command line as well as the configuration. Use the options `--plugins` and `--plugin-dirs` respectively.

Add the reserved plugin name `bundled` to `vast.plugins` to enable load all bundled plugins, i.e., static or dynamic plugins built alongside VAST, or use `--plugins=bundled` on the command line. The reserved plugin name `all` causes all bundled and external plugins to be loaded, i.e., all shared libraries matching `libvast-plugin-*` from the configured `vast.plugin-dirs`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1703](https://github.com/tenzir/tenzir/pull/1703).

#### Add option for configurable post-start hooks

[Section titled “Add option for configurable post-start hooks”](#add-option-for-configurable-post-start-hooks)

The new option `vast.start.commands` allows for specifying an ordered list of VAST commands that run after successful startup. The effect is the same as first starting a node, and then using another VAST client to issue commands. This is useful for commands that have side effects that cannot be expressed through the config file, e.g., starting a source inside the VAST server that listens on a socket or reads packets from a network interface.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1699](https://github.com/tenzir/tenzir/pull/1699).

### Changes

[Section titled “Changes”](#changes)

#### Rework the plugin loading logic

[Section titled “Rework the plugin loading logic”](#rework-the-plugin-loading-logic-1)

VAST no longer loads static plugins by default. Generally, VAST now treats static plugins and bundled dynamic plugins equally, allowing users to enable or disable static plugins as needed for their deployments.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1703](https://github.com/tenzir/tenzir/pull/1703).

#### PRs 1721-1734

[Section titled “PRs 1721-1734”](#prs-1721-1734)

VAST merges lists from configuration files. E.g., running VAST with `--plugins=some-plugin` and `vast.plugins: [other-plugin]` in the configuration now results in both `some-plugin` and `other-plugin` being loaded (sorted by the usual precedence), instead of just `some-plugin`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1721](https://github.com/tenzir/tenzir/pull/1721).

#### Change chat from Gitter to Slack

[Section titled “Change chat from Gitter to Slack”](#change-chat-from-gitter-to-slack)

The VAST community chat moved from Gitter to Slack. [Join us](http://slack.tenzir.com) in the `#vast` channel for vibrant discussions.

By [@mavam](https://github.com/mavam) in [#1696](https://github.com/tenzir/tenzir/pull/1696).

#### Remove support for building without Arrow

[Section titled “Remove support for building without Arrow”](#remove-support-for-building-without-arrow)

Apache Arrow is now a required dependency. The previously deprecated build option `-DVAST_ENABLE_ARROW=OFF` no longer exists.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1683](https://github.com/tenzir/tenzir/pull/1683).

#### Fixup repository-internal symlinks in Dockerfile

[Section titled “Fixup repository-internal symlinks in Dockerfile”](#fixup-repository-internal-symlinks-in-dockerfile)

The [tenzir/vast](https://hub.docker.com/r/tenzir/vast) Docker image bundles the PCAP plugin.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1705](https://github.com/tenzir/tenzir/pull/1705).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix log rotation threshold option

[Section titled “Fix log rotation threshold option”](#fix-log-rotation-threshold-option)

The `vast.log-rotation-threshold` option was silently ignored, causing VAST to always use the default log rotation threshold of 10 MiB. The option works as expected now.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1709](https://github.com/tenzir/tenzir/pull/1709).

#### Fix a bunch of smaller issues

[Section titled “Fix a bunch of smaller issues”](#fix-a-bunch-of-smaller-issues)

Building plugins against an installed VAST no longer requires manually specifying `-DBUILD_SHARED_LIBS=ON`. The option is now correctly enabled by default for external plugins.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1697](https://github.com/tenzir/tenzir/pull/1697).

#### Remove dead condition and fix Docker release tags

[Section titled “Remove dead condition and fix Docker release tags”](#remove-dead-condition-and-fix-docker-release-tags)

Additional tags for the [tenzir/vast](https://hub.docker.com/r/tenzir/vast) Docker image for the release versions exist, e.g., `tenzir/vast:2021.05.27`.

By [@0snap](https://github.com/0snap) in [#1711](https://github.com/tenzir/tenzir/pull/1711).

#### Fix shutdown hang in sources on SIGTERM/SIGINT

[Section titled “Fix shutdown hang in sources on SIGTERM/SIGINT”](#fix-shutdown-hang-in-sources-on-sigtermsigint)

Import processes no longer hang on receiving SIGINT or SIGKILL. Instead, they shut down properly after flushing yet to be processed data.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1718](https://github.com/tenzir/tenzir/pull/1718).

#### Make unix dgram metrics sink connectionless

[Section titled “Make unix dgram metrics sink connectionless”](#make-unix-dgram-metrics-sink-connectionless)

The UDS metrics sink continues to send data when the receiving socket is recreated.

By [@tobim](https://github.com/tobim) in [#1702](https://github.com/tenzir/tenzir/pull/1702).

#### Handle arbitrary types in bloom filter synopsis

[Section titled “Handle arbitrary types in bloom filter synopsis”](#handle-arbitrary-types-in-bloom-filter-synopsis)

VAST no longer crashes when querying for string fields with non-string values. Instead, an error message warns the user about an invalid query.

By [@lava](https://github.com/lava) in [#1685](https://github.com/tenzir/tenzir/pull/1685).

#### Handle quoted strings in CSV parser

[Section titled “Handle quoted strings in CSV parser”](#handle-quoted-strings-in-csv-parser)

The `import csv` command handles quoted fields correctly. Previously, the quotes were part of the parsed value, and field separators in quoted strings caused the parser to fail.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1712](https://github.com/tenzir/tenzir/pull/1712).

# VAST 2021.07.29

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.07.29).

### Features

[Section titled “Features”](#features)

#### Support import filter expressions

[Section titled “Support import filter expressions”](#support-import-filter-expressions)

VAST now supports import filter expressions. They act as the dual to export query expressions: `vast import suricata '#type == "suricata.alert"' < eve.json` will import only `suricata.alert` events, discarding all other events.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1742](https://github.com/tenzir/tenzir/pull/1742).

#### Use a unique version for plugins

[Section titled “Use a unique version for plugins”](#use-a-unique-version-for-plugins)

Plugin versions are now unique to facilitate debugging. They consist of three optional parts: (1) the CMake project version of the plugin, (2) the Git revision of the last commit that touched the plugin, and (3) a `dirty` suffix for uncommited changes to the plugin. Plugin developers no longer need to specify the version manually in the plugin entrypoint.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1764](https://github.com/tenzir/tenzir/pull/1764).

#### Add per-layout metrics to imports

[Section titled “Add per-layout metrics to imports”](#add-per-layout-metrics-to-imports)

VAST now exports per-layout import metrics under the key `<reader>.events.<layout-name>` in addition to the regular `<reader>.events`. This makes it easier to understand the event type distribution.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1781](https://github.com/tenzir/tenzir/pull/1781).

#### Fix Docker image builds for arm64

[Section titled “Fix Docker image builds for arm64”](#fix-docker-image-builds-for-arm64)

VAST now supports the *arm64* architecture.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1773](https://github.com/tenzir/tenzir/pull/1773).

#### Enable Broker plugin by default for Nix

[Section titled “Enable Broker plugin by default for Nix”](#enable-broker-plugin-by-default-for-nix)

The static binary now bundles the Broker plugin.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1789](https://github.com/tenzir/tenzir/pull/1789).

#### Publish a tenzir/vast-dev Docker image

[Section titled “Publish a tenzir/vast-dev Docker image”](#publish-a-tenzirvast-dev-docker-image)

VAST now comes with a [`tenzir/vast-dev`](https://hub.docker.com/r/tenzir/vast-dev) Docker image in addition to the regular [`tenzir/vast`](https://hub.docker.com/r/tenzir/vast). The `vast-dev` image targets development contexts, e.g., when building additional plugins. The image contains all build-time dependencies of VAST and runs as `root` rather than the `vast` user.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1749](https://github.com/tenzir/tenzir/pull/1749).

#### Add Zeek Broker reader plugin

[Section titled “Add Zeek Broker reader plugin”](#add-zeek-broker-reader-plugin)

The new [Broker](https://github.com/zeek/broker) plugin enables seamless log ingestion from [Zeek](https://github.com/zeek/zeek) to VAST via a TCP socket. Broker is Zeek’s messaging library and the plugin turns VAST into a Zeek [logger node](https://docs.zeek.org/en/master/frameworks/cluster.html#logger). Use `vast import broker` to establish a connection to a Zeek node and acquire logs.

By [@mavam](https://github.com/mavam) in [#1758](https://github.com/tenzir/tenzir/pull/1758).

#### PRs 1720-1762-1802

[Section titled “PRs 1720-1762-1802”](#prs-1720-1762-1802)

VAST has new a `store_plugin` type for custom store backends that hold the raw data of a partition. The new setting `vast.store-backend` controls the selection of the store implementation, which has a default value is `segment-store`. This is still an opt-in feature: unless the configuration value is set, VAST defaults to the old implementation.

By [@lava](https://github.com/lava) in [#1720](https://github.com/tenzir/tenzir/pull/1720).

#### Fix bug in bitmap offset computation

[Section titled “Fix bug in bitmap offset computation”](#fix-bug-in-bitmap-offset-computation)

`lsvast` now prints extended information for hash indexes.

By [@lava](https://github.com/lava) in [#1755](https://github.com/tenzir/tenzir/pull/1755).

#### Remove /etc as hardcoded sysconfdir from Nix build

[Section titled “Remove /etc as hardcoded sysconfdir from Nix build”](#remove-etc-as-hardcoded-sysconfdir-from-nix-build)

Installing VAST now includes a `vast.yaml.example` configuration file listing all available options.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1777](https://github.com/tenzir/tenzir/pull/1777).

### Changes

[Section titled “Changes”](#changes)

#### Compile with C++20

[Section titled “Compile with C++20”](#compile-with-c20)

From now on VAST is compiled with the C++20 language standard. Minimum compiler versions have increased to GCC 10, Clang 11, and AppleClang 12.0.5.

By [@tobim](https://github.com/tobim) in [#1768](https://github.com/tenzir/tenzir/pull/1768).

#### Bump minimum Debian requirement to Bullseye

[Section titled “Bump minimum Debian requirement to Bullseye”](#bump-minimum-debian-requirement-to-bullseye)

VAST no longer officially supports Debian Buster with GCC-8. In CI, VAST now runs on Debian Bullseye with GCC-10. The provided Docker images now use `debian:bullseye-slim` as base image. Users that require Debian Buster support should use the provided static builds instead.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1765](https://github.com/tenzir/tenzir/pull/1765).

#### Disable auto-vectorization in prebuilt Docker images

[Section titled “Disable auto-vectorization in prebuilt Docker images”](#disable-auto-vectorization-in-prebuilt-docker-images)

The `vast` binaries in our [prebuilt Docker images](http://hub.docker.com/r/tenzir/vast) no longer contain AVX instructions for increased portability. Building the image locally continues to add supported auto-vectorization flags automatically.

The following new build options exist: `VAST_ENABLE_AUTO_VECTORIZATION` enables/disables all auto-vectorization flags, and `VAST_ENABLE_SSE_INSTRUCTIONS` enables `-msse`; similar options exist for SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, and AVX2.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1778](https://github.com/tenzir/tenzir/pull/1778).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Support unique plugin versions for Nix and Docker

[Section titled “Support unique plugin versions for Nix and Docker”](#support-unique-plugin-versions-for-nix-and-docker)

The official Docker image and static binary distribution of VAST now produce the correct version output for plugins from the `vast version` command.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1799](https://github.com/tenzir/tenzir/pull/1799).

#### PRs 1804-1809

[Section titled “PRs 1804-1809”](#prs-1804-1809)

The disk budget feature no longer triggers a rare segfault while deleting partitions.

By [@tobim](https://github.com/tobim) in [#1804](https://github.com/tenzir/tenzir/pull/1804).

#### Fix sorting of plugins by name

[Section titled “Fix sorting of plugins by name”](#fix-sorting-of-plugins-by-name)

A regression caused VAST’s plugins to be loaded in random order, which printed a warning about mismatching plugins between client and server. The order is now deterministic.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1756](https://github.com/tenzir/tenzir/pull/1756).

#### Propagate VAST\_ENABLE\_JOURNALD\_LOGGING setting to config header

[Section titled “Propagate VAST\_ENABLE\_JOURNALD\_LOGGING setting to config header”](#propagate-vast_enable_journald_logging-setting-to-config-header)

The `VAST_ENABLE_JOURNALD_LOGGING` CMake option is no longer ignored.

By [@lava](https://github.com/lava) in [#1780](https://github.com/tenzir/tenzir/pull/1780).

#### Print the import transformer status

[Section titled “Print the import transformer status”](#print-the-import-transformer-status)

The the `status` command now prints information about input and output transformations.

By [@tobim](https://github.com/tobim) in [#1748](https://github.com/tenzir/tenzir/pull/1748).

#### Fix bug in bitmap offset computation

[Section titled “Fix bug in bitmap offset computation”](#fix-bug-in-bitmap-offset-computation-1)

Queries against fields using a `#index=hash` attribute could have missed some results. Fixing a bug in the offset calculation during bitmap processing resolved the issue.

By [@lava](https://github.com/lava) in [#1755](https://github.com/tenzir/tenzir/pull/1755).

#### Fix error message about /dev/null-backend on startup

[Section titled “Fix error message about /dev/null-backend on startup”](#fix-error-message-about-devnull-backend-on-startup)

A `[*** LOG ERROR #0001 ***]` error message on startup under Linux no longer occurs.

By [@lava](https://github.com/lava) in [#1754](https://github.com/tenzir/tenzir/pull/1754).

#### Make the source shutdown instantaneous

[Section titled “Make the source shutdown instantaneous”](#make-the-source-shutdown-instantaneous)

Import processes now respond quicker. Shutdown requests are no longer delayed when the server process has busy imports, and metrics reports are now written in a timely manner.

Particularly busy imports caused the shutdown of the server process to hang, if import processes were still running or had not yet flushed all data. The server now shuts down correctly in these cases.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1771](https://github.com/tenzir/tenzir/pull/1771).

#### Fix loading of the optional OpenSSL module

[Section titled “Fix loading of the optional OpenSSL module”](#fix-loading-of-the-optional-openssl-module)

Configuring VAST to use CAF’s built-in OpenSSL module via the `caf.openssl.*` options now works again as expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1740](https://github.com/tenzir/tenzir/pull/1740).

#### Normalize GNUInstallDirs for external plugins

[Section titled “Normalize GNUInstallDirs for external plugins”](#normalize-gnuinstalldirs-for-external-plugins)

Plugins built against an external libvast no longer require the `CMAKE_INSTALL_LIBDIR` to be specified as a path relative to the configured `CMAKE_INSTALL_PREFIX`. This fixes an issue with plugins in separate packages for some package managers, e.g., Nix.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1786](https://github.com/tenzir/tenzir/pull/1786).

#### Dont abort JSON import when encountering non-objects

[Section titled “Dont abort JSON import when encountering non-objects”](#dont-abort-json-import-when-encountering-non-objects)

VAST does not abort JSON imports anymore when encountering something other than a JSON object, e.g., a number or a string. Instead, VAST skips the offending line.

By [@lava](https://github.com/lava) in [#1759](https://github.com/tenzir/tenzir/pull/1759).

#### Remove /etc as hardcoded sysconfdir from Nix build

[Section titled “Remove /etc as hardcoded sysconfdir from Nix build”](#remove-etc-as-hardcoded-sysconfdir-from-nix-build-1)

The static binary no longer behaves differently than the regular build with regards to its configuration directories: system-wide configuration files now reside in `<prefix>/etc/vast/vast.yaml` rather than `/etc/vast/vast.yaml`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1777](https://github.com/tenzir/tenzir/pull/1777).

# VAST 2021.08.26

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.08.26).

### Features

[Section titled “Features”](#features)

#### Restore unique IDs for partition-local stores

[Section titled “Restore unique IDs for partition-local stores”](#restore-unique-ids-for-partition-local-stores)

The `segment-store` store backend works correctly with `vast get` and `vast explore`.

By [@lava](https://github.com/lava) in [#1805](https://github.com/tenzir/tenzir/pull/1805).

#### PRs 1819-1833

[Section titled “PRs 1819-1833”](#prs-1819-1833)

VAST can now process Eve JSON events of type `suricata.packet` that Suricata emits when the config option `tagged-packets` is set and a rule tags a packet using, e.g., `tag:session,5,packets;`.

By [@satta](https://github.com/satta) in [#1819](https://github.com/tenzir/tenzir/pull/1819).

### Changes

[Section titled “Changes”](#changes)

#### Support building against fmt 8.x

[Section titled “Support building against fmt 8.x”](#support-building-against-fmt-8x)

VAST now supports building against fmt 8 and spdlog 1.9.2, and now requires at least fmt 7.1.3.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1846](https://github.com/tenzir/tenzir/pull/1846).

#### Keep layer-2 framing when reading PCAP payload

[Section titled “Keep layer-2 framing when reading PCAP payload”](#keep-layer-2-framing-when-reading-pcap-payload)

VAST no longer strips link-layer framing when ingesting PCAPs. The stored payload is the raw PCAP packet. Similarly, `vast export pcap` now includes a Ethernet link-layer framing, per libpcap’s `DLT_EN10MB` link type.

By [@mavam](https://github.com/mavam) in [#1797](https://github.com/tenzir/tenzir/pull/1797).

#### Improve rendering of error messages & fix record to map conversion

[Section titled “Improve rendering of error messages & fix record to map conversion”](#improve-rendering-of-error-messages--fix-record-to-map-conversion)

Strings in error or warning log messages are no longer escaped, greatly improving readability of messages containing nested error contexts.

By [@tobim](https://github.com/tobim) in [#1842](https://github.com/tenzir/tenzir/pull/1842).

#### Align suricata.dhcp with the latest eve.log schema

[Section titled “Align suricata.dhcp with the latest eve.log schema”](#align-suricatadhcp-with-the-latest-evelog-schema)

VAST now ships with an updated schema type for the `suricata.dhcp` event, covering all fields of the extended output.

By [@tobim](https://github.com/tobim) in [#1854](https://github.com/tenzir/tenzir/pull/1854).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix logging in systemd when built without support

[Section titled “Fix logging in systemd when built without support”](#fix-logging-in-systemd-when-built-without-support)

VAST now only switches to journald style logging by default when it is actually supported.

By [@tobim](https://github.com/tobim) in [#1857](https://github.com/tenzir/tenzir/pull/1857).

#### Include in-process sources/sinks in status output

[Section titled “Include in-process sources/sinks in status output”](#include-in-process-sourcessinks-in-status-output)

The output of VAST status now includes status information for sources and sinks spawned in the VAST node, i.e., via `vast spawn source|sink <format>` rather than `vast import|export <format>`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1852](https://github.com/tenzir/tenzir/pull/1852).

#### Print memory counts in bytes instead of kB

[Section titled “Print memory counts in bytes instead of kB”](#print-memory-counts-in-bytes-instead-of-kb)

The memory counts in the output of `vast status` now represent bytes consistently, as opposed to a mix of bytes and kilobytes.

By [@tobim](https://github.com/tobim) in [#1862](https://github.com/tenzir/tenzir/pull/1862).

#### Fix plugin versions in prebuilt Docker images

[Section titled “Fix plugin versions in prebuilt Docker images”](#fix-plugin-versions-in-prebuilt-docker-images)

Plugins in the prebuilt Docker images no longer show `unspecified` as their version.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1828](https://github.com/tenzir/tenzir/pull/1828).

#### Use /etc as sysconfdir for install prefix /usr

[Section titled “Use /etc as sysconfdir for install prefix /usr”](#use-etc-as-sysconfdir-for-install-prefix-usr)

In order to align with the [GNU Coding Standards](https://www.gnu.org/prep/standards/html_node/Directory-Variables.html), the static binary (and other relocatable binaries) now uses `/etc` as sysconfdir for installations to `/usr/bin/vast`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1856](https://github.com/tenzir/tenzir/pull/1856).

#### Add missing concepts for Suricata events

[Section titled “Add missing concepts for Suricata events”](#add-missing-concepts-for-suricata-events)

Previously missing fields of suricata event types are now part of the concept definitions of `net.src.ip`, `net.src.port`, `net.dst.ip`, `net.dst.port`, `net.app`, `net.proto`, `net.community_id`, `net.vlan`, and `net.packets`.

By [@tobim](https://github.com/tobim) in [#1798](https://github.com/tenzir/tenzir/pull/1798).

#### Support quoted non-string fields in the CSV parser

[Section titled “Support quoted non-string fields in the CSV parser”](#support-quoted-non-string-fields-in-the-csv-parser)

The CSV parser now correctly parses quoted fields in non-string types. E.g., `"127.0.0.1"` in CSV now successfully parsers when a matching schema contains an `address` type field.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1858](https://github.com/tenzir/tenzir/pull/1858).

#### Gracefully handle malformed segments at startup

[Section titled “Gracefully handle malformed segments at startup”](#gracefully-handle-malformed-segments-at-startup)

Invalid segment files will no longer crash VAST at startup.

By [@tobim](https://github.com/tobim) in [#1820](https://github.com/tenzir/tenzir/pull/1820).

#### Support native plugins in the static binary

[Section titled “Support native plugins in the static binary”](#support-native-plugins-in-the-static-binary)

The `segment-store` store backend and built-in transform steps (`hash`, `replace`, and `delete`) now function correctly in static VAST binaries.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1850](https://github.com/tenzir/tenzir/pull/1850).

#### Interpret metrics paths relative to the db root

[Section titled “Interpret metrics paths relative to the db root”](#interpret-metrics-paths-relative-to-the-db-root)

The configuration options `vast.metrics.{file,uds}-sink.path` now correctly specify paths relative to the database directory of VAST, rather than the current working directory of the VAST server.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1848](https://github.com/tenzir/tenzir/pull/1848).

# VAST 2021.09.30

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.09.30).

### Features

[Section titled “Features”](#features)

#### Let empty queries export everything

[Section titled “Let empty queries export everything”](#let-empty-queries-export-everything)

The query argument to the export and count commands may now be omitted, which causes the commands to operate on all data. Note that this may be a very expensive operation, so use with caution.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1879](https://github.com/tenzir/tenzir/pull/1879).

#### Add Zeek writer plugin

[Section titled “Add Zeek writer plugin”](#add-zeek-writer-plugin)

The `broker` plugin is now a also *writer* plugin on top of being already a *reader* plugin. The new plugin enables exporting query results directly into a a Zeek process, e.g., to write Zeek scripts that incorporate context from the past. Run `vast export broker <expr>` to ship events via Broker that Zeek dispatches under the event `VAST::data(layout: string, data: any)`.

By [@mavam](https://github.com/mavam) in [#1863](https://github.com/tenzir/tenzir/pull/1863).

#### Add the streaming and query info to the index status

[Section titled “Add the streaming and query info to the index status”](#add-the-streaming-and-query-info-to-the-index-status)

The output of `vast status --detailed` now contains information about queries that are currently processed in the index.

By [@tobim](https://github.com/tobim) in [#1881](https://github.com/tenzir/tenzir/pull/1881).

#### Add tool to regenerate .mdx files

[Section titled “Add tool to regenerate .mdx files”](#add-tool-to-regenerate-mdx-files)

The new tool `mdx-regenerate` allows operators to re-create all `.mdx` files in a database directory to the latest file format version while VAST is running. This is useful for advanced users in preparation for version upgrades that bump the format version.

By [@lava](https://github.com/lava) in [#1866](https://github.com/tenzir/tenzir/pull/1866).

#### Bundle an example configuration file with plugins

[Section titled “Bundle an example configuration file with plugins”](#bundle-an-example-configuration-file-with-plugins)

If present in the plugin source directory, the build scaffolding now automatically installs `<plugin>.yaml.example` files, commenting out every line so the file has no effect. This serves as documentation for operators that can modify the installed file in-place.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1860](https://github.com/tenzir/tenzir/pull/1860).

#### Show loaded config files in status output

[Section titled “Show loaded config files in status output”](#show-loaded-config-files-in-status-output)

Running `vat status --detailed` now lists all loaded configuration files under `system.config-files`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1871](https://github.com/tenzir/tenzir/pull/1871).

### Changes

[Section titled “Changes”](#changes)

#### Make partition-local stores the default

[Section titled “Make partition-local stores the default”](#make-partition-local-stores-the-default)

The default store backend now is `segment-store` in order to enable the use of partition transforms in the future. To continue using the (now deprecated) legacy store backend, set `vast.store-backend` to archive.

By [@lava](https://github.com/lava) in [#1876](https://github.com/tenzir/tenzir/pull/1876).

#### Install example configuration files to datarootdir

[Section titled “Install example configuration files to datarootdir”](#install-example-configuration-files-to-datarootdir)

Example configuration files are now installed to the datarootdir as opposed to the sysconfdir in order to avoid overriding previously installed configuration files.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1880](https://github.com/tenzir/tenzir/pull/1880).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix possible garbage in status command output

[Section titled “Fix possible garbage in status command output”](#fix-possible-garbage-in-status-command-output)

The status command no longer occasionally contains garbage keys when the VAST server is under high load.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1872](https://github.com/tenzir/tenzir/pull/1872).

#### Fix response promises for disk monitor deletion

[Section titled “Fix response promises for disk monitor deletion”](#fix-response-promises-for-disk-monitor-deletion)

The disk monitor no longer fails to delete segments of particularly busy partitions with the `segment-store` store backend.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1892](https://github.com/tenzir/tenzir/pull/1892).

#### Tailor expressions in filter operation

[Section titled “Tailor expressions in filter operation”](#tailor-expressions-in-filter-operation)

Import filter expressions now work correctly with queries using field extractors, e.g., `vast import suricata 'event_type == "alert"' < path/to/eve.json`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1885](https://github.com/tenzir/tenzir/pull/1885).

#### Disallow unsupported field meta extractor predicates

[Section titled “Disallow unsupported field meta extractor predicates”](#disallow-unsupported-field-meta-extractor-predicates)

Expression predicates of the `#field` type now produce error messages instead of empty result sets for operations that are not supported.

By [@tobim](https://github.com/tobim) in [#1886](https://github.com/tenzir/tenzir/pull/1886).

#### Fix possible desync in pending queries map

[Section titled “Fix possible desync in pending queries map”](#fix-possible-desync-in-pending-queries-map)

The index now correctly cancels pending queries when the requester dies.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1884](https://github.com/tenzir/tenzir/pull/1884).

#### Fix ignoring of remote sources and sinks for status

[Section titled “Fix ignoring of remote sources and sinks for status”](#fix-ignoring-of-remote-sources-and-sinks-for-status)

Remote sources and sinks are no longer erroneously included in the output of VAST status.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1873](https://github.com/tenzir/tenzir/pull/1873).

# VAST 2021.11.18

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.11.18).

### Features

[Section titled “Features”](#features)

#### PRs 1929-1947

[Section titled “PRs 1929-1947”](#prs-1929-1947)

The export command now has a `--low-priority` option to reduce the priority of the request while query backlogs are being worked down.

By [@tobim](https://github.com/tobim) in [#1929](https://github.com/tenzir/tenzir/pull/1929).

#### Partition transforms

[Section titled “Partition transforms”](#partition-transforms)

A new ‘apply’ handler in the index gives plugin authors the ability to apply transforms over entire partitions. Previously, transforms were limited to streams of table slice during import or export.

By [@lava](https://github.com/lava) in [#1887](https://github.com/tenzir/tenzir/pull/1887).

#### Add metrics to the index

[Section titled “Add metrics to the index”](#add-metrics-to-the-index)

The keys `query.backlog.normal` and `query.backlog.low` have been added to the metrics output. The values indicate the number of quries that are currently in the backlog.

By [@tobim](https://github.com/tobim) in [#1942](https://github.com/tenzir/tenzir/pull/1942).

### Changes

[Section titled “Changes”](#changes)

#### Introduce a query backlog in the index

[Section titled “Introduce a query backlog in the index”](#introduce-a-query-backlog-in-the-index)

The `max-queries` configuration option now works at a coarser granularity. It used to limit the number of queries that could simultaneously retrieve data, but it now sets the number of queries that can be processed at the same time.

By [@tobim](https://github.com/tobim) in [#1896](https://github.com/tenzir/tenzir/pull/1896).

#### Update xxHash and hashing APIs

[Section titled “Update xxHash and hashing APIs”](#update-xxhash-and-hashing-apis)

VAST no longer vendors [xxHash](https://github.com/Cyan4973/xxHash), which is now a regular required dependency. Internally, VAST switched its default hash function to XXH3, providing a speedup of up to 3x.

By [@mavam](https://github.com/mavam) in [#1905](https://github.com/tenzir/tenzir/pull/1905).

#### Bump minimum required CMake version to 3.18

[Section titled “Bump minimum required CMake version to 3.18”](#bump-minimum-required-cmake-version-to-318)

Building VAST from source now requires CMake 3.18+.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1914](https://github.com/tenzir/tenzir/pull/1914).

#### Prefer reading query from stdin if available

[Section titled “Prefer reading query from stdin if available”](#prefer-reading-query-from-stdin-if-available)

A recently added features allows for exporting everything when no query is provided. We’ve restricted this to prefer reading a query from stdin if available. Additionally, conflicting ways to read the query now trigger errors.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1917](https://github.com/tenzir/tenzir/pull/1917).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Automatically add the skip attribute to records in lists

[Section titled “Automatically add the skip attribute to records in lists”](#automatically-add-the-skip-attribute-to-records-in-lists)

VAST no longer tries to create indexes for fields of type `list<record{...}>` as that wasn’t supported in the first place.

By [@tobim](https://github.com/tobim) in [#1933](https://github.com/tenzir/tenzir/pull/1933).

#### Increase the partition erase timeout to 1 minute

[Section titled “Increase the partition erase timeout to 1 minute”](#increase-the-partition-erase-timeout-to-1-minute)

The timeout duration to delete partitions has been increased to one minute, reducing the frequency of warnings for hitting this timeout significantly.

By [@tobim](https://github.com/tobim) in [#1897](https://github.com/tenzir/tenzir/pull/1897).

#### Change `suricata.dns` schema to match current DNS structure

[Section titled “Change suricata.dns schema to match current DNS structure”](#change-suricatadns-schema-to-match-current-dns-structure)

The `suricata.dns` schema has been updated to match the currently used EVE-JSON structure output by recent Suricata versions.

By [@satta](https://github.com/satta) in [#1919](https://github.com/tenzir/tenzir/pull/1919).

#### Load static plugins only when enabled

[Section titled “Load static plugins only when enabled”](#load-static-plugins-only-when-enabled)

Static plugins are no longer always loaded, but rather need to be explicitly enabled as documented. To restore the behavior from before this bug fix, set `vast.plugins: [bundled]` in your configuration file.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1959](https://github.com/tenzir/tenzir/pull/1959).

#### Update xxHash and hashing APIs

[Section titled “Update xxHash and hashing APIs”](#update-xxhash-and-hashing-apis-1)

When reading IPv6 addresses from PCAP data, only the first 4 bytes have been considered. VAST now stores all 16 bytes.

By [@mavam](https://github.com/mavam) in [#1905](https://github.com/tenzir/tenzir/pull/1905).

#### Fix deletion of segments if CWD != dbdir

[Section titled “Fix deletion of segments if CWD != dbdir”](#fix-deletion-of-segments-if-cwd--dbdir)

Store files now get deleted correctly if the database directory differs from the working directory.

By [@tobim](https://github.com/tobim) in [#1912](https://github.com/tenzir/tenzir/pull/1912).

#### Avoid references to record fields in fill\_status\_map

[Section titled “Avoid references to record fields in fill\_status\_map”](#avoid-references-to-record-fields-in-fill_status_map)

Debug builds of VAST no longer segfault on a status request with the `--debug` option.

By [@tobim](https://github.com/tobim) in [#1915](https://github.com/tenzir/tenzir/pull/1915).

# VAST 2021.12.16

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/2021.12.16).

### Features

[Section titled “Features”](#features)

#### PRs 1987-1992

[Section titled “PRs 1987-1992”](#prs-1987-1992)

Metrics events now optionally contain a metadata field that is a key-value mapping of string to string, allowing for finer-grained introspection. For now this enables correlation of metrics events and individual queries. A set of new metrics for query lookup use this feature to include the query ID.

By [@tobim](https://github.com/tobim) in [#1987](https://github.com/tenzir/tenzir/pull/1987).

#### Make JSON field selectors configurable

[Section titled “Make JSON field selectors configurable”](#make-json-field-selectors-configurable)

JSON field selectors are now configurable instead of being hard-coded for Suricata Eve JSON and Zeek Streaming JSON. E.g., `vast import json --selector=event_type:suricata` is now equivalent to `vast import suricata`. This allows for easier integration of JSONL data containing a field that indicates its type.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1974](https://github.com/tenzir/tenzir/pull/1974).

#### Emit version column along with metrics

[Section titled “Emit version column along with metrics”](#emit-version-column-along-with-metrics)

All metrics events now contain the version of VAST. Additionally, VAST now emits startup and shutdown metrics at the start and stop of the VAST server.

By [@6yozo](https://github.com/6yozo) in [#1973](https://github.com/tenzir/tenzir/pull/1973).

### Changes

[Section titled “Changes”](#changes)

#### Type FlatBuffers

[Section titled “Type FlatBuffers”](#type-flatbuffers)

VAST’s internal type system has a new on-disk data representation. While we still support reading older databases, reverting to an older version of VAST will not be possible after this change. Alongside this change, we’ve implemented numerous fixes and streamlined handling of field name lookups, which now more consistently handles the dot-separator. E.g., the query `#field == "ip"` still matches the field `source.ip`, but no longer the field `source_ip`. The change is also performance-relevant in the long-term: For data persisted from previous versions of VAST we convert to the new type system on the fly, and for newly ingested data we now have near zero-cost deserialization for types, which should result in an overall speedup once the old data is rotated out by the disk monitor.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1888](https://github.com/tenzir/tenzir/pull/1888).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix JSON default selector for nested records

[Section titled “Fix JSON default selector for nested records”](#fix-json-default-selector-for-nested-records)

The field-based default selector of the JSON import now correctly matches types with nested record types.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#1988](https://github.com/tenzir/tenzir/pull/1988).

# Next

Unreleased changes.

# VAST v1.0.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v1.0.0).

### Features

[Section titled “Features”](#features)

#### Introduce the `#import_time` meta extractor

[Section titled “Introduce the #import\_time meta extractor”](#introduce-the-import_time-meta-extractor)

The `#import_time` meta extractor allows for querying events based on the time they arrived at the VAST server process. It may only be used for comparisons with [time value literals](https://vast.io/docs/understand/query-language/expressions#values), e.g., `vast export json '#import_time > 1 hour ago'` exports all events that were imported within the last hour as NDJSON.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2019](https://github.com/tenzir/tenzir/pull/2019).

#### Add a `--omit-nulls` option to the JSON export

[Section titled “Add a --omit-nulls option to the JSON export”](#add-a---omit-nulls-option-to-the-json-export)

The new `--omit-nulls` option to the `vast export json` command causes VAST to skip over fields in JSON objects whose value is `null` when rendering them.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2004](https://github.com/tenzir/tenzir/pull/2004).

#### Create projection plugin

[Section titled “Create projection plugin”](#create-projection-plugin)

VAST has a new transform step: `project`, which keeps the fields with configured key suffixes and removes the rest from the input. At the same time, the `delete` transform step can remove not only one but multiple fields from the input based on the configured key suffixes.

By [@6yozo](https://github.com/6yozo) in [#2000](https://github.com/tenzir/tenzir/pull/2000).

#### Implement a selection transform plugin

[Section titled “Implement a selection transform plugin”](#implement-a-selection-transform-plugin)

VAST has a new transform step: `select`, which keeps rows matching the configured expression and removes the rest from the input.

By [@6yozo](https://github.com/6yozo) in [#2014](https://github.com/tenzir/tenzir/pull/2014).

### Changes

[Section titled “Changes”](#changes)

#### Upgrade the minimum Arrow dependency to 6.0

[Section titled “Upgrade the minimum Arrow dependency to 6.0”](#upgrade-the-minimum-arrow-dependency-to-60)

Building VAST now requires Arrow >= 6.0.

By [@dispanser](https://github.com/dispanser) in [#2033](https://github.com/tenzir/tenzir/pull/2033).

#### Prepare repository for VAST v1.0.0-rc1

[Section titled “Prepare repository for VAST v1.0.0-rc1”](#prepare-repository-for-vast-v100-rc1)

VAST no longer uses calendar-based versioning. Instead, it uses a semantic versioning scheme. A new VERSIONING.md document installed alongside VAST explores the semantics in-depth.

Plugins now have a separate version. The build scaffolding installs README.md and CHANGELOG.md files in the plugin source tree root automatically.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2035](https://github.com/tenzir/tenzir/pull/2035).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Create intermediate dirs for db-directory and respect schema-dirs in bare mode

[Section titled “Create intermediate dirs for db-directory and respect schema-dirs in bare mode”](#create-intermediate-dirs-for-db-directory-and-respect-schema-dirs-in-bare-mode)

VAST no longer ignores the `--schema-dirs` option when using `--bare-mode`.

Starting VAST no longer fails if creating the database directory requires creating intermediate directories.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2046](https://github.com/tenzir/tenzir/pull/2046).

#### Start the telemetry loop of the index correctly

[Section titled “Start the telemetry loop of the index correctly”](#start-the-telemetry-loop-of-the-index-correctly)

The index now emits the metrics `query.backlog.{low,normal}` and `query.workers.{idle,busy}` reliably.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2032](https://github.com/tenzir/tenzir/pull/2032).

# VAST v1.1.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v1.1.0).

### Features

[Section titled “Features”](#features)

#### PRs 2064-2082

[Section titled “PRs 2064-2082”](#prs-2064-2082)

The built-in `select` and `project` transform steps now correctly handle dropping all rows and columns respectively, effectively deleting the input data.

By [@lava](https://github.com/lava) in [#2064](https://github.com/tenzir/tenzir/pull/2064).

#### Add new query language plugin

[Section titled “Add new query language plugin”](#add-new-query-language-plugin)

VAST has a new *query language* plugin type that allows for adding additional query language frontends. The plugin performs one function: compile user input into a VAST expression. The new `sigma` plugin demonstrates usage of this plugin type.

By [@mavam](https://github.com/mavam) in [#2074](https://github.com/tenzir/tenzir/pull/2074).

#### Implement a generic aggregation transform step

[Section titled “Implement a generic aggregation transform step”](#implement-a-generic-aggregation-transform-step)

The new built-in `rename` transform step allows for renaming event types during a transformation. This is useful when you want to ensure that a repeatedly triggered transformation does not affect already transformed events.

The new `aggregate` transform plugin allows for flexibly grouping and aggregating events. We recommend using it alongside the [`compaction` plugin](https://vast.io/docs/about/use-cases/data-aging), e.g., for rolling up events into a more space-efficient representation after a certain amount of time.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2076](https://github.com/tenzir/tenzir/pull/2076).

### Changes

[Section titled “Changes”](#changes)

#### Correctly handle partition transforms without output

[Section titled “Correctly handle partition transforms without output”](#correctly-handle-partition-transforms-without-output)

We fixed an issue where partition transforms that erase complete partitions trigger an internal assertion failure.

By [@lava](https://github.com/lava) in [#2123](https://github.com/tenzir/tenzir/pull/2123).

#### Log actor names together with the unique actor ID

[Section titled “Log actor names together with the unique actor ID”](#log-actor-names-together-with-the-unique-actor-id)

Actor names in log messages now have an `-ID` suffix to make it easier to tell multiple instances of the same actor apart, e.g., `exporter-42`.

By [@tobim](https://github.com/tobim) in [#2119](https://github.com/tenzir/tenzir/pull/2119).

#### Deprecate the msgpack table slice

[Section titled “Deprecate the msgpack table slice”](#deprecate-the-msgpack-table-slice)

The `msgpack` encoding option is now deprecated. VAST issues a warning on startup and automatically uses the `arrow` encoding instead. A future version of VAST will remove this option entirely.

The experimental aging feature is now deprecated. The [compaction plugin](https://vast.io/docs/about/use-cases/data-aging) offers a superset of the aging functionality.

By [@tobim](https://github.com/tobim) in [#2087](https://github.com/tenzir/tenzir/pull/2087).

#### Add new query language plugin

[Section titled “Add new query language plugin”](#add-new-query-language-plugin-1)

VAST no longer attempts to intepret query expressions as Sigma rules automatically. Instead, this functionality moved to a dedicated `sigma` query language plugin that must explicitly be enabled at build time.

By [@mavam](https://github.com/mavam) in [#2074](https://github.com/tenzir/tenzir/pull/2074).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Improve name lookup in meta index to reduce FPs

[Section titled “Improve name lookup in meta index to reduce FPs”](#improve-name-lookup-in-meta-index-to-reduce-fps)

A performance bug in the first stage of query evaluation caused VAST to return too many candidate partitions when querying for a field suffix. For example, a query for the `ts` field commonly used in Zeek logs also included partitions for `netflow.pkts` from `suricata.netflow` events. This bug no longer exists, resulting in a considerable speedup of affected queries.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2086](https://github.com/tenzir/tenzir/pull/2086).

#### Send initial db state to new partition creation listeners

[Section titled “Send initial db state to new partition creation listeners”](#send-initial-db-state-to-new-partition-creation-listeners)

We fixed a bug that potentially resulted in the wrong subset of partitions to be considered during query evaluation.

By [@lava](https://github.com/lava) in [#2103](https://github.com/tenzir/tenzir/pull/2103).

#### Fix hanging queries

[Section titled “Fix hanging queries”](#fix-hanging-queries)

VAST does not lose query capacity when backlogged queries are cancelled any more.

By [@tobim](https://github.com/tobim) in [#2092](https://github.com/tenzir/tenzir/pull/2092).

#### Adjust index statistics for partition transforms

[Section titled “Adjust index statistics for partition transforms”](#adjust-index-statistics-for-partition-transforms)

VAST now correctly adjusts the index statistics when applying partition transforms.

By [@lava](https://github.com/lava) in [#2097](https://github.com/tenzir/tenzir/pull/2097).

# VAST v1.1.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v1.1.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Backport bug fixes for a v1.1.1 release

[Section titled “Backport bug fixes for a v1.1.1 release”](#backport-bug-fixes-for-a-v111-release)

The disk monitor now correctly continues deleting until below the low water mark after a partition failed to delete.

We fixed a rarely occurring race condition caused query workers to become stuck after delivering all results until the corresponding client process terminated.

Queries that timed out or were externally terminated while in the query backlog and with more than five unhandled candidate partitions no longer permanently get stuck.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2160](https://github.com/tenzir/tenzir/pull/2160).

# VAST v1.1.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v1.1.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix race condition with exporter timeouts

[Section titled “Fix race condition with exporter timeouts”](#fix-race-condition-with-exporter-timeouts)

Terminating or timing out exports during the catalog lookup no longer causes query workers to become stuck indefinitely.

By [@lava](https://github.com/lava) in [#2165](https://github.com/tenzir/tenzir/pull/2165).

# VAST v2.0.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v2.0.0).

### Features

[Section titled “Features”](#features)

#### Clean up transform steps (and native plugins generally)

[Section titled “Clean up transform steps (and native plugins generally)”](#clean-up-transform-steps-and-native-plugins-generally)

The `replace` transform step now allows for setting values of complex types, e.g., lists or records.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2228](https://github.com/tenzir/tenzir/pull/2228).

#### Print segment contents with lsvast

[Section titled “Print segment contents with lsvast”](#print-segment-contents-with-lsvast)

The `lsvast` tool now prints the whole store contents when given a store file as an argument.

By [@lava](https://github.com/lava) in [#2247](https://github.com/tenzir/tenzir/pull/2247).

#### Use dedicated partitions for each layout

[Section titled “Use dedicated partitions for each layout”](#use-dedicated-partitions-for-each-layout)

VAST now creates one active partition per layout, rather than having a single active partition for all layouts.

The new option `vast.active-partition-timeout` controls the time after which an active partition is flushed to disk. The timeout may hit before the partition size reaches `vast.max-partition-size`, allowing for an additional temporal control for data freshness. The active partition timeout defaults to 1 hour.

By [@tobim](https://github.com/tobim) in [#2096](https://github.com/tenzir/tenzir/pull/2096).

#### Allow fine-grained meta index configuration

[Section titled “Allow fine-grained meta index configuration”](#allow-fine-grained-meta-index-configuration)

The new `vast.index` section in the configuration supports adjusting the false-positive rate of first-stage lookups for individual fields, allowing users to optimize the time/space trade-off for expensive queries.

By [@lava](https://github.com/lava) in [#2065](https://github.com/tenzir/tenzir/pull/2065).

#### Add a grand total event counter to the status output

[Section titled “Add a grand total event counter to the status output”](#add-a-grand-total-event-counter-to-the-status-output)

The output of `vast status` now displays the total number of events stored under the key `index.statistics.events.total`.

By [@6yozo](https://github.com/6yozo) in [#2133](https://github.com/tenzir/tenzir/pull/2133).

#### Backport bug fixes for a v1.1.1 release

[Section titled “Backport bug fixes for a v1.1.1 release”](#backport-bug-fixes-for-a-v111-release)

The disk monitor has new status entries `blacklist` and \`blacklist

* size\` containing information about partitions failed to be erased.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2160](https://github.com/tenzir/tenzir/pull/2160).

#### Support environment variables as alternate config mechanism

[Section titled “Support environment variables as alternate config mechanism”](#support-environment-variables-as-alternate-config-mechanism)

VAST has now complete support for passing environment variables as alternate path to configuration files. Environment variables have *lower* precedence than CLI arguments and *higher* precedence than config files. Variable names of the form `VAST_FOO__BAR_BAZ` map to `vast.foo.bar-baz`, i.e., `__` is a record separator and `_` translates to `-`. This does not apply to the prefix `VAST_`, which is considered the application identifier. Only variables with non-empty values are considered.

By [@mavam](https://github.com/mavam) in [#2162](https://github.com/tenzir/tenzir/pull/2162).

#### Implement support for transforms that apply to every type and use compaction for aging

[Section titled “Implement support for transforms that apply to every type and use compaction for aging”](#implement-support-for-transforms-that-apply-to-every-type-and-use-compaction-for-aging)

VAST v1.0 deprecated the experimental aging feature. Given popular demand we’ve decided to un-deprecate it, and to actually implement it on top of the same building blocks the compaction mechanism uses. This means that it is now fully working and no longer considered experimental.

By [@lava](https://github.com/lava) in [#2186](https://github.com/tenzir/tenzir/pull/2186).

### Changes

[Section titled “Changes”](#changes)

#### Remove the get subcommand

[Section titled “Remove the get subcommand”](#remove-the-get-subcommand)

We removed the experimental `vast get` command. It relied on an internal unique event ID that was only exposed to the user in debug messages. This removal is a preparatory step towards a simplification of some of the internal workings of VAST.

By [@tobim](https://github.com/tobim) in [#2121](https://github.com/tenzir/tenzir/pull/2121).

#### Mark `experimental` encoding as `arrow.v2`

[Section titled “Mark experimental encoding as arrow.v2”](#mark-experimental-encoding-as-arrowv2)

VAST’s internal data model now completely preserves the nesting of the stored data when using the `arrow` encoding, and maps the pattern, address, subnet, and enumeration types onto Arrow extension types rather than using the underlying representation directly. This change enables use of the `export arrow` command without needing information about VAST’s type system.

Transform steps that add or modify columns now transform the columns in-place rather than at the end, preserving the nesting structure of the original data.

The deprecated `msgpack` encoding no longer exists. Data imported using the `msgpack` encoding can still be accessed, but new data will always use the `arrow` encoding.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2159](https://github.com/tenzir/tenzir/pull/2159).

#### Minimize the threadpool for client commands

[Section titled “Minimize the threadpool for client commands”](#minimize-the-threadpool-for-client-commands)

Client commands such as `vast export` or `vast status` now create less threads at runtime, reducing the risk of hitting system resource limits.

By [@tobim](https://github.com/tobim) in [#2193](https://github.com/tenzir/tenzir/pull/2193).

#### Deploy VAST to AWS Lambda

[Section titled “Deploy VAST to AWS Lambda”](#deploy-vast-to-aws-lambda)

VAST ships experimental Terraform scripts to deploy on AWS Lambda and Fargate.

By [@rdettai](https://github.com/rdettai) in [#2108](https://github.com/tenzir/tenzir/pull/2108).

#### Fix CLI verbosity shorthands

[Section titled “Fix CLI verbosity shorthands”](#fix-cli-verbosity-shorthands)

The command line option `--verbosity` has the new name `--console-verbosity`. This synchronizes the CLI interface with the configuration file that solely understands the option `vast.console-verbosity`.

By [@mavam](https://github.com/mavam) in [#2178](https://github.com/tenzir/tenzir/pull/2178).

#### Remove the “catalog” and “catalog-bytes” keys from the index status

[Section titled “Remove the “catalog” and “catalog-bytes” keys from the index status”](#remove-the-catalog-and-catalog-bytes-keys-from-the-index-status)

The `index` section in the status output no longer contains the `catalog` and `catalog-bytes` keys. The information is already present in the top-level `catalog` section.

By [@tobim](https://github.com/tobim) in [#2233](https://github.com/tenzir/tenzir/pull/2233).

#### Rename `meta-index` to `catalog`

[Section titled “Rename meta-index to catalog”](#rename-meta-index-to-catalog)

The `meta-index` is now called the `catalog`. This affects multiple metrics and entries in the output of `vast status`, and the configuration option `vast.meta-index-fp-rate`, which is now called `vast.catalog-fp-rate`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2128](https://github.com/tenzir/tenzir/pull/2128).

#### Eploit synergies when evaluating many queries at the same time

[Section titled “Eploit synergies when evaluating many queries at the same time”](#eploit-synergies-when-evaluating-many-queries-at-the-same-time)

We revised the query scheduling logic to exploit synergies when multiple queries run at the same time. In that vein, we updated the related metrics with more accurate names to reflect the new mechanism. The new keys `scheduler.partition.materializations`, `scheduler.partition.scheduled`, and `scheduler.partition.lookups` provide periodic counts of partitions loaded from disk and scheduled for lookup, and the overall number of queries issued to partitions, respectively. The keys `query.workers.idle`, and `query.workers.busy` were renamed to `scheduler.partition.remaining-capacity`, and `scheduler.partition.current-lookups`. Finally, the key `scheduler.partition.pending` counts the number of currently pending partitions. It is still possible to opt-out of the new scheduling algorithm with the (deprecated) option `--use-legacy-query-scheduler`.

By [@tobim](https://github.com/tobim) in [#2117](https://github.com/tenzir/tenzir/pull/2117).

#### Bump the minimum version of Apache Arrow to 7.0

[Section titled “Bump the minimum version of Apache Arrow to 7.0”](#bump-the-minimum-version-of-apache-arrow-to-70)

VAST now requires Apache Arrow >= v7.0.0.

By [@tobim](https://github.com/tobim) in [#2122](https://github.com/tenzir/tenzir/pull/2122).

#### Clean up transform steps (and native plugins generally)

[Section titled “Clean up transform steps (and native plugins generally)”](#clean-up-transform-steps-and-native-plugins-generally-1)

Multiple transform steps now have new names: `select` is now called `where`, `delete` is now called `drop`, `project` is now called `put`, and `aggregate` is now called `summarize`. This breaking change is in preparation for an upcoming feature that improves the capability of VAST’s query language.

The `layout-names` option of the `rename` transform step was renamed `schemas`. The step now additonally supports renaming `fields`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2228](https://github.com/tenzir/tenzir/pull/2228).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Make man-page creation more robust

[Section titled “Make man-page creation more robust”](#make-man-page-creation-more-robust)

The `vast(1)` man-page is no longer empty for VAST distributions with static binaries.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2190](https://github.com/tenzir/tenzir/pull/2190).

#### Treat list options in env variables consistently

[Section titled “Treat list options in env variables consistently”](#treat-list-options-in-env-variables-consistently)

Environment variables for options that specify lists now consistently use comma-separators and respect escaping with backslashes.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2236](https://github.com/tenzir/tenzir/pull/2236).

#### Reduce the default log queue size for client commands

[Section titled “Reduce the default log queue size for client commands”](#reduce-the-default-log-queue-size-for-client-commands)

We optimized the queue size of the logger for commands other than `vast start`. Client commands now show a significant reduction in memory usage and startup time.

By [@tobim](https://github.com/tobim) in [#2176](https://github.com/tenzir/tenzir/pull/2176).

#### Lift selector field requirements for JSON import

[Section titled “Lift selector field requirements for JSON import”](#lift-selector-field-requirements-for-json-import)

The JSON import no longer rejects non-string selector fields. Instead, it always uses the textual JSON representation as a selector. E.g., the JSON object `{id:1,...}` imported via `vast import json --selector=id:mymodule` now matches the schema named `mymodule.1` rather than erroring because the `id` field is not a string.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2255](https://github.com/tenzir/tenzir/pull/2255).

#### Correctly terminate the explore command

[Section titled “Correctly terminate the explore command”](#correctly-terminate-the-explore-command)

The `explore` command now properly terminates after the requested number of results are delivered.

By [@tobim](https://github.com/tobim) in [#2120](https://github.com/tenzir/tenzir/pull/2120).

#### Load stores lazily

[Section titled “Load stores lazily”](#load-stores-lazily)

The `count --estimate` erroneously materialized store files from disk, resulting in an unneeded performance penalty. VAST now answers approximate count queries by solely consulting the relevant index files.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2146](https://github.com/tenzir/tenzir/pull/2146).

#### Add support for reals in CSV without dot

[Section titled “Add support for reals in CSV without dot”](#add-support-for-reals-in-csv-without-dot)

The CSV parser no longer fails when encountering integers when floating point values were expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2184](https://github.com/tenzir/tenzir/pull/2184).

#### Fix query pruning in the catalog

[Section titled “Fix query pruning in the catalog”](#fix-query-pruning-in-the-catalog)

The query optimizer incorrectly transformed queries with conjunctions or disjunctions with several operands testing against the same string value, leading to missing result. This was rarely an issue in practice before the introduction of homogenous partitions with the v2.0 release.

By [@lava](https://github.com/lava) in [#2264](https://github.com/tenzir/tenzir/pull/2264).

#### Don’t send null pointers when erasing whole partitions

[Section titled “Don’t send null pointers when erasing whole partitions”](#dont-send-null-pointers-when-erasing-whole-partitions)

VAST no longer sometimes crashes when aging or compaction erase whole partitions.

By [@lava](https://github.com/lava) in [#2227](https://github.com/tenzir/tenzir/pull/2227).

#### Ignore types unrelated to the configuration in the summarize plugin

[Section titled “Ignore types unrelated to the configuration in the summarize plugin”](#ignore-types-unrelated-to-the-configuration-in-the-summarize-plugin)

Transform steps removing all nested fields from a record leaving only empty nested records no longer cause VAST to crash.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2258](https://github.com/tenzir/tenzir/pull/2258).

#### Fix race condition with exporter timeouts

[Section titled “Fix race condition with exporter timeouts”](#fix-race-condition-with-exporter-timeouts)

Some queries could get stuck when an importer would time out during the meta index lookup. This race condition no longer exists.

By [@lava](https://github.com/lava) in [#2167](https://github.com/tenzir/tenzir/pull/2167).

#### Stop accepting new queries after initiating shutdown

[Section titled “Stop accepting new queries after initiating shutdown”](#stop-accepting-new-queries-after-initiating-shutdown)

VAST servers no longer accept queries after initiating shutdown. This fixes a potential infinite hang if new queries were coming in faster than VAST was able to process them.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2215](https://github.com/tenzir/tenzir/pull/2215).

#### Use the timestamp type for inferred event timestamp fields in the Zeek reader

[Section titled “Use the timestamp type for inferred event timestamp fields in the Zeek reader”](#use-the-timestamp-type-for-inferred-event-timestamp-fields-in-the-zeek-reader)

The `import zeek` command now correctly marks the event timestamp using the `timestamp` type alias for all inferred schemas.

By [@tobim](https://github.com/tobim) in [#2155](https://github.com/tenzir/tenzir/pull/2155).

# VAST v2.1.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v2.1.0).

### Features

[Section titled “Features”](#features)

#### Parquet store plugin

[Section titled “Parquet store plugin”](#parquet-store-plugin)

A new parquet store plugin allows VAST to store its data as parquet files, increasing storage efficiency at the expense of higher deserialization costs. Storage requirements for the VAST database is reduced by another 15-20% compared to the existing segment store with Zstd compression enabled. CPU usage for suricata import is up \~ 10%, mostly related to the more expensive serialization. Deserialization (reading) of a partition is significantly more expensive, increasing CPU utilization by about 100%, and should be carefully considered and compared to the potential reduction in storage cost and I/O operations.

By [@dispanser](https://github.com/dispanser) in [#2284](https://github.com/tenzir/tenzir/pull/2284).

#### Report by schema metrics from the importer

[Section titled “Report by schema metrics from the importer”](#report-by-schema-metrics-from-the-importer)

VAST now produces additional metrics under the keys `ingest.events`, `ingest.duration` and `ingest.rate`. Each of those gets issued once for every schema that VAST ingested during the measurement period. Use the `metadata_schema` key to disambiguate the metrics.

By [@tobim](https://github.com/tobim) in [#2274](https://github.com/tenzir/tenzir/pull/2274).

#### Compress in-memory slices with Zstd

[Section titled “Compress in-memory slices with Zstd”](#compress-in-memory-slices-with-zstd)

VAST now compresses data with Zstd. When persisting data to the segment store, the default configuration achieves over 2x space savings. When transferring data between client and server processes, compression reduces the amount of transferred data by up to 5x. This allowed us to increase the default partition size from 1,048,576 to 4,194,304 events, and the default number of events in a single batch from 1,024 to 65,536. The performance increase comes at the cost of a \~20% memory footprint increase at peak load. Use the option `vast.max-partition-size` to tune this space-time tradeoff.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2268](https://github.com/tenzir/tenzir/pull/2268).

#### Add percentage of total number of events to index status

[Section titled “Add percentage of total number of events to index status”](#add-percentage-of-total-number-of-events-to-index-status)

The index statistics in `vast status --detailed` now show the event distribution per schema as a percentage of the total number of events in addition to the per-schema number, e.g., for `suricata.flow` events under the key `index.statistics.layouts.suricata.flow.percentage`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2351](https://github.com/tenzir/tenzir/pull/2351).

#### PRs 2360-2363

[Section titled “PRs 2360-2363”](#prs-2360-2363)

The output `vast status --detailed` now shows metadata from all partitions under the key `.catalog.partitions`. Additionally, the catalog emits metrics under the key `catalog.num-events` and `catalog.num-partitions` containing the number of events and partitions respectively. The metrics contain the schema name in the field `metadata_schema` and the (internal) partition version in the field `metadata_partition-version`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2360](https://github.com/tenzir/tenzir/pull/2360).

#### Base image with Closed Source plugins

[Section titled “Base image with Closed Source plugins”](#base-image-with-closed-source-plugins)

The VAST Cloud CLI can now authenticate to the Tenzir private registry and download the vast-pro image (including plugins such as Matcher). The deployment script can now be configured to use a specific image and can thus be set to use vast-pro.

By [@rdettai](https://github.com/rdettai) in [#2415](https://github.com/tenzir/tenzir/pull/2415).

#### Add new debugging features to VAST tools

[Section titled “Add new debugging features to VAST tools”](#add-new-debugging-features-to-vast-tools)

The `lsvast` tool can now print contents of individual `.mdx` files. It now has an option to print raw Bloom filter contents of string and IP address synopses.

The `mdx-regenerate` tool was renamed to `vast-regenerate` and can now also regenerate an index file from a list of partition UUIDs.

By [@lava](https://github.com/lava) in [#2260](https://github.com/tenzir/tenzir/pull/2260).

#### PRs 2334-KaanSK

[Section titled “PRs 2334-KaanSK”](#prs-2334-kaansk)

PyVAST now supports running client commands for VAST servers running in a container environment, if no local VAST binary is available. Specify the `container` keyword to customize this behavior. It defaults to `{"runtime": "docker", "name": "vast"}`.

By [@KaanSK](https://github.com/KaanSK) in [#2334](https://github.com/tenzir/tenzir/pull/2334).

#### Add index metric for created active partitions

[Section titled “Add index metric for created active partitions”](#add-index-metric-for-created-active-partitions)

VAST emits the new metric `partition.events-written` when writing a partition to disk. The metric’s value is the number of events written, and the `metadata_schema` field contains the name of the partition’s schema.

By [@lava](https://github.com/lava) in [#2302](https://github.com/tenzir/tenzir/pull/2302).

#### Improve usability of CSV format

[Section titled “Improve usability of CSV format”](#improve-usability-of-csv-format)

The `csv` import gained a new `--seperator='x'` option that defaults to `','`. Set it to `'\t'` to import tab-separated values, or `' '` to import space-separated values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2336](https://github.com/tenzir/tenzir/pull/2336).

#### Add a `rebuild` command plugin

[Section titled “Add a rebuild command plugin”](#add-a-rebuild-command-plugin)

The new `rebuild` command rebuilds old partitions to take advantage of improvements in newer VAST versions. Rebuilding takes place in the VAST server in the background. This process merges partitions up to the configured `max-partition-size`, turns VAST v1.x’s heterogeneous into VAST v2.x’s homogenous partitions, migrates all data to the currently configured `store-backend`, and upgrades to the most recent internal batch encoding and indexes.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2321](https://github.com/tenzir/tenzir/pull/2321).

#### Compress serialized indexers

[Section titled “Compress serialized indexers”](#compress-serialized-indexers)

VAST now compresses on-disk indexes with Zstd, resulting in a 50-80% size reduction depending on the type of indexes used, and reducing the overall index size to below the raw data size. This improves retention spans significantly. For example, using the default configuration, the indexes for `suricata.ftp` events now use 75% less disk space, and `suricata.flow` 30% less.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2346](https://github.com/tenzir/tenzir/pull/2346).

#### Add optional status command filters

[Section titled “Add optional status command filters”](#add-optional-status-command-filters)

The `status` command now supports filtering by component name. E.g., `vast status importer index` only shows the status of the importer and index components.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2288](https://github.com/tenzir/tenzir/pull/2288).

### Changes

[Section titled “Changes”](#changes)

#### Parquet store plugin

[Section titled “Parquet store plugin”](#parquet-store-plugin-1)

VAST now requires Arrow >= v8.0.0.

By [@dispanser](https://github.com/dispanser) in [#2284](https://github.com/tenzir/tenzir/pull/2284).

#### Always format time values with microsecond precision

[Section titled “Always format time values with microsecond precision”](#always-format-time-values-with-microsecond-precision)

VAST will from now on always format `time` and `timestamp` values with six decimal places (microsecond precision). The old behavior used a precision that depended on the actual value. This may require action for downstream tooling like metrics collectors that expect nanosecond granularity.

By [@tobim](https://github.com/tobim) in [#2380](https://github.com/tenzir/tenzir/pull/2380).

#### Remove legacy index from VAST

[Section titled “Remove legacy index from VAST”](#remove-legacy-index-from-vast)

The `vast.use-legacy-query-scheduler` option is now ignored because the legacy query scheduler has been removed.

By [@lava](https://github.com/lava) in [#2312](https://github.com/tenzir/tenzir/pull/2312).

#### Deprecate the archive store-backend

[Section titled “Deprecate the archive store-backend”](#deprecate-the-archive-store-backend)

The `vast.store-backend` configuration option no longer supports `archive`, and instead always uses the superior `segment-store` instead. Events stored in the archive will continue to be available in queries.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2290](https://github.com/tenzir/tenzir/pull/2290).

#### Write homogenous partitions from the partition transformer

[Section titled “Write homogenous partitions from the partition transformer”](#write-homogenous-partitions-from-the-partition-transformer)

Partition transforms now always emit homogenous partitions, i.e., one schema per partition. This makes compaction and aging more efficient.

By [@lava](https://github.com/lava) in [#2277](https://github.com/tenzir/tenzir/pull/2277).

#### Add new debugging features to VAST tools

[Section titled “Add new debugging features to VAST tools”](#add-new-debugging-features-to-vast-tools-1)

The `mdx-regenerate` tool is no longer part of VAST binary releases.

By [@lava](https://github.com/lava) in [#2260](https://github.com/tenzir/tenzir/pull/2260).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Improve index crash recovery

[Section titled “Improve index crash recovery”](#improve-index-crash-recovery)

We improved the mechanism to recover the database state after an unclean shutdown.

By [@tobim](https://github.com/tobim) in [#2394](https://github.com/tenzir/tenzir/pull/2394).

#### Use fast\_float to parse reals

[Section titled “Use fast\_float to parse reals”](#use-fast_float-to-parse-reals)

The parser for `real` values now understands scientific notation, e.g., `1.23e+42`.

By [@tobim](https://github.com/tobim) in [#2332](https://github.com/tenzir/tenzir/pull/2332).

#### Prefer CLI over config file for vast.plugins

[Section titled “Prefer CLI over config file for vast.plugins”](#prefer-cli-over-config-file-for-vastplugins)

The command-line options `--plugins`, `--plugin-dirs`, and `--schema-dirs` now correctly overwrite their corresponding configuration options.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2289](https://github.com/tenzir/tenzir/pull/2289).

#### Fix crash in query evaluation for new partitions

[Section titled “Fix crash in query evaluation for new partitions”](#fix-crash-in-query-evaluation-for-new-partitions)

VAST no longer crashes when a query arrives at a newly created active partition in the time window between the partition creation and the first event arriving at the partition.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2295](https://github.com/tenzir/tenzir/pull/2295).

#### Respect the default fp-rate setting

[Section titled “Respect the default fp-rate setting”](#respect-the-default-fp-rate-setting)

VAST now reads the default false-positive rate for sketches correctly. This broke accidentally with the v2.0 release. The option moved from `vast.catalog-fp-rate` to `vast.index.default-fp-rate`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2325](https://github.com/tenzir/tenzir/pull/2325).

#### Parse time from JSON strings containing numbers

[Section titled “Parse time from JSON strings containing numbers”](#parse-time-from-json-strings-containing-numbers)

The JSON import now treats `time` and `duration` fields correctly for JSON strings containing a number, i.e., the JSON string `"1654735756"` now behaves just like the JSON number `1654735756` and for a `time` field results in the value `2022-06-09T00:49:16.000Z`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2340](https://github.com/tenzir/tenzir/pull/2340).

#### The index shall not quit on write errors

[Section titled “The index shall not quit on write errors”](#the-index-shall-not-quit-on-write-errors)

VAST will no longer terminate when it can’t write any more data to disk. Incoming data will still be accepted but discarded. We encourage all users to enable the disk-monitor or compaction features as a proper solution to this problem.

By [@tobim](https://github.com/tobim) in [#2376](https://github.com/tenzir/tenzir/pull/2376).

#### Fall back to string when parsing config options from environment

[Section titled “Fall back to string when parsing config options from environment”](#fall-back-to-string-when-parsing-config-options-from-environment)

Setting the environment variable `VAST_ENDPOINT` to `host:port` pair no longer fails on startup with a parse error.

By [@dispanser](https://github.com/dispanser) in [#2305](https://github.com/tenzir/tenzir/pull/2305).

#### Improve usability of CSV format

[Section titled “Improve usability of CSV format”](#improve-usability-of-csv-format-1)

The `csv` import no longer crashes when the CSV file contains columns not present in the selected schema. Instead, it imports these columns as strings.

`vast export csv` now renders enum columns in their string representation instead of their internal numerical representation.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2336](https://github.com/tenzir/tenzir/pull/2336).

#### Allow missing value indices in partition flatbuffer

[Section titled “Allow missing value indices in partition flatbuffer”](#allow-missing-value-indices-in-partition-flatbuffer)

VAST no longer crashes when importing `map` or `pattern` data annotated with the `#skip` attribute.

By [@lava](https://github.com/lava) in [#2286](https://github.com/tenzir/tenzir/pull/2286).

#### Fix occasional shutdown hangs

[Section titled “Fix occasional shutdown hangs”](#fix-occasional-shutdown-hangs)

VAST no longer hangs when it is shut down while still importing events.

By [@tobim](https://github.com/tobim) in [#2324](https://github.com/tenzir/tenzir/pull/2324).

#### Support environment variables for plugin options

[Section titled “Support environment variables for plugin options”](#support-environment-variables-for-plugin-options)

VAST no longer ignores environment variables for plugin-specific options. E.g., the environment variable `VAST_PLUGINS__FOO__BAR` now correctly refers to the `bar` option of the `foo` plugin, i.e., `plugins.foo.bar`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2390](https://github.com/tenzir/tenzir/pull/2390).

# VAST v2.2.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v2.2.0).

### Features

[Section titled “Features”](#features)

#### Summarize operator with pluggable aggregation functions

[Section titled “Summarize operator with pluggable aggregation functions”](#summarize-operator-with-pluggable-aggregation-functions)

The `summarize` operator supports three new aggregation functions: `sample` takes the first value in every group, `distinct` filters out duplicate values, and `count` yields the number of values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2417](https://github.com/tenzir/tenzir/pull/2417).

#### Implement a `flush` command

[Section titled “Implement a flush command”](#implement-a-flush-command)

The new `flush` command causes VAST to decommission all currently active partitions, i.e., write all active partitions to disk immediately regardless of their size or the active partition timeout. This is particularly useful for testing, or when needing to guarantee in automated scripts that input is available for operations that only work on persisted passive partitions. The `flush` command returns only after all active partitions were flushed to disk.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2396](https://github.com/tenzir/tenzir/pull/2396).

#### Introduce `select` / `replace` / `extend` operators

[Section titled “Introduce select / replace / extend operators”](#introduce-select--replace--extend-operators)

The new `extend` pipeline operator allows for adding new fields with fixed values to data.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2423](https://github.com/tenzir/tenzir/pull/2423).

#### Run commands from scripts

[Section titled “Run commands from scripts”](#run-commands-from-scripts)

The cloud execution commands (`run-lambda` and `execute-command`) now accept scripts from file-like handles. To improve the usability of this feature, the whole host file system is now mounted into the CLI container.

By [@rdettai](https://github.com/rdettai) in [#2446](https://github.com/tenzir/tenzir/pull/2446).

#### Support dropping entire schemas in `drop` operator

[Section titled “Support dropping entire schemas in drop operator”](#support-dropping-entire-schemas-in-drop-operator)

The `drop` pipeline operator now drops entire schemas spcefied by name in the `schemas` configuration key in addition to dropping fields by extractors in the `fields` configuration key.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2419](https://github.com/tenzir/tenzir/pull/2419).

### Changes

[Section titled “Changes”](#changes)

#### Add clean command

[Section titled “Add clean command”](#add-clean-command)

An `init` command was added to `vast-cloud` to help getting out of inconsistent Terraform states.

By [@rdettai](https://github.com/rdettai) in [#2435](https://github.com/tenzir/tenzir/pull/2435).

#### Implement Apache Parquet & Apache Feather V2 stores

[Section titled “Implement Apache Parquet & Apache Feather V2 stores”](#implement-apache-parquet--apache-feather-v2-stores)

Metrics for VAST’s store lookups now use the keys `{active,passive}-store.lookup.{runtime,hits}`. The store type metadata field now distinguishes between the various supported store types, e.g., `parquet`, `feather`, or `segment-store`, rather than containing `active` or `passive`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2413](https://github.com/tenzir/tenzir/pull/2413).

#### Rename transform to pipeline

[Section titled “Rename transform to pipeline”](#rename-transform-to-pipeline)

Transforms are now called pipelines. In your configuration, replace `transform` with `pipeline` in all keys.

By [@dispanser](https://github.com/dispanser) in [#2429](https://github.com/tenzir/tenzir/pull/2429).

#### Introduce `select` / `replace` / `extend` operators

[Section titled “Introduce select / replace / extend operators”](#introduce-select--replace--extend-operators-1)

The `put` pipeline operator is now called `select`, as we’ve abandoned plans to integrate the functionality of `replace` into it.

The `replace` pipeline operator now supports multiple replacements in one configuration, which aligns the behavior with other operators.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2423](https://github.com/tenzir/tenzir/pull/2423).

#### Summarize operator with pluggable aggregation functions

[Section titled “Summarize operator with pluggable aggregation functions”](#summarize-operator-with-pluggable-aggregation-functions-1)

The `summarize` pipeline operator is now a builtin; the previously bundled `summarize` plugin no longer exists. Aggregation functions in the `summarize` operator are now plugins, which makes them easily extensible. The syntax of `summarize` now supports specification of output field names, similar to SQL’s `AS` in `SELECT f(x) AS name`.

The undocumented `count` pipeline operator no longer exists.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2417](https://github.com/tenzir/tenzir/pull/2417).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Make partition deletion resilient against oversize

[Section titled “Make partition deletion resilient against oversize”](#make-partition-deletion-resilient-against-oversize)

VAST is now able to detect corrupt index files and will attempt to repair them on startup.

By [@tobim](https://github.com/tobim) in [#2431](https://github.com/tenzir/tenzir/pull/2431).

#### Make transform application transactional

[Section titled “Make transform application transactional”](#make-transform-application-transactional)

We fixed a race condition when VAST crashed while applying a partition transform, leading to data duplication.

By [@lava](https://github.com/lava) in [#2465](https://github.com/tenzir/tenzir/pull/2465).

#### Properly indicate failure in the rebuild command

[Section titled “Properly indicate failure in the rebuild command”](#properly-indicate-failure-in-the-rebuild-command)

The rebuild command no longer crashes on failure, and displays the encountered error instead.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2466](https://github.com/tenzir/tenzir/pull/2466).

#### Fix missing options sometimes not causing an error

[Section titled “Fix missing options sometimes not causing an error”](#fix-missing-options-sometimes-not-causing-an-error)

Missing arguments for the `--plugins`, `--plugin-dirs`, and `--schema-dirs` command line options no longer cause VAST to crash occasionally.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2470](https://github.com/tenzir/tenzir/pull/2470).

#### Fix `vast.export.json.omit-nulls` for nested records

[Section titled “Fix vast.export.json.omit-nulls for nested records”](#fix-vastexportjsonomit-nulls-for-nested-records)

The JSON export with `--omit-nulls` now correctly handles nested records whose first field is `null` instead of dropping them entirely.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2447](https://github.com/tenzir/tenzir/pull/2447).

#### Render reals with at least one decimal place

[Section titled “Render reals with at least one decimal place”](#render-reals-with-at-least-one-decimal-place)

VAST will export `real` values in JSON consistently with at least one decimal place.

By [@patszt](https://github.com/patszt) in [#2393](https://github.com/tenzir/tenzir/pull/2393).

# VAST v2.3.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v2.3.0).

### Features

[Section titled “Features”](#features)

#### Cloud matchers

[Section titled “Cloud matchers”](#cloud-matchers)

We can now use matchers in AWS using the vast-cloud CLI matcher plugin.

By [@rdettai](https://github.com/rdettai) in [#2473](https://github.com/tenzir/tenzir/pull/2473).

#### Make the connection timeout configurable

[Section titled “Make the connection timeout configurable”](#make-the-connection-timeout-configurable)

The new `vast.connection-timeout` option allows for configuring the timeout VAST clients use when connecting to a VAST server. The value defaults to 10s; setting it to a zero duration causes produces an infinite timeout.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2499](https://github.com/tenzir/tenzir/pull/2499).

#### Make the `rebuild` command more reliable

[Section titled “Make the rebuild command more reliable”](#make-the-rebuild-command-more-reliable)

VAST now continuously rebuilds outdated and merges undersized partitions in the background. The new option `vast.automatic-rebuild` controls how many resources to spend on this. To disable this behavior, set the option to 0; the default is 1.

Rebuilding now emits metrics under the keys `rebuilder.partitions.{remaining,rebuilding,completed}`. The `vast status rebuild` command additionally shows information about the ongoing rebuild.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2493](https://github.com/tenzir/tenzir/pull/2493).

#### Implement optional dense indexes

[Section titled “Implement optional dense indexes”](#implement-optional-dense-indexes)

VAST’s partition indexes are now optional, allowing operators to control the trade-off between disk-usage and query performance for every field.

By [@patszt](https://github.com/patszt) in [#2430](https://github.com/tenzir/tenzir/pull/2430).

### Changes

[Section titled “Changes”](#changes)

#### Lower the impact of low-priority queries

[Section titled “Lower the impact of low-priority queries”](#lower-the-impact-of-low-priority-queries)

We improved the operability of VAST servers under high load from automated low-priority queries. VAST now considers queries issued with `--low-priority`, such as automated retro-match queries, with even less priority compared to regular queries (down from 33.3% to 4%) and internal high-priority queries used for rebuilding and compaction (down from 12.5% to 1%).

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2484](https://github.com/tenzir/tenzir/pull/2484).

#### Make the `rebuild` command more reliable

[Section titled “Make the rebuild command more reliable”](#make-the-rebuild-command-more-reliable-1)

The default value for `vast.active-partition-timeout` is now 5 minutes (down from 1 hour), causing VAST to persist underful partitions earlier.

We split the `vast rebuild` command into two: `vast rebuild start` and `vast rebuild stop`. Rebuild orchestration now runs server-side, and only a single rebuild may run at a given time. We also made it more intuitive to use: `--undersized` now implies `--all`, and a new `--detached` option allows for running rebuilds in the background.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2493](https://github.com/tenzir/tenzir/pull/2493).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Respect `--connection-timeout` in more places

[Section titled “Respect --connection-timeout in more places”](#respect---connection-timeout-in-more-places)

Configuration options representing durations with an associated command-line option like `vast.connection-timeout` and `--connection-timeout` were not picked up from configuration files or environment variables. This now works as expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2503](https://github.com/tenzir/tenzir/pull/2503).

#### Add active partition actor to unpersisted partitions on decomission

[Section titled “Add active partition actor to unpersisted partitions on decomission”](#add-active-partition-actor-to-unpersisted-partitions-on-decomission)

VAST no longer occasionally prints warnings about no longer available partitions when queries run concurrently to imports.

By [@patszt](https://github.com/patszt) in [#2500](https://github.com/tenzir/tenzir/pull/2500).

#### Implement a flatbuffer container class to hold excess table slices in segments

[Section titled “Implement a flatbuffer container class to hold excess table slices in segments”](#implement-a-flatbuffer-container-class-to-hold-excess-table-slices-in-segments)

VAST can now store data in segments bigger than 2GiB in size each.

VAST can now store column indexes that are bigger than 2GiB.

By [@lava](https://github.com/lava) in [#2449](https://github.com/tenzir/tenzir/pull/2449).

#### Implement optional dense indexes

[Section titled “Implement optional dense indexes”](#implement-optional-dense-indexes-1)

VAST properly processes queries for fields with `skip` attribute.

By [@patszt](https://github.com/patszt) in [#2430](https://github.com/tenzir/tenzir/pull/2430).

#### Wait until all stores have exited before finishing a partition transform

[Section titled “Wait until all stores have exited before finishing a partition transform”](#wait-until-all-stores-have-exited-before-finishing-a-partition-transform)

Fixed a race condition where the output of a partition transform could be reused before it was fully written to disk, for example when running `vast rebuild`.

By [@lava](https://github.com/lava) in [#2543](https://github.com/tenzir/tenzir/pull/2543).

#### Display store load failures to the user

[Section titled “Display store load failures to the user”](#display-store-load-failures-to-the-user)

Partitions now fail early when their stores fail to load from disk, detailing what went wrong in an error message.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2507](https://github.com/tenzir/tenzir/pull/2507).

#### Activate cloud plugins explicitely

[Section titled “Activate cloud plugins explicitely”](#activate-cloud-plugins-explicitely)

We changed the way `vast-cloud` is loading its cloud plugins to make it more explicit. This avoids inconsitent defaults assigned to variables when using core commands on specific plugins.

By [@rdettai](https://github.com/rdettai) in [#2510](https://github.com/tenzir/tenzir/pull/2510).

#### Don’t abort startup if individual partitions fail to load

[Section titled “Don’t abort startup if individual partitions fail to load”](#dont-abort-startup-if-individual-partitions-fail-to-load)

The `rebuild` command, automatic rebuilds, and compaction are now much faster, and match the performance of the `import` command for building indexes.

By [@tobim](https://github.com/tobim) in [#2515](https://github.com/tenzir/tenzir/pull/2515).

# VAST v2.3.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v2.3.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Properly recover corrupted partition data on startup

[Section titled “Properly recover corrupted partition data on startup”](#properly-recover-corrupted-partition-data-on-startup)

VAST now properly regenerates any corrupted, oversized partitions it encounters during startup, provided that the corresponding store files are available. These files could be produced by versions up to and including VAST v2.2, when using configurations with an increased maximum partition size.

By [@lava](https://github.com/lava) in [#2631](https://github.com/tenzir/tenzir/pull/2631).

#### Abort the response promise if running into oversized partitions

[Section titled “Abort the response promise if running into oversized partitions”](#abort-the-response-promise-if-running-into-oversized-partitions)

We fixed an indefinite hang that occurred when attempting to apply a pipeline to a partition that is not a valid flatbuffer.

By [@lava](https://github.com/lava) in [#2624](https://github.com/tenzir/tenzir/pull/2624).

# VAST v2.4.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v2.4.0).

### Features

[Section titled “Features”](#features)

#### Cloud MISP

[Section titled “Cloud MISP”](#cloud-misp)

VAST Cloud has now a MISP plugin that enables to add a MISP instance to the cloud stack.

By [@rdettai](https://github.com/rdettai) in [#2548](https://github.com/tenzir/tenzir/pull/2548).

#### Make data predicate evaluation column-major

[Section titled “Make data predicate evaluation column-major”](#make-data-predicate-evaluation-column-major)

Queries without acceleration from a dense index run significantly faster, e.g., initial tests show a 2x performance improvement for substring queries.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2730](https://github.com/tenzir/tenzir/pull/2730).

#### PRs 2567-2614-2638-3681

[Section titled “PRs 2567-2614-2638-3681”](#prs-2567-2614-2638-3681)

The new experimental web plugin offers a RESTful API to VAST and a bundled web user interface in Svelte.

By [@lava](https://github.com/lava) in [#2567](https://github.com/tenzir/tenzir/pull/2567).

#### Rebatch undersized batches when rebuilding partitions

[Section titled “Rebatch undersized batches when rebuilding partitions”](#rebatch-undersized-batches-when-rebuilding-partitions)

Rebuilding partitions now additionally rebatches the contained events to `vast.import.batch-size` events per batch, which accelerates queries against partitions that previously had undersized batches.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2583](https://github.com/tenzir/tenzir/pull/2583).

#### PRs 2513-2738

[Section titled “PRs 2513-2738”](#prs-2513-2738)

We now distribute VAST also as Debian Package with every new release. The Debian package automatically installs a systemd service and creates a `vast` user for the VAST process.

By [@tobim](https://github.com/tobim) in [#2513](https://github.com/tenzir/tenzir/pull/2513).

#### Add “-total” metric keys for schema-dependent metrics

[Section titled “Add “-total” metric keys for schema-dependent metrics”](#add--total-metric-keys-for-schema-dependent-metrics)

VAST has three new metrics: `catalog.num-partitions-total`, `catalog.num-events-total`, and `ingest-total` that sum up all schema-based metrics by their respective schema-based metric counterparts.

By [@Dakostu](https://github.com/Dakostu) in [#2682](https://github.com/tenzir/tenzir/pull/2682).

#### Disable building unit tests in Dockerfile

[Section titled “Disable building unit tests in Dockerfile”](#disable-building-unit-tests-in-dockerfile)

VAST Cloud can now expose HTTP services using Cloudflare Access.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2578](https://github.com/tenzir/tenzir/pull/2578).

#### Emit metrics from the filesystem actor

[Section titled “Emit metrics from the filesystem actor”](#emit-metrics-from-the-filesystem-actor)

VAST now emits metrics for filesystem access under the keys `posix-filesystem.{checks,writes,reads,mmaps,erases,moves}.{successful,failed,bytes}`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2572](https://github.com/tenzir/tenzir/pull/2572).

#### Enable configuration of the zstd compression level for feather store

[Section titled “Enable configuration of the zstd compression level for feather store”](#enable-configuration-of-the-zstd-compression-level-for-feather-store)

VAST has a new configuration setting, `vast.zstd-compression-level`, to control the compression level of the Zstd algorithm used in both the Feather and Parquet store backends. The default level is set by the Apache Arrow library, and for Parquet is no longer explicitly defaulted to `9`.

By [@dispanser](https://github.com/dispanser) in [#2623](https://github.com/tenzir/tenzir/pull/2623).

#### PRs 2574-2652

[Section titled “PRs 2574-2652”](#prs-2574-2652)

VAST now ships a Docker Compose file. In particular, the Docker Compose stack now has a TheHive integration that can run VAST queries as a Cortex Analyzer.

By [@KaanSK](https://github.com/KaanSK) in [#2574](https://github.com/tenzir/tenzir/pull/2574).

### Changes

[Section titled “Changes”](#changes)

#### Move the version string into a central JSON file

[Section titled “Move the version string into a central JSON file”](#move-the-version-string-into-a-central-json-file)

Building VAST from source now requires CMake 3.19 or greater.

By [@tobim](https://github.com/tobim) in [#2582](https://github.com/tenzir/tenzir/pull/2582).

#### Make `feather` the default store-backend

[Section titled “Make feather the default store-backend”](#make-feather-the-default-store-backend)

The default store backend of VAST is now `feather`. Reading from VAST’s custom `segment-store` backend is still transparently supported, but new partitions automatically write to the Apache Feather V2 backend instead.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2587](https://github.com/tenzir/tenzir/pull/2587).

#### Change default endpoint to 127.0.0.1

[Section titled “Change default endpoint to 127.0.0.1”](#change-default-endpoint-to-127001)

We changed the default VAST endpoint from `localhost` to `127.0.0.1`. This ensures the listening address is deterministic and not dependent on the host-specific IPv4 and IPv6 resolution. For example, resolving `localhost` yields a list of addresses, and if VAST fails to bind on the first (e.g., to due to a lingering socket) it would silently go to the next. Taking name resolution out of the equation fixes such issues. Set the option `vast.endpoint` to override the default endpoint.

By [@lava](https://github.com/lava) in [#2512](https://github.com/tenzir/tenzir/pull/2512).

#### Load “all” plugins by default & allow “empty” values

[Section titled “Load “all” plugins by default & allow “empty” values”](#load-all-plugins-by-default--allow-empty-values)

VAST now loads all plugins by default. To revert to the old behavior, explicitly set the `vast.plugins` option to have no value.

By [@Dakostu](https://github.com/Dakostu) in [#2689](https://github.com/tenzir/tenzir/pull/2689).

#### Add memory-usage to index and catalog telemetry reports

[Section titled “Add memory-usage to index and catalog telemetry reports”](#add-memory-usage-to-index-and-catalog-telemetry-reports)

VAST now emits per-component memory usage metrics under the keys `index.memory-usage` and `catalog.memory-usage`.

By [@patszt](https://github.com/patszt) in [#2471](https://github.com/tenzir/tenzir/pull/2471).

#### Remove PyVAST in favor of new Python bindings

[Section titled “Remove PyVAST in favor of new Python bindings”](#remove-pyvast-in-favor-of-new-python-bindings)

We removed PyVAST from the code base in favor of the new Python bindings. PyVAST continues to work as a thin wrapper around the VAST binary, but will no longer be released alongside VAST.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2674](https://github.com/tenzir/tenzir/pull/2674).

#### Rename `vast dump` to `vast show`

[Section titled “Rename vast dump to vast show”](#rename-vast-dump-to-vast-show)

The `vast dump` command is now called `vast show`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2686](https://github.com/tenzir/tenzir/pull/2686).

#### Arrow 10.0.0 support

[Section titled “Arrow 10.0.0 support”](#arrow-1000-support)

Building VAST from source now requires [Apache Arrow 10.0](https://arrow.apache.org/blog/2022/10/31/10.0.0-release/) or newer.

By [@Dakostu](https://github.com/Dakostu) in [#2685](https://github.com/tenzir/tenzir/pull/2685).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add a timeout to the UDS metric sink

[Section titled “Add a timeout to the UDS metric sink”](#add-a-timeout-to-the-uds-metric-sink)

The UDS metrics sink no longer deadlocks due to suspended listeners.

By [@tobim](https://github.com/tobim) in [#2635](https://github.com/tenzir/tenzir/pull/2635).

#### Remove caf::skip usages

[Section titled “Remove caf::skip usages”](#remove-cafskip-usages)

Rebuilding of heterogeneous partition no longer freezes the entire rebuilder on pipeline failures.

By [@patszt](https://github.com/patszt) in [#2530](https://github.com/tenzir/tenzir/pull/2530).

#### Fix a connection error message

[Section titled “Fix a connection error message”](#fix-a-connection-error-message)

The error message on connection failure now contains a correctly formatted target endpoint.

By [@tobim](https://github.com/tobim) in [#2609](https://github.com/tenzir/tenzir/pull/2609).

#### Remove the shutdown grace period

[Section titled “Remove the shutdown grace period”](#remove-the-shutdown-grace-period)

VAST no longer attempts to hard-kill itself if the shutdown did not finish within the configured grace period. The option `vast.shutdown-grace-period` no longer exists. We recommend setting `TimeoutStopSec=180` in the VAST systemd service definition to restore the previous behavior.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2568](https://github.com/tenzir/tenzir/pull/2568).

#### Don’t abort startup if individual partitions fail to load

[Section titled “Don’t abort startup if individual partitions fail to load”](#dont-abort-startup-if-individual-partitions-fail-to-load)

VAST now skips unreadable partitions while starting up, instead of aborting the initialization routine.

By [@tobim](https://github.com/tobim) in [#2515](https://github.com/tenzir/tenzir/pull/2515).

#### Allow read access to user home dir in the systemd unit

[Section titled “Allow read access to user home dir in the systemd unit”](#allow-read-access-to-user-home-dir-in-the-systemd-unit)

The systemd service no longer fails if the home directory of the vast user is not in `/var/lib/vast`.

By [@tobim](https://github.com/tobim) in [#2734](https://github.com/tenzir/tenzir/pull/2734).

#### Clear failed partitions from the cache

[Section titled “Clear failed partitions from the cache”](#clear-failed-partitions-from-the-cache)

VAST now ejects partitions from the LRU cache if they fail to load with an I/O error.

By [@lava](https://github.com/lava) in [#2642](https://github.com/tenzir/tenzir/pull/2642).

# VAST v2.4.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v2.4.1).

### Features

[Section titled “Features”](#features)

#### Make feather stores read incrementally

[Section titled “Make feather stores read incrementally”](#make-feather-stores-read-incrementally)

VAST’s Feather store now yields initial results much faster and performs better when running queries affecting a large number of partitions by doing smaller incremental disk reads as needed rather than one large disk read upfront.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2805](https://github.com/tenzir/tenzir/pull/2805).

# VAST v2.4.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v2.4.2).

### Changes

[Section titled “Changes”](#changes)

#### Reduce conflict potential between rebuilding and queries

[Section titled “Reduce conflict potential between rebuilding and queries”](#reduce-conflict-potential-between-rebuilding-and-queries)

VAST’s rebuilding and compaction features now interfere less with queries.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3047](https://github.com/tenzir/tenzir/pull/3047).

# VAST v3.0.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v3.0.0).

### Features

[Section titled “Features”](#features)

#### Add `vast import arrow` enabling ingestion of arrow IPC format

[Section titled “Add vast import arrow enabling ingestion of arrow IPC format”](#add-vast-import-arrow-enabling-ingestion-of-arrow-ipc-format)

VAST now imports Arrow IPC data, which is the same format it already supports for export.

By [@dispanser](https://github.com/dispanser) in [#2707](https://github.com/tenzir/tenzir/pull/2707).

#### Add CORS preflight request handling

[Section titled “Add CORS preflight request handling”](#add-cors-preflight-request-handling)

The experimental web frontend now correctly responds to CORS preflight requests. To configure CORS behavior, the new `vast.web.cors-allowed-origin` config option can be used.

By [@lava](https://github.com/lava) in [#2944](https://github.com/tenzir/tenzir/pull/2944).

#### Support pattern case insensitivity in Sigma plugin

[Section titled “Support pattern case insensitivity in Sigma plugin”](#support-pattern-case-insensitivity-in-sigma-plugin)

The `sigma` plugin now treats Sigma strings as case-insensitive patterns during the transpilation process.

By [@Dakostu](https://github.com/Dakostu) in [#2974](https://github.com/tenzir/tenzir/pull/2974).

#### Make it easy to create docker images with Nix

[Section titled “Make it easy to create docker images with Nix”](#make-it-easy-to-create-docker-images-with-nix)

We now offer a `tenzir/vast-slim` image as an alternative to the `tenzir/vast` image. The image is minimal in size and supports the same features as the regular image, but does not support building additional plugins against it and mounting in additional plugins.

By [@tobim](https://github.com/tobim) in [#2742](https://github.com/tenzir/tenzir/pull/2742).

#### Add ‘pseudonymize’ pipeline operator

[Section titled “Add ‘pseudonymize’ pipeline operator”](#add-pseudonymize-pipeline-operator)

The new `pseudonymize` pipeline operator pseudonymizes IP addresses in user-specified fields.

By [@Dakostu](https://github.com/Dakostu) in [#2719](https://github.com/tenzir/tenzir/pull/2719).

#### Implement support for `:string == /pattern/` queries

[Section titled “Implement support for :string == /pattern/ queries”](#implement-support-for-string--pattern-queries)

Queries of the forms `:string == /pattern/`, `field == /pattern/`, `#type == /pattern/`, and their respective negations now work as expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2769](https://github.com/tenzir/tenzir/pull/2769).

#### Add options to omit empty values when exporting as JSON

[Section titled “Add options to omit empty values when exporting as JSON”](#add-options-to-omit-empty-values-when-exporting-as-json)

The JSON export format gained the options `--omit-empty-records`, `--omit-empty-lists`, and `--omit-empty-maps`, which cause empty records, lists, and maps not to be rendered respectively. The options may be combined together with the existing `--omit-nulls` option. Use `--omit-empty` to set all four flags at once.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2856](https://github.com/tenzir/tenzir/pull/2856).

#### Add ‘pipeline’ parameter and schematized format to export endpoint

[Section titled “Add ‘pipeline’ parameter and schematized format to export endpoint”](#add-pipeline-parameter-and-schematized-format-to-export-endpoint)

The `/export` family of endpoints now accepts an optional `pipeline` parameter to specify an ad-hoc pipeline that should be applied to the exported data.

By [@lava](https://github.com/lava) in [#2773](https://github.com/tenzir/tenzir/pull/2773).

#### Support case insensitivity in patterns

[Section titled “Support case insensitivity in patterns”](#support-case-insensitivity-in-patterns)

Patterns now support case insensitivity by adding `i` to the pattern string, e.g. `/^\w{3}$/i`.

By [@Dakostu](https://github.com/Dakostu) in [#2951](https://github.com/tenzir/tenzir/pull/2951).

#### Add a new /openapi.json endpoint to the web plugin

[Section titled “Add a new /openapi.json endpoint to the web plugin”](#add-a-new-openapijson-endpoint-to-the-web-plugin)

The experimental web plugin now serves its own API specification at the new ‘/openapi.json’ endpoint.

By [@lava](https://github.com/lava) in [#2981](https://github.com/tenzir/tenzir/pull/2981).

#### Eliminate shutdown lag from the signal monitor

[Section titled “Eliminate shutdown lag from the signal monitor”](#eliminate-shutdown-lag-from-the-signal-monitor)

The new `/query` endpoint for the experimental REST API allows users to receive query data in multiple steps, as opposed to a oneshot export.

By [@tobim](https://github.com/tobim) in [#2766](https://github.com/tenzir/tenzir/pull/2766).

#### Implement `head` and `taste` operators

[Section titled “Implement head and taste operators”](#implement-head-and-taste-operators)

The new `head` and `taste` operators limit results to the specified number of events. The `head` operator applies this limit for all events, and the `taste` operator applies it per schema. Both operators take the limit as an optional argument, with the default value being 10.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2891](https://github.com/tenzir/tenzir/pull/2891).

#### PRs 2877-2904-2907

[Section titled “PRs 2877-2904-2907”](#prs-2877-2904-2907)

The `export` and `import` commands now support an optional pipeline string that allows for chaining pipeline operators together and executing such a pipeline on outgoing and incoming data. This feature is experimental and the syntax is subject to change without notice. New operators are only available in the new pipeline syntax, and the old YAML syntax is deprecated.

By [@Dakostu](https://github.com/Dakostu) in [#2877](https://github.com/tenzir/tenzir/pull/2877).

#### Add extractor predicates

[Section titled “Add extractor predicates”](#add-extractor-predicates)

Extractors such as `x` and `:T` can now expand to the predicates `x != null` and `:T != null`, respectively.

By [@jachris](https://github.com/jachris) in [#2984](https://github.com/tenzir/tenzir/pull/2984).

#### Implement a retry mechanism for VAST clients failing to connect to the server

[Section titled “Implement a retry mechanism for VAST clients failing to connect to the server”](#implement-a-retry-mechanism-for-vast-clients-failing-to-connect-to-the-server)

We changed VAST client processes to attempt connecting to a VAST server multiple times until the configured connection timeout (`vast.connection-timeout`, defaults to 5 minutes) runs out. A fixed delay between connection attempts (`vast.connection-retry-delay`, defaults to 3 seconds) ensures that clients to not stress the server too much. Set the connection timeout to zero to let VAST client attempt connecting indefinitely, and the delay to zero to disable the retry mechanism.

By [@patszt](https://github.com/patszt) in [#2835](https://github.com/tenzir/tenzir/pull/2835).

#### Install Python bindings along with VAST

[Section titled “Install Python bindings along with VAST”](#install-python-bindings-along-with-vast)

VAST installations and packages now include Python bindings in a site-package under `<install-prefix>/lib/python*/site-packages/vast`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2636](https://github.com/tenzir/tenzir/pull/2636).

#### Add CEF reader plugin

[Section titled “Add CEF reader plugin”](#add-cef-reader-plugin)

The `cef` import format allows for reading events in the Common Event Format (CEF) via `vast import cef < cef.log`.

By [@mavam](https://github.com/mavam) in [#2216](https://github.com/tenzir/tenzir/pull/2216).

### Changes

[Section titled “Changes”](#changes)

#### Add ‘pseudonymize’ pipeline operator

[Section titled “Add ‘pseudonymize’ pipeline operator”](#add-pseudonymize-pipeline-operator-1)

OpenSSL is now a required dependency.

By [@Dakostu](https://github.com/Dakostu) in [#2719](https://github.com/tenzir/tenzir/pull/2719).

#### Remove broker plugin

[Section titled “Remove broker plugin”](#remove-broker-plugin)

We removed the broker plugin that enabled direct Zeek 3.x log transfer to VAST. The plugin will return in the future rewritten for Zeek 5+.

By [@patszt](https://github.com/patszt) in [#2796](https://github.com/tenzir/tenzir/pull/2796).

#### PRs 2769-2873

[Section titled “PRs 2769-2873”](#prs-2769-2873)

The match operator `~`, its negation `!~`, and the `pattern` type no longer exist. Use queries of the forms `lhs == /rhs/` and `lhs != /rhs/` instead for queries using regular expressions.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2769](https://github.com/tenzir/tenzir/pull/2769).

#### Replace ‘nil’ with ‘null’

[Section titled “Replace ‘nil’ with ‘null’”](#replace-nil-with-null)

The non-value literal in expressions has a new syntax: `null` replaces its old representation `nil`. For example, the query `x != nil` is no longer valid; use `x != null` instead.

By [@Dakostu](https://github.com/Dakostu) in [#2999](https://github.com/tenzir/tenzir/pull/2999).

#### Deprecate `vast.pipeline-triggers`

[Section titled “Deprecate vast.pipeline-triggers”](#deprecate-vastpipeline-triggers)

The `vast.pipeline-triggers` option is deprecated; while it continues to work as-is, support for it will be removed in the next release. Use the new inline import and export pipelines instead. They will return as more generally applicable node ingress and egress pipelines in the future.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3008](https://github.com/tenzir/tenzir/pull/3008).

#### Remove the `#field` meta extractor

[Section titled “Remove the #field meta extractor”](#remove-the-field-meta-extractor)

The `#field` meta extractor no longer exists. Use `X != null` over `#field == "X"` to check for existence for the field `X`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2776](https://github.com/tenzir/tenzir/pull/2776).

#### Rename count, int, real, and addr to uint64, int64, double, and ip respectively

[Section titled “Rename count, int, real, and addr to uint64, int64, double, and ip respectively”](#rename-count-int-real-and-addr-to-uint64-int64-double-and-ip-respectively)

The builtin types `count`, `int`, `real`, and `addr` were renamed to `uint64`, `int64`, `double`, and `ip` respectively. For backwards-compatibility, VAST still supports parsing the old type tokens in schema files.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2864](https://github.com/tenzir/tenzir/pull/2864).

#### Use CAF streaming in export command

[Section titled “Use CAF streaming in export command”](#use-caf-streaming-in-export-command)

The `explore` and `pivot` commands are now unavailable. They will be reintroduced as pipeline operators in the future.

By [@patszt](https://github.com/patszt) in [#2898](https://github.com/tenzir/tenzir/pull/2898).

#### Introduce a potpourri of smaller improvements

[Section titled “Introduce a potpourri of smaller improvements”](#introduce-a-potpourri-of-smaller-improvements)

Plugin names are now case-insensitive.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2832](https://github.com/tenzir/tenzir/pull/2832).

#### Introduce a potpourri of smaller improvements

[Section titled “Introduce a potpourri of smaller improvements”](#introduce-a-potpourri-of-smaller-improvements-1)

VAST now ignores the previously deprecated options `vast.meta-index-fp-rate`, `vast.catalog-fp-rate`, `vast.transforms` and `vast.transform-triggers`. Similarly, setting `vast.store-backend` to `segment-store` now results in an error rather than a graceful fallback to the default store.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2832](https://github.com/tenzir/tenzir/pull/2832).

#### Provide an Ansible role for VAST

[Section titled “Provide an Ansible role for VAST”](#provide-an-ansible-role-for-vast)

VAST now comes with a role definition for Ansible. You can find it directly in the `ansible` subdirectory.

By [@tobim](https://github.com/tobim) in [#2604](https://github.com/tenzir/tenzir/pull/2604).

#### Rename `identity` operator to `pass`

[Section titled “Rename identity operator to pass”](#rename-identity-operator-to-pass)

We renamed the `identity` operator to `pass`.

By [@jachris](https://github.com/jachris) in [#2980](https://github.com/tenzir/tenzir/pull/2980).

#### PRs 2922-2927

[Section titled “PRs 2922-2927”](#prs-2922-2927)

We removed the frontend prototype bundled with the web plugin Some parts of the frontend that we have in development are designed to be closed-source, and it is easier to develop at the current development stage in a single repository that is not bound to the release process of VAST itself. An open-source version of the frontend may return in the future.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2922](https://github.com/tenzir/tenzir/pull/2922).

#### Update response format of the /export endpoint

[Section titled “Update response format of the /export endpoint”](#update-response-format-of-the-export-endpoint)

For the experimental REST API, the result format of the `/export` endpoint was modified: The `num_events` key was renamed to `num-events`, and the `version` key was removed.

By [@lava](https://github.com/lava) in [#2899](https://github.com/tenzir/tenzir/pull/2899).

#### Change boolean literals to `true` and `false`

[Section titled “Change boolean literals to true and false”](#change-boolean-literals-to-true-and-false)

Boolean literals in expressions have a new syntax: `true` and `false` replace the old representations `T` and `F`. For example, the query `suricata.alert.alerted == T` is no longer valid; use `suricata.alert.alerted == true` instead.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2844](https://github.com/tenzir/tenzir/pull/2844).

#### Make the map type inaccessible to users

[Section titled “Make the map type inaccessible to users”](#make-the-map-type-inaccessible-to-users)

The `map` type no longer exists: instead of `map<T, U>`, use the equivalent `list<record{ key: T, value: U }>`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2976](https://github.com/tenzir/tenzir/pull/2976).

#### Switch default TCP port to 5158

[Section titled “Switch default TCP port to 5158”](#switch-default-tcp-port-to-5158)

From now on VAST will use TCP port 5158 for its native inter process communication. This change avoids collisions from dynamic port allocation on Linux systems.

By [@tobim](https://github.com/tobim) in [#2998](https://github.com/tenzir/tenzir/pull/2998).

#### PRs 2807-2848

[Section titled “PRs 2807-2848”](#prs-2807-2848)

Blocking imports now imply that ingested data gets persisted to disk before the the `vast import` process exits.

By [@tobim](https://github.com/tobim) in [#2807](https://github.com/tenzir/tenzir/pull/2807).

#### Remove the /export endpoint

[Section titled “Remove the /export endpoint”](#remove-the-export-endpoint)

The REST API does not contain the `/export` and `/export/with-schemas` endpoints anymore. Any previous queries using those endpoints have to be sent to the `/query` endpoint now.

By [@Dakostu](https://github.com/Dakostu) in [#2990](https://github.com/tenzir/tenzir/pull/2990).

#### Merge the type-registry into the catalog

[Section titled “Merge the type-registry into the catalog”](#merge-the-type-registry-into-the-catalog)

`vast status` no longer shows type registry-related information. Instead, refer to `vast show` for detailed type metadata information.

By [@Dakostu](https://github.com/Dakostu) in [#2745](https://github.com/tenzir/tenzir/pull/2745).

#### PRs 2693-2923

[Section titled “PRs 2693-2923”](#prs-2693-2923)

Building VAST now requires CAF 0.18.7. VAST supports setting advanced options for CAF directly in its configuration file under the `caf` section. If you were using any of these, compare them against the bundled `vast.yaml.example` file to see if you need to make any changes. The change has (mostly positive) [performance and stability implications](https://www.actor-framework.org/blog/2021-01/benchmarking-0.18/) throughout VAST, especially in high-load scenarios.

By [@patszt](https://github.com/patszt) in [#2693](https://github.com/tenzir/tenzir/pull/2693).

#### Align output of the Zeek TSV reader with schemas

[Section titled “Align output of the Zeek TSV reader with schemas”](#align-output-of-the-zeek-tsv-reader-with-schemas)

The bundled Zeek schema no longer includes the `_path` field included in Zeek JSON. Use `#type == "zeek.foo"` over `_path == "foo"` for querying data ingested using `vast import zeek-json`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2887](https://github.com/tenzir/tenzir/pull/2887).

#### PRs 2778-2797-2798

[Section titled “PRs 2778-2797-2798”](#prs-2778-2797-2798)

VAST no longer supports reading partitions created with VAST versions older than VAST v2.2. Since VAST v2.2, VAST continuously upgrades old partitions to the most recent internal format while running.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2778](https://github.com/tenzir/tenzir/pull/2778).

#### Move event distribution statistics to the catalog

[Section titled “Move event distribution statistics to the catalog”](#move-event-distribution-statistics-to-the-catalog)

The per-schema event distribution moved from `index.statistics.layouts` to `catalog.schemas`, and additionally includes information about the import time range and the number of partitions VAST knows for the schema. The number of events per schema no longer includes events that are yet unpersisted.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2852](https://github.com/tenzir/tenzir/pull/2852).

#### Move taxonomy resolution to the catalog

[Section titled “Move taxonomy resolution to the catalog”](#move-taxonomy-resolution-to-the-catalog)

`vast status` does not work anymore with an embedded node (i.e., spawned with the `-N` parameter).

By [@Dakostu](https://github.com/Dakostu) in [#2771](https://github.com/tenzir/tenzir/pull/2771).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix rounding issue when printing duration types

[Section titled “Fix rounding issue when printing duration types”](#fix-rounding-issue-when-printing-duration-types)

We fixed incorrect printing of human-readable durations in some edge cases. E.g., the value 1.999s was rendered as 1.1s instead of the expected 2.0s. This bug affected the JSON and CSV export formats, and all durations printed in log messages or the status command.

By [@patszt](https://github.com/patszt) in [#2906](https://github.com/tenzir/tenzir/pull/2906).

#### Fix linux PID file check on startup

[Section titled “Fix linux PID file check on startup”](#fix-linux-pid-file-check-on-startup)

VAST no longer ignores existing PID lock files on Linux.

By [@lava](https://github.com/lava) in [#2861](https://github.com/tenzir/tenzir/pull/2861).

#### Add a workaround to fix CAF OpenSSL options

[Section titled “Add a workaround to fix CAF OpenSSL options”](#add-a-workaround-to-fix-caf-openssl-options)

Options passed in the `caf.openssl` section in the configuration file or as `VAST_CAF__OPENSSL__*` environment variables are no longer ignored.

By [@tobim](https://github.com/tobim) in [#2908](https://github.com/tenzir/tenzir/pull/2908).

#### Align output of the Zeek TSV reader with schemas

[Section titled “Align output of the Zeek TSV reader with schemas”](#align-output-of-the-zeek-tsv-reader-with-schemas-1)

The Zeek TSV reader now respects the schema files in the bundled `zeek.schema` file, and produces data of the same schema as the Zeek JSON reader. E.g., instead of producing a top-level ip field `id.orig_h`, the reader now produces a top-level record field `id` that contains the ip field `orig_h`, effectively unflattening the data.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2887](https://github.com/tenzir/tenzir/pull/2887).

#### Prevent query\_processor from hanging when there are no candidate partitions

[Section titled “Prevent query\_processor from hanging when there are no candidate partitions”](#prevent-query_processor-from-hanging-when-there-are-no-candidate-partitions)

The VAST client will now terminate properly when using the `count` command with a query which delivers zero results.

By [@Dakostu](https://github.com/Dakostu) in [#2924](https://github.com/tenzir/tenzir/pull/2924).

#### Run start commands asynchronously

[Section titled “Run start commands asynchronously”](#run-start-commands-asynchronously)

The start commands specified with the `vast.start.commands` option are now run aynchronously. This means that commands that block indefinitely will no longer prevent execution of subsequent commands, and allow for correct signal handling.

By [@lava](https://github.com/lava) in [#2868](https://github.com/tenzir/tenzir/pull/2868).

#### Bump CAF to version 0.18.6

[Section titled “Bump CAF to version 0.18.6”](#bump-caf-to-version-0186)

Attempting to connect with thousands of clients around the same time sometimes crashed the VAST server. This no longer occurs.

By [@patszt](https://github.com/patszt) in [#2693](https://github.com/tenzir/tenzir/pull/2693).

#### Remove the transformer actor

[Section titled “Remove the transformer actor”](#remove-the-transformer-actor)

Pipelines that reduce the number of events do not prevent `vast export` processes that have a `max-events` limit from terminating any more.

By [@Dakostu](https://github.com/Dakostu) in [#2896](https://github.com/tenzir/tenzir/pull/2896).

#### Infer non-config types in extend and replace operators

[Section titled “Infer non-config types in extend and replace operators”](#infer-non-config-types-in-extend-and-replace-operators)

The `replace` and `extend` pipeline operators wrongly inferred IP address, subnet, pattern, and map values as strings. They are now inferred correctly. To force a value to be inferred as a string, wrap it inside double quotes.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2768](https://github.com/tenzir/tenzir/pull/2768).

#### Trigger new compaction runs immediately on error

[Section titled “Trigger new compaction runs immediately on error”](#trigger-new-compaction-runs-immediately-on-error)

Compaction now retries immediately on failure instead of waiting for the configured scan interval to expire again.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3006](https://github.com/tenzir/tenzir/pull/3006).

#### Fix some shutdown issues in the web plugin

[Section titled “Fix some shutdown issues in the web plugin”](#fix-some-shutdown-issues-in-the-web-plugin)

The web plugin now reacts correctly to CTRL-C by stopping itself.

By [@lava](https://github.com/lava) in [#2860](https://github.com/tenzir/tenzir/pull/2860).

#### Fix infinite recursion in the record algebra parser

[Section titled “Fix infinite recursion in the record algebra parser”](#fix-infinite-recursion-in-the-record-algebra-parser)

VAST no longer crashes when it encounters an invalid type expression in a schema.

By [@tobim](https://github.com/tobim) in [#2977](https://github.com/tenzir/tenzir/pull/2977).

#### Introduce a potpourri of smaller improvements

[Section titled “Introduce a potpourri of smaller improvements”](#introduce-a-potpourri-of-smaller-improvements-2)

VAST now shuts down instantly when metrics are enabled instead of being held alive for up to the duration of the telemetry interval (10 seconds).

By [@dominiklohmann](https://github.com/dominiklohmann) in [#2832](https://github.com/tenzir/tenzir/pull/2832).

# VAST v3.0.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v3.0.1).

### Features

[Section titled “Features”](#features)

#### Allow `/* ... */` comments in the VAST language

[Section titled “Allow /\* ... \*/ comments in the VAST language”](#allow----comments-in-the-vast-language)

The VAST language now supports comments using the familiar `/* comment */` notation. This makes it easy to document multi-line pipelines inline.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3011](https://github.com/tenzir/tenzir/pull/3011).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Include outdated but not undersized partitions in automatic rebuilds

[Section titled “Include outdated but not undersized partitions in automatic rebuilds”](#include-outdated-but-not-undersized-partitions-in-automatic-rebuilds)

Automatic partition rebuilding both updates partitions with an outdated storage format and merges undersized partitions continuously in the background. This now also works as expected for outdated but not undersized partitions.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3020](https://github.com/tenzir/tenzir/pull/3020).

#### Avoid crashing when reading a pre-2.0 partition

[Section titled “Avoid crashing when reading a pre-2.0 partition”](#avoid-crashing-when-reading-a-pre-20-partition)

VAST no longer crashes when reading an unsupported partition from VAST v1.x. Instead, the partition is ignored correctly. Since v2.2 VAST automatically rebuilds partitions in the background to ensure compatibility.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3018](https://github.com/tenzir/tenzir/pull/3018).

# VAST v3.0.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v3.0.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix usage of moved from variable in rebuild metrics

[Section titled “Fix usage of moved from variable in rebuild metrics”](#fix-usage-of-moved-from-variable-in-rebuild-metrics)

VAST no longer miscalculates the `rebuild` metrics.

By [@patszt](https://github.com/patszt) in [#3026](https://github.com/tenzir/tenzir/pull/3026).

# VAST v3.0.3

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v3.0.3).

### Features

[Section titled “Features”](#features)

#### PRs 3004-3010

[Section titled “PRs 3004-3010”](#prs-3004-3010)

The new `vast exec` command executes a pipeline locally. It takes a single argument representing a closed pipeline, and immediately executes it. This is the foundation for a new, pipeline-first VAST, in which most operations are expressed as pipelines.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3004](https://github.com/tenzir/tenzir/pull/3004).

### Changes

[Section titled “Changes”](#changes)

#### Reduce conflict potential between rebuilding and queries

[Section titled “Reduce conflict potential between rebuilding and queries”](#reduce-conflict-potential-between-rebuilding-and-queries)

VAST’s rebuilding and compaction features now interfere less with queries. This patch was also backported as [VAST v2.4.2](https://vast.io/changelog#v242) to enable a smoother upgrade from to VAST v3.x.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3047](https://github.com/tenzir/tenzir/pull/3047).

#### Add Boost as a dependency

[Section titled “Add Boost as a dependency”](#add-boost-as-a-dependency)

VAST now depends on the Boost C++ libraries.

By [@tobim](https://github.com/tobim) in [#3043](https://github.com/tenzir/tenzir/pull/3043).

# VAST v3.0.4

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v3.0.4).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix partition selection for the `rebuild` command

[Section titled “Fix partition selection for the rebuild command”](#fix-partition-selection-for-the-rebuild-command)

Automatic rebuilds now correctly consider only outdated or undersized partitions.

The `--all` flag for the `rebuild` command now consistently causes all partitions to be rebuilt, aligning its functionality with its documentation.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3083](https://github.com/tenzir/tenzir/pull/3083).

# VAST v3.1.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v3.1.0).

### Features

[Section titled “Features”](#features)

#### Introduce a `version` source operator

[Section titled “Introduce a version source operator”](#introduce-a-version-source-operator)

The `vast exec` command now supports implicit sinks for pipelines that end in events or bytes: `write json --pretty` and `save file -`, respectively.

The `--pretty` option for the JSON printer enables multi-line output.

The new `version` source operator yields a single event containing VAST’s version and a list of enabled plugins.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3123](https://github.com/tenzir/tenzir/pull/3123).

#### Implement the `measure` operator

[Section titled “Implement the measure operator”](#implement-the-measure-operator)

The `inspect` operator replaces the events or bytes it receives with incremental metrics describing the input.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3093](https://github.com/tenzir/tenzir/pull/3093).

#### PRs 3036-3039-3089

[Section titled “PRs 3036-3039-3089”](#prs-3036-3039-3089)

The `put` operator is the new companion to the existing `extend` and `replace` operators. It specifies the output fields exactly, referring either to input fields with an extractor, metadata with a selector, or a fixed value.

The `extend` and `replace` operators now support assigning extractors and selectors in addition to just fixed values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3036](https://github.com/tenzir/tenzir/pull/3036).

#### Add `directory` saver

[Section titled “Add directory saver”](#add-directory-saver)

The new `directory` sink creates a directory with a file for each schema in the specified format.

By [@Dakostu](https://github.com/Dakostu) in [#3098](https://github.com/tenzir/tenzir/pull/3098).

#### Introduce the `count_distinct` aggregation function

[Section titled “Introduce the count\_distinct aggregation function”](#introduce-the-count_distinct-aggregation-function)

The `count_distinct` aggregation function returns the number of distinct, non-null values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3068](https://github.com/tenzir/tenzir/pull/3068).

#### Expose the lower-level `load`, `parse`, `print`, and `save` operators

[Section titled “Expose the lower-level load, parse, print, and save operators”](#expose-the-lower-level-load-parse-print-and-save-operators)

The new `from <connector> [read <format>]`, `read <format> [from <connector>]`, `write <format> [to <connector>]`, and `to <connector> [write <format>]` operators bring together a connector and a format to prduce and consume events, respectively. Their lower-level building blocks `load <connector>`, `parse <format>`, `print <format>`, and `save <connector>` enable expert users to operate on raw byte streams directly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3079](https://github.com/tenzir/tenzir/pull/3079).

#### Add `unique` operator

[Section titled “Add unique operator”](#add-unique-operator)

The newly-added `unique` operator removes adjacent duplicates.

By [@jachris](https://github.com/jachris) in [#3051](https://github.com/tenzir/tenzir/pull/3051).

#### Add Feather and Parquet parsers and printers

[Section titled “Add Feather and Parquet parsers and printers”](#add-feather-and-parquet-parsers-and-printers)

The `feather` and `parquet` formats allow for reading and writing events from and to the Apache Feather V2 and Apache Parquet files, respectively.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3103](https://github.com/tenzir/tenzir/pull/3103).

#### Implement `xsv` parser & printer

[Section titled “Implement xsv parser & printer”](#implement-xsv-parser--printer)

The `xsv` format enables the user to parse and print character-separated values, with the additional `csv`, `tsv` and `ssv` formats as sane defaults.

By [@Dakostu](https://github.com/Dakostu) in [#3104](https://github.com/tenzir/tenzir/pull/3104).

#### Implement a distributed pipeline executor

[Section titled “Implement a distributed pipeline executor”](#implement-a-distributed-pipeline-executor)

Pipelines may now span across multiple processes. This will enable upcoming operators that do not just run locally in the `vast exec` process, but rather connect to a VAST node and partially run in that node. The new operator modifiers `remote` and `local` allow expert users to control where parts of their pipeline run explicitly, e.g., to offload compute to a more powerful node. Potentially unsafe use of these modifiers requires setting `vast.allow-unsafe-pipelines` to `true` in the configuration file.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3119](https://github.com/tenzir/tenzir/pull/3119).

#### Add new CEF parser plugin

[Section titled “Add new CEF parser plugin”](#add-new-cef-parser-plugin)

The `cef` parser allows for using the CEF format with the new pipelines.

By [@jachris](https://github.com/jachris) in [#3110](https://github.com/tenzir/tenzir/pull/3110).

#### Upgrade exporter to use new pipelines

[Section titled “Upgrade exporter to use new pipelines”](#upgrade-exporter-to-use-new-pipelines)

The `vast export` command now accepts the new pipelines as input. Furthermore, `vast export <expr>` is now deprecated in favor of `vast export 'where <expr>'`.

By [@jachris](https://github.com/jachris) in [#3076](https://github.com/tenzir/tenzir/pull/3076).

#### Implement a `zeek-tsv` format

[Section titled “Implement a zeek-tsv format”](#implement-a-zeek-tsv-format)

The `zeek-tsv` format parses and prints Zeek’s native tab-separated value (TSV) representation of logs.

By [@Dakostu](https://github.com/Dakostu) in [#3114](https://github.com/tenzir/tenzir/pull/3114).

#### Upgrade partition transformer to new pipelines

[Section titled “Upgrade partition transformer to new pipelines”](#upgrade-partition-transformer-to-new-pipelines)

User-defined operator aliases make pipelines easier to use by enabling users to encapsulate a pipelinea into a new operator. To define a user-defined operator alias, add an entry to the `vast.operators` section of your configuration.

Compaction now makes use of the new pipeline operators, and allows pipelines to be defined inline instead in addition to the now deprecated `vast.pipelines` configuration section.

By [@jachris](https://github.com/jachris) in [#3064](https://github.com/tenzir/tenzir/pull/3064).

#### Add `tail` operator

[Section titled “Add tail operator”](#add-tail-operator)

The new `tail` pipeline operator limits all latest events to a specified number. The operator takes the limit as an optional argument, with the default value being 10.

By [@Dakostu](https://github.com/Dakostu) in [#3050](https://github.com/tenzir/tenzir/pull/3050).

#### PRs 3085-3088-3097

[Section titled “PRs 3085-3088-3097”](#prs-3085-3088-3097)

The new `file` connector enables the user to process file input/output as data in a pipeline. This includes regular files, UDS files as well as `stdin/stdout`.

By [@jachris](https://github.com/jachris) in [#3085](https://github.com/tenzir/tenzir/pull/3085).

### Changes

[Section titled “Changes”](#changes)

#### Restart the systemd service on failure

[Section titled “Restart the systemd service on failure”](#restart-the-systemd-service-on-failure)

The bundled systemd service is now configured to restart VAST in case of a failure.

By [@tobim](https://github.com/tobim) in [#3058](https://github.com/tenzir/tenzir/pull/3058).

#### Add support for user-defined operator aliases

[Section titled “Add support for user-defined operator aliases”](#add-support-for-user-defined-operator-aliases)

The `vast.operators` section in the configuration file supersedes the now deprecated `vast.pipelines` section and more generally enables user-defined operators. Defined operators now must use the new, textual format introduced with VAST v3.0, and are available for use in all places where pipelines are supported.

By [@jachris](https://github.com/jachris) in [#3067](https://github.com/tenzir/tenzir/pull/3067).

#### Upgrade exporter to use new pipelines

[Section titled “Upgrade exporter to use new pipelines”](#upgrade-exporter-to-use-new-pipelines-1)

The `exporter.*` metrics no longer exist, and will return in a future release as a more generic instrumentation mechanism for all pipelines.

By [@jachris](https://github.com/jachris) in [#3076](https://github.com/tenzir/tenzir/pull/3076).

#### Update query endpoint to use new pipeline executor

[Section titled “Update query endpoint to use new pipeline executor”](#update-query-endpoint-to-use-new-pipeline-executor)

The `/query` REST endpoint no longer accepts an expression at the start of the query. Instead, use `where <expr> | ...`.

By [@jachris](https://github.com/jachris) in [#3015](https://github.com/tenzir/tenzir/pull/3015).

#### Remove configuration-defined import/export pipelines

[Section titled “Remove configuration-defined import/export pipelines”](#remove-configuration-defined-importexport-pipelines)

As already announced with the VAST v3.0 release, the `vast.pipeline-triggers` option now no longer functions. The feature will be replaced with node ingress/egress pipelines that fit better into a multi-node model than the previous feature that was built under the assumption of a client/server model with a single server.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3052](https://github.com/tenzir/tenzir/pull/3052).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Introduce the `count_distinct` aggregation function

[Section titled “Introduce the count\_distinct aggregation function”](#introduce-the-count_distinct-aggregation-function-1)

The `distinct` function silently performed a different operation on lists, returning the distinct non-null elements in the list rather than operating on the list itself. This special-casing no longer exists, and instead the function now operates on the lists itself. This feature will return in the future as unnesting on the extractor level via `distinct(field[])`, but for now it has to go to make the `distinct` aggregation function work consistently.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3068](https://github.com/tenzir/tenzir/pull/3068).

#### Mark some CAF types as nodiscard

[Section titled “Mark some CAF types as nodiscard”](#mark-some-caf-types-as-nodiscard)

Tokens created with `vast web generate-token` now persist correctly, and work across restarts of VAST.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3086](https://github.com/tenzir/tenzir/pull/3086).

#### Fix subnet queries for some subnets

[Section titled “Fix subnet queries for some subnets”](#fix-subnet-queries-for-some-subnets)

VAST incorrectly handled subnets using IPv6 addresses for which an equivalent IPv4 address existed. This is now done correctly. For example, the query `where :ip !in ::ffff:0:0/96` now returns all events containing an IP address that cannot be represented as an IPv4 address. As an additional safeguard, the VAST language no longer allows for constructing subnets for IPv4 addresses with lengths greater than 32.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3060](https://github.com/tenzir/tenzir/pull/3060).

#### Set minimum timestamp of partitions properly

[Section titled “Set minimum timestamp of partitions properly”](#set-minimum-timestamp-of-partitions-properly)

Some pipelines in compaction caused transformed partitions to be treated as if they were older than they were supposed to be, causing them to be picked up again for deletion too early. This bug no longer exists, and compacted partitions are now considered at most as old as the oldest event before compaction.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3141](https://github.com/tenzir/tenzir/pull/3141).

#### Align endpoints between regular and slim Docker images

[Section titled “Align endpoints between regular and slim Docker images”](#align-endpoints-between-regular-and-slim-docker-images)

The `tenzir/vast` image now listens on `0.0.0.0:5158` instead of `127.0.0.1:5158` by default, which aligns the behavior with the `tenzir/vast-slim` image.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3137](https://github.com/tenzir/tenzir/pull/3137).

#### Fix remaining partitions counter in the rebuilder

[Section titled “Fix remaining partitions counter in the rebuilder”](#fix-remaining-partitions-counter-in-the-rebuilder)

The `rebuilder.partitions.remaining` metric sometimes reported wrong values when partitions for at least one schema did not need to be rebuilt. We aligned the metrics with the actual functionality.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3147](https://github.com/tenzir/tenzir/pull/3147).

#### Bump vast-plugins to a95e420

[Section titled “Bump vast-plugins to a95e420”](#bump-vast-plugins-to-a95e420)

The matcher plugin no longer causes deadlocks through detached matcher clients.

By [@tobim](https://github.com/tobim) in [#3115](https://github.com/tenzir/tenzir/pull/3115).

# Tenzir Node v4.0.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.0.0).

### Features

[Section titled “Features”](#features)

#### Add the enumerate operator

[Section titled “Add the enumerate operator”](#add-the-enumerate-operator)

The new `enumerate` operator prepends a column with the row number of the input records.

By [@mavam](https://github.com/mavam) in [#3142](https://github.com/tenzir/tenzir/pull/3142).

#### Add colors to JSON printer

[Section titled “Add colors to JSON printer”](#add-colors-to-json-printer)

The `json` printer can now colorize its output by providing the `-C|--color-output` option, and explicitly disable coloring via `-M|--monochrome-output`.

By [@mavam](https://github.com/mavam) in [#3343](https://github.com/tenzir/tenzir/pull/3343).

#### Add `show` operator

[Section titled “Add show operator”](#add-show-operator)

The new `show` source operator makes it possible to gather meta information about Tenzir. For example, the provided introspection capabilities allow for emitting existing formats, connectors, and operators.

By [@mavam](https://github.com/mavam) in [#3414](https://github.com/tenzir/tenzir/pull/3414).

#### Rename `#type` to `#schema` and introduce `#schema_id`

[Section titled “Rename #type to #schema and introduce #schema\_id”](#rename-type-to-schema-and-introduce-schema_id)

The new `#schema_id` meta extractor returns a unique fingerprint for the schema.

By [@jachris](https://github.com/jachris) in [#3183](https://github.com/tenzir/tenzir/pull/3183).

#### Expose the `batch` operator underlying rebuild

[Section titled “Expose the batch operator underlying rebuild”](#expose-the-batch-operator-underlying-rebuild)

The `batch <limit>` operator allows expert users to control batch sizes in pipelines explicitly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3391](https://github.com/tenzir/tenzir/pull/3391).

#### Revamp packet acquisition and parsing

[Section titled “Revamp packet acquisition and parsing”](#revamp-packet-acquisition-and-parsing)

The new `nic` plugin provides a loader that acquires packets from a network interface card using libpcap. It emits chunks of data in the PCAP file format so that the `pcap` parser can process them as if packets come from a trace file.

The new `decapsulate` operator processes events of type `pcap.packet` and emits new events of type `tenzir.packet` that contain the decapsulated PCAP packet with packet header fields from the link, network, and transport layer. The operator also computes a Community ID.

By [@mavam](https://github.com/mavam) in [#3263](https://github.com/tenzir/tenzir/pull/3263).

#### Add —append and —real-time to directory saver

[Section titled “Add —append and —real-time to directory saver”](#add-append-and-real-time-to-directory-saver)

The `directory` saver now supports the two arguments `-a|--append` and `-r|--realtime` that have the same semantics as they have for the `file` saver: open files in the directory in append mode (instead of overwriting) and flush the output buffers on every update.

By [@mavam](https://github.com/mavam) in [#3379](https://github.com/tenzir/tenzir/pull/3379).

#### Use `load -` and `read json` as implicit sources

[Section titled “Use load - and read json as implicit sources”](#use-load---and-read-json-as-implicit-sources)

Pipelines executed locally with `tenzir` now use `load -` and `read json` as implicit sources. This complements `save -` and `write json --pretty` as implicit sinks.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3329](https://github.com/tenzir/tenzir/pull/3329).

#### Fix sporadic stalling of pipelines

[Section titled “Fix sporadic stalling of pipelines”](#fix-sporadic-stalling-of-pipelines)

The pipeline manager now accepts empty strings for the optional `name`. The `/create` endpoint returns a list of diagnostics if pipeline creation fails, and if `start_when_created` is set, the endpoint now returns only after the pipeline execution has been fully started. The `/list` endpoint now returns the diagnostics collected for every pipeline so far. The `/delete` endpoint now returns an empty object if the request is successful.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3264](https://github.com/tenzir/tenzir/pull/3264).

#### Add a `--schema` option to the JSON parser

[Section titled “Add a --schema option to the JSON parser”](#add-a---schema-option-to-the-json-parser)

The `--schema` option for the JSON parser allows for setting the target schema explicitly by name.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3295](https://github.com/tenzir/tenzir/pull/3295).

#### Expose pipeline operator metrics in execution node and pipeline executor

[Section titled “Expose pipeline operator metrics in execution node and pipeline executor”](#expose-pipeline-operator-metrics-in-execution-node-and-pipeline-executor)

Pipeline metrics (total ingress/egress amount and average rate per second) are now visible in the `pipeline-manager`, via the `metrics` field in the `/pipeline/list` endpoint result.

By [@Dakostu](https://github.com/Dakostu) in [#3376](https://github.com/tenzir/tenzir/pull/3376).

#### Implement `top` and `rare`

[Section titled “Implement top and rare”](#implement-top-and-rare)

The `top <field>` operator makes it easy to find the most common values for the given field. Likewise, `rare <field>` returns the least common values for the given field.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3176](https://github.com/tenzir/tenzir/pull/3176).

#### Implement the `unflatten` operator

[Section titled “Implement the unflatten operator”](#implement-the-unflatten-operator)

The `unflatten [<separator>]` operator unflattens data structures by creating nested records out of fields whose names contain a `<separator>`.

By [@Dakostu](https://github.com/Dakostu) in [#3304](https://github.com/tenzir/tenzir/pull/3304).

#### Implement a `sort` operator

[Section titled “Implement a sort operator”](#implement-a-sort-operator)

The new `sort` operator allows for arranging events by field, in ascending and descending order. The current version is still “beta” and has known limitations.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3155](https://github.com/tenzir/tenzir/pull/3155).

#### Add a `--cumulative` option to the `measure` operator

[Section titled “Add a --cumulative option to the measure operator”](#add-a---cumulative-option-to-the-measure-operator)

The `measure` operator now returns running totals with the `--cumulative` option.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3156](https://github.com/tenzir/tenzir/pull/3156).

#### Change `summarize` to operate across schemas

[Section titled “Change summarize to operate across schemas”](#change-summarize-to-operate-across-schemas)

The `summarize` operator now works across multiple schemas and can combine events of different schemas into one group. It now also treats missing columns as having `null` values.

The `by` clause of `summarize` is now optional. If it is omitted, all events are assigned to the same group.

By [@jachris](https://github.com/jachris) in [#3250](https://github.com/tenzir/tenzir/pull/3250).

#### Add diagnostics (and some other improvements)

[Section titled “Add diagnostics (and some other improvements)”](#add-diagnostics-and-some-other-improvements)

In addition to `tenzir "<pipeline>"`, there now is `tenzir -f <file>`, which loads and executes the pipeline defined in the given file.

The pipeline parser now emits helpful and visually pleasing diagnostics.

By [@jachris](https://github.com/jachris) in [#3223](https://github.com/tenzir/tenzir/pull/3223).

#### Implement the `serve` operator and `/serve` endpoint

[Section titled “Implement the serve operator and /serve endpoint”](#implement-the-serve-operator-and-serve-endpoint)

The `serve` operator and `/serve` endpoint supersede the experimental `/query` endpoint. The operator is a sink for events, and bridges a pipeline into a RESTful interface from which events can be pulled incrementally.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3180](https://github.com/tenzir/tenzir/pull/3180).

#### Apply the changes from the new `pipeline_manager` plugin

[Section titled “Apply the changes from the new pipeline\_manager plugin”](#apply-the-changes-from-the-new-pipeline_manager-plugin)

The new *pipeline-manager* is a proprietary plugin that allows for creating, updating and persisting pipelines. The included RESTful interface allows for easy access and modification of these pipelines.

By [@Dakostu](https://github.com/Dakostu) in [#3164](https://github.com/tenzir/tenzir/pull/3164).

#### Implement a fallback parser mechanism for extensions that don’t have …

[Section titled “Implement a fallback parser mechanism for extensions that don’t have …”](#implement-a-fallback-parser-mechanism-for-extensions-that-dont-have)

The `json` parser now servers as a fallback parser for all files whose extension do not have any default parser in Tenzir.

By [@Dakostu](https://github.com/Dakostu) in [#3422](https://github.com/tenzir/tenzir/pull/3422).

#### Avoid crashing when reading a pre-2.0 partition

[Section titled “Avoid crashing when reading a pre-2.0 partition”](#avoid-crashing-when-reading-a-pre-20-partition)

The `flatten [<separator>]` operator flattens nested data structures by joining nested records with the specified separator (defaults to `.`) and merging lists.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3018](https://github.com/tenzir/tenzir/pull/3018).

#### PRs 3128-3173-3193

[Section titled “PRs 3128-3173-3193”](#prs-3128-3173-3193)

The sink operator `import` persists events in a VAST node.

The source operator `export` retrieves events from a VAST node.

The `repeat` operator repeats its input a given number of times.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3128](https://github.com/tenzir/tenzir/pull/3128).

#### Improve metrics (and some other things)

[Section titled “Improve metrics (and some other things)”](#improve-metrics-and-some-other-things)

The `sort` operator now also works for `ip` and `enum` fields.

`tenzir --dump-metrics '<pipeline>'` prints a performance overview of the executed pipeline on stderr at the end.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3390](https://github.com/tenzir/tenzir/pull/3390).

#### A collection of minor UX improvements

[Section titled “A collection of minor UX improvements”](#a-collection-of-minor-ux-improvements)

The `--timeout` option for the `vast status` command allows for defining how long VAST waits for components to report their status. The option defaults to 10 seconds.

By [@tobim](https://github.com/tobim) in [#3162](https://github.com/tenzir/tenzir/pull/3162).

#### Unroll the Zeek TSV header parsing loop

[Section titled “Unroll the Zeek TSV header parsing loop”](#unroll-the-zeek-tsv-header-parsing-loop)

The `zeek-tsv` parser sometimes failed to parse Zeek TSV logs, wrongly reporting that the header ended too early. This bug no longer exists.

By [@Dakostu](https://github.com/Dakostu) in [#3291](https://github.com/tenzir/tenzir/pull/3291).

### Changes

[Section titled “Changes”](#changes)

#### Rename package artifacts from vast to tenzir

[Section titled “Rename package artifacts from vast to tenzir”](#rename-package-artifacts-from-vast-to-tenzir)

The Debian package for Tenzir replaces previous VAST installations and attempts to migrate existing data from VAST to Tenzir in the process. You can opt-out of this migration by creating the file `/var/lib/vast/disable-migration`.

By [@tobim](https://github.com/tobim) in [#3203](https://github.com/tenzir/tenzir/pull/3203).

#### Change Arrow extension type and metadata prefixes

[Section titled “Change Arrow extension type and metadata prefixes”](#change-arrow-extension-type-and-metadata-prefixes)

We now register extension types as `tenzir.ip`, `tenzir.subnet`, and `tenzir.enumeration` instead of `vast.address`, `vast.subnet`, and `vast.enumeration`, respectively. Arrow schema metadata now has a `TENZIR:` prefix instead of a `VAST:` prefix.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3208](https://github.com/tenzir/tenzir/pull/3208).

#### Introduce the `tenzir` and `tenzird` binaries

[Section titled “Introduce the tenzir and tenzird binaries”](#introduce-the-tenzir-and-tenzird-binaries)

VAST is now called Tenzir. The `tenzir` binary replaces `vast exec` to execute a pipeline. The `tenzird` binary replaces `vast start` to start a node. The `tenzirctl` binary continues to offer all functionality that `vast` previously offered until all commands have been migrated to pipeline operators.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3187](https://github.com/tenzir/tenzir/pull/3187).

#### Delete `delete_when_stopped` from the pipeline manager

[Section titled “Delete delete\_when\_stopped from the pipeline manager”](#delete-delete_when_stopped-from-the-pipeline-manager)

The `delete_when_stopped` flag was removed from the pipeline manager REST API.

By [@jachris](https://github.com/jachris) in [#3292](https://github.com/tenzir/tenzir/pull/3292).

#### Transform `read` and `write` into `parse` and `print`

[Section titled “Transform read and write into parse and print”](#transform-read-and-write-into-parse-and-print)

The `parse` and `print` operators have been renamed to `read` and `write`, respectively. The `read ... [from ...]` and `write ... [to ...]` operators are not available anymore. If you did not specify a connector, you can continue using `read ...` and `write ...` in many cases. Otherwise, use `from ... [read ...]` and `to ... [write ...]` instead.

By [@jachris](https://github.com/jachris) in [#3365](https://github.com/tenzir/tenzir/pull/3365).

#### Rename `#type` to `#schema` and introduce `#schema_id`

[Section titled “Rename #type to #schema and introduce #schema\_id”](#rename-type-to-schema-and-introduce-schema_id-1)

The `#type` meta extractor was renamed to `#schema`.

By [@jachris](https://github.com/jachris) in [#3183](https://github.com/tenzir/tenzir/pull/3183).

#### Tune defaults and demo-node experience

[Section titled “Tune defaults and demo-node experience”](#tune-defaults-and-demo-node-experience)

We reduced the default `batch-timeout` from ten seconds to one second in to improve the user experience of interactive pipelines with data aquisition.

We reduced the default `active-partition-timeout` from 5 minutes to 30 seconds to reduce the time until data is persisted.

By [@tobim](https://github.com/tobim) in [#3320](https://github.com/tenzir/tenzir/pull/3320).

#### Remove old commands

[Section titled “Remove old commands”](#remove-old-commands)

The `stop` command no longer exists. Shut down VAST nodes using CTRL-C instead.

The `version` command no longer exists. Use the more powerful `version` pipeline operator instead.

The `spawn source` and `spawn sink` commands no longer exist. To import data remotely, run a pipeline in the form of `remote from … | … | import`, and to export data remotely, run a pipeline in the form of `export | … | remote to …`.

The lower-level `peer`, `kill`, and `send` commands no longer exist.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3166](https://github.com/tenzir/tenzir/pull/3166).

#### Remove `lsvast`

[Section titled “Remove lsvast”](#remove-lsvast)

The debugging utility `lsvast` no longer exists. Pipelines replace most of its functionality.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3211](https://github.com/tenzir/tenzir/pull/3211).

#### Revamp packet acquisition and parsing

[Section titled “Revamp packet acquisition and parsing”](#revamp-packet-acquisition-and-parsing-1)

We reimplemented the old `pcap` plugin as a format. The command `tenzir-ctl import pcap` no longer works. Instead, the new `pcap` plugin provides a parser that emits `pcap.packet` events, as well as a printer that generates a PCAP file when provided with these events.

By [@mavam](https://github.com/mavam) in [#3263](https://github.com/tenzir/tenzir/pull/3263).

#### Add colors to JSON printer

[Section titled “Add colors to JSON printer”](#add-colors-to-json-printer-1)

We removed the `--pretty` option from the `json` printer. This option is now the default. To switch to NDJSON, use `-c|--compact-output`.

By [@mavam](https://github.com/mavam) in [#3343](https://github.com/tenzir/tenzir/pull/3343).

#### Change `summarize` to operate across schemas

[Section titled “Change summarize to operate across schemas”](#change-summarize-to-operate-across-schemas-1)

The aggregation functions in a `summarize` operator can now receive only a single extractor instead of multiple ones.

The behavior for absent columns and aggregations across multiple schemas was changed.

By [@jachris](https://github.com/jachris) in [#3250](https://github.com/tenzir/tenzir/pull/3250).

#### Remove the `prefix()` function from the REST endpoint plugin API

[Section titled “Remove the prefix() function from the REST endpoint plugin API”](#remove-the-prefix-function-from-the-rest-endpoint-plugin-api)

We removed the `rest_endpoint_plugin::prefix()` function from the public API of the `rest_endpoint_plugin` class. For a migration, existing users should prepend the prefix manually to all endpoints defined by their plugin.

By [@lava](https://github.com/lava) in [#3221](https://github.com/tenzir/tenzir/pull/3221).

#### Implement the `serve` operator and `/serve` endpoint

[Section titled “Implement the serve operator and /serve endpoint”](#implement-the-serve-operator-and-serve-endpoint-1)

The default port of the web plugin changed from 42001 to 5160. This change avoids collisions from dynamic port allocation on Linux systems.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3180](https://github.com/tenzir/tenzir/pull/3180).

#### Switch /status to POST

[Section titled “Switch /status to POST”](#switch-status-to-post)

The HTTP method of the status endpoint in the experimental REST API is now `POST`.

By [@tobim](https://github.com/tobim) in [#3194](https://github.com/tenzir/tenzir/pull/3194).

#### Add diagnostics (and some other improvements)

[Section titled “Add diagnostics (and some other improvements)”](#add-diagnostics-and-some-other-improvements-1)

We changed the default connector of `read <format>` and `write <format>` for all formats to `stdin` and `stdout`, respectively.

We removed language plugins in favor of operator-based integrations.

The interface of the operator, loader, parser, printer and saver plugins was changed.

By [@jachris](https://github.com/jachris) in [#3223](https://github.com/tenzir/tenzir/pull/3223).

#### Improve low-load memory consumption

[Section titled “Improve low-load memory consumption”](#improve-low-load-memory-consumption)

The default interval between two automatic rebuilds is now set to 2 hours and can be configured with the `rebuild-interval` option.

By [@tobim](https://github.com/tobim) in [#3377](https://github.com/tenzir/tenzir/pull/3377).

#### Remove previously deprecated options

[Section titled “Remove previously deprecated options”](#remove-previously-deprecated-options)

The previously deprecated options `tenzir.pipelines` (replaced with `tenzir.operators`) and `tenzir.pipeline-triggers` (no replacement) no longer exist.

The previously deprecated deprecated types `addr`, `count`, `int`, and `real` (replaced with `ip`, `uint64`, `int64`, and `double`, respectively) no longer exist.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3358](https://github.com/tenzir/tenzir/pull/3358).

#### Rename default database directory to `tenzir.db`

[Section titled “Rename default database directory to tenzir.db”](#rename-default-database-directory-to-tenzirdb)

The default database directory moved from `vast.db` to `tenzir.db`. Use the option `tenzir.db-directory` to manually set the database directory path.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3212](https://github.com/tenzir/tenzir/pull/3212).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix shutdown of sources and importer

[Section titled “Fix shutdown of sources and importer”](#fix-shutdown-of-sources-and-importer)

Import processes sometimes failed to shut down automatically when the node exited. They now shut down reliably.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3207](https://github.com/tenzir/tenzir/pull/3207).

#### Fix rare crash when transforming sliced nested arrays

[Section titled “Fix rare crash when transforming sliced nested arrays”](#fix-rare-crash-when-transforming-sliced-nested-arrays)

Using transformation operators like `summarize`, `sort`, `put`, `extend`, or `replace` no longer sometimes crashes after a preceding `head` or `tail` operator when referencing a nested field.

The `tail` operator sometimes returned more events than specified. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3171](https://github.com/tenzir/tenzir/pull/3171).

#### Add a changelog entry for the compaction fix

[Section titled “Add a changelog entry for the compaction fix”](#add-a-changelog-entry-for-the-compaction-fix)

We fixed a bug in the compation plugin that prevented it from applying the configured weights when it was used for the first time on a database.

By [@tobim](https://github.com/tobim) in [#3185](https://github.com/tenzir/tenzir/pull/3185).

#### Fix reconnect attempts for remote pipelines

[Section titled “Fix reconnect attempts for remote pipelines”](#fix-reconnect-attempts-for-remote-pipelines)

Starting a remote pipeline with `vast exec` failed when the node was not reachable yet. Like other commands, executing a pipeline now waits until the node is reachable before starting.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3188](https://github.com/tenzir/tenzir/pull/3188).

# Tenzir Node v4.0.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.0.1).

### Features

[Section titled “Features”](#features)

#### Implement \`replace

[Section titled “Implement \`replace”](#implement-replace)

It is now possible to replace the schema name with `replace #schema="new_name"`.

By [@jachris](https://github.com/jachris) in [#3451](https://github.com/tenzir/tenzir/pull/3451).

# Tenzir Node v4.1.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.1.0).

### Features

[Section titled “Features”](#features)

#### Make `show pipelines` a thing

[Section titled “Make show pipelines a thing”](#make-show-pipelines-a-thing)

The new `show pipelines` aspect displays a list of all managed pipelines.

By [@mavam](https://github.com/mavam) in [#3457](https://github.com/tenzir/tenzir/pull/3457).

#### Bump the tenzir-plugins submodule pointer to include the pipeline manager’s failure and rendered diagnostics functionality

[Section titled “Bump the tenzir-plugins submodule pointer to include the pipeline manager’s failure and rendered diagnostics functionality”](#bump-the-tenzir-plugins-submodule-pointer-to-include-the-pipeline-managers-failure-and-rendered-diagnostics-functionality)

The `rendered` field in the pipeline manager diagnostics delivers a displayable version of the diagnostic’s error message.

Pipelines that encounter an error during execution are now in a new `failed` rather than `stopped` state.

By [@Dakostu](https://github.com/Dakostu) in [#3479](https://github.com/tenzir/tenzir/pull/3479).

#### Introduce an experimental `sigma` operator

[Section titled “Introduce an experimental sigma operator”](#introduce-an-experimental-sigma-operator)

The new `sigma` operator filters its input with Sigma rules and outputs matching events alongside the matched rule.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3138](https://github.com/tenzir/tenzir/pull/3138).

#### Implement `compress` and `decompress` operators

[Section titled “Implement compress and decompress operators”](#implement-compress-and-decompress-operators)

The `compress [--level <level>] <codec>` and `decompress <codec>` operators enable streaming compression and decompression in pipelines for `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3443](https://github.com/tenzir/tenzir/pull/3443).

#### Bump the tenzir-plugins submodule pointer to include the pipeline manager’s resuming and pausing functionality

[Section titled “Bump the tenzir-plugins submodule pointer to include the pipeline manager’s resuming and pausing functionality”](#bump-the-tenzir-plugins-submodule-pointer-to-include-the-pipeline-managers-resuming-and-pausing-functionality)

The `pause` action in the `/pipeline/update` endpoint suspends a pipeline and sets its state to `paused`. Resume it with the `start` action.

Newly created pipelines are now in a new `created` rather than `stopped` state.

By [@Dakostu](https://github.com/Dakostu) in [#3471](https://github.com/tenzir/tenzir/pull/3471).

#### Implement `show config`

[Section titled “Implement show config”](#implement-show-config)

The `show config` aspect returns the configuration currently in use, combining options set in the configuration file, the command-line, environment options.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3455](https://github.com/tenzir/tenzir/pull/3455).

### Changes

[Section titled “Changes”](#changes)

#### Refactor `show` aspects and rewrite `version`

[Section titled “Refactor show aspects and rewrite version”](#refactor-show-aspects-and-rewrite-version)

The `version` operator no longer exists. Use `show version` to get the Tenzir version instead. The additional information that `version` produced is now available as `show build`, `show dependencies`, and `show plugins`.

By [@mavam](https://github.com/mavam) in [#3442](https://github.com/tenzir/tenzir/pull/3442).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Tweak the execution node behavior

[Section titled “Tweak the execution node behavior”](#tweak-the-execution-node-behavior)

Pipeline operators that create output independent of their input now emit their output instantly instead of waiting for receiving further input. This makes the `shell` operator more reliable.

The `show <aspect>` operator wrongfully required unsafe pipelines to be allowed for some aspects. This is now fixed.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3470](https://github.com/tenzir/tenzir/pull/3470).

# Tenzir Node v4.10.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.10.0).

### Features

[Section titled “Features”](#features)

#### Fix partial specialization of S3 configuration in URL

[Section titled “Fix partial specialization of S3 configuration in URL”](#fix-partial-specialization-of-s3-configuration-in-url)

S3 access and secret keys can now be specified in the S3 plugin’s configuration file.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4001](https://github.com/tenzir/tenzir/pull/4001).

#### Add configurable pipelines

[Section titled “Add configurable pipelines”](#add-configurable-pipelines)

We made it possible to set pipelines declaratively in the `tenzir.yaml` configuration file.

By [@tobim](https://github.com/tobim) in [#4006](https://github.com/tenzir/tenzir/pull/4006).

#### Allow using `and`, `or`, and `not` in expressions

[Section titled “Allow using and, or, and not in expressions”](#allow-using-and-or-and-not-in-expressions)

The `where` operator now supports using `and`, `or`, and `not` as alternatives to `&&`, `||`, and `!` in expressions.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3993](https://github.com/tenzir/tenzir/pull/3993).

#### Introduce multi-arch Docker images

[Section titled “Introduce multi-arch Docker images”](#introduce-multi-arch-docker-images)

The `tenzir/tenzir` and `tenzir/tenzir-node` Docker images now run natively on arm64 in addition to amd64.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3989](https://github.com/tenzir/tenzir/pull/3989).

#### Implement multi-field lookups for context implementations

[Section titled “Implement multi-field lookups for context implementations”](#implement-multi-field-lookups-for-context-implementations)

The `enrich` and `lookup` operators now support type extractors, concepts, and comma-separated lists of fields as arguments to `--field`.

By [@Dakostu](https://github.com/Dakostu) in [#3968](https://github.com/tenzir/tenzir/pull/3968).

### Changes

[Section titled “Changes”](#changes)

#### Improve metrics collection

[Section titled “Improve metrics collection”](#improve-metrics-collection)

Nodes now collect CPU, disk, memory, and process metrics every second instead of every ten seconds, improving the usability of metrics with the `chart` operator. Memory metrics now work as expected on macOS.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3982](https://github.com/tenzir/tenzir/pull/3982).

#### Introduce the `no-location-overrides` option

[Section titled “Introduce the no-location-overrides option”](#introduce-the-no-location-overrides-option)

We’ve replaced the `tenzir.allow-unsafe-pipelines` option with the `tenzir.no-location-overrides` option with an inverted default. The new option is a less confusing default for new users and more accurately describes what the option does, namely preventing operator locations to be overriden.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3978](https://github.com/tenzir/tenzir/pull/3978).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Count nulls and absent values in `top` and `rare`

[Section titled “Count nulls and absent values in top and rare”](#count-nulls-and-absent-values-in-top-and-rare)

The `top` and `rare` operators now correctly count null and absent values. Previously, they emitted a single event with a count of zero when any null or absent values were included in the input.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3990](https://github.com/tenzir/tenzir/pull/3990).

#### Fix a TOCTOU bug that caused the index to fail

[Section titled “Fix a TOCTOU bug that caused the index to fail”](#fix-a-toctou-bug-that-caused-the-index-to-fail)

Tenzir nodes sometimes failed when trying to canonicalize file system paths before opening them when the disk-monitor or compaction rotated them out. This is now handled gracefully.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3994](https://github.com/tenzir/tenzir/pull/3994).

#### Fix partial specialization of S3 configuration in URL

[Section titled “Fix partial specialization of S3 configuration in URL”](#fix-partial-specialization-of-s3-configuration-in-url-1)

The S3 connector no longer ignores the default credentials provider for the current user when any arguments are specified in the URI explicitly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4001](https://github.com/tenzir/tenzir/pull/4001).

#### Fix panic on parsing invalid syslog messages

[Section titled “Fix panic on parsing invalid syslog messages”](#fix-panic-on-parsing-invalid-syslog-messages)

Parsing an invalid syslog message (using the schema `syslog.unknown`) no longer causes a crash.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#4012](https://github.com/tenzir/tenzir/pull/4012).

#### Make `remote python` work

[Section titled “Make remote python work”](#make-remote-python-work)

The `python` operator now works with when using the `remote` location override.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3999](https://github.com/tenzir/tenzir/pull/3999).

#### Prevent tcp socket inheritance to child processes

[Section titled “Prevent tcp socket inheritance to child processes”](#prevent-tcp-socket-inheritance-to-child-processes)

We fixed a problem with the TCP connector that caused pipeline restarts on the same port to fail if running `shell` or `python` operators were present.

By [@tobim](https://github.com/tobim) in [#3998](https://github.com/tenzir/tenzir/pull/3998).

#### Fix crash in `sigma` operator for non-existent file

[Section titled “Fix crash in sigma operator for non-existent file”](#fix-crash-in-sigma-operator-for-non-existent-file)

The `sigma` operator sometimes crashed when pointed to a non-existent file or directory. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4010](https://github.com/tenzir/tenzir/pull/4010).

# Tenzir Node v4.10.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.10.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix crash when using configured pipelines for the first time

[Section titled “Fix crash when using configured pipelines for the first time”](#fix-crash-when-using-configured-pipelines-for-the-first-time)

When upgrading from a previous version to Tenzir v4.10 and using configured pipelines for the first time, the node sometimes crashed on startup. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4020](https://github.com/tenzir/tenzir/pull/4020).

# Tenzir Node v4.10.3

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.10.3).

### Changes

[Section titled “Changes”](#changes)

#### Update the submodule pointers to include periodic platform reconnects

[Section titled “Update the submodule pointers to include periodic platform reconnects”](#update-the-submodule-pointers-to-include-periodic-platform-reconnects)

Tenzir nodes no longer attempt reconnecting to app.tenzir.com immediately upon failure, but rather wait before reconnecting.

By [@Dakostu](https://github.com/Dakostu) in [#3997](https://github.com/tenzir/tenzir/pull/3997).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Ignore events in `lookup` that did not bind

[Section titled “Ignore events in lookup that did not bind”](#ignore-events-in-lookup-that-did-not-bind)

The `lookup` operator no longer tries to match internal metrics and diagnostics events.

The `lookup` operator no longer returns events for which none of the provided fields exist.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4028](https://github.com/tenzir/tenzir/pull/4028).

# Tenzir Node v4.10.4

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.10.4).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix loading of large contexts

[Section titled “Fix loading of large contexts”](#fix-loading-of-large-contexts)

Using `context load` with large context files no longer causes a crash.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#4033](https://github.com/tenzir/tenzir/pull/4033).

#### Isolate environment in python operator

[Section titled “Isolate environment in python operator”](#isolate-environment-in-python-operator)

The code passed to the `python` operator no longer fails to resolve names when the local and global scope are both used.

By [@jachris](https://github.com/jachris) in [#4036](https://github.com/tenzir/tenzir/pull/4036).

#### Fix a crash in the Sigma operator

[Section titled “Fix a crash in the Sigma operator”](#fix-a-crash-in-the-sigma-operator)

The `sigma` operator crashed for some rules when trying to attach the rule to the matched event. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4034](https://github.com/tenzir/tenzir/pull/4034).

#### Improve HTTP transfer abstraction

[Section titled “Improve HTTP transfer abstraction”](#improve-http-transfer-abstraction)

The `http` saver now correctly sets the `Content-Length` header when issuing HTTP requests.

By [@mavam](https://github.com/mavam) in [#4031](https://github.com/tenzir/tenzir/pull/4031).

# Tenzir Node v4.11.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.11.0).

### Features

[Section titled “Features”](#features)

#### Add a `files` source

[Section titled “Add a files source”](#add-a-files-source)

The new `files` source lists file information for a given directory.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4035](https://github.com/tenzir/tenzir/pull/4035).

#### Add the `set` operator for upserting fields

[Section titled “Add the set operator for upserting fields”](#add-the-set-operator-for-upserting-fields)

The new `set` operator upserts fields, i.e., acts like `replace` for existing fields and like `extend` for new fields. It also supports setting the schema name explicitly via `set #schema="new-name"`.

The `put` operator now supports setting the schema name explicitly via `put #schema="new-name"`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4057](https://github.com/tenzir/tenzir/pull/4057).

#### Introduce `--replace`, `--separate`, and `--yield` for contexts

[Section titled “Introduce --replace, --separate, and --yield for contexts”](#introduce---replace---separate-and---yield-for-contexts)

The `--replace` option for the `enrich` operator causes the input values to be replaced with their context instead of extending the event with the context, resulting in a leaner output.

The `--separate` option makes the `enrich` and `lookup` operators handle each field individually, duplicating the event for each relevant field, and returning at most one context per output event.

The `--yield <field>` option allows for adding only a part of a context with the `enrich` and `lookup` operators. For example, with a `geoip` context with a MaxMind country database, `--yield registered_country.iso_code` will cause the enrichment to only consist of the country’s ISO code.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4040](https://github.com/tenzir/tenzir/pull/4040).

#### Introduce the `every` operator modifier

[Section titled “Introduce the every operator modifier”](#introduce-the-every-operator-modifier)

The `every <interval>` operator modifier executes a source operator repeatedly. For example, `every 1h from http://foo.com/bar` polls an endpoint every hour.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4050](https://github.com/tenzir/tenzir/pull/4050).

#### Add SQS Connector

[Section titled “Add SQS Connector”](#add-sqs-connector)

The new `sqs` connector makes it possible to read from and write to Amazon SQS queues.

By [@mavam](https://github.com/mavam) in [#3819](https://github.com/tenzir/tenzir/pull/3819).

#### Add an email saver

[Section titled “Add an email saver”](#add-an-email-saver)

The new `email` saver allows for sending pipeline data via email by connecting to a mail server via SMTP or SMTPS.

By [@mavam](https://github.com/mavam) in [#4041](https://github.com/tenzir/tenzir/pull/4041).

### Changes

[Section titled “Changes”](#changes)

#### Introduce `--replace`, `--separate`, and `--yield` for contexts

[Section titled “Introduce --replace, --separate, and --yield for contexts”](#introduce---replace---separate-and---yield-for-contexts-1)

The `enrich` and `lookup` operators now include the metadata in every context object to accomodate the new `--replace` and `--separate` options. Previously, the metadata was available once in the output field.

The `mode` field in the enrichments returned from the `lookup` operator is now `lookup.retro`, `lookup.live`, or `lookup.snapshot` depending on the mode.

The `bloom-filter` context now always returns `true` or `null` for the context instead of embedding the result in a record with a single `data` field.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4040](https://github.com/tenzir/tenzir/pull/4040).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Introduce `--replace`, `--separate`, and `--yield` for contexts

[Section titled “Introduce --replace, --separate, and --yield for contexts”](#introduce---replace---separate-and---yield-for-contexts-2)

`drop` and `select` silently ignored all but the first match of the specified type extractors and concepts. This no longer happens. For example, `drop :time` drops all fields with type `time` from events.

Enriching a field in adjacent events in `lookup` and `enrich` with a `lookup-table` context sometimes crashed when the lookup-table referred to values of different types.

The `geoip` context sometimes returned incorrect values. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4040](https://github.com/tenzir/tenzir/pull/4040).

#### Add SQS Connector

[Section titled “Add SQS Connector”](#add-sqs-connector-1)

Source operators that do not quit on their own only freed their resources after they had emitted an additional output, even after the pipeline had already exited. This sometimes caused errors when restarting pipelines, and in rare cases caused Tenzir nodes to hang on shutdown. This no longer happens, and the entire pipeline shuts down at once.

By [@mavam](https://github.com/mavam) in [#3819](https://github.com/tenzir/tenzir/pull/3819).

#### Fix disk metrics with custom state directories

[Section titled “Fix disk metrics with custom state directories”](#fix-disk-metrics-with-custom-state-directories)

Disk metrics now work correctly for deployments with a customized state directory.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4058](https://github.com/tenzir/tenzir/pull/4058).

#### Fix `from <url>` with username

[Section titled “Fix from \<url> with username”](#fix-from-url-with-username)

`from <url>` now also works when the url specifies username and password.

By [@jachris](https://github.com/jachris) in [#4043](https://github.com/tenzir/tenzir/pull/4043).

#### Fix an off-by-one error when loading persisted contexts

[Section titled “Fix an off-by-one error when loading persisted contexts”](#fix-an-off-by-one-error-when-loading-persisted-contexts)

We fixed a bug that caused every second context to become unavailable after a restarting the node.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4045](https://github.com/tenzir/tenzir/pull/4045).

#### Fix invalid assertion in `compress` operator

[Section titled “Fix invalid assertion in compress operator”](#fix-invalid-assertion-in-compress-operator)

The `compress` and `to` operators no longer fail when compression is unable to further reduce the size of a batch of bytes.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4048](https://github.com/tenzir/tenzir/pull/4048).

# Tenzir Node v4.11.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.11.2).

### Changes

[Section titled “Changes”](#changes)

#### PRs 4073-satta

[Section titled “PRs 4073-satta”](#prs-4073-satta)

The `python` operator now requires Python 3.9 (down from Python 3.10) or newer, making it available on more systems.

By [@satta](https://github.com/satta) in [#4073](https://github.com/tenzir/tenzir/pull/4073).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fail late in the python operator setup

[Section titled “Fail late in the python operator setup”](#fail-late-in-the-python-operator-setup)

The `python` operator often failed with a 504 Gateway Timeout error on app.tenzir.com when first run. This no longer happens.

By [@tobim](https://github.com/tobim) in [#4066](https://github.com/tenzir/tenzir/pull/4066).

# Tenzir Node v4.12.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.12.0).

### Features

[Section titled “Features”](#features)

#### PRs 4133-4138-satta

[Section titled “PRs 4133-4138-satta”](#prs-4133-4138-satta)

The `suricata` parser’s schema now more accurately reflects Suricata’s Eve JSON output, adding many fields that were previously missing.

By [@satta](https://github.com/satta) in [#4133](https://github.com/tenzir/tenzir/pull/4133).

#### Add value grouping to `chart` and remove `--title`

[Section titled “Add value grouping to chart and remove --title”](#add-value-grouping-to-chart-and-remove---title)

Some charts supported by the `chart` operator (`bar`, `line`, and `area`) now have a `--position` argument, with the possible values of `grouped` and `stacked`.

By [@jachris](https://github.com/jachris) in [#4119](https://github.com/tenzir/tenzir/pull/4119).

#### Add a `--timeout <duration>` option to `batch`

[Section titled “Add a --timeout \<duration> option to batch”](#add-a---timeout-duration-option-to-batch)

The `batch` operator gained a new `--timeout <duration>` option that controls the maixmum latency for withholding events for batching.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4095](https://github.com/tenzir/tenzir/pull/4095).

#### Add configurable contexts

[Section titled “Add configurable contexts”](#add-configurable-contexts)

You can now define contexts and their creation parameters in the `tenzir.contexts` section of the configuration file.

By [@tobim](https://github.com/tobim) in [#4126](https://github.com/tenzir/tenzir/pull/4126).

#### Add `show schemas` to display all available schemas

[Section titled “Add show schemas to display all available schemas”](#add-show-schemas-to-display-all-available-schemas)

The `show schemas` operator lists all unique schemas of events stored at the node.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4131](https://github.com/tenzir/tenzir/pull/4131).

#### Introduce the BITZ format

[Section titled “Introduce the BITZ format”](#introduce-the-bitz-format)

The `bitz` format resembles Tenzir’s internal wire format. It enables lossless and quick transfer of events between Tenzir nodes through any connector.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4079](https://github.com/tenzir/tenzir/pull/4079).

#### Add a delay to retrying failed pipelines

[Section titled “Add a delay to retrying failed pipelines”](#add-a-delay-to-retrying-failed-pipelines)

Stopping a failed pipeline now moves it into the stopped state in the app and through the `/pipeline/update` API, stopping automatic restarts on failure.

Pipelines now restart on failure at most every minute. The new API parameter `retry_delay` is available in the `/pipeline/create`, `/pipeline/launch`, and `/pipeline/update` APIs to customize this value. For configured pipelines, the new `restart-on-error` option supersedes the previous `autostart.failed` option and may be set either to a boolean or to a duration, with the former using the default retry delay and the latter using a custom one.

The output of `show pipelines` and the `/pipeline/list` API now includes the start time of the pipeline in the field `start_time`, the newly added retry delay in the field `retry_delay`, and whether the pipeline is hidden from the overview page on app.tenzir.com in the field `hidden`.

By [@Dakostu](https://github.com/Dakostu) in [#4108](https://github.com/tenzir/tenzir/pull/4108).

#### Add `unroll` operator

[Section titled “Add unroll operator”](#add-unroll-operator)

The `unroll` operator transforms an event that contains a list into a sequence of events where each output event contains one of the list elements.

By [@jachris](https://github.com/jachris) in [#4078](https://github.com/tenzir/tenzir/pull/4078).

#### Add UDP connector

[Section titled “Add UDP connector”](#add-udp-connector)

The new `udp` connector comes with a loader and saver to read bytes from and write bytes to a UDP socket.

By [@mavam](https://github.com/mavam) in [#4067](https://github.com/tenzir/tenzir/pull/4067).

#### Support 0mq inproc sockets

[Section titled “Support 0mq inproc sockets”](#support-0mq-inproc-sockets)

The `0mq` connector now supports `inproc` socket endpoint URLs, allowing you to create arbitrary publish/subscribe topologies within a node. For example, `save zmq inproc://foo` writes messages to the in-process socket named `foo`.

By [@mavam](https://github.com/mavam) in [#4117](https://github.com/tenzir/tenzir/pull/4117).

#### Generalize `every` to work with all operators

[Section titled “Generalize every to work with all operators”](#generalize-every-to-work-with-all-operators)

The `every <duration>` operator modifier now supports all operators, turning blocking operators like `tail`, `sort` or `summarize` into operators that emit events every `<duration>`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4109](https://github.com/tenzir/tenzir/pull/4109).

#### Add multi-line syslog message support

[Section titled “Add multi-line syslog message support”](#add-multi-line-syslog-message-support)

Syslog messages spanning multiple lines are now supported.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#4080](https://github.com/tenzir/tenzir/pull/4080).

#### Add `deduplicate` operator

[Section titled “Add deduplicate operator”](#add-deduplicate-operator)

The `deduplicate` operator allows removing duplicate events based on specific fields.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#4068](https://github.com/tenzir/tenzir/pull/4068).

### Changes

[Section titled “Changes”](#changes)

#### Support 0mq inproc sockets

[Section titled “Support 0mq inproc sockets”](#support-0mq-inproc-sockets-1)

The `0mq` connector no longer automatically monitors TCP sockets to wait until at least one remote peer is present. Explicitly pass `--monitor` for this behavior.

By [@mavam](https://github.com/mavam) in [#4117](https://github.com/tenzir/tenzir/pull/4117).

#### Remove metrics from `/pipeline/list`

[Section titled “Remove metrics from /pipeline/list”](#remove-metrics-from-pipelinelist)

The `show pipelines` operator and `/pipeline/list` endpoint no longer include pipeline metrics. We recommend using the `metrics` operator instead, which offers the same data in a more flexible way.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4114](https://github.com/tenzir/tenzir/pull/4114).

#### Add value grouping to `chart` and remove `--title`

[Section titled “Add value grouping to chart and remove --title”](#add-value-grouping-to-chart-and-remove---title-1)

In the `chart` operator, unless otherwise specified, every field but the first one is taken to be a value for the Y-axis, instead of just the second one.

If the value for `-x`/`--name` or `-y`/`--value` is explicitly specified, the other one must now be too.

The `--title` option is removed from `chart`. Titles can instead be set directly in the web interface.

By [@jachris](https://github.com/jachris) in [#4119](https://github.com/tenzir/tenzir/pull/4119).

#### Remove many deprecated things

[Section titled “Remove many deprecated things”](#remove-many-deprecated-things)

The `tenzir-ctl count <expr>` command no longer exists. It has long been deprecated and superseded by pipelines of the form `export | where <expr> | summarize count(.)`.

The deprecated `tenzir-ctl status` command and the corresponding `/status` endpoint no longer exist. They have been superseded by the `show` and `metrics` operators that provide more detailed insight.

The deprecated `tenzir.aging-frequency` and `tenzir.aging-query` options no longer exist. We recommend using the compaction or disk monitor mechanisms instead to delete persisted events.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4103](https://github.com/tenzir/tenzir/pull/4103).

#### Add multi-line syslog message support

[Section titled “Add multi-line syslog message support”](#add-multi-line-syslog-message-support-1)

Lines of input containing an invalid syslog messages are now assumed to be a continuation of a message on a previous line, if there’s any.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#4080](https://github.com/tenzir/tenzir/pull/4080).

#### Remove events output from many context operators

[Section titled “Remove events output from many context operators”](#remove-events-output-from-many-context-operators)

The `context create`, `context reset`, `context update`, and `context load` operators no return information about the context. Pipelines ending with these operators will now be considered closed, and you will be asked to deploy them in the Explorer. Previously, users commonly added `discard` after these operators to force this behavior.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4143](https://github.com/tenzir/tenzir/pull/4143).

#### Implement feather printer and parser

[Section titled “Implement feather printer and parser”](#implement-feather-printer-and-parser)

The `feather` format now reads and writes Arrow IPC streams in addition to Feather files, and no longer requires random access to a file to function, making the format streamable with both `read feather` and `write feather`.

By [@balavinaithirthan](https://github.com/balavinaithirthan) in [#4089](https://github.com/tenzir/tenzir/pull/4089).

#### Implement a `parquet` parser and printer

[Section titled “Implement a parquet parser and printer”](#implement-a-parquet-parser-and-printer)

The `parquet` format more efficiently reads and writes Parquet files. The format is streamable for `write parquet`.

By [@balavinaithirthan](https://github.com/balavinaithirthan) in [#4116](https://github.com/tenzir/tenzir/pull/4116).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Make `python` operator not discard fields that start with an underscore

[Section titled “Make python operator not discard fields that start with an underscore”](#make-python-operator-not-discard-fields-that-start-with-an-underscore)

The `python` operator no longer discards field that start with an underscore.

By [@jachris](https://github.com/jachris) in [#4085](https://github.com/tenzir/tenzir/pull/4085).

#### Support parallel connections in `from tcp`

[Section titled “Support parallel connections in from tcp”](#support-parallel-connections-in-from-tcp)

The `tcp` connector now supports accepting multiple connections in parallel when used with the `from` operator, parsing data separately per connection.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4084](https://github.com/tenzir/tenzir/pull/4084).

#### Fix shutdown of connected pipelines alongside node

[Section titled “Fix shutdown of connected pipelines alongside node”](#fix-shutdown-of-connected-pipelines-alongside-node)

Pipelines run with the `tenzir` binary that connected to a Tenzir Node did sometimes not shut down correctly when the node shut down. This now happens reliably.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4093](https://github.com/tenzir/tenzir/pull/4093).

#### Remove wrong EXPOSE in Dockerfile

[Section titled “Remove wrong EXPOSE in Dockerfile”](#remove-wrong-expose-in-dockerfile)

Tenzir Docker images no longer expose 5158/tcp by default, as this prevented running multiple containers in the same network or in host mode.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4099](https://github.com/tenzir/tenzir/pull/4099).

#### Fix unflattening of empty records and `null` records

[Section titled “Fix unflattening of empty records and null records”](#fix-unflattening-of-empty-records-and-null-records)

Empty records and `null` values of record type are now correctly unflattened.

By [@jachris](https://github.com/jachris) in [#4104](https://github.com/tenzir/tenzir/pull/4104).

#### Remove events output from many context operators

[Section titled “Remove events output from many context operators”](#remove-events-output-from-many-context-operators-1)

The `enrich` operator sometimes stopped working when it encountered an event for which the specified fields did not exist. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4143](https://github.com/tenzir/tenzir/pull/4143).

#### Fix verification of large FlatBuffers tables

[Section titled “Fix verification of large FlatBuffers tables”](#fix-verification-of-large-flatbuffers-tables)

Lookup tables with more than 1M entries failed to load after the node was restarted. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4137](https://github.com/tenzir/tenzir/pull/4137).

#### Fix a race condition in `/serve`

[Section titled “Fix a race condition in /serve”](#fix-a-race-condition-in-serve)

We fixed a bug that caused the explorer to sometimes show 504 Gateway Timeout errors for pipelines where the first result took over two seconds to arrive.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4123](https://github.com/tenzir/tenzir/pull/4123).

#### Shut down node a configured pipeline fails to start

[Section titled “Shut down node a configured pipeline fails to start”](#shut-down-node-a-configured-pipeline-fails-to-start)

Nodes now shut down with a non-zero exit code when pipelines configured as part of the `tenzir.yaml` file fail to start, making such configuration errors easier to spot.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4097](https://github.com/tenzir/tenzir/pull/4097).

#### Fix HTTP saver Content-Length computation

[Section titled “Fix HTTP saver Content-Length computation”](#fix-http-saver-content-length-computation)

The `http` saver now correctly sets the `Content-Length` header value for HTTP POST requests.

By [@mavam](https://github.com/mavam) in [#4134](https://github.com/tenzir/tenzir/pull/4134).

#### Fix the schema name in `show contexts`

[Section titled “Fix the schema name in show contexts”](#fix-the-schema-name-in-show-contexts)

The schema name of events returned by `show contexts` sometimes did not match the type of the context. This now works reliably.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4082](https://github.com/tenzir/tenzir/pull/4082).

#### Fix JSON printer handling of `inf` and `nan`

[Section titled “Fix JSON printer handling of inf and nan”](#fix-json-printer-handling-of-inf-and-nan)

The JSON printer previously printed invalid JSON for `inf` and `nan`, which means that `serve` could sometimes emit invalid JSON, which is not handled well by platform/app. Instead, we now emit `null`.

By [@jachris](https://github.com/jachris) in [#4087](https://github.com/tenzir/tenzir/pull/4087).

#### Fix HTTP PUT with empty request body

[Section titled “Fix HTTP PUT with empty request body”](#fix-http-put-with-empty-request-body)

We fixed a bug in the `http` saver that prevented sending HTTP PUT requests with an empty request body.

By [@mavam](https://github.com/mavam) in [#4092](https://github.com/tenzir/tenzir/pull/4092).

#### Fix `python` deadlock for empty input

[Section titled “Fix python deadlock for empty input”](#fix-python-deadlock-for-empty-input)

The `python` operator no longer deadlocks when given an empty program.

By [@jachris](https://github.com/jachris) in [#4086](https://github.com/tenzir/tenzir/pull/4086).

# Tenzir Node v4.12.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.12.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Consolidate community and enterprise edition builds

[Section titled “Consolidate community and enterprise edition builds”](#consolidate-community-and-enterprise-edition-builds)

We fixed a misconfiguration that caused the `publish` and `subscribe` operators not to be available in the statically linked Linux builds.

We fixed a crash on startup when selectively enabling or disabling plugins when at least two plugins with dependent plugins were disabled.

By [@tobim](https://github.com/tobim) in [#4149](https://github.com/tenzir/tenzir/pull/4149).

# Tenzir Node v4.12.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.12.2).

### Features

[Section titled “Features”](#features)

#### Add an operator for dumping schemas

[Section titled “Add an operator for dumping schemas”](#add-an-operator-for-dumping-schemas)

The chart operator now accepts the flags `--x-axis-type` and `--y-axis-type` for `bar`, `line`, and `area` charts, with the possible values being `log` and `linear`, with `linear` as the default value. Setting these flags defines the scale (logarithmic or linear) on the Tenzir App chart visualization.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4147](https://github.com/tenzir/tenzir/pull/4147).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Use consistent attributes for y-axis in `chart`

[Section titled “Use consistent attributes for y-axis in chart”](#use-consistent-attributes-for-y-axis-in-chart)

The `chart` operator failed to render a chart when the y-axis was not specified explicitly and the events contained more than two top-level fields. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4173](https://github.com/tenzir/tenzir/pull/4173).

#### Find syntax errors before record batch reading

[Section titled “Find syntax errors before record batch reading”](#find-syntax-errors-before-record-batch-reading)

The python operator now checks for syntax errors on operator start up.

By [@balavinaithirthan](https://github.com/balavinaithirthan) in [#4139](https://github.com/tenzir/tenzir/pull/4139).

#### Fix reopening ports while subprocesses are open

[Section titled “Fix reopening ports while subprocesses are open”](#fix-reopening-ports-while-subprocesses-are-open)

We fixed a bug that prevented restarts of pipelines containing a listening connector under specific circumstances.

By [@tobim](https://github.com/tobim) in [#4170](https://github.com/tenzir/tenzir/pull/4170).

#### Chain the execution node start response promise to the node lifetime

[Section titled “Chain the execution node start response promise to the node lifetime”](#chain-the-execution-node-start-response-promise-to-the-node-lifetime)

The retry delay now works for pipelines that fail during startup.

By [@Dakostu](https://github.com/Dakostu) in [#4171](https://github.com/tenzir/tenzir/pull/4171).

#### Restore implicit `read json` in `from tcp`

[Section titled “Restore implicit read json in from tcp”](#restore-implicit-read-json-in-from-tcp)

We accidentally removed the implicit `read json` from `from tcp` in Tenzir v4.12. The shortform now works as expected again.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4175](https://github.com/tenzir/tenzir/pull/4175).

#### Fix shutdown of `every <interval> <transformation|sink>`

[Section titled “Fix shutdown of every \<interval> \<transformation|sink>”](#fix-shutdown-of-every-interval-transformationsink)

Transformations or sinks used with the `every` operator modifier did not shut down correctly when exhausting their input. This now work as expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4166](https://github.com/tenzir/tenzir/pull/4166).

# Tenzir Node v4.13.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.13.0).

### Features

[Section titled “Features”](#features)

#### Add new loading mechanism for GeoIP context

[Section titled “Add new loading mechanism for GeoIP context”](#add-new-loading-mechanism-for-geoip-context)

The `geoip` context now supports loading in a MaxMind database with `context load <ctx>`. For example, `load s3://my-bucket/file.mmdb | context load my-ctx` makes the GeoIP context use a remotely stored database.

By [@balavinaithirthan](https://github.com/balavinaithirthan) in [#4158](https://github.com/tenzir/tenzir/pull/4158).

#### Add LEEF parser

[Section titled “Add LEEF parser”](#add-leef-parser)

The new `leef` parser supports parsing Log Event Extended Format (LEEF) version 1.0 and 2.0 events, e.g., `LEEF:1.0|Microsoft|MSExchange|4.0 SP1|15345|src=192.0.2.0\tdst=172.50.123.1`.

By [@mavam](https://github.com/mavam) in [#4178](https://github.com/tenzir/tenzir/pull/4178).

#### Add `cron` operator

[Section titled “Add cron operator”](#add-cron-operator)

The `cron "<cron expression>"` operator modifier executes an operator on a schedule. For example, `cron "* */10 * * * MON-FRI" from https://example.org/api` queries an endpoint on every 10th minute, Monday through Friday.

By [@IyeOnline](https://github.com/IyeOnline) in [#4192](https://github.com/tenzir/tenzir/pull/4192).

#### Add `--precise` mode to JSON parser

[Section titled “Add --precise mode to JSON parser”](#add---precise-mode-to-json-parser)

The `json` parser has a new `--precise` flag, which ensures that the layout of the emitted events precisely match the input. For example, it guarantees that no additional `null` fields will be added. This mode is implicitly enabled when using `read gelf`.

By [@jachris](https://github.com/jachris) in [#4169](https://github.com/tenzir/tenzir/pull/4169).

### Changes

[Section titled “Changes”](#changes)

#### Remove the —clear parameter for lookup table contexts

[Section titled “Remove the —clear parameter for lookup table contexts”](#remove-the-clear-parameter-for-lookup-table-contexts)

The `--clear` parameter for clearing lookup table contexts during an update no longer exists. It has been superseded by the more robust `context reset` operator.

By [@Dakostu](https://github.com/Dakostu) in [#4179](https://github.com/tenzir/tenzir/pull/4179).

#### Fix a hang on shutdown and remove deprecated things

[Section titled “Fix a hang on shutdown and remove deprecated things”](#fix-a-hang-on-shutdown-and-remove-deprecated-things)

The deprecated `matcher` plugin no longer exists. Use the superior `lookup` operator and contexts instead.

The deprecated `tenzir-ctl import` and `tenzir-ctl export` commands no longer exists. They have been fully superseded by pipelines in the form `… | import` and `export | …`, respectively.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4187](https://github.com/tenzir/tenzir/pull/4187).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add LEEF parser

[Section titled “Add LEEF parser”](#add-leef-parser-1)

The `syslog` parser no longer crops messages at unprintable characters, such as tab (`\t`).

The `syslog` parser no longer eagerly attempts to grab an application name from the content, fixing issues when combined with CEF and LEEF.

By [@mavam](https://github.com/mavam) in [#4178](https://github.com/tenzir/tenzir/pull/4178).

#### Fix CSV/XSV format printing the header once for each batch

[Section titled “Fix CSV/XSV format printing the header once for each batch”](#fix-csvxsv-format-printing-the-header-once-for-each-batch)

The CSV, TSV, and SSV printers no longer erroneously print the header multiple times when more than one event batch of events arrives.

By [@jachris](https://github.com/jachris) in [#4195](https://github.com/tenzir/tenzir/pull/4195).

#### Update the repository to include retry delay-related bug fixes

[Section titled “Update the repository to include retry delay-related bug fixes”](#update-the-repository-to-include-retry-delay-related-bug-fixes)

Some pipelines did not restart on failure. The retry mechanism now works for all kinds of failures.

Pipelines that are configured to automatically restart on failure can now be stopped explicitly. Stopping a failed pipeline now always changes its state to the stopped state.

By [@Dakostu](https://github.com/Dakostu) in [#4184](https://github.com/tenzir/tenzir/pull/4184).

#### Fix a hang on shutdown and remove deprecated things

[Section titled “Fix a hang on shutdown and remove deprecated things”](#fix-a-hang-on-shutdown-and-remove-deprecated-things-1)

Startup failures caused by invalid pipelines or contexts deployed as code in the configuration file sometimes caused the node to hang instead of shutting down with an error message. The node now shuts down as expected when this happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4187](https://github.com/tenzir/tenzir/pull/4187).

#### Make python venv creation independent from the user

[Section titled “Make python venv creation independent from the user”](#make-python-venv-creation-independent-from-the-user)

A permission error caused `python` operator to fail when it was previously used by another system user with the same set of requirements. There now is a one Python environment per user and set of requirements.

By [@tobim](https://github.com/tobim) in [#4189](https://github.com/tenzir/tenzir/pull/4189).

# Tenzir Node v4.13.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.13.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### PRs 4212-satta

[Section titled “PRs 4212-satta”](#prs-4212-satta)

The `amqp` connector now properly signals more errors caused, for example, by networking issues. This enables pipelines using this connector to trigger their retry behavior.

By [@satta](https://github.com/satta) in [#4212](https://github.com/tenzir/tenzir/pull/4212).

#### Improve `slice` with positive begin and negative end

[Section titled “Improve slice with positive begin and negative end”](#improve-slice-with-positive-begin-and-negative-end)

The `slice` operator no longer waits for all input to arrive when used with a positive begin and a negative (or missing) end value, which rendered it unusable with infinite inputs. Instead, the operator now yields results earlier.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4210](https://github.com/tenzir/tenzir/pull/4210).

#### Relax `transform_columns`

[Section titled “Relax transform\_columns”](#relax-transform_columns)

The `enrich`, `drop`, `extend`, `replace`, and `deduplicate` operators failed for empty input events. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4215](https://github.com/tenzir/tenzir/pull/4215).

#### Remove stream managers when decommissioning partitions

[Section titled “Remove stream managers when decommissioning partitions”](#remove-stream-managers-when-decommissioning-partitions)

The node’s CPU usage increased ever so slightly with every persisted partition, eventually causing imports and exports to get stuck. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4214](https://github.com/tenzir/tenzir/pull/4214).

# Tenzir Node v4.14.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.14.0).

### Features

[Section titled “Features”](#features)

#### Add timeout options to `summarize`

[Section titled “Add timeout options to summarize”](#add-timeout-options-to-summarize)

The `summarize` operator gained two new options: `timeout` and `update-timeout`, which enable streaming aggregations. They specifiy the maximum time a bucket in the operator may exist, tracked from the arrival of the first and last event in the bucket, respectively. The `timeout` is useful to guarantee that events are held back no more than the specified duration, and the `update-timeout` is useful to finish aggregations earlier in cases where events that would be sorted into the same buckets arrive within the specified duration, allowing results to be seen earlier.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4209](https://github.com/tenzir/tenzir/pull/4209).

#### Add statistical aggregation functions

[Section titled “Add statistical aggregation functions”](#add-statistical-aggregation-functions)

The new `mean` aggregation function computes the mean of grouped numeric values.

The new `approximate_median` aggregation function computes an approximate median of grouped numeric values using the t-digest algorithm.

The new `stddev` and `variance` aggregation functions compute the standard deviation and variance of grouped numeric values, respectively.

The new `collect` aggregation function collects a list of all non-null grouped values. Unlike `distinct`, this function does not remove dulicates and the results may appear in any order.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4208](https://github.com/tenzir/tenzir/pull/4208).

#### Implement strides for the `slice` operator

[Section titled “Implement strides for the slice operator”](#implement-strides-for-the-slice-operator)

The `slice` operator now supports strides in the form of `slice <begin>:<end>:<stride>`. Negative strides reverse the event order. The new `reverse` operator is a short form of `slice ::-1` and reverses the event order.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4216](https://github.com/tenzir/tenzir/pull/4216).

### Changes

[Section titled “Changes”](#changes)

#### Change the syntax of the `slice` operator

[Section titled “Change the syntax of the slice operator”](#change-the-syntax-of-the-slice-operator)

The `slice` operator now expects its arguments in the form `<begin>:<end>`, where either the begin or the end value may be omitted. For example, `slice 10:` returns all but the first 10 events, `slice 10:20` returns events 10 to 20 (exclusive), and `slice :-10` returns all but the last 10 events.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4211](https://github.com/tenzir/tenzir/pull/4211).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Remove the superfluous path separators when using S3 or GS connectors

[Section titled “Remove the superfluous path separators when using S3 or GS connectors”](#remove-the-superfluous-path-separators-when-using-s3-or-gs-connectors)

Paths for `s3` and `gs` connectors are not broken anymore during loading/saving.

By [@Dakostu](https://github.com/Dakostu) in [#4222](https://github.com/tenzir/tenzir/pull/4222).

#### Make syslog parser more lenient

[Section titled “Make syslog parser more lenient”](#make-syslog-parser-more-lenient)

The `syslog` parser incorrectly identified a message without hostname and tag as one with hostname and no tag. This resulted in a hostname with a trailing colon, e.g., `zscaler-nss:`. In such messages, the parser now correctly sets the hostname to `null` and assigns `zscaler-nss` as tag/app, without the trailing colon.

By [@mavam](https://github.com/mavam) in [#4225](https://github.com/tenzir/tenzir/pull/4225).

# Tenzir Node v4.15.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.15.0).

### Features

[Section titled “Features”](#features)

#### Add label definitions to configured pipelines

[Section titled “Add label definitions to configured pipelines”](#add-label-definitions-to-configured-pipelines)

Pipelines configured as code in the `tenzir.yaml` configuration file may now contain labels.

By [@tobim](https://github.com/tobim) in [#4247](https://github.com/tenzir/tenzir/pull/4247).

#### Add an option for writing JSON arrays of objects

[Section titled “Add an option for writing JSON arrays of objects”](#add-an-option-for-writing-json-arrays-of-objects)

Use `write json --arrays-of-objects` to write JSON arrays per batch of events instead of JSON objects per event.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4249](https://github.com/tenzir/tenzir/pull/4249).

#### Build an RPM package

[Section titled “Build an RPM package”](#build-an-rpm-package)

We now offer an RPM package for RedHat Linux and its derivatives.

By [@tobim](https://github.com/tobim) in [#4188](https://github.com/tenzir/tenzir/pull/4188).

#### Support multiple fields in `sort`

[Section titled “Support multiple fields in sort”](#support-multiple-fields-in-sort)

The `sort` operator now supports sorting by multiple fields.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4242](https://github.com/tenzir/tenzir/pull/4242).

#### Add an option to `export` both past and future events

[Section titled “Add an option to export both past and future events”](#add-an-option-to-export-both-past-and-future-events)

The `export`, `metrics`, and `diagnostics` operators now features a `--retro` flag. This flag will make the operators first export past events, even when `--live` is set. Specify both options explicitly to first return past events and then immediately switch into live mode.

By [@IyeOnline](https://github.com/IyeOnline) in [#4203](https://github.com/tenzir/tenzir/pull/4203).

#### Allow editing definitions of managed pipelines

[Section titled “Allow editing definitions of managed pipelines”](#allow-editing-definitions-of-managed-pipelines)

The `/pipeline/update` API endpoint now supports updating definitions of existing pipelines.

By [@Dakostu](https://github.com/Dakostu) in [#4196](https://github.com/tenzir/tenzir/pull/4196).

#### Support IP-in-subnet queries in lookup tables

[Section titled “Support IP-in-subnet queries in lookup tables”](#support-ip-in-subnet-queries-in-lookup-tables)

The `lookup-table` context now performs longest-prefix matches when the table key is of type `subnet` and the to-be-enriched field of type `ip`. For example, a lookup table with key `10.0.0.0/8` will match when enriching the IP address `10.1.1.1`.

By [@mavam](https://github.com/mavam) in [#4051](https://github.com/tenzir/tenzir/pull/4051).

#### Support disabling TLS in `https` connector

[Section titled “Support disabling TLS in https connector”](#support-disabling-tls-in-https-connector)

The `https` connector supports the new options `--skip-peer-verification` and `--skip-hostname-verification` to disable verification of the peer’s certificate and verification of the certificate hostname.

By [@mavam](https://github.com/mavam) in [#4248](https://github.com/tenzir/tenzir/pull/4248).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Support multiple table slice schemas when dumping contexts

[Section titled “Support multiple table slice schemas when dumping contexts”](#support-multiple-table-slice-schemas-when-dumping-contexts)

`context inspect` will not crash anymore when encountering contexts that contain multi-schema data.

By [@Dakostu](https://github.com/Dakostu) in [#4236](https://github.com/tenzir/tenzir/pull/4236).

#### Add an option to `export` both past and future events

[Section titled “Add an option to export both past and future events”](#add-an-option-to-export-both-past-and-future-events-1)

`export --live` no longer buffers the last batch of event that was imported, and instead immediately returns all imported events.

By [@IyeOnline](https://github.com/IyeOnline) in [#4203](https://github.com/tenzir/tenzir/pull/4203).

#### Add label definitions to configured pipelines

[Section titled “Add label definitions to configured pipelines”](#add-label-definitions-to-configured-pipelines-1)

Pipelines configured as code no longer always restart with the node. Instead, just like for other pipelines, they only restart when they were running before the node shut down.

By [@tobim](https://github.com/tobim) in [#4247](https://github.com/tenzir/tenzir/pull/4247).

# Tenzir Node v4.15.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.15.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix the demo node Docker image’s entrypoint

[Section titled “Fix the demo node Docker image’s entrypoint”](#fix-the-demo-node-docker-images-entrypoint)

We fixed a regression that caused demo nodes not to start for Tenzir v4.15.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4258](https://github.com/tenzir/tenzir/pull/4258).

# Tenzir Node v4.15.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.15.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Do not persist pipeline state updates on node shutdown

[Section titled “Do not persist pipeline state updates on node shutdown”](#do-not-persist-pipeline-state-updates-on-node-shutdown)

Some *Running* pipelines were considered *Completed* when the node shut down, causing them not to start up again automatically when the node restarted. Now, the node only considers pipelines *Completed* that entered the state on their own before the node’s shutdown.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4261](https://github.com/tenzir/tenzir/pull/4261).

# Tenzir Node v4.16.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.16.0).

### Features

[Section titled “Features”](#features)

#### Enable selective deletion of lookup table entries

[Section titled “Enable selective deletion of lookup table entries”](#enable-selective-deletion-of-lookup-table-entries)

For `lookup-table` contexts, the new `--erase` option for `context update` enables selective deletion of lookup table entries.

The `context update` operator now defaults the `--key <field>` option to the first field in the input when no field is explicitly specified.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4274](https://github.com/tenzir/tenzir/pull/4274).

#### Add percentile aggregation functions

[Section titled “Add percentile aggregation functions”](#add-percentile-aggregation-functions)

The `p99`, `p95`, `p90`, `p75`, and `p50` aggregation functions calculate commonly used percentiles of grouped values in the `summarize` operator.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4273](https://github.com/tenzir/tenzir/pull/4273).

#### Support multiple publishers

[Section titled “Support multiple publishers”](#support-multiple-publishers)

The `publish` operator’s topics no longer have to be unique. Instead, any number of pipelines may use the `publish` operator with the same topic. This enables multi-producer, multi-consumer (MPMC) event routing, where streams of events from different pipelines can now be merged back together in addition to being split.

Inter-pipeline data transfer with the `publish` and `subscribe` operators is now as fast as intra-pipeline data transfer between pipeline operators and utilizes the same amount of memory.

Back pressure now propagates from subscribers back to publishers, i.e., if a pipeline with a `subscribe` operator is too slow then all pipelines with matching `publish` operators will be slowed down to a matching speed. This limits the memory usage of `publish` operators and prevents data loss.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4270](https://github.com/tenzir/tenzir/pull/4270).

### Changes

[Section titled “Changes”](#changes)

#### Add percentile aggregation functions

[Section titled “Add percentile aggregation functions”](#add-percentile-aggregation-functions-1)

The `approximate_median` aggregation function is now called `median`. We found the longer name, despite being more accurate, to be rather unintuitive.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4273](https://github.com/tenzir/tenzir/pull/4273).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Handle loading of configured and non-configured contexts with the same name

[Section titled “Handle loading of configured and non-configured contexts with the same name”](#handle-loading-of-configured-and-non-configured-contexts-with-the-same-name)

Configured and non-configured contexts with the same name will not cause non-deterministic behavior upon loading anymore. The node will shut down instead.

By [@Dakostu](https://github.com/Dakostu) in [#4224](https://github.com/tenzir/tenzir/pull/4224).

#### Evaluate `ip == subnet` predicates

[Section titled “Evaluate ip == subnet predicates”](#evaluate-ip--subnet-predicates)

Predicates of the form `ip == subnet` and `ip in [subnet1, subnet2, …]` now work as expected.

The `lookup` operator now correctly handles subnet keys when using the `--retro` or `--snapshot` options.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4268](https://github.com/tenzir/tenzir/pull/4268).

# Tenzir Node v4.17.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.17.0).

### Features

[Section titled “Features”](#features)

#### Fix a potential crash in `enrich --replace`

[Section titled “Fix a potential crash in enrich --replace”](#fix-a-potential-crash-in-enrich---replace)

The `enrich` operator no longer crashes when it is used to replace a field value with a context value of a different type and the context is not able to provide a substitute for all inputs.

By [@tobim](https://github.com/tobim) in [#4291](https://github.com/tenzir/tenzir/pull/4291).

#### Implement the `azure-log-analytics` plugin

[Section titled “Implement the azure-log-analytics plugin”](#implement-the-azure-log-analytics-plugin)

The new `azure-log-analytics` operator makes it possible to upload events to supported or custom tables in Microsoft Azure.

By [@Dakostu](https://github.com/Dakostu) in [#4281](https://github.com/tenzir/tenzir/pull/4281).

#### Make the parallel level in `lookup` configurable

[Section titled “Make the parallel level in lookup configurable”](#make-the-parallel-level-in-lookup-configurable)

The `lookup` operator gained a new `--parallel <level>` option controlling the number of partitions the operator is allowed to open at once for retrospective lookups. This can significantly increase performance at the cost of higher resource usage. The option defaults to 3. To restore the previous behavior, set the option to 1.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4300](https://github.com/tenzir/tenzir/pull/4300).

#### Add the `ttl` to the `/pipeline/list` API

[Section titled “Add the ttl to the /pipeline/list API”](#add-the-ttl-to-the-pipelinelist-api)

The `/pipeline/list` API now includes a new `ttl` field showing the TTL of the pipeline. The remaining TTL moved from `ttl_expires_in_ns` to a `remaining_ttl` field, aligning the output of the API with the `show pipelines` operator.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4314](https://github.com/tenzir/tenzir/pull/4314).

#### Allow pip to write to stdout in venv creation

[Section titled “Allow pip to write to stdout in venv creation”](#allow-pip-to-write-to-stdout-in-venv-creation)

We fixed bug that caused python-pip to fail when creating the runtime environment for the python operator.

By [@tobim](https://github.com/tobim) in [#4279](https://github.com/tenzir/tenzir/pull/4279).

#### Add a `rendered` field to diagnostics

[Section titled “Add a rendered field to diagnostics”](#add-a-rendered-field-to-diagnostics)

Newly created diagnostics returned from the `diagnostics` now contain a `rendered` field that contains a rendered form of the diagnostic. To restore the previous behavior, use `diagnostics | drop rendered`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4290](https://github.com/tenzir/tenzir/pull/4290).

#### Fix healthcheck in docker-compose.yaml

[Section titled “Fix healthcheck in docker-compose.yaml”](#fix-healthcheck-in-docker-composeyaml)

`context update <name>` for `lookup-table` contexts now supports per-entry timeouts. The `--create-timeout <duration>` option sets the time after which lookup table entries expire, and the `--update-timeout <duration>` option sets the time after which lookup table entries expire if they are not accessed.

By [@lo-chr](https://github.com/lo-chr) in [#5126](https://github.com/tenzir/tenzir/pull/5126).

#### Print Operator

[Section titled “Print Operator”](#print-operator)

The `print` operator allows for printing record fields as strings with any format.

By [@balavinaithirthan](https://github.com/balavinaithirthan) in [#4265](https://github.com/tenzir/tenzir/pull/4265).

### Changes

[Section titled “Changes”](#changes)

#### Remove built-in type aliases

[Section titled “Remove built-in type aliases”](#remove-built-in-type-aliases)

The built-in type aliases `timestamp` and `port` for `time` and `uint64`, respectively, no longer exist. They were an artifact of Tenzir from before it supported schema inference in most parsers, and did not play well with many operators when used together with inferred types from other parsers.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4299](https://github.com/tenzir/tenzir/pull/4299).

#### Include hidden pipelines in `show pipelines`

[Section titled “Include hidden pipelines in show pipelines”](#include-hidden-pipelines-in-show-pipelines)

`show pipelines` now includes “hidden” pipelines run by the by the Tenzir Platform or through the API. These pipelines usually run background jobs, so they’re intentionally hidden from the `/pipeline/list` API.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4309](https://github.com/tenzir/tenzir/pull/4309).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Make `ip == subnet` and `string == pattern` commutative

[Section titled “Make ip == subnet and string == pattern commutative”](#make-ip--subnet-and-string--pattern-commutative)

`subnet == ip` and `pattern == string` predicates now behave just like `ip == subnet` and `string == pattern` predicates.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4280](https://github.com/tenzir/tenzir/pull/4280).

#### Fix start abort error message

[Section titled “Fix start abort error message”](#fix-start-abort-error-message)

Errors during pipeline startup are properly propagated instead of being replaced by `error: failed to run` in some situations.

By [@jachris](https://github.com/jachris) in [#4288](https://github.com/tenzir/tenzir/pull/4288).

#### Add missing `-X` option for `kafka` saver

[Section titled “Add missing -X option for kafka saver”](#add-missing--x-option-for-kafka-saver)

The `-X` option for overriding configuration options for `librdkafka` now works the `kafka` saver as well. Previously, the option was only exposed for the loader, unlike advertised in the documentation.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4317](https://github.com/tenzir/tenzir/pull/4317).

#### Tone down execution node backoff behavior

[Section titled “Tone down execution node backoff behavior”](#tone-down-execution-node-backoff-behavior)

We fixed a regression that caused excess CPU usage for some operators when idle. This was most visible with the `subscribe`, `export`, `metrics`, `diagnostics`, `lookup` and `enrich` operators.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4297](https://github.com/tenzir/tenzir/pull/4297).

#### Implement the `azure-log-analytics` plugin

[Section titled “Implement the azure-log-analytics plugin”](#implement-the-azure-log-analytics-plugin-1)

The `https` and related savers now signal an error when the saver-related upload fails.

By [@Dakostu](https://github.com/Dakostu) in [#4281](https://github.com/tenzir/tenzir/pull/4281).

#### Fix using `summarize … by x` when `x` is of type `null`

[Section titled “Fix using summarize … by x when x is of type null”](#fix-using-summarize--by-x-when-x-is-of-type-null)

The `summarize` operator no longer crashes when grpuping by a field of type `null`, i.e., a field whose type could not be inferred because all of its values were `null`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4289](https://github.com/tenzir/tenzir/pull/4289).

# Tenzir Node v4.17.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.17.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Revert “Allow components to depend on other components (#4295)”

[Section titled “Revert “Allow components to depend on other components (#4295)””](#revert-allow-components-to-depend-on-other-components-4295)

We fixed a bug in Tenzir v4.17 that caused some nodes to error on startup with an “unreachable” error.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4322](https://github.com/tenzir/tenzir/pull/4322).

# Tenzir Node v4.17.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.17.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### PRs 4295-4322-4325

[Section titled “PRs 4295-4322-4325”](#prs-4295-4322-4325)

We fixed a bug that very rarely caused configured pipelines using contexts to fail starting up because the used context was not available, and similarly to fail shutting down because the used context was no longer available before the pipeline was shut down.

By [@tobim](https://github.com/tobim) in [#4295](https://github.com/tenzir/tenzir/pull/4295).

#### Fix minor issues with the diagnostics and metrics collector

[Section titled “Fix minor issues with the diagnostics and metrics collector”](#fix-minor-issues-with-the-diagnostics-and-metrics-collector)

We fixed an issue where diagnostics were not properly propagated and thus not available to the `diagnostics` operator.

By [@jachris](https://github.com/jachris) in [#4326](https://github.com/tenzir/tenzir/pull/4326).

# Tenzir Node v4.17.3

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.17.3).

### Features

[Section titled “Features”](#features)

#### Add an operator for partition candidate checks

[Section titled “Add an operator for partition candidate checks”](#add-an-operator-for-partition-candidate-checks)

The `partitions [<expr>]` source operator supersedes `show partitions` (now deprecated) and supports an optional expression as a positional argument for showing only the partitions that would be considered in `export | where <expr>`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4329](https://github.com/tenzir/tenzir/pull/4329).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix deletion of removed configured contexts

[Section titled “Fix deletion of removed configured contexts”](#fix-deletion-of-removed-configured-contexts)

We fixed a bug in Tenzir v4.17.2 that sometimes caused the deletion of on-disk state of configured contexts on startup.

By [@tobim](https://github.com/tobim) in [#4330](https://github.com/tenzir/tenzir/pull/4330).

# Tenzir Node v4.17.4

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.17.4).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix configured pipelines causing a crash when they contain a syntax error

[Section titled “Fix configured pipelines causing a crash when they contain a syntax error”](#fix-configured-pipelines-causing-a-crash-when-they-contain-a-syntax-error)

Shutting down a node no longer sets managed pipelines to the completed state unintentionally.

Configured pipelines with retry on error enabled will not trigger an assertion anymore when they fail to launch.

By [@Dakostu](https://github.com/Dakostu) in [#4334](https://github.com/tenzir/tenzir/pull/4334).

#### Fix two concurrency issues related to child process creation

[Section titled “Fix two concurrency issues related to child process creation”](#fix-two-concurrency-issues-related-to-child-process-creation)

We fixed a bug that caused a “Bad file descriptor” error from the python operator, when multiple instances of it were started simultaneously.

By [@tobim](https://github.com/tobim) in [#4333](https://github.com/tenzir/tenzir/pull/4333).

# Tenzir Node v4.18.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.18.0).

### Features

[Section titled “Features”](#features)

#### Add an optional `name` argument to the `metrics` operator

[Section titled “Add an optional name argument to the metrics operator”](#add-an-optional-name-argument-to-the-metrics-operator)

The `metrics` operator now optionally takes a metric name as an argument. For example, `metrics cpu` only shows CPU metrics. This is functionally equivalent to `metrics | where #schema == "tenzir.metrics.cpu"`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4369](https://github.com/tenzir/tenzir/pull/4369).

#### Introduce API metrics

[Section titled “Introduce API metrics”](#introduce-api-metrics)

The new `tenzir.metrics.api` metrics record every API call made to a Tenzir Node.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4368](https://github.com/tenzir/tenzir/pull/4368).

#### Emit connection status metrics from the platform

[Section titled “Emit connection status metrics from the platform”](#emit-connection-status-metrics-from-the-platform)

The `tenzir.metrics.platform` metrics records every second whether the connection to the Tenzir Platform is working as expected from the node’s perspective.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4374](https://github.com/tenzir/tenzir/pull/4374).

#### PRs 4339-4365

[Section titled “PRs 4339-4365”](#prs-4339-4365)

The `publish`, `subscribe`, `import`, `export`, `lookup` and `enrich` operators deliver their own, operator-specific metrics now.

By [@Dakostu](https://github.com/Dakostu) in [#4339](https://github.com/tenzir/tenzir/pull/4339).

### Changes

[Section titled “Changes”](#changes)

#### Raise the default and max timeouts for `/serve`

[Section titled “Raise the default and max timeouts for /serve”](#raise-the-default-and-max-timeouts-for-serve)

We raised the default and maximum long-polling timeouts for `/serve` from 2s and 5s to 5s and 10s, respectively.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4370](https://github.com/tenzir/tenzir/pull/4370).

#### Prepare the pipeline manager for TQLv2

[Section titled “Prepare the pipeline manager for TQLv2”](#prepare-the-pipeline-manager-for-tqlv2)

Diagnostics from managed pipelines are now deduplicated, showing each diagnostic at most once for each run.

By [@jachris](https://github.com/jachris) in [#4348](https://github.com/tenzir/tenzir/pull/4348).

#### Push expressions into `subscribe` for better metrics

[Section titled “Push expressions into subscribe for better metrics”](#push-expressions-into-subscribe-for-better-metrics)

Pipeline activity for pipelines starting with `subscribe | where <expr>` will no longer report ingress that does not match the provided filter expression.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4349](https://github.com/tenzir/tenzir/pull/4349).

#### Revamp the `export` operator

[Section titled “Revamp the export operator”](#revamp-the-export-operator)

The previously deprecated `--low-priority` option for the `export` operator no longer exists. The new `--parallel <level>` option allows tuning how many worker threads the operator uses at most for querying persisted events.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4365](https://github.com/tenzir/tenzir/pull/4365).

#### Simplify the node actor setup

[Section titled “Simplify the node actor setup”](#simplify-the-node-actor-setup)

The deprecated `vast` symlink for the `tenzir-ctl` binary that offeres backwards compatiblity with versions older than Tenzir v4—when it was called VAST—no longer exists.

The deprecated `tenzir.db-directory` option no longer exists. Use `tenzir.state-directory` instead.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4343](https://github.com/tenzir/tenzir/pull/4343).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add missing time format specifiers for the static binary

[Section titled “Add missing time format specifiers for the static binary”](#add-missing-time-format-specifiers-for-the-static-binary)

The time parser now accepts the `%F`, `%g`, `%G`, `%u`, `%V`, `%z`, and `%Z` format specifiers.

By [@tobim](https://github.com/tobim) in [#4366](https://github.com/tenzir/tenzir/pull/4366).

#### Fix open partition tracking in the `lookup` operator

[Section titled “Fix open partition tracking in the lookup operator”](#fix-open-partition-tracking-in-the-lookup-operator)

We fixed a rare bug that caused the `lookup` operator to exit unexpectedly when using a high value for the operator’s `--parallel` option.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4363](https://github.com/tenzir/tenzir/pull/4363).

#### Allow immediate restarts of the TCP listen connector

[Section titled “Allow immediate restarts of the TCP listen connector”](#allow-immediate-restarts-of-the-tcp-listen-connector)

The `tcp` connector no longer fails in listen mode when you try to restart it directly after stopping it.

By [@tobim](https://github.com/tobim) in [#4367](https://github.com/tenzir/tenzir/pull/4367).

#### Honor proxy settings in the SQS connector

[Section titled “Honor proxy settings in the SQS connector”](#honor-proxy-settings-in-the-sqs-connector)

The SQS connector now honors system proxy settings.

By [@tobim](https://github.com/tobim) in [#4359](https://github.com/tenzir/tenzir/pull/4359).

#### Fix possible crash when one of multiple subscribers disconnects

[Section titled “Fix possible crash when one of multiple subscribers disconnects”](#fix-possible-crash-when-one-of-multiple-subscribers-disconnects)

We fixed a rare crash when one of multiple `subscribe` operators for the same topic disconnected while at least one of the other subscribers was overwhelmed and asked for corresponding publishers to throttle.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4346](https://github.com/tenzir/tenzir/pull/4346).

#### Remove the zero-size check in the split\_at\_null() input loop

[Section titled “Remove the zero-size check in the split\_at\_null() input loop”](#remove-the-zero-size-check-in-the-split_at_null-input-loop)

We fixed a rarely occurring issue in the `gelf` parser that led to parsing errors for some events.

By [@Dakostu](https://github.com/Dakostu) in [#4341](https://github.com/tenzir/tenzir/pull/4341).

#### Push expressions into `subscribe` for better metrics

[Section titled “Push expressions into subscribe for better metrics”](#push-expressions-into-subscribe-for-better-metrics-1)

Pipelines of the form `export --live | where <expr>` failed to filter with type extractors or concepts. This now works as expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4349](https://github.com/tenzir/tenzir/pull/4349).

# Tenzir Node v4.18.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.18.1).

### Features

[Section titled “Features”](#features)

#### Allow disabling node-to-node connections

[Section titled “Allow disabling node-to-node connections”](#allow-disabling-node-to-node-connections)

Setting the `tenzir.endpoint` option to `false` now causes the node not to listen for node-to-node connections. Previously, the port was always exposed for other nodes or `tenzir` processes to connect.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4380](https://github.com/tenzir/tenzir/pull/4380).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Do not optimize `deduplicate`

[Section titled “Do not optimize deduplicate”](#do-not-optimize-deduplicate)

We fixed a bug that caused `deduplicate <fields...> --distance <distance>` to sometimes produce incorrect results when followed by `where <expr>` with an expression that filters on the deduplicated fields.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4379](https://github.com/tenzir/tenzir/pull/4379).

#### Don’t terminate `export` when used with `every`

[Section titled “Don’t terminate export when used with every”](#dont-terminate-export-when-used-with-every)

Pipelines that use the `every` modifier with the `export` operator no longer terminate after the first run.

By [@tobim](https://github.com/tobim) in [#4382](https://github.com/tenzir/tenzir/pull/4382).

# Tenzir Node v4.18.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.18.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Send a hard shutdown to the export bridge

[Section titled “Send a hard shutdown to the export bridge”](#send-a-hard-shutdown-to-the-export-bridge)

We fixed a memory leak in `export` that was introduced with `v4.18.1`.

By [@tobim](https://github.com/tobim) in [#4389](https://github.com/tenzir/tenzir/pull/4389).

# Tenzir Node v4.18.3

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.18.3).

### Changes

[Section titled “Changes”](#changes)

#### Prevent unbounded memory usage in `export --live`

[Section titled “Prevent unbounded memory usage in export --live”](#prevent-unbounded-memory-usage-in-export---live)

`metrics export` now includes an additional field that shows the number of queued events in the pipeline.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4396](https://github.com/tenzir/tenzir/pull/4396).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Use `append_array_slice` everywhere

[Section titled “Use append\_array\_slice everywhere”](#use-append_array_slice-everywhere)

Fixed an issue where `null` records were sometimes transformed into non-null records with `null` fields.

We fixed an issue that sometimes caused `subscribe` to fail when multiple `publish` operators pushed to the same topic at the exact same time.

By [@jachris](https://github.com/jachris) in [#4394](https://github.com/tenzir/tenzir/pull/4394).

#### Prevent unbounded memory usage in `export --live`

[Section titled “Prevent unbounded memory usage in export --live”](#prevent-unbounded-memory-usage-in-export---live-1)

We fixed a bug that caused a potentially unbounded memory usage in `export --live`, `metrics --live`, and `diagnostics --live`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4396](https://github.com/tenzir/tenzir/pull/4396).

# Tenzir Node v4.18.4

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.18.4).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Break back pressure in hidden subscribers

[Section titled “Break back pressure in hidden subscribers”](#break-back-pressure-in-hidden-subscribers)

The `subscribe` operator no longer propagates back pressure to its corresponding `publish` operators when part of a pipeline that runs in the background, i.e., is not visible on the overview page on app.tenzir.com. An invisible subscriber should never be able to slow down a publisher.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4399](https://github.com/tenzir/tenzir/pull/4399).

# Tenzir Node v4.18.5

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.18.5).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add new `unflatten` implementation

[Section titled “Add new unflatten implementation”](#add-new-unflatten-implementation)

The `unflatten` operator now correctly preserves field order and overwrites in case of a name conflict.

By [@jachris](https://github.com/jachris) in [#4405](https://github.com/tenzir/tenzir/pull/4405).

# Tenzir Node v4.19.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.19.0).

### Features

[Section titled “Features”](#features)

#### Add the `buffer` operator for breaking back pressure

[Section titled “Add the buffer operator for breaking back pressure”](#add-the-buffer-operator-for-breaking-back-pressure)

The `buffer` operator buffers up to the specified number of events in an in-memory buffer. By default, operators in a pipeline run only when their downstream operators want to receive input. This mechanism is called back pressure. The `buffer` operator effectively breaks back pressure by storing up to the specified number of events in memory, always requesting more input, which allows upstream operators to run uninterruptedly even in case the downstream operators of the buffer are unable to keep up. This allows pipelines to handle data spikes more easily.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4404](https://github.com/tenzir/tenzir/pull/4404).

#### Add a package manager

[Section titled “Add a package manager”](#add-a-package-manager)

The new `package` operator allows for adding and removing packages, a combination of pipelines and contexts deployed to a node as a set. Nodes load packages installed to `<configdir>/tenzir/package/<package-name>/package.yaml` on startup.

By [@lava](https://github.com/lava) in [#4344](https://github.com/tenzir/tenzir/pull/4344).

### Changes

[Section titled “Changes”](#changes)

#### Remove the `use_simple_format` option for `/serve`

[Section titled “Remove the use\_simple\_format option for /serve”](#remove-the-use_simple_format-option-for-serve)

The `/serve` endpoint now always uses the simple output format for schema definitions. The option `use_simple_format` is now ignored.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4411](https://github.com/tenzir/tenzir/pull/4411).

#### Refactor python operator setup

[Section titled “Refactor python operator setup”](#refactor-python-operator-setup)

The `python` operator now resolves dependencies with every fresh pipeline run. Just restart your pipeline to upgrade to the latest available versions of your Python modules.

The `python` operator no longer uses `pip` but rather [`uv`](https://github.com/astral-sh/uv). In case you set custom environment variables for `pip` you need to exchange those with alternative settings that work with `uv`.

By [@tobim](https://github.com/tobim) in [#4336](https://github.com/tenzir/tenzir/pull/4336).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add the `buffer` operator for breaking back pressure

[Section titled “Add the buffer operator for breaking back pressure”](#add-the-buffer-operator-for-breaking-back-pressure-1)

Metrics emitted towards the end of an operator’s runtime were sometimes not recorded correctly. This now works reliably.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4404](https://github.com/tenzir/tenzir/pull/4404).

# Tenzir Node v4.19.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.19.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Explicitly handle AMQP heartbeats in saver

[Section titled “Explicitly handle AMQP heartbeats in saver”](#explicitly-handle-amqp-heartbeats-in-saver)

Activating heartbeats via `-X`/`--set` on an `amqp` saver triggered socket errors if the interval between sent messages was larger than the heartbeat interval. This has been fixed by handling heartbeat communication correctly in such cases.

By [@satta](https://github.com/satta) in [#4428](https://github.com/tenzir/tenzir/pull/4428).

# Tenzir Node v4.19.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.19.2).

### Features

[Section titled “Features”](#features)

#### Add a throttle operator

[Section titled “Add a throttle operator”](#add-a-throttle-operator)

The throttle operator allows for limiting the bandwidth of a pipeline.

By [@lava](https://github.com/lava) in [#4448](https://github.com/tenzir/tenzir/pull/4448).

### Changes

[Section titled “Changes”](#changes)

#### Add new expert-only options to control demand

[Section titled “Add new expert-only options to control demand”](#add-new-expert-only-options-to-control-demand)

We’ve made some changes that optimize Tenzir’s memory usage. Pipeline operators that emit very small batches of events or bytes at a high frequency now use less memory. The `serve` operator’s internal buffer is now soft-capped at 1Ki instead of 64Ki events, aligning the buffer size with the default upper limit for the number of events that can be fetched at once from `/serve`. The `export`, `metrics`, and `diagnostics` operators now handle back pressure better and utilize less memory in situations where the node has many small partitions. For expert users, the new `tenzir.demand` configuration section allows for controlling how eagerly operators demand input from their upstream operators. Lowering the demand reduces the peak memory usage of pipelines at some performance cost.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4447](https://github.com/tenzir/tenzir/pull/4447).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Emit metrics for events that get emitted as part of push() in the subscribe operator

[Section titled “Emit metrics for events that get emitted as part of push() in the subscribe operator”](#emit-metrics-for-events-that-get-emitted-as-part-of-push-in-the-subscribe-operator)

The `subscribe` operator now delivers metrics more consistently.

By [@Dakostu](https://github.com/Dakostu) in [#4439](https://github.com/tenzir/tenzir/pull/4439).

# Tenzir Node v4.19.3

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.19.3).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Always store pipeline state for configured packaged pipelines

[Section titled “Always store pipeline state for configured packaged pipelines”](#always-store-pipeline-state-for-configured-packaged-pipelines)

Pipelines from packages now correctly remember their last run number and last state when the reinstalling the package.

By [@lava](https://github.com/lava) in [#4479](https://github.com/tenzir/tenzir/pull/4479).

# Tenzir Node v4.19.4

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.19.4).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add packages plugin to list in ‘overlay.nix’

[Section titled “Add packages plugin to list in ‘overlay.nix’”](#add-packages-plugin-to-list-in-overlaynix)

The `packages` plugin is now available in the static binary release artifacts.

By [@lava](https://github.com/lava) in [#4490](https://github.com/tenzir/tenzir/pull/4490).

# Tenzir Node v4.19.5

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.19.5).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix excessive CPU usage of `serve`

[Section titled “Fix excessive CPU usage of serve”](#fix-excessive-cpu-usage-of-serve)

The `serve` operator no longer uses an excessive amount of CPU.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4499](https://github.com/tenzir/tenzir/pull/4499).

# Tenzir Node v4.19.6

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.19.6).

### Features

[Section titled “Features”](#features)

#### Implement the `assert` operator

[Section titled “Implement the assert operator”](#implement-the-assert-operator)

The `tenzir` command-line utility gained a new option `--strict`, causing it to exit with a non-zero exit code for pipelines that emit at least one warning.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4506](https://github.com/tenzir/tenzir/pull/4506).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix a potentially uncaught exception in `shell`

[Section titled “Fix a potentially uncaught exception in shell”](#fix-a-potentially-uncaught-exception-in-shell)

We fixed a bug in the `shell` operator that could cause the process to crash when breaking its pipe. Now, the operator shuts down with an error diagnostic instead.

Pipelines with the `python` operator now deploy more quickly, as their deployment no longer waits for the virtual environment to be set up successfully.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4508](https://github.com/tenzir/tenzir/pull/4508).

#### Fix `slice 1:-1` for exactly one event

[Section titled “Fix slice 1:-1 for exactly one event”](#fix-slice-1-1-for-exactly-one-event)

The `slice` operator no longer crashes when used with a positive begin and negative end value when operating on less events than `-end`, e.g., when working on a single event and using `slice 0:-1`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4505](https://github.com/tenzir/tenzir/pull/4505).

# Tenzir Node v4.2.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.2.0).

### Features

[Section titled “Features”](#features)

#### Add a round of loaders: `http`, `https`, `ftp`, `ftps`

[Section titled “Add a round of loaders: http, https, ftp, ftps”](#add-a-round-of-loaders-http-https-ftp-ftps)

The new connectors `http`, `https`, `ftp`, and `ftps` simplify using remote files in pipelines via HTTP(S) and FTP(S).

By [@mavam](https://github.com/mavam) in [#3499](https://github.com/tenzir/tenzir/pull/3499).

#### Support parsing of concatenated PCAPs

[Section titled “Support parsing of concatenated PCAPs”](#support-parsing-of-concatenated-pcaps)

The `pcap` parser can now process a stream of concatenated PCAP files. On the command line, you can now parse traces with `cat *.pcap | tenzir 'read pcap'`. When providing `--emit-file-headers`, each intermediate file header yields a separate event.

The `nic` loader has a new option `--emit-file-headers` that prepends a PCAP file header for every batch of bytes that the loader produces, yielding a stream of concatenated PCAP files.

By [@mavam](https://github.com/mavam) in [#3513](https://github.com/tenzir/tenzir/pull/3513).

#### Implement the GCS connector plugin

[Section titled “Implement the GCS connector plugin”](#implement-the-gcs-connector-plugin)

The new `gcs` connector enables the user to import/export file data from/to GCS buckets.

By [@Dakostu](https://github.com/Dakostu) in [#3498](https://github.com/tenzir/tenzir/pull/3498).

#### Support `show nics` to see network interfaces

[Section titled “Support show nics to see network interfaces”](#support-show-nics-to-see-network-interfaces)

You can now write `show nics` to get a list of network interfaces. Use `show nics | select name` to a get a list of possible interface names for `from nic`.

By [@mavam](https://github.com/mavam) in [#3517](https://github.com/tenzir/tenzir/pull/3517).

#### Add `lines` parser

[Section titled “Add lines parser”](#add-lines-parser)

The new `lines` parser splits its input at newline characters and produces events with a single field containing the line.

By [@mavam](https://github.com/mavam) in [#3511](https://github.com/tenzir/tenzir/pull/3511).

#### Implement a ZeroMQ connector

[Section titled “Implement a ZeroMQ connector”](#implement-a-zeromq-connector)

The new `zmq` connector ships with a saver and loader for interacting with ZeroMQ. The loader (source) implements a connecting `SUB` socket and the saver (sink) a binding `PUB` socket. The `--bind` or `--connect` flags make it possible to control the direction of connection establishment.

By [@mavam](https://github.com/mavam) in [#3497](https://github.com/tenzir/tenzir/pull/3497).

#### Implement the s3 connector

[Section titled “Implement the s3 connector”](#implement-the-s3-connector)

The new `s3` connector enables the user to import/export file data from/to S3 buckets.

By [@Dakostu](https://github.com/Dakostu) in [#3496](https://github.com/tenzir/tenzir/pull/3496).

### Changes

[Section titled “Changes”](#changes)

#### Do not drop the `data` field in `decapsulate`

[Section titled “Do not drop the data field in decapsulate”](#do-not-drop-the-data-field-in-decapsulate)

The `decapsulate` operator no longer drops the PCAP packet data in incoming events.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3515](https://github.com/tenzir/tenzir/pull/3515).

#### Support parsing of concatenated PCAPs

[Section titled “Support parsing of concatenated PCAPs”](#support-parsing-of-concatenated-pcaps-1)

The long option name `--emit-file-header` of the `pcap` parser is now called `--emit-file-headers` (plural) to streamline it with the `nic` loader and the new capability to process concatenated PCAP files.

By [@mavam](https://github.com/mavam) in [#3513](https://github.com/tenzir/tenzir/pull/3513).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Bump submodule pointer to include pipeline manager deserialization update

[Section titled “Bump submodule pointer to include pipeline manager deserialization update”](#bump-submodule-pointer-to-include-pipeline-manager-deserialization-update)

Pipelines now show up in the “stopped” instead of the “created” state after the node restarted.

By [@Dakostu](https://github.com/Dakostu) in [#3487](https://github.com/tenzir/tenzir/pull/3487).

# Tenzir Node v4.20.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.20.0).

### Features

[Section titled “Features”](#features)

#### Perform individual catalog lookups in `lookup`

[Section titled “Perform individual catalog lookups in lookup”](#perform-individual-catalog-lookups-in-lookup)

The `lookup` operator is now smarter about retroactive lookups for frequently updated contexts and avoids loading data from disk multiple times for context updates that arrive shortly after one another.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4535](https://github.com/tenzir/tenzir/pull/4535).

#### Implement the `cache` operator

[Section titled “Implement the cache operator”](#implement-the-cache-operator)

The `cache` operator is a transformation that passes through events, creating an in-memory cache of events on the first use. On subsequent uses, the operator signals upstream operators no to start at all, and returns the cached events immediately. The operator may also be used as a source for reading from a cache only, or as a sink for writing to a cache only.

The `/pipeline/launch` operator features four new parameters `cache_id`, `cache_capacity`,`cache_ttl`, and `cache_max_ttl`. If a `cache_id` is specified, the pipeline’s implicit sink will use the `cache` operator under the hood. At least one of `serve_id` and `cache_id` must be specified.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4515](https://github.com/tenzir/tenzir/pull/4515).

#### Remove the legacy metrics system

[Section titled “Remove the legacy metrics system”](#remove-the-legacy-metrics-system)

The new `rebuild` metrics contain information about running partition rebuilds.

The `ingest` metrics contain information about all ingested events and their schema. This is slightly different from the existing `import` metrics, which track only events imported via the `import` operator, and are separate per pipeline.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4381](https://github.com/tenzir/tenzir/pull/4381).

#### Implement unstoppable pipelines

[Section titled “Implement unstoppable pipelines”](#implement-unstoppable-pipelines)

The new `unstoppable` flag allows for pipelines to run and repeat indefinitely without the ability to stop or pause.

By [@Dakostu](https://github.com/Dakostu) in [#4513](https://github.com/tenzir/tenzir/pull/4513).

### Changes

[Section titled “Changes”](#changes)

#### Remove the legacy metrics system

[Section titled “Remove the legacy metrics system”](#remove-the-legacy-metrics-system-1)

The previously deprecated legacy metrics system configured via the `tenzir.metrics` configuration section no longer exists. Use the `metrics` operator instead.

`lookup` metrics no longer contain the `snapshot` field; instead, the values show in the `retro` field.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4381](https://github.com/tenzir/tenzir/pull/4381).

#### Remove special character escaping from `lines_printer`

[Section titled “Remove special character escaping from lines\_printer”](#remove-special-character-escaping-from-lines_printer)

The `lines` printer now does not perform any escaping and is no longer an alias to the `ssv` printer. Additionally, nulls are skipped, instead of being printed as `-`.

By [@raxyte](https://github.com/raxyte) in [#4520](https://github.com/tenzir/tenzir/pull/4520).

#### PRs 4455-4549

[Section titled “PRs 4455-4549”](#prs-4455-4549)

The `show` operator is deprecated. Use the operator `<aspect>` instead of `show <aspect>`. The information from `show dependencies` and `show build` is now available in the `version` operator.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4455](https://github.com/tenzir/tenzir/pull/4455).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix a crash when starting `export` on shutdown

[Section titled “Fix a crash when starting export on shutdown”](#fix-a-crash-when-starting-export-on-shutdown)

We fixed an issue where the `export`, `metrics`, or `diagnostics` operators crashed the node when started while the node was shutting down or after an unexpected filesystem error occurred. This happened frequently while using the Tenzir Platform, which subscribes to metrics and diagnostics automatically.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4530](https://github.com/tenzir/tenzir/pull/4530).

#### Perform individual catalog lookups in `lookup`

[Section titled “Perform individual catalog lookups in lookup”](#perform-individual-catalog-lookups-in-lookup-1)

We fixed a bug that sometimes caused the `retro.queued_events` value in `lookup` metrics to stop going down again.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4535](https://github.com/tenzir/tenzir/pull/4535).

#### Fix data parser precedence

[Section titled “Fix data parser precedence”](#fix-data-parser-precedence)

IPv6 addresses with a prefix that is a valid duration, for example `2dff::` with the prefix `2d`, now correctly parse as an IP instead of a string.

By [@jachris](https://github.com/jachris) in [#4523](https://github.com/tenzir/tenzir/pull/4523).

#### Fix regression in `azure-log-analytics`

[Section titled “Fix regression in azure-log-analytics”](#fix-regression-in-azure-log-analytics)

We fixed a regression introduced in Tenzir v4.19.2 in the `azure-log-analytics` operator that prevented it from starting correctly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4516](https://github.com/tenzir/tenzir/pull/4516).

#### Fix crash for heterogeneous subnet lookup tables

[Section titled “Fix crash for heterogeneous subnet lookup tables”](#fix-crash-for-heterogeneous-subnet-lookup-tables)

`context inspect <ctx>` no longer crashes for lookup table contexts with values of multiple schemas when using subnets as keys.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4531](https://github.com/tenzir/tenzir/pull/4531).

# Tenzir Node v4.20.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.20.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `/pipeline/launch` when no cache is provided

[Section titled “Fix /pipeline/launch when no cache is provided”](#fix-pipelinelaunch-when-no-cache-is-provided)

We fixed a regression introduced with Tenzir v4.20 that sometimes caused the Tenzir Platform to fail to fetch results from pipelines.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4554](https://github.com/tenzir/tenzir/pull/4554).

# Tenzir Node v4.20.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.20.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Allow binding to low ports in systemd

[Section titled “Allow binding to low ports in systemd”](#allow-binding-to-low-ports-in-systemd)

The systemd unit now allows binding to privileged ports by default via the ambient capability `CAP_NET_BIND_SERVICE`.

By [@tobim](https://github.com/tobim) in [#4580](https://github.com/tenzir/tenzir/pull/4580).

#### Add OCSF schema type definitions

[Section titled “Add OCSF schema type definitions”](#add-ocsf-schema-type-definitions)

The empty record type is no longer rejected in schema definitions.

By [@jachris](https://github.com/jachris) in [#4558](https://github.com/tenzir/tenzir/pull/4558).

#### Fix a use-after-free in the `xsv` parser

[Section titled “Fix a use-after-free in the xsv parser”](#fix-a-use-after-free-in-the-xsv-parser)

We fixed a potential crash in the `csv`, `ssv`, and `tsv` parsers for slowly arriving inputs.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4570](https://github.com/tenzir/tenzir/pull/4570).

#### Fix the `azure-log-analytics` operator (again)

[Section titled “Fix the azure-log-analytics operator (again)”](#fix-the-azure-log-analytics-operator-again)

The `azure-log-analytics` operator sometimes errored on startup complaining about am unknown `window` option. This no longer occurs.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4578](https://github.com/tenzir/tenzir/pull/4578).

#### Fix compile error with fmt 11.0.2

[Section titled “Fix compile error with fmt 11.0.2”](#fix-compile-error-with-fmt-1102)

We fixed a bug that caused the Demo Node package not to be pre-installed correctly when using the `tenzir/tenzir-demo` Docker image.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4559](https://github.com/tenzir/tenzir/pull/4559).

#### Set `SO_REUSEADDR` in the UDP connector

[Section titled “Set SO\_REUSEADDR in the UDP connector”](#set-so_reuseaddr-in-the-udp-connector)

Restarting pipelines with the `udp` connector no longer fails to bind to the socket.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4579](https://github.com/tenzir/tenzir/pull/4579).

# Tenzir Node v4.20.3

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.20.3).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix a logic error for retried requests in `/serve`

[Section titled “Fix a logic error for retried requests in /serve”](#fix-a-logic-error-for-retried-requests-in-serve)

The `/serve` endpoint now gracefully handles retried requests with the same continuation token, returning the same result for each request.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4585](https://github.com/tenzir/tenzir/pull/4585).

#### Remove the delay between importing and exporting events

[Section titled “Remove the delay between importing and exporting events”](#remove-the-delay-between-importing-and-exporting-events)

We fixed a bug where the `export`, `metrics`, and `diagnostics` operators were sometimes missing events from up to the last 30 seconds. In the Tenzir Platform, this showed itself as a gap in activity sparkbars upon loading the page.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4583](https://github.com/tenzir/tenzir/pull/4583).

# Tenzir Node v4.21.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.21.0).

### Features

[Section titled “Features”](#features)

#### Add a —null option to the lines parser

[Section titled “Add a —null option to the lines parser”](#add-a-null-option-to-the-lines-parser)

The `lines` parser can now handle null delimited “lines” with the `--null` flag.

By [@tobim](https://github.com/tobim) in [#4603](https://github.com/tenzir/tenzir/pull/4603).

#### Support bytes inputs in the `buffer` operator

[Section titled “Support bytes inputs in the buffer operator”](#support-bytes-inputs-in-the-buffer-operator)

The `buffer` operator now works with bytes inputs in addition to the existing support for events inputs.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4594](https://github.com/tenzir/tenzir/pull/4594).

#### Precise Parsing

[Section titled “Precise Parsing”](#precise-parsing)

The CEF, CSV, GELF, JSON, KV, LEEF, Suricata, Syslog, XSV, YAML and Zeek JSON parsers now properly adhere to the schema of the read data. Previously, parsers would merge heterogeneous input into a single, growing schema, inserting nulls for fields that did not exist in some events.

The `fluent-bit` source now properly adheres to the schema of the read data.

The CEF, CSV, GELF, JSON, KV, LEEF, Suricata, Syslog, XSV, YAML and Zeek JSON parsers now all support the `--schema`, `--selector` flags to parse their data according to some given schema, as well as various other flags to more precisely control their output schema.

By [@IyeOnline](https://github.com/IyeOnline) in [#4527](https://github.com/tenzir/tenzir/pull/4527).

#### Metrics for TCP connections

[Section titled “Metrics for TCP connections”](#metrics-for-tcp-connections)

`metrics tcp` shows metrics for TCP connections, emitted once every second per connection. The metrics contains the reads and writes on the socket and the number of bytes transmitted.

By [@tobim](https://github.com/tobim) in [#4564](https://github.com/tenzir/tenzir/pull/4564).

#### Dynamically grow simdjson buffer if necessary

[Section titled “Dynamically grow simdjson buffer if necessary”](#dynamically-grow-simdjson-buffer-if-necessary)

The JSON parser is now able to also handle extremely large events when not using the NDJSON or GELF mode.

By [@IyeOnline](https://github.com/IyeOnline) in [#4590](https://github.com/tenzir/tenzir/pull/4590).

#### Make the kv-parser consider quotes when looking for separators

[Section titled “Make the kv-parser consider quotes when looking for separators”](#make-the-kv-parser-consider-quotes-when-looking-for-separators)

The `kv` parser now allows for keys and values to be enclosed in double quotes: Split matches within quotes will not be considered. Quotes will be trimmed of keys and values. For example `"key"="nested = value, fun"` will now successfully parse as `{ "key" : "nested = value, fun" }`.

By [@IyeOnline](https://github.com/IyeOnline) in [#4591](https://github.com/tenzir/tenzir/pull/4591).

#### Implement the azure-blob-storage connector

[Section titled “Implement the azure-blob-storage connector”](#implement-the-azure-blob-storage-connector)

The new `azure-blob-storage` connector allows reading from and writing to Azure Blob Storage via an URI.

By [@IyeOnline](https://github.com/IyeOnline) in [#4617](https://github.com/tenzir/tenzir/pull/4617).

### Changes

[Section titled “Changes”](#changes)

#### Prefer recent partitions for retro lookups

[Section titled “Prefer recent partitions for retro lookups”](#prefer-recent-partitions-for-retro-lookups)

The `lookup` operator now prefers recent data in searches for lookups against historical data instead of using the order in which context updates arrive.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4636](https://github.com/tenzir/tenzir/pull/4636).

#### Switch the index to basic messaging

[Section titled “Switch the index to basic messaging”](#switch-the-index-to-basic-messaging)

We removed the unused `--snapshot` option from the `lookup` operator.

By [@tobim](https://github.com/tobim) in [#4613](https://github.com/tenzir/tenzir/pull/4613).

#### Precise Parsing

[Section titled “Precise Parsing”](#precise-parsing-1)

The JSON parser’s `--precise` option is now deprecated, as the “precise” mode is the new default. Use `--merge` to get the previous “imprecise” behavior.

The JSON parser’s `--no-infer` option has been renamed to `--schema-only`. The old name is deprecated and will be removed in the future.

By [@IyeOnline](https://github.com/IyeOnline) in [#4527](https://github.com/tenzir/tenzir/pull/4527).

#### Stabilize the `bitz` format

[Section titled “Stabilize the bitz format”](#stabilize-the-bitz-format)

Tenzir’s internal wire format `bitz` is now considered stable. Note that the format underwent significant changes as part of its stabilization, and is incompatible with `bitz` from Tenzir Node v4.20 and older.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4633](https://github.com/tenzir/tenzir/pull/4633).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Stabilize the `bitz` format

[Section titled “Stabilize the bitz format”](#stabilize-the-bitz-format-1)

We fixed a very rare crash in the zero-copy parser implementation of `read feather` and `read parquet` that was caused by releasing shared memory too early.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4633](https://github.com/tenzir/tenzir/pull/4633).

#### Keep from tcp pipelines running on connection failures

[Section titled “Keep from tcp pipelines running on connection failures”](#keep-from-tcp-pipelines-running-on-connection-failures)

Pipelines starting with `from tcp` no longer enter the failed state when an error occurrs in one of the connections.

By [@tobim](https://github.com/tobim) in [#4602](https://github.com/tenzir/tenzir/pull/4602).

#### Stop using connection timeout to get node components

[Section titled “Stop using connection timeout to get node components”](#stop-using-connection-timeout-to-get-node-components)

The `import` and `partitions` operators and the `tenzir-ctl rebuild` command no longer occasionally fail with request timeouts when the node is under high load.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4597](https://github.com/tenzir/tenzir/pull/4597).

#### Make `read json --arrays-of-objects` faster

[Section titled “Make read json --arrays-of-objects faster”](#make-read-json---arrays-of-objects-faster)

We fixed an accidentally quadratic scaling with the number of top-level array elements in `read json --arrays-of-objects`. As a result, using this option will now be much faster.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4601](https://github.com/tenzir/tenzir/pull/4601).

#### Precise Parsing

[Section titled “Precise Parsing”](#precise-parsing-2)

We fixed various edge cases in parsers where values would not be properly parsed as typed data and were stored as plain text instead. No input data was lost, but no valuable type information was gained either.

By [@IyeOnline](https://github.com/IyeOnline) in [#4527](https://github.com/tenzir/tenzir/pull/4527).

# Tenzir Node v4.21.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.21.1).

### Features

[Section titled “Features”](#features)

#### Make the grok parser precise

[Section titled “Make the grok parser precise”](#make-the-grok-parser-precise)

The `grok` parser now allows better control over the schema inference.

The `grok` parser can now be directly used when reading input, allowing for `read grok`.

By [@IyeOnline](https://github.com/IyeOnline) in [#4657](https://github.com/tenzir/tenzir/pull/4657).

#### Add `sample` operator

[Section titled “Add sample operator”](#add-sample-operator)

A new `sample` operator now provides the ability to dynamically sample input data based on the frequency of the receiving events allowing relative sampling in situations of varying load.

By [@raxyte](https://github.com/raxyte) in [#4645](https://github.com/tenzir/tenzir/pull/4645).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Switch the index to basic messaging

[Section titled “Switch the index to basic messaging”](#switch-the-index-to-basic-messaging)

We fixed a bug that sometimes caused the `tenzir-node` process to hang on shutdown. This was most likely to happen when the node shut down immediately after starting up, e.g., because of an invalid configuration file.

By [@tobim](https://github.com/tobim) in [#4613](https://github.com/tenzir/tenzir/pull/4613).

#### Fix double-closing fds in the python operator

[Section titled “Fix double-closing fds in the python operator”](#fix-double-closing-fds-in-the-python-operator)

Fixed a bug in the python operator that could lead to random valid file descriptors in the parent process being closed prematurely.

By [@lava](https://github.com/lava) in [#4646](https://github.com/tenzir/tenzir/pull/4646).

#### Enable azure-blob-storage in the DEB and RPM packages

[Section titled “Enable azure-blob-storage in the DEB and RPM packages”](#enable-azure-blob-storage-in-the-deb-and-rpm-packages)

The `azure-blob-storage` connector is now also available in the static linux binary distributions.

By [@tobim](https://github.com/tobim) in [#4649](https://github.com/tenzir/tenzir/pull/4649).

#### Fix incorrect context updates count in lookup metrics

[Section titled “Fix incorrect context updates count in lookup metrics”](#fix-incorrect-context-updates-count-in-lookup-metrics)

We fixed a bug that caused the `context_updates` field in `metrics lookup` to be reported once per field specified in the corresponding `lookup` operator instead of being reported once per operator in total.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4655](https://github.com/tenzir/tenzir/pull/4655).

# Tenzir Node v4.22.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.22.0).

### Features

[Section titled “Features”](#features)

#### Implement Google Cloud Pub/Sub connectors

[Section titled “Implement Google Cloud Pub/Sub connectors”](#implement-google-cloud-pubsub-connectors)

The new `google-cloud-pubsub` connectors allow subscribing to a Google Cloud Pub/Sub subscription and publishing to a Google Cloud Pub/Sub topic.

By [@IyeOnline](https://github.com/IyeOnline) in [#4656](https://github.com/tenzir/tenzir/pull/4656).

#### Add metrics for select component actors

[Section titled “Add metrics for select component actors”](#add-metrics-for-select-component-actors)

We added low-level actor metrics that help admins track the system health over time.

By [@tobim](https://github.com/tobim) in [#4668](https://github.com/tenzir/tenzir/pull/4668).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Clean up old venvs in the python operator

[Section titled “Clean up old venvs in the python operator”](#clean-up-old-venvs-in-the-python-operator)

The node now wipes its cache directory whenever it restarts.

By [@lava](https://github.com/lava) in [#4669](https://github.com/tenzir/tenzir/pull/4669).

#### Check cURL response codes and do not deliver data on error

[Section titled “Check cURL response codes and do not deliver data on error”](#check-curl-response-codes-and-do-not-deliver-data-on-error)

We fixed a bug in the HTTP connectors, that caused them to not respect the http response codes.

By [@IyeOnline](https://github.com/IyeOnline) in [#4660](https://github.com/tenzir/tenzir/pull/4660).

# Tenzir Node v4.22.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.22.1).

### Features

[Section titled “Features”](#features)

#### Add a few new aggregation functions for TQL2

[Section titled “Add a few new aggregation functions for TQL2”](#add-a-few-new-aggregation-functions-for-tql2)

We added three new, TQL2-exclusive aggregation functions: `first`, `last`, and `mode`. The functions return the first, last, and most common non-null value per group, respectively.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4679](https://github.com/tenzir/tenzir/pull/4679).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Implement `and` and `or` for `null_type`

[Section titled “Implement and and or for null\_type”](#implement-and-and-or-for-null_type)

The boolean operators `and`/`or` now work correctly for the type `null`. Previously, `null and false` evaluated to `null`, and a warning was emitted. Now, it evaluates to `false` without a warning.

By [@raxyte](https://github.com/raxyte) in [#4689](https://github.com/tenzir/tenzir/pull/4689).

#### Ensure cache directory is writable for multiple users

[Section titled “Ensure cache directory is writable for multiple users”](#ensure-cache-directory-is-writable-for-multiple-users)

Using the `tenzir` process from multiple users on the same host sometimes failed because the cache directory was not writable for all users. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4694](https://github.com/tenzir/tenzir/pull/4694).

#### Return instantly in `/serve` if pipeline fails early

[Section titled “Return instantly in /serve if pipeline fails early”](#return-instantly-in-serve-if-pipeline-fails-early)

The `/serve` endpoint now returns instantly when its pipeline fails before the endpoint is used for the first time. In the Tenzir Platform this causes the load more button in the Explorer to correctly stop showing for pipelines that fail shortly after starting.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4688](https://github.com/tenzir/tenzir/pull/4688).

# Tenzir Node v4.22.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.22.2).

### Features

[Section titled “Features”](#features)

#### Implement `<list>.sort()` and `<record>.sort()`

[Section titled “Implement \<list>.sort() and \<record>.sort()”](#implement-listsort-and-recordsort)

The new `sort` method sorts fields in records by name and lists by values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4704](https://github.com/tenzir/tenzir/pull/4704).

#### Add a `value_counts` aggregation function

[Section titled “Add a value\_counts aggregation function”](#add-a-value_counts-aggregation-function)

The new `value_counts` aggregation function returns a list of values and their frequency.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4701](https://github.com/tenzir/tenzir/pull/4701).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Enable google-cloud-pubsub connector in the Docker Image

[Section titled “Enable google-cloud-pubsub connector in the Docker Image”](#enable-google-cloud-pubsub-connector-in-the-docker-image)

The `google-cloud-pubsub` connector and TQL2 operators `load_google_cloud_pubsub` `save_google_cloud_pubsub` operators are now available in the Docker image.

By [@IyeOnline](https://github.com/IyeOnline) in [#4690](https://github.com/tenzir/tenzir/pull/4690).

#### Add a `value_counts` aggregation function

[Section titled “Add a value\_counts aggregation function”](#add-a-value_counts-aggregation-function-1)

We fixed a bug that caused the `mode` aggregation function to sometimes ignore some input values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4701](https://github.com/tenzir/tenzir/pull/4701).

#### Allow reinstantiating the `buffer` operator

[Section titled “Allow reinstantiating the buffer operator”](#allow-reinstantiating-the-buffer-operator)

We fixed a bug in the `buffer` operator that caused it to break when restarting a pipeline or using multiple buffers in a “parallel” context, such as in `load_tcp`’s pipeline argument.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4702](https://github.com/tenzir/tenzir/pull/4702).

#### Explicitly close ending TCP connection sockets

[Section titled “Explicitly close ending TCP connection sockets”](#explicitly-close-ending-tcp-connection-sockets)

We fixed a bug that sometimes prevented incoming connections from `load_tcp` from closing properly.

By [@tobim](https://github.com/tobim) in [#4674](https://github.com/tenzir/tenzir/pull/4674).

# Tenzir Node v4.23.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.23.0).

### Features

[Section titled “Features”](#features)

#### Implement `in` for `list_type`

[Section titled “Implement in for list\_type”](#implement-in-for-list_type)

The relational operator `in` now supports checking for existence of an element in a list. For example, `where x in ["important", "values"]` is functionally equivalent to `where x == "important" or x == "values"`.

By [@raxyte](https://github.com/raxyte) in [#4691](https://github.com/tenzir/tenzir/pull/4691).

#### Add universal function call syntax

[Section titled “Add universal function call syntax”](#add-universal-function-call-syntax)

TQL now supports “universal function call syntax,” which means that every method is callable as a function and every function with at least one positional argument is callable as a method.

By [@jachris](https://github.com/jachris) in [#4730](https://github.com/tenzir/tenzir/pull/4730).

#### Implement `floor` and `ceil` functions

[Section titled “Implement floor and ceil functions”](#implement-floor-and-ceil-functions)

`ceil` and `floor` join the existing `round` function for rounding numbers, durations, and timestamps upwards and downwards, respectively.

By [@raxyte](https://github.com/raxyte) in [#4712](https://github.com/tenzir/tenzir/pull/4712).

#### Add `load_balance` operator

[Section titled “Add load\_balance operator”](#add-load_balance-operator)

The new `load_balance` operator distributes events over a set of subpipelines.

By [@jachris](https://github.com/jachris) in [#4720](https://github.com/tenzir/tenzir/pull/4720).

#### Port kafka connector to TQL2

[Section titled “Port kafka connector to TQL2”](#port-kafka-connector-to-tql2)

New `load_kafka` and `save_kafka` operators enable seamless integration with Apache Kafka in TQL2.

By [@raxyte](https://github.com/raxyte) in [#4725](https://github.com/tenzir/tenzir/pull/4725).

#### Add spread syntax `...expr` for lists

[Section titled “Add spread syntax ...expr for lists”](#add-spread-syntax-expr-for-lists)

The spread syntax `...` can now be used inside lists to expand one list into another. For example, `[1, ...[2, 3]]` evaluates to `[1, 2, 3]`.

By [@jachris](https://github.com/jachris) in [#4729](https://github.com/tenzir/tenzir/pull/4729).

#### Expose new hash functions

[Section titled “Expose new hash functions”](#expose-new-hash-functions)

We’ve added new hash functions for commonly used algorithms: `hash_md5`, `hash_sha1`, `hash_sha224`, `hash_sha256`, `hash_sha384`, `hash_sha512`, `hash_xxh3`.

By [@mavam](https://github.com/mavam) in [#4705](https://github.com/tenzir/tenzir/pull/4705).

#### Add a `splunk` sink operator

[Section titled “Add a splunk sink operator”](#add-a-splunk-sink-operator)

The new `to_splunk` sink operator writes data to Splunk HEC endpoint.

By [@IyeOnline](https://github.com/IyeOnline) in [#4719](https://github.com/tenzir/tenzir/pull/4719).

### Changes

[Section titled “Changes”](#changes)

#### `splunk` fixes & consistent operator naming

[Section titled “splunk fixes & consistent operator naming”](#splunk-fixes--consistent-operator-naming)

We renamed the TQL2 `azure_log_analytics` operator to `to_azure_log_analytics`.

We renamed the TQL2 `velociraptor` operator to `from_velociraptor`.

By [@IyeOnline](https://github.com/IyeOnline) in [#4726](https://github.com/tenzir/tenzir/pull/4726).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Correctly handle duplicate serve requests and tune some logs

[Section titled “Correctly handle duplicate serve requests and tune some logs”](#correctly-handle-duplicate-serve-requests-and-tune-some-logs)

We eliminated a rare crash in the `serve` operator that was introduced in v4.20.3.

By [@tobim](https://github.com/tobim) in [#4715](https://github.com/tenzir/tenzir/pull/4715).

#### Fix TQL2 `summarize` with no groups and no input

[Section titled “Fix TQL2 summarize with no groups and no input”](#fix-tql2-summarize-with-no-groups-and-no-input)

TQL2’s `summarize` now returns a single event when used with no groups and no input events just like in TQL1, making `from [] | summarize count=count()` return `{count: 0}` instead of nothing.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4709](https://github.com/tenzir/tenzir/pull/4709).

#### Make `str(enum)` return the name of the enum entry

[Section titled “Make str(enum) return the name of the enum entry”](#make-strenum-return-the-name-of-the-enum-entry)

The `str` function no longer returns the numeric index of an enumeration value. Instead, the result is now the actual name associated with that value.

By [@jachris](https://github.com/jachris) in [#4717](https://github.com/tenzir/tenzir/pull/4717).

# Tenzir Node v4.23.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.23.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Only wipe the cache directory contents but not the dir itself

[Section titled “Only wipe the cache directory contents but not the dir itself”](#only-wipe-the-cache-directory-contents-but-not-the-dir-itself)

The node doesn’t try to recreate its cache directory on startup anymore, avoiding permissions issues on systems with strict access control.

By [@lava](https://github.com/lava) in [#4742](https://github.com/tenzir/tenzir/pull/4742).

#### Use separate volumes in docker-compose

[Section titled “Use separate volumes in docker-compose”](#use-separate-volumes-in-docker-compose)

The `docker compose` setup now uses separate local volumes for each `tenzir` directory. This fixes a bug where restarting the container resets installed packages or pipelines.

By [@sunnewehr](https://github.com/sunnewehr) in [#4749](https://github.com/tenzir/tenzir/pull/4749).

#### Normalize expressions for live and unpersisted data

[Section titled “Normalize expressions for live and unpersisted data”](#normalize-expressions-for-live-and-unpersisted-data)

We fixed a crash in pipelines that use the `export` operator and a subsequent `where` filter with certain expressions.

By [@tobim](https://github.com/tobim) in [#4774](https://github.com/tenzir/tenzir/pull/4774).

#### Build the `parquet` plugin in the Dockerfile

[Section titled “Build the parquet plugin in the Dockerfile”](#build-the-parquet-plugin-in-the-dockerfile)

The `parquet` plugin is now available in the `tenzir/tenzir` and `tenzir/tenzir-node` Docker images.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4760](https://github.com/tenzir/tenzir/pull/4760).

#### Fix configuration options for the kafka plugin

[Section titled “Fix configuration options for the kafka plugin”](#fix-configuration-options-for-the-kafka-plugin)

We fixed a bug in the kafka plugin so that it no longer wrongly splits config options from the `yaml` files at the dot character.

By [@tobim](https://github.com/tobim) in [#4761](https://github.com/tenzir/tenzir/pull/4761).

#### Fix syslog parser not yielding on infinite streams

[Section titled “Fix syslog parser not yielding on infinite streams”](#fix-syslog-parser-not-yielding-on-infinite-streams)

We fixed a bug causing the `syslog` parser to never yield events until the input stream ended.

By [@IyeOnline](https://github.com/IyeOnline) in [#4777](https://github.com/tenzir/tenzir/pull/4777).

#### Fix bugs in `where` when predicate evaluates to `null`

[Section titled “Fix bugs in where when predicate evaluates to null”](#fix-bugs-in-where-when-predicate-evaluates-to-null)

We fixed a bug in TQL2’s `where` operator that made it sometimes return incorrect results for events for which the predicate evaluated to `null`. Now, the operator consistently warns when this happens and drops the events.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4785](https://github.com/tenzir/tenzir/pull/4785).

# Tenzir Node v4.24.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.24.0).

### Features

[Section titled “Features”](#features)

#### Implement `encode_base64` and `decode_base64`

[Section titled “Implement encode\_base64 and decode\_base64”](#implement-encode_base64-and-decode_base64)

The new functions `encode_base64` and `decode_base64` encode and decode blobs and strings as Base64.

By [@raxyte](https://github.com/raxyte) in [#4806](https://github.com/tenzir/tenzir/pull/4806).

#### Implement `append`, `prepend`, and `concatenate`

[Section titled “Implement append, prepend, and concatenate”](#implement-append-prepend-and-concatenate)

The new `append`, `prepend`, and `concatenate` functions add an element to the end of a list, to the front of a list, and merge two lists, respectively. `xs.append(y)` is equivalent to `[...xs, y]`, `xs.prepend(y)` is equivalent to `[y, ...xs]`, and `concatenate(xs, ys)` is equivalent to `[...xs, ..ys]`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4792](https://github.com/tenzir/tenzir/pull/4792).

#### Implement `otherwise(<expr>, <expr>)`

[Section titled “Implement otherwise(\<expr>, \<expr>)”](#implement-otherwiseexpr-expr)

The function `otherwise(primary:any, fallback:any)` provides a simple way to specify a `fallback` expression when the `primary` expression evaluates to `null`.

By [@raxyte](https://github.com/raxyte) in [#4794](https://github.com/tenzir/tenzir/pull/4794).

#### Port `unroll` to TQL2

[Section titled “Port unroll to TQL2”](#port-unroll-to-tql2)

The `unroll` operator is now available in TQL2. It takes a field of type list, and duplicates the surrounding event for every element of the list.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4736](https://github.com/tenzir/tenzir/pull/4736).

#### Support decapsulating SLL2 packets

[Section titled “Support decapsulating SLL2 packets”](#support-decapsulating-sll2-packets)

The `decapsulate` function now handles SLL2 frames (Linux cooked capture encapsulation).

By [@mavam](https://github.com/mavam) in [#4744](https://github.com/tenzir/tenzir/pull/4744).

#### Implement `where` and `map` on lists

[Section titled “Implement where and map on lists”](#implement-where-and-map-on-lists)

The `<list>.map(<capture>, <expression>)` function replaces each value from `<list>` with the value from `<expression>`. Within `<expression>`, the elements are available as `<capture>`. For example, to add 5 to all elements in the list `xs`, use `xs = xs.map(x, x + 5)`.

The `<list>.where(<capture>, <predicate>)` removes all elements from `<list>` for which the `<predicate>` evaluates to `false`. Within `<predicate>`, the elements are available as `<capture>`. For example, to remove all elements smaller than 3 from the list `xs`, use `xs = xs.where(x, x >= 3)`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4788](https://github.com/tenzir/tenzir/pull/4788).

#### Implement `encode_hex()` and `decode_hex()`

[Section titled “Implement encode\_hex() and decode\_hex()”](#implement-encode_hex-and-decode_hex)

The functions `encode_hex` and `decode_hex` transform strings and blobs to/from their hexadecimal byte representation.

By [@raxyte](https://github.com/raxyte) in [#4815](https://github.com/tenzir/tenzir/pull/4815).

#### Port Contexts to TQL2

[Section titled “Port Contexts to TQL2”](#port-contexts-to-tql2)

The contexts feature is now available in TQL2. It has undergone significant changes to make use of TQL2’s more powerful expressions. Contexts are shared between TQL1 and TQL2 pipelines. All operators are grouped in the `context` module, including the `enrich` and `show contexts` operators, which are now called `context::enrich` and `context::list`, respectively. To create a new context, use the `context::create_lookup_table`, `context::create_bloom_filter`, or `context::create_geoip` operators.

Lookup table contexts now support separate create, write, and read timeouts via the `create_timeout`, `write_timeout`, and `read_timeout` options, respectively. The options are exclusive to contexts updated with TQL2’s `context::update` operator.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4753](https://github.com/tenzir/tenzir/pull/4753).

#### Improve `to_splunk` TLS functionality

[Section titled “Improve to\_splunk TLS functionality”](#improve-to_splunk-tls-functionality)

The `to_splunk` operator now supports the `cacert`, `certfile`, and `keyfile` options to provide certificates for the TLS connection.

By [@raxyte](https://github.com/raxyte) in [#4825](https://github.com/tenzir/tenzir/pull/4825).

#### Implement `--limit` flag for the `chart` operator

[Section titled “Implement --limit flag for the chart operator”](#implement---limit-flag-for-the-chart-operator)

The `--limit` option for the TQL1 `chart` operator controls the previously hardcoded upper limit on the number of events in a chart. The option defaults to 10,000 events.

By [@IyeOnline](https://github.com/IyeOnline) in [#4757](https://github.com/tenzir/tenzir/pull/4757).

#### Add `parse_time` and `format_time` methods

[Section titled “Add parse\_time and format\_time methods”](#add-parse_time-and-format_time-methods)

The new `parse_time` and `format_time` functions transform strings into timestamps and vice versa.

By [@mavam](https://github.com/mavam) in [#4576](https://github.com/tenzir/tenzir/pull/4576).

#### Implement `x[y]` record indexing

[Section titled “Implement x\[y\] record indexing”](#implement-xy-record-indexing)

Indexing records with string expressions is now supported.

By [@raxyte](https://github.com/raxyte) in [#4795](https://github.com/tenzir/tenzir/pull/4795).

#### Implement `split`, `split_regex`, and `join`

[Section titled “Implement split, split\_regex, and join”](#implement-split-split_regex-and-join)

The `split` and `split_regex` functions split a string into a list of strings based on a delimiter or a regular expression, respectively.

The `join` aggregation function concatenates a strings into a single string, optionally separated by a delimiter.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4799](https://github.com/tenzir/tenzir/pull/4799).

#### Fix crash in `context::enrich` for heterogeneous enrichments

[Section titled “Fix crash in context::enrich for heterogeneous enrichments”](#fix-crash-in-contextenrich-for-heterogeneous-enrichments)

The `network` function returns the network address of a CIDR subnet. For example, `192.168.0.0/16.network()` returns `192.168.0.0`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4828](https://github.com/tenzir/tenzir/pull/4828).

#### Introduce a `zip` function for merging lists

[Section titled “Introduce a zip function for merging lists”](#introduce-a-zip-function-for-merging-lists)

The `zip` function merges two lists into a single list of a record with two fields `left` and `right`. For example, `zip([1, 2], [3, 4])` returns `[{left: 1, right: 3}, {left: 2, right: 4}]`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4803](https://github.com/tenzir/tenzir/pull/4803).

#### PRs 4716-4807

[Section titled “PRs 4716-4807”](#prs-4716-4807)

The following operators are now available in TQL2 for loading and saving: `load_amqp`, `save_amqp`, `load_ftp`, `save_ftp`, `load_nic`, `load_s3`, `save_s3`, `load_sqs`, `save_sqs`, `load_udp`, `save_udp`, `load_zmq`, `save_zmq`, `save_tcp` and `save_email`.

The following new operators are available in TQL2 to convert event streams to byte streams in various formats: `write_csv`, `write_feather`, `write_json`, `write_lines`, `write_ndjson`, `write_parquet`, `write_pcap`, `write_ssv`, `write_tsv`, `write_xsv`, `write_yaml`, `write_zeek_tsv`.

By [@raxyte](https://github.com/raxyte) in [#4716](https://github.com/tenzir/tenzir/pull/4716).

#### Allow aggregation functions to be called on lists

[Section titled “Allow aggregation functions to be called on lists”](#allow-aggregation-functions-to-be-called-on-lists)

Aggregation functions now work on lists. For example, `[1, 2, 3].sum()` will return `6`, and `["foo", "bar", "baz"].map(x, x == "bar").any()` will return `true`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4821](https://github.com/tenzir/tenzir/pull/4821).

#### Port `unordered`, `local`, and `remote` to TQL2

[Section titled “Port unordered, local, and remote to TQL2”](#port-unordered-local-and-remote-to-tql2)

The `local` and `remote` operators allow for overriding the location of a pipeline. Local operators prefer running at a client `tenzir` process, and remote operators prefer running at a remote `tenzir-node` process. These operators are primarily intended for testing purposes.

The `unordered` operator throws away the order of events in a pipeline. This causes some operators to run faster, e.g., `read_ndjson` is able to parse events out of order through this. This operator is primarily intended for testing purposes, as most of the time the ordering requirements are inferred from subsequent operators in the pipeline.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4835](https://github.com/tenzir/tenzir/pull/4835).

### Changes

[Section titled “Changes”](#changes)

#### Stop URL-encoding pub/sub topics

[Section titled “Stop URL-encoding pub/sub topics”](#stop-url-encoding-pubsub-topics)

The topics provided to the `publish` and `subscribe` operators now exactly match the `topic` field in the corresponding metrics.

Using `publish` and `subscribe` without an explicitly provided topic now uses the topic `main` as opposed to an implementation-defined special name.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4738](https://github.com/tenzir/tenzir/pull/4738).

#### Finish porting loaders, printers and savers

[Section titled “Finish porting loaders, printers and savers”](#finish-porting-loaders-printers-and-savers)

The option `ndjson` for `write_json` operator has been removed in favor of a new operator `write_ndjson`.

By [@raxyte](https://github.com/raxyte) in [#4762](https://github.com/tenzir/tenzir/pull/4762).

#### PRs 4741-4746

[Section titled “PRs 4741-4746”](#prs-4741-4746)

The functions `ocsf_category_name`, `ocsf_category_uid`, `ocsf_class_name`, and `ocsf_class_uid` are now called `ocsf::category_name`, `ocsf::category_uid`, `ocsf::class_name`, and `ocsf::class_uid`, respectively. Similarly, the `package_add`, `package_remove`, `packages`, and `show pipelines` operators are now called `package::add`, `package::remove`, `package::list`, and `pipeline::list`, respectively.

By [@jachris](https://github.com/jachris) in [#4741](https://github.com/tenzir/tenzir/pull/4741).

#### Improve names for the `cache` operator’s timeout options

[Section titled “Improve names for the cache operator’s timeout options”](#improve-names-for-the-cache-operators-timeout-options)

The `cache` operator’s `ttl` and `max_ttl` options are now called `read_timeout` and `write_timeout`, respectively.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4758](https://github.com/tenzir/tenzir/pull/4758).

#### Prepare small fixes for release

[Section titled “Prepare small fixes for release”](#prepare-small-fixes-for-release)

The new `string` function now replaces the `str` function. The older `str` name will be available as an alias for some time for compatibility but will be removed in a future release.

By [@raxyte](https://github.com/raxyte) in [#4834](https://github.com/tenzir/tenzir/pull/4834).

#### Align argument parser usage format with docs

[Section titled “Align argument parser usage format with docs”](#align-argument-parser-usage-format-with-docs)

The usage string that is reported when an operator or function is being used incorrectly now uses the same format as the documentation.

By [@jachris](https://github.com/jachris) in [#4740](https://github.com/tenzir/tenzir/pull/4740).

#### Improve `to_splunk` TLS functionality

[Section titled “Improve to\_splunk TLS functionality”](#improve-to_splunk-tls-functionality-1)

The `tls_no_verify` option of the `to_splunk` operator is now called `skip_peer_verification`.

By [@raxyte](https://github.com/raxyte) in [#4825](https://github.com/tenzir/tenzir/pull/4825).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add timeout to multiline syslog

[Section titled “Add timeout to multiline syslog”](#add-timeout-to-multiline-syslog)

We fixed an oversight in the syslog parsers, which caused it to not yield an event until the next line came in.

By [@IyeOnline](https://github.com/IyeOnline) in [#4829](https://github.com/tenzir/tenzir/pull/4829).

#### Port Loaders, Printers, Savers

[Section titled “Port Loaders, Printers, Savers”](#port-loaders-printers-savers)

The docs for the `sqs` connector now correctly reflect the default of `3s` for the `--poll-time` option.

By [@raxyte](https://github.com/raxyte) in [#4716](https://github.com/tenzir/tenzir/pull/4716).

#### Ignore whole line when NDJSON parser fails

[Section titled “Ignore whole line when NDJSON parser fails”](#ignore-whole-line-when-ndjson-parser-fails)

The `read_ndjson` operator no longer uses an error-prone mechanism to continue parsing an NDJSON line that contains an error. Instead, the entire line is skipped.

By [@jachris](https://github.com/jachris) in [#4801](https://github.com/tenzir/tenzir/pull/4801).

#### Introduce `{package,pipeline}::list`

[Section titled “Introduce {package,pipeline}::list”](#introduce-packagepipelinelist)

`context inspect` crashed when used to inspect a context that was previously updated with `context update` with an input containing a field of type `enum`. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4746](https://github.com/tenzir/tenzir/pull/4746).

#### Don’t allow manual erasing of contexts from packages

[Section titled “Don’t allow manual erasing of contexts from packages”](#dont-allow-manual-erasing-of-contexts-from-packages)

It is no longer possible to manually remove contexts that are installed as part of a package.

By [@lava](https://github.com/lava) in [#4768](https://github.com/tenzir/tenzir/pull/4768).

#### Fix ODR violation of `tenzir::socket` type

[Section titled “Fix ODR violation of tenzir::socket type”](#fix-odr-violation-of-tenzirsocket-type)

The TQL1 and TQL2 `sockets` operators no longer crash on specific builds.

By [@raxyte](https://github.com/raxyte) in [#4816](https://github.com/tenzir/tenzir/pull/4816).

#### Improve `to_splunk` TLS functionality

[Section titled “Improve to\_splunk TLS functionality”](#improve-to_splunk-tls-functionality-2)

The `max_content_length` option for the `to_splunk` operator was named incorrectly in an earlier version to `send_timeout`. This has now been fixed.

By [@raxyte](https://github.com/raxyte) in [#4825](https://github.com/tenzir/tenzir/pull/4825).

#### Prepend the field added by `enumerate`

[Section titled “Prepend the field added by enumerate”](#prepend-the-field-added-by-enumerate)

The `enumerate` operator now correctly prepends the added index field instead of appending it.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4756](https://github.com/tenzir/tenzir/pull/4756).

#### Add missing `co_yield`s in `save_http`

[Section titled “Add missing co\_yields in save\_http”](#add-missing-co_yields-in-save_http)

The TQL2 `save_http` operator had a bug causing it to fail to connect and get stuck in an infinite loop. This is now fixed and works as expected.

By [@raxyte](https://github.com/raxyte) in [#4833](https://github.com/tenzir/tenzir/pull/4833).

#### Fix `str` function quotes

[Section titled “Fix str function quotes”](#fix-str-function-quotes)

The `str` function no longer adds extra quotes when given a string. For example, `str("") == "\"\""` was changed to `str("") == ""`.

By [@jachris](https://github.com/jachris) in [#4809](https://github.com/tenzir/tenzir/pull/4809).

#### Port Contexts to TQL2

[Section titled “Port Contexts to TQL2”](#port-contexts-to-tql2-1)

The last metric emitted for each run of the `enrich` operator was incorrectly named `tenzir.enrich.metrics` instead of `tenzir.metrics.enrich`, causing it not to be available via `metrics enrich`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4753](https://github.com/tenzir/tenzir/pull/4753).

#### Make `to_hive` a “local” operator

[Section titled “Make to\_hive a “local” operator”](#make-to_hive-a-local-operator)

The `to_hive` operator now correctly writes files relative to the working directory of a `tenzir` client process instead of relative to the node.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4771](https://github.com/tenzir/tenzir/pull/4771).

# Tenzir Node v4.24.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.24.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `last` aggregation

[Section titled “Fix last aggregation”](#fix-last-aggregation)

We fixed the `last` aggregation function to return the last element.

By [@raxyte](https://github.com/raxyte) in [#4855](https://github.com/tenzir/tenzir/pull/4855).

#### Check mmapped chunks for required minimum size

[Section titled “Check mmapped chunks for required minimum size”](#check-mmapped-chunks-for-required-minimum-size)

We fixed a bug introduced with v4.24.0 causing crashes on startup when some of the files in the node’s state directory were smaller than 12 bytes.

By [@lava](https://github.com/lava) in [#4856](https://github.com/tenzir/tenzir/pull/4856).

#### Fix a rare crash in the index actor on startup

[Section titled “Fix a rare crash in the index actor on startup”](#fix-a-rare-crash-in-the-index-actor-on-startup)

We fixed a rare crash on startup that would occur when starting the `tenzir-node` process was so slow that it would try to emit metrics before the component handling metrics was ready.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4846](https://github.com/tenzir/tenzir/pull/4846).

#### Fix operator name for TQL2 `nics`

[Section titled “Fix operator name for TQL2 nics”](#fix-operator-name-for-tql2-nics)

The TQL2 `nics` operator had a bug causing the operator name to be `nic`. This has now been fixed and works as documented.

By [@raxyte](https://github.com/raxyte) in [#4847](https://github.com/tenzir/tenzir/pull/4847).

# Tenzir Node v4.25.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.25.0).

### Features

[Section titled “Features”](#features)

#### Add `to_opensearch` and `to_elasticsearch` sink operators

[Section titled “Add to\_opensearch and to\_elasticsearch sink operators”](#add-to_opensearch-and-to_elasticsearch-sink-operators)

A new operator `to_opensearch` is now available for sending data to OpenSearch-compatible Bulk API providers including ElasticSearch.

By [@raxyte](https://github.com/raxyte) in [#4871](https://github.com/tenzir/tenzir/pull/4871).

#### Add `duration(string) -> duration`

[Section titled “Add duration(string) -> duration”](#add-durationstring---duration)

The new `duration` function now allows to parse expressions resulting in strings as duration values.

By [@raxyte](https://github.com/raxyte) in [#4877](https://github.com/tenzir/tenzir/pull/4877).

#### Introduce a TQL2-only mode

[Section titled “Introduce a TQL2-only mode”](#introduce-a-tql2-only-mode)

Start your Tenzir Node with `tenzir-node --tql2` or set the `TENZIR_TQL2=true` environment variable to enable TQL2-only mode for your node. In this mode, all pipelines will run as TQL2, with the old TQL1 pipelines only being available through the `legacy` operator. In Q1 2025, this option will be enabled by default, and later in 2025 the `legacy` operator and TQL1 support will be removed entirely.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4840](https://github.com/tenzir/tenzir/pull/4840).

#### Parse `x not in y` as `not x in y`

[Section titled “Parse x not in y as not x in y”](#parse-x-not-in-y-as-not-x-in-y)

TQL2 now allows writing `x not in y` as an equivalent to `not (x in y)` for better readability.

By [@raxyte](https://github.com/raxyte) in [#4844](https://github.com/tenzir/tenzir/pull/4844).

#### Implement `ip in subnet` and `subnet in subnet`

[Section titled “Implement ip in subnet and subnet in subnet”](#implement-ip-in-subnet-and-subnet-in-subnet)

Whether an IP address is contained in a subnet can now be checked using expressions such as `1.2.3.4 in 1.2.0.0/16`. Similarly, to check whether a subnet is included in another subnet, use `1.2.0.0/16 in 1.0.0.0/8`.

By [@jachris](https://github.com/jachris) in [#4841](https://github.com/tenzir/tenzir/pull/4841).

#### Implement TQL2 `from` and `to`

[Section titled “Implement TQL2 from and to”](#implement-tql2-from-and-to)

We have added the `from` operator that allows you to easily onboard data from most sources. For example, you can now write `from "https://example.com/file.json.gz"` to automatically deduce the load operator, compression, and format.

We have added the `to` operator that allows you to easily send data to most destinations. For example, you can now write `to "ftps://example.com/file.json.gz"` to automatically deduce the save operator, compression, and format.

You can use the new `subnet(string)` function to parse strings as subnets.

By [@IyeOnline](https://github.com/IyeOnline) in [#4805](https://github.com/tenzir/tenzir/pull/4805).

#### Enhance HTTP connector controls

[Section titled “Enhance HTTP connector controls”](#enhance-http-connector-controls)

Several new options are now available for the `load_http` operator: `data`, `json`, `form`, `skip_peer_verification`, `skip_hostname_verification`, `chunked`, and `multipart`. The `skip_peer_verification` and `skip_hostname_verification` options are now also available for the `save_http` operator.

By [@mavam](https://github.com/mavam) in [#4811](https://github.com/tenzir/tenzir/pull/4811).

#### Custom quotes and doubled quote escaping

[Section titled “Custom quotes and doubled quote escaping”](#custom-quotes-and-doubled-quote-escaping)

The `read_csv`, `read_kv`, `read_ssv`, `read_tsv` and `read_xsv` operators now support custom quote characters.

The `read_csv`, `read_ssv`, `read_tsv` and `read_xsv` operators support doubled quote escaping.

The `read_csv`, `read_ssv`, `read_tsv` and `read_xsv` operators now accept multi-character strings as separators.

The `list_sep` option for the `read_csv`, `read_ssv`, `read_tsv` and `read_xsv` operators can be set to an empty string, which will disable list parsing.

The new `string.parse_leef()` function can be used to parse a string as a LEEF message.

By [@IyeOnline](https://github.com/IyeOnline) in [#4837](https://github.com/tenzir/tenzir/pull/4837).

#### Add Snowflake sink

[Section titled “Add Snowflake sink”](#add-snowflake-sink)

We have added a new `to_snowflake` sink operator, writing events into a [snowflake](https://www.snowflake.com/) table.

By [@IyeOnline](https://github.com/IyeOnline) in [#4589](https://github.com/tenzir/tenzir/pull/4589).

#### Implement `float(number|string)`

[Section titled “Implement float(number|string)”](#implement-floatnumberstring)

Numbers and string expressions containing numbers can now be converted into `float` type values using the `float` function.

By [@raxyte](https://github.com/raxyte) in [#4882](https://github.com/tenzir/tenzir/pull/4882).

#### Port `deduplicate` to TQL2

[Section titled “Port deduplicate to TQL2”](#port-deduplicate-to-tql2)

The `deduplicate` operator in TQL2 to help you remove events with a common key. The operator provides more flexibility than its TQL1 pendant by letting the common key use any expression, not just a field name. You can also control timeouts with finer granularity.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4850](https://github.com/tenzir/tenzir/pull/4850).

#### Add user-defined operators to TQL2

[Section titled “Add user-defined operators to TQL2”](#add-user-defined-operators-to-tql2)

User-defined operators can now be written and used in TQL2. To use TQL2, start your definition with the comment `// tql2`, or use the `--tql2` flag to opt into TQL2 as the default.

By [@jachris](https://github.com/jachris) in [#4884](https://github.com/tenzir/tenzir/pull/4884).

#### Implement `context::erase`

[Section titled “Implement context::erase”](#implement-contexterase)

The `context::erase` operator allows you to selectively remove entries from contexts.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4864](https://github.com/tenzir/tenzir/pull/4864).

#### `save_email` cleanup

[Section titled “save\_email cleanup”](#save_email-cleanup)

The `save_email` now accepts a `tls` option to specify TLS usage when establishing the SMTP connection.

By [@raxyte](https://github.com/raxyte) in [#4848](https://github.com/tenzir/tenzir/pull/4848).

### Changes

[Section titled “Changes”](#changes)

#### Split `compress`/`decompress` into separate operators

[Section titled “Split compress/decompress into separate operators”](#split-compressdecompress-into-separate-operators)

The `compress` and `decompress` operators have been deprecated in favor of separate operators for each compression algorithm. These new operators expose additional options, such as `compress_gzip level=10, format="deflate"`.

By [@IyeOnline](https://github.com/IyeOnline) in [#4876](https://github.com/tenzir/tenzir/pull/4876).

#### Make the expression evaluator support heterogeneous results

[Section titled “Make the expression evaluator support heterogeneous results”](#make-the-expression-evaluator-support-heterogeneous-results)

Functions can now return values of different types for the same input types. For example, `x.otherwise(y)` no longer requires that `x` has the same type as `y`.

By [@jachris](https://github.com/jachris) in [#4839](https://github.com/tenzir/tenzir/pull/4839).

#### Implement TQL2 `from` and `to`

[Section titled “Implement TQL2 from and to”](#implement-tql2-from-and-to-1)

The `topic` argument for `load_kafka` and `save_kafka` is now a positional argument, instead of a named argument.

The array version of `from` that allowed you to create multiple events has been removed. Instead, you can just pass multiple records to `from` now.

By [@IyeOnline](https://github.com/IyeOnline) in [#4805](https://github.com/tenzir/tenzir/pull/4805).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix operator parenthesis continuation

[Section titled “Fix operator parenthesis continuation”](#fix-operator-parenthesis-continuation)

Operator invocations that directly use parenthesis but continue after the closing parenthesis are no longer rejected. For example, `where (x or y) and z` is now being parsed correctly.

By [@jachris](https://github.com/jachris) in [#4885](https://github.com/tenzir/tenzir/pull/4885).

#### Fix handling of empty records in `write_parquet`

[Section titled “Fix handling of empty records in write\_parquet”](#fix-handling-of-empty-records-in-write_parquet)

`write_parquet` now gracefully handles nested empty records by replacing them with nulls. The Apache Parquet format does fundamentally not support empty nested records.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4874](https://github.com/tenzir/tenzir/pull/4874).

#### Make the expression evaluator support heterogeneous results

[Section titled “Make the expression evaluator support heterogeneous results”](#make-the-expression-evaluator-support-heterogeneous-results-1)

Metadata such as `@name` can now be set to a dynamically computed value that does not have to be a constant. For example, if the field `event_name` should be used as the event name, `@name = event_name` now correctly assigns the events their name instead of using the first value.

By [@jachris](https://github.com/jachris) in [#4839](https://github.com/tenzir/tenzir/pull/4839).

#### `save_email` cleanup

[Section titled “save\_email cleanup”](#save_email-cleanup-1)

The `endpoint` argument of the `save_email` operator was documented as optional but was not parsed as so. This has been fixed and the argument is now correctly optional.

By [@raxyte](https://github.com/raxyte) in [#4848](https://github.com/tenzir/tenzir/pull/4848).

#### Fix pipeline manager discarding parse-time warnings

[Section titled “Fix pipeline manager discarding parse-time warnings”](#fix-pipeline-manager-discarding-parse-time-warnings)

Warnings that happen very early during pipeline startup now correctly show up in the Tenzir Platform.

By [@jachris](https://github.com/jachris) in [#4867](https://github.com/tenzir/tenzir/pull/4867).

#### Validate legacy expressions when splitting for predicate pushdown

[Section titled “Validate legacy expressions when splitting for predicate pushdown”](#validate-legacy-expressions-when-splitting-for-predicate-pushdown)

Pipelines that begin with `export | where` followed by an expression that does not depend on the incoming events, such as `export | where 1 == 1`, no longer cause an internal error.

By [@jachris](https://github.com/jachris) in [#4861](https://github.com/tenzir/tenzir/pull/4861).

# Tenzir Node v4.26.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.26.0).

### Features

[Section titled “Features”](#features)

#### Introduce CAF metrics

[Section titled “Introduce CAF metrics”](#introduce-caf-metrics)

`metrics "caf"` offers insights into Tenzir’s underlying actor system. This is primarily aimed at developers for performance benchmarking.

The new `merge` function combines two records. `merge(foo, bar)` is a shorthand for `{...foo, ...bar}`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4897](https://github.com/tenzir/tenzir/pull/4897).

#### TQL printer

[Section titled “TQL printer”](#tql-printer)

You can use the new `write_tql` operator to print events as TQL expressions.

We added `strip` options to `write_json` and `write_ndjson`, allowing you to strip null fields as well as empty records or lists.

By [@IyeOnline](https://github.com/IyeOnline) in [#4921](https://github.com/tenzir/tenzir/pull/4921).

#### Implement `match_regex`

[Section titled “Implement match\_regex”](#implement-match_regex)

You can use the new `string.match_regex(regex:string)` function to check whether a string partially matches a regular expression.

By [@IyeOnline](https://github.com/IyeOnline) in [#4917](https://github.com/tenzir/tenzir/pull/4917).

#### Implement `to_asl` operator

[Section titled “Implement to\_asl operator”](#implement-to_asl-operator)

We added a `to_asl` operator that can be used to send OCSF normalized events to an Amazon Security Lake.

By [@IyeOnline](https://github.com/IyeOnline) in [#4911](https://github.com/tenzir/tenzir/pull/4911).

### Changes

[Section titled “Changes”](#changes)

#### TQL printer

[Section titled “TQL printer”](#tql-printer-1)

The implicit sources and sinks that can be set via commandline options or configuration now use TQL2.

The default implicit event sink now writes TQL values instead of JSON.

By [@IyeOnline](https://github.com/IyeOnline) in [#4921](https://github.com/tenzir/tenzir/pull/4921).

#### Fix overzealous parameter validation in `/pipeline/launch`

[Section titled “Fix overzealous parameter validation in /pipeline/launch”](#fix-overzealous-parameter-validation-in-pipelinelaunch)

Contexts persist less frequently now in the background, reducing their resource usage.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4919](https://github.com/tenzir/tenzir/pull/4919).

#### Improve configured pipeline startup errors

[Section titled “Improve configured pipeline startup errors”](#improve-configured-pipeline-startup-errors)

Errors from the startup of configured pipelines, including those coming from configured packages, now have improved rendering.

By [@jachris](https://github.com/jachris) in [#4886](https://github.com/tenzir/tenzir/pull/4886).

#### Use adaptive resolution and `Z` suffix in timestamp printer

[Section titled “Use adaptive resolution and Z suffix in timestamp printer”](#use-adaptive-resolution-and-z-suffix-in-timestamp-printer)

Timestamps are now printed with a `Z` suffix to indicate that they are relative to UTC. Furthermore, the fractional part of the seconds is no longer always printed using 6 digits. Instead, timestamps that do not have sub-second information no longer have a fractional part. Other timestamps are either printed with 3, 6 or 9 fractional digits, depending on their resolution.

Durations that are printed as minutes now use `min` instead of `m`. Additionally, the fractional part of durations is now printed with full precision instead of being rounded to two digits.

By [@jachris](https://github.com/jachris) in [#4916](https://github.com/tenzir/tenzir/pull/4916).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix CONVERSION part for GROK patterns

[Section titled “Fix CONVERSION part for GROK patterns”](#fix-conversion-part-for-grok-patterns)

We fixed a bug which broke the CONVERSION part of the GROK pattern semantic.

By [@IyeOnline](https://github.com/IyeOnline) in [#4939](https://github.com/tenzir/tenzir/pull/4939).

#### Fix overzealous parameter validation in `/pipeline/launch`

[Section titled “Fix overzealous parameter validation in /pipeline/launch”](#fix-overzealous-parameter-validation-in-pipelinelaunch-1)

We fixed an overzealous parameter validation bug that prevented the use of the `/pipeline/launch` API endpoint when specifying a `cache_id` without a `serve_id` when `definition` contained a definition for a pipeline without a sink.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4919](https://github.com/tenzir/tenzir/pull/4919).

# Tenzir Node v4.27.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.27.0).

### Features

[Section titled “Features”](#features)

#### Add plugin options to enable self-signed platform certificates

[Section titled “Add plugin options to enable self-signed platform certificates”](#add-plugin-options-to-enable-self-signed-platform-certificates)

The `platform` plugin now understands the `skip-peer-verification` and `cacert` options in order to enable connections to self-hosted platform instances with self-signed TLS certificates.

By [@lava](https://github.com/lava) in [#4918](https://github.com/tenzir/tenzir/pull/4918).

#### Implement `chart_line`, `chart_area`, `chart_bar` and `chart_pie`

[Section titled “Implement chart\_line, chart\_area, chart\_bar and chart\_pie”](#implement-chart_line-chart_area-chart_bar-and-chart_pie)

Charting functionality is now available in TQL2 via the new `chart_area`, `chart_bar`, `chart_line` and `chart_pie` operators and the [Tenzir Platform](https://app.tenzir.com).

By [@raxyte](https://github.com/raxyte) in [#4888](https://github.com/tenzir/tenzir/pull/4888).

#### Implement `unroll` for records

[Section titled “Implement unroll for records”](#implement-unroll-for-records)

The `unroll` operator now works for record fields as well as lists. The operator duplicates the surrounding event for every field in the specified record.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4934](https://github.com/tenzir/tenzir/pull/4934).

#### Implement AWS MSK IAM Authentication Mechanism for `{load,save}_kafka`

[Section titled “Implement AWS MSK IAM Authentication Mechanism for {load,save}\_kafka”](#implement-aws-msk-iam-authentication-mechanism-for-loadsave_kafka)

The `load_kafka` and `save_kafka` operators can now authenticate with AWS MSK using IAM via the new `aws_iam` options.

By [@raxyte](https://github.com/raxyte) in [#4944](https://github.com/tenzir/tenzir/pull/4944).

#### Add `batches` to input/output operator metrics

[Section titled “Add batches to input/output operator metrics”](#add-batches-to-inputoutput-operator-metrics)

`metrics "operator"` now includes a new `batches` field under the `input` and `output` records that counts how many batches of events or bytes the metric was generated from.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4962](https://github.com/tenzir/tenzir/pull/4962).

#### Add `tenzir.retention` configuration

[Section titled “Add tenzir.retention configuration”](#add-tenzirretention-configuration)

The new `tenzir.retention.metrics` and `tenzir.retention.diagnostics` configuration options configure how long Tenzir Nodes retain metrics and diagnostics. The policies are checked once every hour.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4949](https://github.com/tenzir/tenzir/pull/4949).

### Changes

[Section titled “Changes”](#changes)

#### Deprecate TQL1

[Section titled “Deprecate TQL1”](#deprecate-tql1)

TQL1 is now deprecated in favor of TQL2. Starting a TQL1 pipeline issues a warning on startup nudging towards upgrading to TQL2, which will become the default in the upcoming Tenzir v5.0 release. This warning cannot be turned off.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4954](https://github.com/tenzir/tenzir/pull/4954).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Implement AWS MSK IAM Authentication Mechanism for `{load,save}_kafka`

[Section titled “Implement AWS MSK IAM Authentication Mechanism for {load,save}\_kafka”](#implement-aws-msk-iam-authentication-mechanism-for-loadsave_kafka-1)

Operators such as `load_kafka`, `load_s3` etc can now correctly read their respective YAML configs.

By [@raxyte](https://github.com/raxyte) in [#4944](https://github.com/tenzir/tenzir/pull/4944).

# Tenzir Node v4.28.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.28.0).

### Features

[Section titled “Features”](#features)

#### Implement `parse_syslog`

[Section titled “Implement parse\_syslog”](#implement-parse_syslog)

You can use the new `parse_syslog` function to parse a string as a syslog message.

By [@IyeOnline](https://github.com/IyeOnline) in [#4980](https://github.com/tenzir/tenzir/pull/4980).

#### Add explicit `tls` options to `to_opensearch` and `to_splunk`

[Section titled “Add explicit tls options to to\_opensearch and to\_splunk”](#add-explicit-tls-options-to-to_opensearch-and-to_splunk)

`to_opensearch` and `to_splunk` now feature an explicit `tls` option.

By [@IyeOnline](https://github.com/IyeOnline) in [#4983](https://github.com/tenzir/tenzir/pull/4983).

#### More parsing functions

[Section titled “More parsing functions”](#more-parsing-functions)

It is now possible to define additional patterns in the `parse_grok` function.

The `read_xsv` family of parsers now accept the `header` as a list of strings as an alternative to a single delimited string.

`read_grok` now accepts additional `pattern_definitions` as either a `record` mapping from pattern name to definition or a `string` of newline separated patterns definitions.

We introduced the `parse_csv`, `parse_kv`, `parse_ssv`, `parse_tsv`, `parse_xsv` and `parse_yaml` functions, allowing you to parse strings as those formats.

The `map` function now handles cases where list elements mapped to different types.

By [@IyeOnline](https://github.com/IyeOnline) in [#4933](https://github.com/tenzir/tenzir/pull/4933).

#### Implement `load_stdin` and `save_stdout`

[Section titled “Implement load\_stdin and save\_stdout”](#implement-load_stdin-and-save_stdout)

The new `load_stdin` operator accepts bytes from standard input, while `save_stdout` writes bytes to standard output.

By [@IyeOnline](https://github.com/IyeOnline) in [#4969](https://github.com/tenzir/tenzir/pull/4969).

### Changes

[Section titled “Changes”](#changes)

#### Stop collecting metrics for hidden pipelines

[Section titled “Stop collecting metrics for hidden pipelines”](#stop-collecting-metrics-for-hidden-pipelines)

`metrics "operator"` no longer includes metrics from hidden pipelines, such as pipelines run under-the-hood by the Tenzir Platform.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4966](https://github.com/tenzir/tenzir/pull/4966).

#### Evict old caches when exceeding capacity limits

[Section titled “Evict old caches when exceeding capacity limits”](#evict-old-caches-when-exceeding-capacity-limits)

Unless specified explicitly, the `cache` has no more default capacity in terms of number of events per cache. Instead, the node now tracks the global cache capacity in number of bytes. This is limited to 1GiB by default, and can be configured with the `tenzir.cache.capacity` option. For practical reasons, we require at least 64MiB of caches.

The default `write_timeout` of caches increased from 1 minute to 10 minutes, and can now be configured with the `tenzir.cache.lifetime` option.

The `/serve` endpoint now returns an additional field `state`, which can be one of `running`, `completed`, or `failed`, indicating the status of the pipeline with the corresponding `serve` operator at the time of the request.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4984](https://github.com/tenzir/tenzir/pull/4984).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### More parsing functions

[Section titled “More parsing functions”](#more-parsing-functions-1)

Re-defining a predefined grok pattern no longer terminates the application.

The `string.parse_json()` function can now parse single numbers or strings instead of only objects.

`read_leef` and `parse_leef` now include the `event_class_id` in their output.

`read_yaml` now properly parses numbers as numbers.

By [@IyeOnline](https://github.com/IyeOnline) in [#4933](https://github.com/tenzir/tenzir/pull/4933).

#### Fix shutdown of the lookup helper actor

[Section titled “Fix shutdown of the lookup helper actor”](#fix-shutdown-of-the-lookup-helper-actor)

We sqashed a bug that prevented the `tenzir-node` process from exiting cleanly while the `lookup` operator was used in a pipeline.

By [@tobim](https://github.com/tobim) in [#4978](https://github.com/tenzir/tenzir/pull/4978).

#### Evict old caches when exceeding capacity limits

[Section titled “Evict old caches when exceeding capacity limits”](#evict-old-caches-when-exceeding-capacity-limits-1)

We fixed an up to 60 seconds hang in requests to the `/serve` endpoint when the request was issued after the pipeline with the corresponding `serve` operator was started and before it finished with an error and without results.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4984](https://github.com/tenzir/tenzir/pull/4984).

# Tenzir Node v4.28.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.28.2).

### Features

[Section titled “Features”](#features)

#### Add `fill` option to chart operators

[Section titled “Add fill option to chart operators”](#add-fill-option-to-chart-operators)

The new `fill` option on `chart_area`, `chart_bar` and `chart_line` allows you to specify a value to replace `null`s with and fill gaps.

By [@raxyte](https://github.com/raxyte) in [#4967](https://github.com/tenzir/tenzir/pull/4967).

#### Fix `from`/`to` not respecting default formats

[Section titled “Fix from/to not respecting default formats”](#fix-fromto-not-respecting-default-formats)

The `from` and `to` operators now assume `http` and `https` URLs to produce or accept JSON, unless the filename in the URL contains a known file extension.

By [@IyeOnline](https://github.com/IyeOnline) in [#4990](https://github.com/tenzir/tenzir/pull/4990).

#### Fix crash in AMQP and add JSON `arrays_of_objects` option

[Section titled “Fix crash in AMQP and add JSON arrays\_of\_objects option”](#fix-crash-in-amqp-and-add-json-arrays_of_objects-option)

The `write_json` and `write_ndjson` operators now have a `arrays_of_objects` option.

By [@IyeOnline](https://github.com/IyeOnline) in [#4994](https://github.com/tenzir/tenzir/pull/4994).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix crash in AMQP and add JSON `arrays_of_objects` option

[Section titled “Fix crash in AMQP and add JSON arrays\_of\_objects option”](#fix-crash-in-amqp-and-add-json-arrays_of_objects-option-1)

We fixed a bug in `load_amqp` and `save_amqp` that prevented the node from starting if they were used in a pipeline configured as code and failed to connect.

By [@IyeOnline](https://github.com/IyeOnline) in [#4994](https://github.com/tenzir/tenzir/pull/4994).

# Tenzir Node v4.29.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.29.0).

### Features

[Section titled “Features”](#features)

#### Implement sub-duration functions

[Section titled “Implement sub-duration functions”](#implement-sub-duration-functions)

New functions `years`, `months`, `weeks`, `days`, `hours`, `minutes`, `seconds`, `milliseconds`, `microseconds` and `nanoseconds` convert a numeric value to the equivalent duration. Their counterpart `count_*` functions calculate how many units can the duration be broken into, i.e. `duration / unit`.

The `abs` function calculates the absolute value for a number or a duration.

By [@raxyte](https://github.com/raxyte) in [#4985](https://github.com/tenzir/tenzir/pull/4985).

#### Implement `base` option for `int()` and `uint()`

[Section titled “Implement base option for int() and uint()”](#implement-base-option-for-int-and-uint)

The new `base` option for `int` and `uint` functions allows parsing hexadecimal numbers in strings.

By [@raxyte](https://github.com/raxyte) in [#5006](https://github.com/tenzir/tenzir/pull/5006).

#### Implement some `print_*` functions

[Section titled “Implement some print\_\* functions”](#implement-some-print_-functions)

The `write_xsv` family of operators now accepts multi-character separators, instead of being restricted to a single character.

We added the `write_kv` operator, allowing you to write events as Key-Value pairs.

We added the functions `any.print_json()` and `any.print_yaml()` to print any value as a JSON or YAML string.

We added the functions `record.print_kv()`, `record.print_csv()`, `record.print_ssv()`, `record.print_tsv()` and `record.print_xsv()` to print records as the respective format.

By [@IyeOnline](https://github.com/IyeOnline) in [#5001](https://github.com/tenzir/tenzir/pull/5001).

### Changes

[Section titled “Changes”](#changes)

#### Compact NDJSON output

[Section titled “Compact NDJSON output”](#compact-ndjson-output)

The output of `write_ndjson` is now more compact and no longer includes unnecessary whitespace. Additionally, `write_json` no longer prints a trailing whitespace after each comma.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5015](https://github.com/tenzir/tenzir/pull/5015).

#### Allow values to be `null` when charting

[Section titled “Allow values to be null when charting”](#allow-values-to-be-null-when-charting)

The `chart_area`, `chart_bar`, and `chart_pie` operators no longer reject null-values. Previously, gaps in charts were only supported for `chart_line`.

By [@raxyte](https://github.com/raxyte) in [#5009](https://github.com/tenzir/tenzir/pull/5009).

#### Implement some `print_*` functions

[Section titled “Implement some print\_\* functions”](#implement-some-print_-functions-1)

The `sep` argument on the `flatten` and `unflatten` functions is now a positional argument, allowing you to simply write `record.flatten("-")`.

The `unflatten` option found on many `read_*` operators and `parse_*` functions is now called `unflatten_separator`.

The `field_sep`, `list_sep` and `null_value` options on the XSV operators and functions (such as `read_xsv`, `write_csv` or `parse_tsv`) are now named arguments on all of them and are called `field_separator`, `list_separator` and `null_value`.

The `field_split` and `list_split` arguments for the `read_kv` operator and `parse_kv` function are now named arguments.

By [@IyeOnline](https://github.com/IyeOnline) in [#5001](https://github.com/tenzir/tenzir/pull/5001).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add to\_splunk to the ce binary packages

[Section titled “Add to\_splunk to the ce binary packages”](#add-to_splunk-to-the-ce-binary-packages)

We now include the `to_splunk` operator in the Tenzir binary packages.

By [@tobim](https://github.com/tobim) in [#5012](https://github.com/tenzir/tenzir/pull/5012).

#### Normalize pushed-up predicates in `subscribe`

[Section titled “Normalize pushed-up predicates in subscribe”](#normalize-pushed-up-predicates-in-subscribe)

We fixed an optimization bug that caused pipelines of the form `subscribe <topic> | where <value> in <field>` to evaluate the predicate `<field> in <value>` instead, returning incorrect results from the pipeline.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5014](https://github.com/tenzir/tenzir/pull/5014).

#### Respect `--color` option in default implicit events sink

[Section titled “Respect --color option in default implicit events sink”](#respect---color-option-in-default-implicit-events-sink)

The implicit events sink of the `tenzir` binary now respects the `--color=[always|never|auto]` option and the `NO_COLOR` environment variable. Previously, color usage was only determined based on whether `stdout` had a TTY attached.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5007](https://github.com/tenzir/tenzir/pull/5007).

#### Fix a crash in `{parse,read}_grok` for invalid patterns

[Section titled “Fix a crash in {parse,read}\_grok for invalid patterns”](#fix-a-crash-in-parseread_grok-for-invalid-patterns)

The `read_grok` operator and `parse_grok` functions no longer crash when providing an invalid Grok expression.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5018](https://github.com/tenzir/tenzir/pull/5018).

#### Fix compilation error handling inside `if`

[Section titled “Fix compilation error handling inside if”](#fix-compilation-error-handling-inside-if)

A compilation error within an `if` statement no longer causes pipelines to crash.

By [@jachris](https://github.com/jachris) in [#5011](https://github.com/tenzir/tenzir/pull/5011).

#### Remove type layering

[Section titled “Remove type layering”](#remove-type-layering)

We fixed a bug that caused `type_id(this)` to return inconsistent values for schemas with metadata attached, e.g., after assigning a schema name via `@name = "new_name"` or using operators like `chart_line` that add custom metadata to a schema for use of the Tenzir Platform. Unfortunately, this may cause charts or tables added to dashboards before Tenzir Platform v1.7 to break. To fix them, click on the action menu on the chart or table on the dashboard, click “Open in Explorer,” and re-add the chart or table to the dashboard. We are sorry about this inconvenience.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5008](https://github.com/tenzir/tenzir/pull/5008).

# Tenzir Node v4.29.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.29.1).

### Features

[Section titled “Features”](#features)

#### Allow assigning ids for pipelines manually

[Section titled “Allow assigning ids for pipelines manually”](#allow-assigning-ids-for-pipelines-manually)

The `/pipeline/create` and `/pipeline/launch` endpoints now accept an optional `id` parameter for assigning the pipeline’s ID manually.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5026](https://github.com/tenzir/tenzir/pull/5026).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Make `drop` not remove empty records

[Section titled “Make drop not remove empty records”](#make-drop-not-remove-empty-records)

Dropping all fields from a record with the `drop` operator no longer removes the record itself. For example, `from {x: {y: 0}} | drop x.y` now returns `{x: {}}` instead of `{}`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5021](https://github.com/tenzir/tenzir/pull/5021).

#### Fix crash in the MSB in merging mode

[Section titled “Fix crash in the MSB in merging mode”](#fix-crash-in-the-msb-in-merging-mode)

We fixed a bug in the `read_xsv` and `parse_xsv` family of operators and functions that caused the parser to fail unexpectedly when the data contained a list (as specified through the list separator) for fields where the provided `schema` did not expect lists.

By [@IyeOnline](https://github.com/IyeOnline) in [#5028](https://github.com/tenzir/tenzir/pull/5028).

#### Fix `tls` option of `to_splunk`

[Section titled “Fix tls option of to\_splunk”](#fix-tls-option-of-to_splunk)

Using the `tls` option of the `to_splunk` operator no longer directly emits an error.

By [@jachris](https://github.com/jachris) in [#5027](https://github.com/tenzir/tenzir/pull/5027).

#### Make UDO resolution order-independent

[Section titled “Make UDO resolution order-independent”](#make-udo-resolution-order-independent)

The resolution of user-defined operator aliases in the `tenzir.operators` section is no longer order-dependent. Previously, an operator `foo` may have depended on an operator `bar`, but not the other way around. This limitation no longer exists.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5029](https://github.com/tenzir/tenzir/pull/5029).

#### Fix startup delay in `from_fluent_bit`

[Section titled “Fix startup delay in from\_fluent\_bit”](#fix-startup-delay-in-from_fluent_bit)

We fixed a bug that caused pipelines with `from_fluent_bit` to not report their startup successfully, causing errors when deploying pipelines starting with the operator through the Tenzir Platform.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5025](https://github.com/tenzir/tenzir/pull/5025).

# Tenzir Node v4.29.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.29.2).

### Features

[Section titled “Features”](#features)

#### Make debugging `load_tcp` pipelines easier

[Section titled “Make debugging load\_tcp pipelines easier”](#make-debugging-load_tcp-pipelines-easier)

The newly added `max_buffered_chunks` for `load_tcp` controls how many reads the operator schedules in advance on the socket. The option defaults to 10.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5040](https://github.com/tenzir/tenzir/pull/5040).

### Changes

[Section titled “Changes”](#changes)

#### Improve panics to show stacktrace in diagnostics and logs

[Section titled “Improve panics to show stacktrace in diagnostics and logs”](#improve-panics-to-show-stacktrace-in-diagnostics-and-logs)

We have improved how internal errors are presented to the user, making it easier to report and analyze bugs.

By [@jachris](https://github.com/jachris) in [#5023](https://github.com/tenzir/tenzir/pull/5023).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Ignore additional fields in package config

[Section titled “Ignore additional fields in package config”](#ignore-additional-fields-in-package-config)

Installing packages no longer fails when packages contain additional fields, and instead warns about the unexpected fields.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5031](https://github.com/tenzir/tenzir/pull/5031).

#### Fix splitting logic for heterogeneous evaluation

[Section titled “Fix splitting logic for heterogeneous evaluation”](#fix-splitting-logic-for-heterogeneous-evaluation)

Expressions that have varying output types for the same input types (mostly the `parse_*` family of functions) no longer trigger an assertion on certain inputs.

By [@jachris](https://github.com/jachris) in [#5043](https://github.com/tenzir/tenzir/pull/5043).

#### Fix hang in `cache` when creating an empty cache

[Section titled “Fix hang in cache when creating an empty cache”](#fix-hang-in-cache-when-creating-an-empty-cache)

The `cache` operator no longer hangs indefinitely when creating a new cache from a pipeline that returned zero events. For example, the pipeline `from {} | head 0 | cache "whoops"` never exited before this fix.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5042](https://github.com/tenzir/tenzir/pull/5042).

#### Avoid idle wakeups in `load_tcp`

[Section titled “Avoid idle wakeups in load\_tcp”](#avoid-idle-wakeups-in-load_tcp)

We fixed a bug that caused unnecessary idle wakeups in the `load_tcp` operator, throwing off scheduling of pipelines using it. Under rare circumstances, this could also lead to partially duplicated output of the operator’s nested pipeline.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5035](https://github.com/tenzir/tenzir/pull/5035).

#### Fix cache eviction always happening on maximally large caches

[Section titled “Fix cache eviction always happening on maximally large caches”](#fix-cache-eviction-always-happening-on-maximally-large-caches)

We fixed a bug in the `cache` operator that caused caches that were capped just short of the `tenzir.cache.capacity` option to still get evicted immediately.

By [@Avaq](https://github.com/Avaq) in [#5039](https://github.com/tenzir/tenzir/pull/5039).

#### Fix mismatch in type metadata after assignments

[Section titled “Fix mismatch in type metadata after assignments”](#fix-mismatch-in-type-metadata-after-assignments)

We fixed a bug that caused a loss of type names for nested fields in assignments, causing field metadata in `write_feather` and `write_parquet` to be missing.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5033](https://github.com/tenzir/tenzir/pull/5033).

#### Change how signed ints are rendered in logs

[Section titled “Change how signed ints are rendered in logs”](#change-how-signed-ints-are-rendered-in-logs)

We fixed a bug in the `from_fluent_bit` and `to_fluent_bit` operators that caused positive integer options to be forwarded with a leading `+`. For example, `options={port: 9200}` forwarded the option `port=+9200` to Fluent Bit.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5037](https://github.com/tenzir/tenzir/pull/5037).

# Tenzir Node v4.3.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.3.0).

### Features

[Section titled “Features”](#features)

#### Add support for reading and writing YAML documents

[Section titled “Add support for reading and writing YAML documents”](#add-support-for-reading-and-writing-yaml-documents)

The `yaml` format supports reading and writing YAML documents and streams.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3456](https://github.com/tenzir/tenzir/pull/3456).

#### PRs 3461-fluent-bit

[Section titled “PRs 3461-fluent-bit”](#prs-3461-fluent-bit)

The new `fluent-bit` source and sink operator provide and interface to the Fluent Bit ecosystem. The source operator maps to a Fluent Bit *input* and the sink operator to a Fluent Bit *output*.

By [@tobim](https://github.com/tobim) in [#3461](https://github.com/tenzir/tenzir/pull/3461).

#### Add pipeline label support to the main repository

[Section titled “Add pipeline label support to the main repository”](#add-pipeline-label-support-to-the-main-repository)

The pipeline manager now supports user-provided labels for pipelines.

By [@Dakostu](https://github.com/Dakostu) in [#3541](https://github.com/tenzir/tenzir/pull/3541).

#### Improve `json` parser, add `null` type, and various fixes

[Section titled “Improve json parser, add null type, and various fixes”](#improve-json-parser-add-null-type-and-various-fixes)

The performance of the `json`, `suricata` and `zeek-json` parsers was improved.

The `json` parser has a new `--raw` flag, which uses the raw type of JSON values instead of trying to infer one. For example, strings with ip addresses are given the type `string` instead of `ip`.

A dedicated `null` type was added.

Empty records are now allowed. Operators that previously discarded empty records (for example, `drop`) now preserve them.

By [@jachris](https://github.com/jachris) in [#3503](https://github.com/tenzir/tenzir/pull/3503).

### Changes

[Section titled “Changes”](#changes)

#### Disable dense indexes

[Section titled “Disable dense indexes”](#disable-dense-indexes)

Tenzir no longer builds dense indexes for imported events. Dense indexes improved query performance at the cost of a higher memory usage. However, over time the performance improvement became smaller due to other improvements in the underlying storage engine.

Tenzir no longer supports models in taxonomies. Since Tenzir v4.0 they were only supported in the deprecated `tenzir-ctl export` and `tenzir-ctl count` commands. We plan to bring the functionality back in the future with more powerful expressions in TQL.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3552](https://github.com/tenzir/tenzir/pull/3552).

#### Disable 0mq socket lingering

[Section titled “Disable 0mq socket lingering”](#disable-0mq-socket-lingering)

We made it easier to reuse the default `zmq` socket endpoint by disabling *socket lingering*, and thereby immediately relinquishing resources when terminating a ZeroMQ pipeline. Changing the linger period from infinite to 0 no longer buffers pending messages in memory after closing a ZeroMQ socket.

By [@mavam](https://github.com/mavam) in [#3536](https://github.com/tenzir/tenzir/pull/3536).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix timing-related crashes in the `web` plugin

[Section titled “Fix timing-related crashes in the web plugin”](#fix-timing-related-crashes-in-the-web-plugin)

The web server will not crash when receiving requests during shutdown anymore.

By [@Dakostu](https://github.com/Dakostu) in [#3553](https://github.com/tenzir/tenzir/pull/3553).

#### Change type of `version` in `suricata.quic` to `string`

[Section titled “Change type of version in suricata.quic to string”](#change-type-of-version-in-suricataquic-to-string)

The type of the `quic.version` field in the built-in `suricata.quic` schema was fixed. It now is a string instead of an integer.

By [@jachris](https://github.com/jachris) in [#3533](https://github.com/tenzir/tenzir/pull/3533).

#### Implement serialization\_plugins for store\_plugins

[Section titled “Implement serialization\_plugins for store\_plugins”](#implement-serialization_plugins-for-store_plugins)

The `parquet` and `feather` formats no longer throw assertions during normal usage anymore.

By [@Dakostu](https://github.com/Dakostu) in [#3537](https://github.com/tenzir/tenzir/pull/3537).

#### Fix the wrong type for the version record type in the `zeek.software` schema

[Section titled “Fix the wrong type for the version record type in the zeek.software schema”](#fix-the-wrong-type-for-the-version-record-type-in-the-zeeksoftware-schema)

The `zeek.software` does not contain an incomplete `version` record type anymore.

The `version.minor` type in the `zeek.software` schema is now a `uint64` instead of a `double` to comply with Zeek’s version structure.

By [@Dakostu](https://github.com/Dakostu) in [#3538](https://github.com/tenzir/tenzir/pull/3538).

#### Improve `json` parser, add `null` type, and various fixes

[Section titled “Improve json parser, add null type, and various fixes”](#improve-json-parser-add-null-type-and-various-fixes-1)

The `json`, `suricata` and `zeek-json` parsers are now more stable and should now parse all inputs correctly.

`null` records are no longer incorrectly transformed into records with `null` fields anymore.

By [@jachris](https://github.com/jachris) in [#3503](https://github.com/tenzir/tenzir/pull/3503).

#### Fix processing of http arguments

[Section titled “Fix processing of http arguments”](#fix-processing-of-http-arguments)

The `http` loader no longer ignores the value user-provided custom headers.

By [@mavam](https://github.com/mavam) in [#3535](https://github.com/tenzir/tenzir/pull/3535).

# Tenzir Node v4.30.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.30.0).

### Features

[Section titled “Features”](#features)

#### Add a global ca-certificates config option

[Section titled “Add a global ca-certificates config option”](#add-a-global-ca-certificates-config-option)

We introduced common TLS settings for all operators that support TLS. The Tenzir config now has a key `cacert`, which will set the CA certificate file for all operators using it. The default for this will be chosen appropriately for the system.

By [@tobim](https://github.com/tobim) in [#5022](https://github.com/tenzir/tenzir/pull/5022).

#### Add `to_clickhouse` operator

[Section titled “Add to\_clickhouse operator”](#add-to_clickhouse-operator)

We have added a new `to_clickhouse` operator, which enables you to easily send events to ClickHouse.

By [@IyeOnline](https://github.com/IyeOnline) in [#5032](https://github.com/tenzir/tenzir/pull/5032).

#### Port `load_gcs` and `save_gcs` to TQL2

[Section titled “Port load\_gcs and save\_gcs to TQL2”](#port-load_gcs-and-save_gcs-to-tql2)

The `load_gcs` and `save_gcs` operators are now available in TQL2 to interact with Google Cloud Storage.

By [@raxyte](https://github.com/raxyte) in [#5054](https://github.com/tenzir/tenzir/pull/5054).

#### Implement an `assert_throughput` operator

[Section titled “Implement an assert\_throughput operator”](#implement-an-assert_throughput-operator)

The new `assert_throughput` operators allows for checking whether a pipeline meets the minimum expected throughput at any place in the pipeline.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5055](https://github.com/tenzir/tenzir/pull/5055).

#### Introduce `metrics "pipeline"`

[Section titled “Introduce metrics "pipeline"”](#introduce-metrics-pipeline)

`metrics "pipeline"` provides an easy way to view the ingress and egress of pipelines. The new metrics show the ingress and egress of every pipeline in windows of ten seconds.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5024](https://github.com/tenzir/tenzir/pull/5024).

### Changes

[Section titled “Changes”](#changes)

#### Add a global ca-certificates config option

[Section titled “Add a global ca-certificates config option”](#add-a-global-ca-certificates-config-option-1)

The `skip_host_verification` option has been removed from the `load_http`, `save_email` and `save_http` operators. Its functionality has been merged into the `skip_peer_verification` option.

By [@tobim](https://github.com/tobim) in [#5022](https://github.com/tenzir/tenzir/pull/5022).

#### Introduce `metrics "pipeline"`

[Section titled “Introduce metrics "pipeline"”](#introduce-metrics-pipeline-1)

`metrics "operator"` is now deprecated. Use `metrics "pipeline"` instead, which offers a pre-aggregated view of pipeline metrics. We plan to remove operator metrics in an upcoming release, as they are too expensive in large-scale deployments.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5024](https://github.com/tenzir/tenzir/pull/5024).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix passing string params to `{from,to}_fluent_bit`

[Section titled “Fix passing string params to {from,to}\_fluent\_bit”](#fix-passing-string-params-to-fromto_fluent_bit)

We fixed a regression that caused strings passed as options to the `from_fluent_bit` and `to_fluent_bit` operators to incorrectly be surrounded by double quotes.

`to_fluent_bit` incorrectly reported zero bytes being pushed to the Fluent Bit engine as an error. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5053](https://github.com/tenzir/tenzir/pull/5053).

#### Fix bug that caused `read_zeek_tsv` to produce invalid fields

[Section titled “Fix bug that caused read\_zeek\_tsv to produce invalid fields”](#fix-bug-that-caused-read_zeek_tsv-to-produce-invalid-fields)

The `read_zeek_tsv` operator sometimes produced an invalid field with the name `\0` for types without a schema specified. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5052](https://github.com/tenzir/tenzir/pull/5052).

#### Fix crash in `from "path/to/file.ndjson"`

[Section titled “Fix crash in from "path/to/file.ndjson"”](#fix-crash-in-from-pathtofilendjson)

The `from` operator no longer incorrectly attempts to use parsers with a known file extension that is a suffix of the actual file extension. For example, `from "file.foojson"` will no longer attempt to use the `json` parser by default, while `from "file.foo.json"` and `from "file.json"` continue to work as expected. This fixes an error for `.ndjson` files, which could previously not decide between the `json` and `ndjson` parsers.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5050](https://github.com/tenzir/tenzir/pull/5050).

# Tenzir Node v4.30.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.30.1).

### Features

[Section titled “Features”](#features)

#### Move rebatching into the `importer` actor

[Section titled “Move rebatching into the importer actor”](#move-rebatching-into-the-importer-actor)

The import buffer timeout is now configurable via the `tenzir.import-buffer-timeout` option. The option defaults to 1 second, and controls how long the `import` operator buffers events for batching before forwarding them. Set the option to `0s` to enable an unbuffered mode with minimal latency, or to a higher value to increase performance.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5056](https://github.com/tenzir/tenzir/pull/5056).

### Changes

[Section titled “Changes”](#changes)

#### Move rebatching into the `importer` actor

[Section titled “Move rebatching into the importer actor”](#move-rebatching-into-the-importer-actor-1)

The default value for the `tenzir.active-partition-timeout` option increased from 30s to 5min. The option controls how long the `import` operators waits until flushing events to disk. In the past, this value was set so low because the `export` operator was only able to access already flushed events. This is no longer the case, removing the need for the low timeout. Note that the `import` operator always immediately flushes events after a pipeline with `import` completes, or when the node shuts down.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5056](https://github.com/tenzir/tenzir/pull/5056).

# Tenzir Node v4.30.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.30.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix fluentbit engine stop logic and disable TLS default

[Section titled “Fix fluentbit engine stop logic and disable TLS default”](#fix-fluentbit-engine-stop-logic-and-disable-tls-default)

Fixed an error in the `{from,to}_fluent_bit` operators that would cause it to fail to start successfully when using an input plugin (in particular the `elasticsearch` plugin) when the TLS setting was enabled without specifying a keyfile.

By [@lava](https://github.com/lava) in [#5070](https://github.com/tenzir/tenzir/pull/5070).

# Tenzir Node v4.30.3

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.30.3).

### Features

[Section titled “Features”](#features)

#### Make schema definitions represent the type system exactly

[Section titled “Make schema definitions represent the type system exactly”](#make-schema-definitions-represent-the-type-system-exactly)

We introduced a new `type_of(x: any) -> record` function that returns the exact type definition of a TQL expression. For example, `this = type_of(this)` replaces an event with its schema’s definition.

The `/serve` endpoint gained a new option `schema`, which can be set to `legacy` (default), `exact`, or `never`. The `legacy` option causes the schema definition to be rendered in a simplified way, which is the current default. The `exact` option causes the schema definitions to be rendered exactly without omitting any information. Set the option to `never` to omit schema definitions entirely.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5062](https://github.com/tenzir/tenzir/pull/5062).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `engine::push` and co. for fluent-bit

[Section titled “Fix engine::push and co. for fluent-bit”](#fix-enginepush-and-co-for-fluent-bit)

The `to_fluent_bit` operator no longer crashes on larger inputs.

By [@raxyte](https://github.com/raxyte) in [#5076](https://github.com/tenzir/tenzir/pull/5076).

#### Fix up `to_azure_log_analytics`

[Section titled “Fix up to\_azure\_log\_analytics”](#fix-up-to_azure_log_analytics)

We fixed a hang in `to_azure_log_analytics` for pipelines that never exhausted their input.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5077](https://github.com/tenzir/tenzir/pull/5077).

# Tenzir Node v4.31.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.31.0).

### Features

[Section titled “Features”](#features)

#### Implement `if … else` expressions and short-circuiting for `and` / `or`

[Section titled “Implement if … else expressions and short-circuiting for and / or”](#implement-if--else-expressions-and-short-circuiting-for-and--or)

Tenzir now supports inline `if … else` expressions in the form `foo if pred`, which returns `foo` if `pred` evaluates to `true`, or `null` otherwise, and `foo if pred else bar`, which instead of falling back to `null` falls back to `bar`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5085](https://github.com/tenzir/tenzir/pull/5085).

#### Implement `from_opensearch`

[Section titled “Implement from\_opensearch”](#implement-from_opensearch)

We added a `from_opensearch` operator that presents a OpenSearch-compatible REST API to enable easy interop with tools that can send data to OpenSearch or Elasticsearch, e.g. Filebeat, Winlogbeat etc.

By [@raxyte](https://github.com/raxyte) in [#5075](https://github.com/tenzir/tenzir/pull/5075).

#### Implement `write_syslog`

[Section titled “Implement write\_syslog”](#implement-write_syslog)

We added a new `write_syslog` operator to format events as syslog messages.

By [@raxyte](https://github.com/raxyte) in [#5083](https://github.com/tenzir/tenzir/pull/5083).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Implement `if … else` expressions and short-circuiting for `and` / `or`

[Section titled “Implement if … else expressions and short-circuiting for and / or”](#implement-if--else-expressions-and-short-circuiting-for-and--or-1)

The binary operators `and` and `or` now skip evaluating their right-hand side when not necessary. For example, `where this.has("foo") and foo == 42` now avoids emitting a warning when `foo` does not exist.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5085](https://github.com/tenzir/tenzir/pull/5085).

#### Hardcode the fluent-bit page size to 24576

[Section titled “Hardcode the fluent-bit page size to 24576”](#hardcode-the-fluent-bit-page-size-to-24576)

The `from_fluent_bit` and `to_fluent_bit` operators no longer crash when trying to handle very large payloads.

By [@tobim](https://github.com/tobim) in [#5084](https://github.com/tenzir/tenzir/pull/5084).

# Tenzir Node v4.31.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.31.2).

### Features

[Section titled “Features”](#features)

#### Add compression support to `to_hive`

[Section titled “Add compression support to to\_hive”](#add-compression-support-to-to_hive)

You can now specify an optional compression method for the files written by the `to_hive` operator.

By [@tobim](https://github.com/tobim) in [#5088](https://github.com/tenzir/tenzir/pull/5088).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Remove 20k element limits from `data_builder`

[Section titled “Remove 20k element limits from data\_builder”](#remove-20k-element-limits-from-data_builder)

We removed the limit of 20,000 elements in lists and records for `read_*` operators and `parse_*` functions.

By [@IyeOnline](https://github.com/IyeOnline) in [#5091](https://github.com/tenzir/tenzir/pull/5091).

# Tenzir Node v4.32.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.32.0).

### Features

[Section titled “Features”](#features)

#### Implement `to_google_secops`

[Section titled “Implement to\_google\_secops”](#implement-to_google_secops)

We now provide an integration for customers with a Google SecOps workspace via the `to_google_secops` operator. This new operator can send logs via the [Chronicle Ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api#unstructuredlogentries).

By [@raxyte](https://github.com/raxyte) in [#5101](https://github.com/tenzir/tenzir/pull/5101).

#### Implement `.?` and `get` for field access without warnings

[Section titled “Implement .? and get for field access without warnings”](#implement--and-get-for-field-access-without-warnings)

The `.?` operator is a new alternative to the `.` operator that allows field access without warnings when the field does not exist or the parent record is `null`. For example, both `foo.bar` and `foo.?bar` return `null` if `foo` is `null`, or if `bar` does not exist, but the latter does not warn about this. Functionally, `foo.?bar` is equivalent to `foo.bar if foo.has("bar")`.

The `get` method on records or lists is an alternative to index expressions that allows for specifying a default value when the list index is out of bounds or the record field is missing. For example, `foo[bar]` is equivalent to `foo.get(bar)`, and `foo[bar] if foo.has(bar) else fallback` is equivalent to `foo.get(bar, fallback)`. This works for both records and lists.

Indexing expressions on records now support numeric indices to access record fields. For example, `this[0]` returns the first field of the top-level record.

The `has` method on records no longer requires the field name to be a constant.

The `config` function replaces the previous `config` operator as a more flexible mechanism to access variables from the configuration file. If you rely on the previous behavior, use `from config()` as a replacement.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5099](https://github.com/tenzir/tenzir/pull/5099).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix a stack-use-after-move in `save_tcp`

[Section titled “Fix a stack-use-after-move in save\_tcp”](#fix-a-stack-use-after-move-in-save_tcp)

The `save_tcp` operator no longer panics or crashes on startup when it cannot connect to the provided hostname and port, and instead produces a helpful error message.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5103](https://github.com/tenzir/tenzir/pull/5103).

#### Check array validity before iterating

[Section titled “Check array validity before iterating”](#check-array-validity-before-iterating)

The `parse_json` function no longer crashes in case it encounters invalid arrays.

By [@tobim](https://github.com/tobim) in [#5100](https://github.com/tenzir/tenzir/pull/5100).

#### Fix error response and lifetime issues in `from_opensearch`

[Section titled “Fix error response and lifetime issues in from\_opensearch”](#fix-error-response-and-lifetime-issues-in-from_opensearch)

We fixed a bug that caused the `from_opensearch` operator to crash on high volume input. Additionally, the operator now correctly responds to requests.

By [@raxyte](https://github.com/raxyte) in [#5096](https://github.com/tenzir/tenzir/pull/5096).

# Tenzir Node v4.32.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.32.1).

### Features

[Section titled “Features”](#features)

#### Make `to_google_secops` max size configurable

[Section titled “Make to\_google\_secops max size configurable”](#make-to_google_secops-max-size-configurable)

The `to_google_secops` operator now has two additonal configuration options to set the `max_request_size` and the `batch_timeout`.

By [@raxyte](https://github.com/raxyte) in [#5108](https://github.com/tenzir/tenzir/pull/5108).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Make `to_google_secops` max size configurable

[Section titled “Make to\_google\_secops max size configurable”](#make-to_google_secops-max-size-configurable-1)

The default max request size has been changed from `4MB` to `1MB` for `to_google_secops` to align with the API.

By [@raxyte](https://github.com/raxyte) in [#5108](https://github.com/tenzir/tenzir/pull/5108).

# Tenzir Node v4.4.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.4.0).

### Features

[Section titled “Features”](#features)

#### Add `show serves` for debugging the `serve` operator

[Section titled “Add show serves for debugging the serve operator”](#add-show-serves-for-debugging-the-serve-operator)

`show serves` displays all currently active serve IDs in the `/serve` API endpoint, showing an overview of active pipelines with an on-demand API.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3596](https://github.com/tenzir/tenzir/pull/3596).

#### Implement a RabbitMQ-based AMQP connector

[Section titled “Implement a RabbitMQ-based AMQP connector”](#implement-a-rabbitmq-based-amqp-connector)

The new `amqp` connector enables interaction with an AMQP 0-9-1 exchange, supporting working with messages as producer (saver) and consumer (loader).

By [@mavam](https://github.com/mavam) in [#3546](https://github.com/tenzir/tenzir/pull/3546).

#### Implement extended deployment options

[Section titled “Implement extended deployment options”](#implement-extended-deployment-options)

The new `ttl_expires_in_ns` shows the remaining time to live for a pipeline in the pipeline manager.

By [@Dakostu](https://github.com/Dakostu) in [#3585](https://github.com/tenzir/tenzir/pull/3585).

#### Add a `yara` operator

[Section titled “Add a yara operator”](#add-a-yara-operator)

The new `yara` operator matches Yara rules on byte streams, producing structured events when rules match.

By [@mavam](https://github.com/mavam) in [#3594](https://github.com/tenzir/tenzir/pull/3594).

#### Add `blob` type for arbitrary binary data

[Section titled “Add blob type for arbitrary binary data”](#add-blob-type-for-arbitrary-binary-data)

The new `blob` type can be used to represent arbitrary binary data.

By [@jachris](https://github.com/jachris) in [#3581](https://github.com/tenzir/tenzir/pull/3581).

#### Add a —live option to the export operator

[Section titled “Add a —live option to the export operator”](#add-a-live-option-to-the-export-operator)

The `export` operator now has a `--live` option to continuously emit events as they are imported instead of those that already reside in the database.

By [@tobim](https://github.com/tobim) in [#3612](https://github.com/tenzir/tenzir/pull/3612).

#### Add a `velociraptor` operator

[Section titled “Add a velociraptor operator”](#add-a-velociraptor-operator)

The new `velociraptor` source supports submitting VQL queries to a Velociraptor server. The operator communicates with the server via gRPC using a mutually authenticated and encrypted connection with client certificates. For example, `velociraptor -q "select * from pslist()"` lists processes and their running binaries.

By [@mavam](https://github.com/mavam) in [#3556](https://github.com/tenzir/tenzir/pull/3556).

#### Update the plugins submodule pointer to include extended & serialized pipeline states

[Section titled “Update the plugins submodule pointer to include extended & serialized pipeline states”](#update-the-plugins-submodule-pointer-to-include-extended--serialized-pipeline-states)

The new `completed` pipeline state in the pipeline manager shows when a pipeline has finished execution.

If the node with running pipelines crashes, they will be marked as `failed` upon restarting.

By [@Dakostu](https://github.com/Dakostu) in [#3554](https://github.com/tenzir/tenzir/pull/3554).

#### Add `events` field to output of `show partitions`

[Section titled “Add events field to output of show partitions”](#add-events-field-to-output-of-show-partitions)

The output of `show partitions` includes a new `events` field that shows the number of events kept in that partition. E.g., the pipeline `show partitions | summarize events=sum(events) by schema` shows the number of events per schema stored at the node.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3580](https://github.com/tenzir/tenzir/pull/3580).

### Changes

[Section titled “Changes”](#changes)

#### Implement extended deployment options

[Section titled “Implement extended deployment options”](#implement-extended-deployment-options-1)

The new `autostart` and `autodelete` parameters for the pipeline manager supersede the `start_when_created` and `restart_with_node` parameters and extend restarting and deletion possibilities for pipelines.

By [@Dakostu](https://github.com/Dakostu) in [#3585](https://github.com/tenzir/tenzir/pull/3585).

#### Add `blob` type for arbitrary binary data

[Section titled “Add blob type for arbitrary binary data”](#add-blob-type-for-arbitrary-binary-data-1)

The `string` type is now restricted to valid UTF-8 strings. Use `blob` for arbitrary binary data.

By [@jachris](https://github.com/jachris) in [#3581](https://github.com/tenzir/tenzir/pull/3581).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Detect and report incomplete object in JSON parser

[Section titled “Detect and report incomplete object in JSON parser”](#detect-and-report-incomplete-object-in-json-parser)

When using `read json`, incomplete objects (e.g., due to truncated files) are now reported as an error instead of silently discarded.

By [@jachris](https://github.com/jachris) in [#3570](https://github.com/tenzir/tenzir/pull/3570).

#### Fix `serve` exiting prematurely

[Section titled “Fix serve exiting prematurely”](#fix-serve-exiting-prematurely)

Pipelines ending with the `serve` operator no longer incorrectly exit 60 seconds after transferring all events to the `/serve` endpoint, but rather wait until all events were fetched from the endpoint.

Shutting down a node immediately after starting it now no longer waits for all partitions to be loaded.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3562](https://github.com/tenzir/tenzir/pull/3562).

#### Fix parsing of subnet columns in zeek-tsv

[Section titled “Fix parsing of subnet columns in zeek-tsv”](#fix-parsing-of-subnet-columns-in-zeek-tsv)

The `zeek-tsv` parser is now able to handle fields of type `subnet` correctly.

By [@tobim](https://github.com/tobim) in [#3606](https://github.com/tenzir/tenzir/pull/3606).

#### Check for duplicate field names in zeek\_tsv\_parser

[Section titled “Check for duplicate field names in zeek\_tsv\_parser”](#check-for-duplicate-field-names-in-zeek_tsv_parser)

Having duplicate field names in `zeek-tsv` data no longer causes a crash, but rather errors out gracefully.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3578](https://github.com/tenzir/tenzir/pull/3578).

#### Fix predicate pushdown in `export` and other small fixes

[Section titled “Fix predicate pushdown in export and other small fixes”](#fix-predicate-pushdown-in-export-and-other-small-fixes)

A regression in Tenzir v4.3 caused exports to often consider all partitions as candidates. Pipelines of the form `export | where <expr>` now work as expected again and only load relevant partitions from disk.

The long option `--skip-empty` for `read lines` now works as documented.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3599](https://github.com/tenzir/tenzir/pull/3599).

#### Upgrade remaining usages of the adaptive table slice builder

[Section titled “Upgrade remaining usages of the adaptive table slice builder”](#upgrade-remaining-usages-of-the-adaptive-table-slice-builder)

The `csv` parsed (or more generally, the `xsv` parser) now attempts to parse fields in order to infer their types.

By [@jachris](https://github.com/jachris) in [#3582](https://github.com/tenzir/tenzir/pull/3582).

# Tenzir Node v4.5.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.5.0).

### Features

[Section titled “Features”](#features)

#### Flush implicitly in the `import` operator

[Section titled “Flush implicitly in the import operator”](#flush-implicitly-in-the-import-operator)

The `import` operator now flushes events to disk automatically before returning, ensuring that they are available immediately for subsequent uses of the `export` operator.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3638](https://github.com/tenzir/tenzir/pull/3638).

#### Add an operator blocklist

[Section titled “Add an operator blocklist”](#add-an-operator-blocklist)

The `tenzir.disable-plugins` option is a list of names of plugins and builtins to explicitly forbid from being used in Tenzir. For example, adding `shell` will prohibit use of the `shell` operator builtin, and adding `kafka` will prohibit use of the `kafka` connector plugin. This allows for a more fine-grained control than the `tenzir.allow-unsafe-pipelines` option.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3642](https://github.com/tenzir/tenzir/pull/3642).

#### Relax type restrictions for queries with numeric literals

[Section titled “Relax type restrictions for queries with numeric literals”](#relax-type-restrictions-for-queries-with-numeric-literals)

In `where <expression>`, the types of numeric literals and numeric fields in an equality or relational comparison must no longer match exactly. The literals `+42`, `42` or `42.0` now compare against fields of types `int64`, `uint64`, and `double` as expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3634](https://github.com/tenzir/tenzir/pull/3634).

#### Improve `summarize` result for empty inputs

[Section titled “Improve summarize result for empty inputs”](#improve-summarize-result-for-empty-inputs)

If the `summarize` operator has no `by` clause, it now returns a result even if there is no input. For example, `summarize num=count(.)` returns an event with `{"num": 0}`. Aggregation functions which do not have a single default value, for example because it would depend on the input type, return `null`.

By [@jachris](https://github.com/jachris) in [#3640](https://github.com/tenzir/tenzir/pull/3640).

#### Always enable time and bool synopses

[Section titled “Always enable time and bool synopses”](#always-enable-time-and-bool-synopses)

Lookups against uint64, int64, double, and duration fields now always use sparse indexes, which improves the performance of `export | where <expression>` for some expressions.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3639](https://github.com/tenzir/tenzir/pull/3639).

#### Add an `api` source operator

[Section titled “Add an api source operator”](#add-an-api-source-operator)

The `api` source operator interacts with Tenzir’s REST API without needing to spin up a web server, making all APIs accessible from within pipelines.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3630](https://github.com/tenzir/tenzir/pull/3630).

### Changes

[Section titled “Changes”](#changes)

#### Use prefix matching instead of suffix matching

[Section titled “Use prefix matching instead of suffix matching”](#use-prefix-matching-instead-of-suffix-matching)

The operators `drop`, `pseudonymize`, `put`, `extend`, `replace`, `rename` and `select` were converted from suffix matching to prefix matching and can therefore address records now.

By [@jachris](https://github.com/jachris) in [#3616](https://github.com/tenzir/tenzir/pull/3616).

#### Always enable time and bool synopses

[Section titled “Always enable time and bool synopses”](#always-enable-time-and-bool-synopses-1)

Sparse indexes for time and bool fields are now always enabled, accelerating lookups against them.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3639](https://github.com/tenzir/tenzir/pull/3639).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Ensure exporter metrics don’t get lost

[Section titled “Ensure exporter metrics don’t get lost”](#ensure-exporter-metrics-dont-get-lost)

The `exporter.*` metrics will now be emitted in case the exporter finishes early.

By [@tobim](https://github.com/tobim) in [#3633](https://github.com/tenzir/tenzir/pull/3633).

#### Rename `--appending` option for `save file` to `--append`

[Section titled “Rename --appending option for save file to --append”](#rename---appending-option-for-save-file-to---append)

The long option `--append` for the `file` and `directory` savers now works as documented. Previously, only the short option worked correctly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3629](https://github.com/tenzir/tenzir/pull/3629).

# Tenzir Node v4.6.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.6.0).

### Features

[Section titled “Features”](#features)

#### Add `apply` operator

[Section titled “Add apply operator”](#add-apply-operator)

The new `apply` operator includes pipelines defined in other files.

By [@jachris](https://github.com/jachris) in [#3677](https://github.com/tenzir/tenzir/pull/3677).

#### Add `parse` operator

[Section titled “Add parse operator”](#add-parse-operator)

The new, experimental `parse` operator applies a parser to the string stored in a given field.

By [@jachris](https://github.com/jachris) in [#3665](https://github.com/tenzir/tenzir/pull/3665).

#### Add `from/load/to/save <uri/file>`

[Section titled “Add from/load/to/save \<uri/file>”](#add-fromloadtosave-urifile)

The operators `from`, `to`, `load`, and `save` support using URLs and file paths directly as their argument. For example, `load https://example.com` means `load https https://example.com`, and `save local-file.json` means `save file local-file.json`.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3608](https://github.com/tenzir/tenzir/pull/3608).

#### Add `syslog` parser

[Section titled “Add syslog parser”](#add-syslog-parser)

The `syslog` parser allows reading both RFC 5424 and RFC 3164 syslog messages.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3645](https://github.com/tenzir/tenzir/pull/3645).

#### Add `yield` operator

[Section titled “Add yield operator”](#add-yield-operator)

The new `yield` operator extracts nested records with the ability to unfold lists.

By [@jachris](https://github.com/jachris) in [#3651](https://github.com/tenzir/tenzir/pull/3651).

#### Introduce an experimental `python` pipeline operator

[Section titled “Introduce an experimental python pipeline operator”](#introduce-an-experimental-python-pipeline-operator)

The `python` operator adds the ability to perform arbitrary event to event transformations with the full power of Python 3.

By [@lava](https://github.com/lava) in [#3592](https://github.com/tenzir/tenzir/pull/3592).

#### Implement a TCP loader

[Section titled “Implement a TCP loader”](#implement-a-tcp-loader)

We added a new `tcp` connector that allows reading raw bytes from TCP or TLS connections.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3664](https://github.com/tenzir/tenzir/pull/3664).

#### Add file extension detection to `from`/`to`

[Section titled “Add file extension detection to from/to”](#add-file-extension-detection-to-fromto)

When using `from <URL>` and `to <URL>` without specifying the format explicitly using a `read`/`write` argument, the default format is determined by the file extension for all loaders and savers, if possible. Previously, that was only done when using the `file` loader/saver. Additionally, if the file name would indicate some sort of compression (e.g. `.gz`), compression and decompression is performed automatically. For example, `from https://example.com/myfile.yml.gz` is expanded to `load https://example.com/myfile.yml.gz | decompress gzip | read yaml` automatically.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3653](https://github.com/tenzir/tenzir/pull/3653).

#### Support `show`-ing all aspects at once

[Section titled “Support show-ing all aspects at once”](#support-show-ing-all-aspects-at-once)

Use `show` without an aspect to return information about all aspects of a node.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3650](https://github.com/tenzir/tenzir/pull/3650).

#### Implement context backends for the contextualizer

[Section titled “Implement context backends for the contextualizer”](#implement-context-backends-for-the-contextualizer)

The closed-source `context` plugin offers a backend functionality for finding matches between data sets.

The new `lookup-table` built-in is a hashtable-based contextualization algorithm that enriches events based on a unique value.

The JSON format has a new `--arrays-of-objects` parameter that allows for parsing a JSON array of JSON objects into an event for each object.

By [@Dakostu](https://github.com/Dakostu) in [#3684](https://github.com/tenzir/tenzir/pull/3684).

#### Add ‘min\_events’ parameters to /serve endpoint

[Section titled “Add ‘min\_events’ parameters to /serve endpoint”](#add-min_events-parameters-to-serve-endpoint)

We optimized the behavior of the ‘serve’ operator to respond quicker and cause less system load for pipelines that take a long time to generate the first result. The new `min_events` parameter can be used to implement long-polling behavior for clients of `/serve`.

By [@lava](https://github.com/lava) in [#3666](https://github.com/tenzir/tenzir/pull/3666).

#### Support comments in xsv parser

[Section titled “Support comments in xsv parser”](#support-comments-in-xsv-parser)

Use `--allow-comments` with the `xsv` parser (incl. `csv`, `tsv`, and `ssv`) to treat lines beginning with `'#'` as comments.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3681](https://github.com/tenzir/tenzir/pull/3681).

#### Add `export --internal` to access metrics

[Section titled “Add export --internal to access metrics”](#add-export---internal-to-access-metrics)

The new `--internal` flag for the `export` operators returns internal events collected by the system, for example pipeline metrics.

By [@jachris](https://github.com/jachris) in [#3619](https://github.com/tenzir/tenzir/pull/3619).

### Changes

[Section titled “Changes”](#changes)

#### Rename pytenzir to tenzir

[Section titled “Rename pytenzir to tenzir”](#rename-pytenzir-to-tenzir)

We renamed the name of our python package from `pytenzir` to `tenzir`.

By [@lava](https://github.com/lava) in [#3660](https://github.com/tenzir/tenzir/pull/3660).

#### Implement a TCP loader

[Section titled “Implement a TCP loader”](#implement-a-tcp-loader-1)

We renamed the `--bind` option of the `zmq` connector to `--listen`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3664](https://github.com/tenzir/tenzir/pull/3664).

#### Consider discard, export, and import as internal operators

[Section titled “Consider discard, export, and import as internal operators”](#consider-discard-export-and-import-as-internal-operators)

Ingress and egress metrics for pipelines now indicate whether the pipeline sent/received events to/from outside of the node with a new `internal` flag. For example, when using the `export` operator, data is entering the pipeline from within the node, so its ingress is considered internal.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3658](https://github.com/tenzir/tenzir/pull/3658).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add `export --internal` to access metrics

[Section titled “Add export --internal to access metrics”](#add-export---internal-to-access-metrics-1)

`export --live` now respects a subsequent `where <expr>` instead of silently discarding the filter expression.

By [@jachris](https://github.com/jachris) in [#3619](https://github.com/tenzir/tenzir/pull/3619).

#### Support lists and null values and empty strings in XSV parser

[Section titled “Support lists and null values and empty strings in XSV parser”](#support-lists-and-null-values-and-empty-strings-in-xsv-parser)

The `csv`, `ssv`, and `tsv` parsers now correctly support empty strings, lists, and null values.

The `tail` operator no longer hangs occasionally.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3687](https://github.com/tenzir/tenzir/pull/3687).

#### Fix `sort` type check

[Section titled “Fix sort type check”](#fix-sort-type-check)

Using the `sort` operator with polymorphic inputs no longer leads to a failing assertion under some circumstances.

By [@jachris](https://github.com/jachris) in [#3655](https://github.com/tenzir/tenzir/pull/3655).

# Tenzir Node v4.6.3

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.6.3).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix debian package installation when a vast state directory exists

[Section titled “Fix debian package installation when a vast state directory exists”](#fix-debian-package-installation-when-a-vast-state-directory-exists)

The Debian package sometimes failed to install, and the bundled systemd unit failed to start with Tenzir v4.6.2. This issue no longer exists.

By [@tobim](https://github.com/tobim) in [#3705](https://github.com/tenzir/tenzir/pull/3705).

# Tenzir Node v4.6.4

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.6.4).

### Features

[Section titled “Features”](#features)

#### Add a `duration` field to operator metrics

[Section titled “Add a duration field to operator metrics”](#add-a-duration-field-to-operator-metrics)

The `tenzir.metrics.operator` metric now contains an additional `duration` field with the timespan over which the metric was collected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3713](https://github.com/tenzir/tenzir/pull/3713).

### Changes

[Section titled “Changes”](#changes)

#### Use systemd provided default paths if available

[Section titled “Use systemd provided default paths if available”](#use-systemd-provided-default-paths-if-available)

When selecting default paths, the `tenzir-node` will now respect the systemd-provided variables `STATE_DIRECTORY`, `CACHE_DIRECTORY` and `LOGS_DIRECTORY` before falling back to `$PWD/tenzir.db`.

By [@tobim](https://github.com/tobim) in [#3714](https://github.com/tenzir/tenzir/pull/3714).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix yaml printer enums

[Section titled “Fix yaml printer enums”](#fix-yaml-printer-enums)

The yaml printer no longer crashes when receiving enums.

By [@jachris](https://github.com/jachris) in [#3719](https://github.com/tenzir/tenzir/pull/3719).

#### Fix RFC3164 (legacy-syslog) parser expecting spaces after `<PRI>`

[Section titled “Fix RFC3164 (legacy-syslog) parser expecting spaces after \<PRI>”](#fix-rfc3164-legacy-syslog-parser-expecting-spaces-after-pri)

The RFC 3164 syslog parser no longer requires a whitespace after the `PRI`-field (part in angle brackets in the beginning of a message).

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3718](https://github.com/tenzir/tenzir/pull/3718).

# Tenzir Node v4.7.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.7.0).

### Features

[Section titled “Features”](#features)

#### Add file data to show partitions

[Section titled “Add file data to show partitions”](#add-file-data-to-show-partitions)

`show partitions` now contains location and size of the `store`, `index`, and `sketch` files of a partition, as well the aggregate size at `diskusage`.

By [@tobim](https://github.com/tobim) in [#3675](https://github.com/tenzir/tenzir/pull/3675).

#### Implement the `geoip` context

[Section titled “Implement the geoip context”](#implement-the-geoip-context)

The new `geoip` context is a built-in that reads MaxMind DB files and uses IP values in events to enrich them with the MaxMind DB geolocation data.

By [@Dakostu](https://github.com/Dakostu) in [#3731](https://github.com/tenzir/tenzir/pull/3731).

#### Show processes and sockets

[Section titled “Show processes and sockets”](#show-processes-and-sockets)

With the new `processes` and `sockets` source operators, you can now get a snapshot of the operating system processes and sockets as pipeline input.

By [@mavam](https://github.com/mavam) in [#3521](https://github.com/tenzir/tenzir/pull/3521).

#### Add `grok` parser

[Section titled “Add grok parser”](#add-grok-parser)

The `grok` parser, for use with the `parse` operator, enables powerful regex-based string dissection.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3683](https://github.com/tenzir/tenzir/pull/3683).

#### Add TCP saver

[Section titled “Add TCP saver”](#add-tcp-saver)

The `tcp` connector is now also a saver in addition to a loader.

By [@mavam](https://github.com/mavam) in [#3727](https://github.com/tenzir/tenzir/pull/3727).

#### Add support for macOS-style syslog messages

[Section titled “Add support for macOS-style syslog messages”](#add-support-for-macos-style-syslog-messages)

The `syslog` parser now supports macOS-style syslog messages.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3692](https://github.com/tenzir/tenzir/pull/3692).

#### Include UDOs in `show operators`

[Section titled “Include UDOs in show operators”](#include-udos-in-show-operators)

`show operators` now shows user-defined operators in addition to operators that ship with Tenzir or as plugins.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3723](https://github.com/tenzir/tenzir/pull/3723).

#### Add `kv` parser

[Section titled “Add kv parser”](#add-kv-parser)

The `kv` parser splits strings into key-value pairs.

By [@jachris](https://github.com/jachris) in [#3646](https://github.com/tenzir/tenzir/pull/3646).

#### Implement the `slice` operator

[Section titled “Implement the slice operator”](#implement-the-slice-operator)

The `slice` operator keeps a range of events within a half-closed interval. Begin and end of the interval can be specified relative to the first or last event.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3703](https://github.com/tenzir/tenzir/pull/3703).

### Changes

[Section titled “Changes”](#changes)

#### Add support for macOS-style syslog messages

[Section titled “Add support for macOS-style syslog messages”](#add-support-for-macos-style-syslog-messages-1)

The events created by the RFC 3164 syslog parser no longer has a `tag` field, but `app_name` and `process_id`.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3692](https://github.com/tenzir/tenzir/pull/3692).

#### Allow empty field names

[Section titled “Allow empty field names”](#allow-empty-field-names)

Records can now have fields where the name is empty.

By [@jachris](https://github.com/jachris) in [#3742](https://github.com/tenzir/tenzir/pull/3742).

#### Show processes and sockets

[Section titled “Show processes and sockets”](#show-processes-and-sockets-1)

The `show` operator now always connects to and runs at a node. Consequently, the `version` and `nics` aspects moved into operators of their own.

By [@mavam](https://github.com/mavam) in [#3521](https://github.com/tenzir/tenzir/pull/3521).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Prevent delays for blocking operators

[Section titled “Prevent delays for blocking operators”](#prevent-delays-for-blocking-operators)

Pipeline operators blocking in their execution sometimes caused results to be delayed. This is no longer the case. This bug fix also reduces the time to first result for pipelines with many operators.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3743](https://github.com/tenzir/tenzir/pull/3743).

# Tenzir Node v4.7.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.7.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Downtone push behavior of pipelines

[Section titled “Downtone push behavior of pipelines”](#downtone-push-behavior-of-pipelines)

We fixed a bug that caused operators that caused an increased memory usage for pipelines with slow operators immediately after a faster operator.

We fixed a bug that caused short-running pipelines to sometimes hang.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3758](https://github.com/tenzir/tenzir/pull/3758).

# Tenzir Node v4.8.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.8.0).

### Features

[Section titled “Features”](#features)

#### Support concepts in more places

[Section titled “Support concepts in more places”](#support-concepts-in-more-places)

Concepts are now supported in more places than just the `where` operator: All operators and concepts that reference fields in events now support them transparently. For example, it is not possible to enrich with a lookup table against all source IP addresses defined in the concept `net.src.ip`, or to group by destination ports across different schemas with the concept `net.dst.port`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3812](https://github.com/tenzir/tenzir/pull/3812).

#### Implement `timeshift` and `delay` operators

[Section titled “Implement timeshift and delay operators”](#implement-timeshift-and-delay-operators)

The new `timeshift` operator adjusts timestamps relative to a given start time, with an optional speedup.

The new `delay` operator delays events relative to a given start time, with an optional speedup.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3701](https://github.com/tenzir/tenzir/pull/3701).

#### Add GELF parser and document Graylog integration

[Section titled “Add GELF parser and document Graylog integration”](#add-gelf-parser-and-document-graylog-integration)

The new `gelf` parser reads a stream of NULL-byte terminated messages in Graylog Extended Log Format (GELF).

By [@mavam](https://github.com/mavam) in [#3768](https://github.com/tenzir/tenzir/pull/3768).

#### Update the main repository to include the pipeline manager autostart changes

[Section titled “Update the main repository to include the pipeline manager autostart changes”](#update-the-main-repository-to-include-the-pipeline-manager-autostart-changes)

Pipeline states in the `/pipeline` API will not change upon node shutdown anymore. When a node restarts afterwards, previously running pipelines will continue to run while paused pipelines will load in a stopped state.

By [@Dakostu](https://github.com/Dakostu) in [#3785](https://github.com/tenzir/tenzir/pull/3785).

#### Add node health metrics

[Section titled “Add node health metrics”](#add-node-health-metrics)

A Tenzir node will now automatically collect and store metrics about disk, cpu and memory usage of the host machine.

By [@lava](https://github.com/lava) in [#3736](https://github.com/tenzir/tenzir/pull/3736).

#### Add `xsv --no-header`

[Section titled “Add xsv --no-header”](#add-xsv---no-header)

The `csv`, `tsv`, `ssv` and `xsv` printers now support not printing a header line with the `--no-header` option.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3821](https://github.com/tenzir/tenzir/pull/3821).

#### Add savers for curl connectors

[Section titled “Add savers for curl connectors”](#add-savers-for-curl-connectors)

The `http` and `https` loaders now also have savers to send data from a pipeline to a remote API.

The `http` and `https` connectors have a new flag `--form` to submit the request body URL-encoded. This also changes the Content-Type header to `application/x-www-form-urlencoded`.

By [@mavam](https://github.com/mavam) in [#3539](https://github.com/tenzir/tenzir/pull/3539).

#### Allow setting the header for csv, tsv, and ssv manually

[Section titled “Allow setting the header for csv, tsv, and ssv manually”](#allow-setting-the-header-for-csv-tsv-and-ssv-manually)

The `csv`, `tsv`, `ssv` and `xsv` parsers now support setting the header line manually with the `--header` option.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3778](https://github.com/tenzir/tenzir/pull/3778).

#### Implement the `diagnostics` operator

[Section titled “Implement the diagnostics operator”](#implement-the-diagnostics-operator)

The new `diagnostics` operator provides information about diagnostics that a pipeline may encounter during its lifetime.

By [@Dakostu](https://github.com/Dakostu) in [#3828](https://github.com/tenzir/tenzir/pull/3828).

#### Fix `export --live` and introduce `metrics`

[Section titled “Fix export --live and introduce metrics”](#fix-export---live-and-introduce-metrics)

The `metrics` operator returns internal metrics events generated in a Tenzir node. Use `metrics --live` to get a feed of metrics as they are being generated.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3790](https://github.com/tenzir/tenzir/pull/3790).

#### Add support for Cisco Firepower syslog messages

[Section titled “Add support for Cisco Firepower syslog messages”](#add-support-for-cisco-firepower-syslog-messages)

The RFC 3164 syslog parser now supports years in the message timestamp.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3833](https://github.com/tenzir/tenzir/pull/3833).

#### Improve handling of open file descriptors

[Section titled “Improve handling of open file descriptors”](#improve-handling-of-open-file-descriptors)

On Linux systems, the process metrics now have an additional value `open_fds` showing the number of file descriptors opened by the node.

By [@lava](https://github.com/lava) in [#3784](https://github.com/tenzir/tenzir/pull/3784).

#### Implement the `lookup` operator

[Section titled “Implement the lookup operator”](#implement-the-lookup-operator)

The new `lookup` operator performs live filtering of the import feed using a context, and translates context updates into historical queries. This effectively enables live and retro matching in a single operator.

By [@Dakostu](https://github.com/Dakostu) in [#3721](https://github.com/tenzir/tenzir/pull/3721).

#### Add time parser

[Section titled “Add time parser”](#add-time-parser)

The `time` parser allows parsing datetimes and timestamps from arbitrary strings using a `strptime`-like format string.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3738](https://github.com/tenzir/tenzir/pull/3738).

### Changes

[Section titled “Changes”](#changes)

#### Switch from JSON to MsgPack data transport

[Section titled “Switch from JSON to MsgPack data transport”](#switch-from-json-to-msgpack-data-transport)

The `fluent-bit` source operator no longer performs JSON conversion from Fluent Bit prior to processing an event. Instead, it directly processes the MsgPack data that Fluent Bit uses internally for more robust and quicker event delivery.

By [@mavam](https://github.com/mavam) in [#3770](https://github.com/tenzir/tenzir/pull/3770).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Print Zeek TSV metadata when schema changes

[Section titled “Print Zeek TSV metadata when schema changes”](#print-zeek-tsv-metadata-when-schema-changes)

The `zeek-tsv` printer incorrectly emitted metadata too frequently. It now only writes opening and closing tags when it encounters a new schema.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3836](https://github.com/tenzir/tenzir/pull/3836).

#### Optimize pipeline when using `/pipeline/launch`

[Section titled “Optimize pipeline when using /pipeline/launch”](#optimize-pipeline-when-using-pipelinelaunch)

The `/pipeline/launch` endpoint now optimizes the pipeline before starting it.

By [@jachris](https://github.com/jachris) in [#3801](https://github.com/tenzir/tenzir/pull/3801).

#### Fail properly when transfer breaks

[Section titled “Fail properly when transfer breaks”](#fail-properly-when-transfer-breaks)

Failing transfers using `http(s)` and `ftp(s)` connectors now properly return an error when the transfer broke. For example, `from http://does.not.exist` no longer returns silently a success.

By [@mavam](https://github.com/mavam) in [#3842](https://github.com/tenzir/tenzir/pull/3842).

#### Fix blob parsing with padding

[Section titled “Fix blob parsing with padding”](#fix-blob-parsing-with-padding)

When reading Base64-encoded JSON strings with the `blob` type, `=` padding is now accepted.

By [@jachris](https://github.com/jachris) in [#3765](https://github.com/tenzir/tenzir/pull/3765).

#### Fix the `tenzir/tenzir:latest-slim` image

[Section titled “Fix the tenzir/tenzir:latest-slim image”](#fix-the-tenzirtenzirlatest-slim-image)

The `tenzir/tenzir:latest-slim` Docker image now sets a default `TENZIR_CACHE_DIRECTORY` automatically.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3764](https://github.com/tenzir/tenzir/pull/3764).

#### Display failing pipeline diagnostics in `/serve`

[Section titled “Display failing pipeline diagnostics in /serve”](#display-failing-pipeline-diagnostics-in-serve)

The `/serve` API now displays why a pipeline became unavailable in an error case instead of showing a generic error message. This causes runtime errors in pipelines to show up in the Explorer on app.tenzir.com.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3788](https://github.com/tenzir/tenzir/pull/3788).

#### Fix `export --live` and introduce `metrics`

[Section titled “Fix export --live and introduce metrics”](#fix-export---live-and-introduce-metrics-1)

`export --live` sometimes got stuck, failing to deliver events. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3790](https://github.com/tenzir/tenzir/pull/3790).

#### Override lookup-table context entries for duplicate keys

[Section titled “Override lookup-table context entries for duplicate keys”](#override-lookup-table-context-entries-for-duplicate-keys)

Updating entries of a `lookup-table` context now overrides values with duplicate keys instead of ignoring them.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3808](https://github.com/tenzir/tenzir/pull/3808).

# Tenzir Node v4.8.1

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.8.1).

### Features

[Section titled “Features”](#features)

#### Support setting profiles in the Velociraptor config

[Section titled “Support setting profiles in the Velociraptor config”](#support-setting-profiles-in-the-velociraptor-config)

The `velociraptor` operator gained a new `--profile <profile>` option to support multiple configured Velociraptor instances. To opt into using profiles, move your Velociraptor configuration in `<configdir>/tenzir/plugin/velociraptor.yaml` from `<config>` to `profiles.<profile>.<config>`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3848](https://github.com/tenzir/tenzir/pull/3848).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Enable the AMQP plugin in the static binary

[Section titled “Enable the AMQP plugin in the static binary”](#enable-the-amqp-plugin-in-the-static-binary)

The `amqp` connector plugin was incorrectly packaged and unavailable in some build configurations. The plugin is now available in all builds.

Failing to create the virtualenv of the `python` operator caused subsequent uses of the `python` operator to silently fail. This no longer happens.

The Debian package now depends on `python3-venv`, which is required for the `python` operator to create its virtualenv.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3854](https://github.com/tenzir/tenzir/pull/3854).

# Tenzir Node v4.8.2

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.8.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Remove restriction for unflattening into empty field names

[Section titled “Remove restriction for unflattening into empty field names”](#remove-restriction-for-unflattening-into-empty-field-names)

The `unflatten` operator no longer ignores fields that begin or end with the separator.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3814](https://github.com/tenzir/tenzir/pull/3814).

#### Schedule idle operators less aggressively

[Section titled “Schedule idle operators less aggressively”](#schedule-idle-operators-less-aggressively)

Some idle source operators and loaders, e.g., `from tcp://localhost:3000` where no data arrives via TCP, consumed excessive amounts of CPU. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3865](https://github.com/tenzir/tenzir/pull/3865).

# Tenzir Node v4.9.0

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v4.9.0).

### Features

[Section titled “Features”](#features)

#### Implement `context load`, `context save`, and `context reset`

[Section titled “Implement context load, context save, and context reset”](#implement-context-load-context-save-and-context-reset)

The `context reset` operator allows for clearing the state of a context.

The `context save` and `context load` operators allow serializing and deserializing the state of a context to/from bytes.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3908](https://github.com/tenzir/tenzir/pull/3908).

#### Add new ‘—file’ option to the python operator

[Section titled “Add new ‘—file’ option to the python operator”](#add-new-file-option-to-the-python-operator)

The `python` operator gained a new `--file` flag that allows loading python code from a file instead of providing it as part of the pipeline definition.

By [@lava](https://github.com/lava) in [#3901](https://github.com/tenzir/tenzir/pull/3901).

#### Add Bloom filter context

[Section titled “Add Bloom filter context”](#add-bloom-filter-context)

The new `bloom-filter` context represents large sets in a space-efficient manner.

By [@mavam](https://github.com/mavam) in [#3834](https://github.com/tenzir/tenzir/pull/3834).

#### Improve the `export` operator

[Section titled “Improve the export operator”](#improve-the-export-operator)

The `export` operator gained a `--low-priority` option, which causes it to interfere less with regular priority exports at the cost of potentially running slower.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3909](https://github.com/tenzir/tenzir/pull/3909).

#### Handle nested fields and integers as selectors in JSON parser

[Section titled “Handle nested fields and integers as selectors in JSON parser”](#handle-nested-fields-and-integers-as-selectors-in-json-parser)

The `--selector` option of the `json` parser now works with nested fields, and integer fields.

By [@jachris](https://github.com/jachris) in [#3900](https://github.com/tenzir/tenzir/pull/3900).

#### Add `lines` printer

[Section titled “Add lines printer”](#add-lines-printer)

The `lines` printer enables simple line-delimited formatting of events.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3847](https://github.com/tenzir/tenzir/pull/3847).

#### Add the `openapi` operator

[Section titled “Add the openapi operator”](#add-the-openapi-operator)

The `openapi` source operator generates Tenzir’s OpenAPI specification. Use `openapi | to ./openapi.yaml` to generate a file with the canonical format.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3898](https://github.com/tenzir/tenzir/pull/3898).

#### Add structured\_data to syslog output

[Section titled “Add structured\_data to syslog output”](#add-structured_data-to-syslog-output)

The `structured_data` field in RFC 5424-style syslog messages is now parsed and included in the output.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3871](https://github.com/tenzir/tenzir/pull/3871).

#### Implement a context content dumping mechanism

[Section titled “Implement a context content dumping mechanism”](#implement-a-context-content-dumping-mechanism)

The new `context inspect <context-name>` command dumps a specific context’s user-provided data, usually the context’s content.

By [@Dakostu](https://github.com/Dakostu) in [#3893](https://github.com/tenzir/tenzir/pull/3893).

#### Support parsing numeric timestamps since epoch

[Section titled “Support parsing numeric timestamps since epoch”](#support-parsing-numeric-timestamps-since-epoch)

When specifying a schema with a field typed as `time #unit=<unit>`, numeric values will be interpreted as offsets from the epoch.

By [@jachris](https://github.com/jachris) in [#3927](https://github.com/tenzir/tenzir/pull/3927).

#### Add running and paused times to pipeline metrics

[Section titled “Add running and paused times to pipeline metrics”](#add-running-and-paused-times-to-pipeline-metrics)

Operator metrics now separately track the time that an operator was paused or running in the `time_paused` and `time_running` values in addition to the wall-clock time in `time_total`. Throughput rates now exclude the paused time from their calculation.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3940](https://github.com/tenzir/tenzir/pull/3940).

#### Rewrite `chart` and `set-attributes` operators

[Section titled “Rewrite chart and set-attributes operators”](#rewrite-chart-and-set-attributes-operators)

The `chart` operator adds metadata to the schema of the input events, enabling rendering events as bar, area, line, or pie charts on app.tenzir.com.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3866](https://github.com/tenzir/tenzir/pull/3866).

#### Update the main repository to include timestamped pipelines

[Section titled “Update the main repository to include timestamped pipelines”](#update-the-main-repository-to-include-timestamped-pipelines)

`show pipelines` and the `/pipeline` API endpoints now include `created_at` and `last_modified` fields that track the pipeline’s creation and last manual modification time, respectively. Pipelines created with older versions of Tenzir will use the start time of the node as their creation time.

By [@Dakostu](https://github.com/Dakostu) in [#3869](https://github.com/tenzir/tenzir/pull/3869).

#### Implement more malleable lookup data for contexts

[Section titled “Implement more malleable lookup data for contexts”](#implement-more-malleable-lookup-data-for-contexts)

The context match events now contain a new field `mode` that states the lookup mode of this particular match.

The `enrich` operator gained a `--filter` option, which causes it to exclude enriched events that do not contain a context.

By [@Dakostu](https://github.com/Dakostu) in [#3920](https://github.com/tenzir/tenzir/pull/3920).

#### Update the main repository to include the pipeline run ID

[Section titled “Update the main repository to include the pipeline run ID”](#update-the-main-repository-to-include-the-pipeline-run-id)

Managed pipelines now contain a new `total_runs` parameter that counts all started runs. The new `run` field is available in the events delivered by the `metrics` and `diagnostics` operators.

By [@Dakostu](https://github.com/Dakostu) in [#3883](https://github.com/tenzir/tenzir/pull/3883).

### Changes

[Section titled “Changes”](#changes)

#### Context versioning

[Section titled “Context versioning”](#context-versioning)

The binary format used by contexts for saving on disk on node shutdown is now versioned. A node can support loading of multiple different versions, and automigrate between them.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3945](https://github.com/tenzir/tenzir/pull/3945).

#### Remove reader, writer, and language plugin types

[Section titled “Remove reader, writer, and language plugin types”](#remove-reader-writer-and-language-plugin-types)

We removed the `tenzir-ctl start` subcommand. Users should switch to the `tenzir-node` command instead, which accepts the same arguments and presents the same command-line interface.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3899](https://github.com/tenzir/tenzir/pull/3899).

#### Disable colors if `NO_COLOR` or not a terminal

[Section titled “Disable colors if NO\_COLOR or not a terminal”](#disable-colors-if-no_color-or-not-a-terminal)

Color escape codes are no longer emitted if `NO_COLOR` is set to a non-empty value, or when the output device is not a terminal.

By [@jachris](https://github.com/jachris) in [#3952](https://github.com/tenzir/tenzir/pull/3952).

#### Allow plugins to bundle further builtins

[Section titled “Allow plugins to bundle further builtins”](#allow-plugins-to-bundle-further-builtins)

Plugins may now depend on other plugins. Plugins with unmet dependencies are automatically disabled. For example, the `lookup` and `enrich` plugins now depend on the `context` plugin. Run `show plugins` to see all available plugins and their dependencies.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3877](https://github.com/tenzir/tenzir/pull/3877).

#### Replace `tenzir.db-directory` with `tenzir.state-directory`

[Section titled “Replace tenzir.db-directory with tenzir.state-directory”](#replace-tenzirdb-directory-with-tenzirstate-directory)

The option `tenzir.db-directory` is deprecated in favor of the `tenzir.state-directory` option and will be removed in the future.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3889](https://github.com/tenzir/tenzir/pull/3889).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add support for commas in seconds in the time data parser

[Section titled “Add support for commas in seconds in the time data parser”](#add-support-for-commas-in-seconds-in-the-time-data-parser)

Commas are now allowed as subsecond separators in timestamps in TQL. Previously, only dots were allowed, but ISO 8601 allows for both.

By [@eliaskosunen](https://github.com/eliaskosunen) in [#3903](https://github.com/tenzir/tenzir/pull/3903).

#### Update the repository to include lookup lifetime fixes

[Section titled “Update the repository to include lookup lifetime fixes”](#update-the-repository-to-include-lookup-lifetime-fixes)

Retroactive lookups will now properly terminate when they have finished.

By [@Dakostu](https://github.com/Dakostu) in [#3910](https://github.com/tenzir/tenzir/pull/3910).

#### Make `/serve` more consistent

[Section titled “Make /serve more consistent”](#make-serve-more-consistent)

The `/serve` API sometimes returned an empty string for the next continuation token instead of `null` when there are no further results to fetch. It now consistently returns `null`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3885](https://github.com/tenzir/tenzir/pull/3885).

#### Prevent duplicate fields in schema

[Section titled “Prevent duplicate fields in schema”](#prevent-duplicate-fields-in-schema)

Invalid schema definitions, where a record contains the same key multiple times, are now detected and rejected.

By [@jachris](https://github.com/jachris) in [#3929](https://github.com/tenzir/tenzir/pull/3929).

#### Gracefully handle misaligned header and values in `xsv` parser

[Section titled “Gracefully handle misaligned header and values in xsv parser”](#gracefully-handle-misaligned-header-and-values-in-xsv-parser)

The `xsv` parser (and by extension the `csv`, `tsv`, and `ssv` parsers) skipped lines that had a mismatch between the number of values contained and the number of fields defined in the header. Instead, it now fills in `null` values for missing values and, if the new `--auto-expand` option is set, also adds new header fields for excess values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3874](https://github.com/tenzir/tenzir/pull/3874).

#### Fix restart on failure

[Section titled “Fix restart on failure”](#fix-restart-on-failure)

The option to automatically restart on failure did not correctly trigger for pipelines that failed an operator emitted an error diagnostic, a new mechanism for improved error messages introduced with Tenzir v4.8. Such pipelines now restart automatically as expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3947](https://github.com/tenzir/tenzir/pull/3947).

#### Fix logger deadlock in python tests

[Section titled “Fix logger deadlock in python tests”](#fix-logger-deadlock-in-python-tests)

We fixed a rare deadlock by changing the internal logger behavior from blocking until the oldest messages were consumed to overwriting them.

By [@lava](https://github.com/lava) in [#3911](https://github.com/tenzir/tenzir/pull/3911).

#### Improve the `export` operator

[Section titled “Improve the export operator”](#improve-the-export-operator-1)

We fixed a bug that under rare circumstances led to an indefinite hang when using a high-volume source followed by a slow transformation and a fast sink.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#3909](https://github.com/tenzir/tenzir/pull/3909).

# TQL2, Always

Tenzir Node v5.0 makes TQL2 the default and only option throughout Tenzir.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.0.0).

### Features

[Section titled “Features”](#features)

#### Implement `from_http`

[Section titled “Implement from\_http”](#implement-from_http)

`from_http <host:port>, server=true` creates an HTTP/1.1 server that listens on a specified hostname and port. In the future, the `load_http` operator’s HTTP client will be integrated with this operator as well, eventually superseding `load_http`.

By [@raxyte](https://github.com/raxyte) in [#5114](https://github.com/tenzir/tenzir/pull/5114).

#### Add optimizations for `if`

[Section titled “Add optimizations for if”](#add-optimizations-for-if)

The `mo` suffix allows for specifying durations in terms of months. For example, `3mo` returns a duration equivalent to three months. The unit has alternative spellings `month` and `months`, and is defined as one twelfth of a year.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5110](https://github.com/tenzir/tenzir/pull/5110).

#### Fix panic in `head 0 | write_json arrays_of_objects=true`

[Section titled “Fix panic in head 0 | write\_json arrays\_of\_objects=true”](#fix-panic-in-head-0--write_json-arrays_of_objectstrue)

`write_json arrays_of_objects=true` now works correctly with an empty input, returning an empty JSON array instead of running into a panic.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5115](https://github.com/tenzir/tenzir/pull/5115).

#### Implement `move` operator

[Section titled “Implement move operator”](#implement-move-operator)

We added a new `move` operator that moves a field into another, effectively a smart renaming such as `ctx.message=status.msg` moves the values from `status.msg` into the field `message` of a record `ctx` and drops `status.msg`.

By [@raxyte](https://github.com/raxyte) in [#5117](https://github.com/tenzir/tenzir/pull/5117).

#### Add a `hidden` field to `diagnostics`

[Section titled “Add a hidden field to diagnostics”](#add-a-hidden-field-to-diagnostics)

The output of the `diagnostics` operator now includes an additional `hidden` field that is set to `true` for pipelines that are not visible on the Pipelines page of the Tenzir Platform, e.g., because they’re run under-the-hood by the platform or interactively in the Explorer.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5119](https://github.com/tenzir/tenzir/pull/5119).

### Changes

[Section titled “Changes”](#changes)

#### Make TQL2 the default

[Section titled “Make TQL2 the default”](#make-tql2-the-default)

TQL2 is now the default and only option for writing pipelines with Tenzir. The environment variable `TENZIR_TQL2`, the configuration option `tenzir.tql2` have no more effect. Using the command-line option `--tql2` results in an error.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5086](https://github.com/tenzir/tenzir/pull/5086).

#### Implement `file_content(path:string)`

[Section titled “Implement file\_content(path:string)”](#implement-file_contentpathstring)

We added a new function `file_contents` to read bytes from a file with an absolute path.

By [@raxyte](https://github.com/raxyte) in [#5111](https://github.com/tenzir/tenzir/pull/5111).

#### Add optimizations for `if`

[Section titled “Add optimizations for if”](#add-optimizations-for-if-1)

`1y` in TQL now equals 365.2425 days, which is the average length of a year in the Gregorian calendar. This aligns the duration literal with the `years` function and how the Tenzir Platform renders durations.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5110](https://github.com/tenzir/tenzir/pull/5110).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Add optimizations for `if`

[Section titled “Add optimizations for if”](#add-optimizations-for-if-2)

Operators that interact with state in the node that is not local to the pipeline, e.g., `context::update`, now properly work when used inside an `if` statement. Previously, pipelines of the form `if … { context::update … }` failed at runtime.

Branches in `if` statement no longer run on a single thread, and instead properly participate in the thread pool. This fixes performance problems when running complex pipelines inside branches. Note that this causes the output of the `if` operator to be unordered between its branches.

Literal values of type `time` in expressions failed to parse when they used subsecond prevision or a time-zone offset. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5110](https://github.com/tenzir/tenzir/pull/5110).

# ClickHouse Fixes

This release improves the integration with ClickHouse.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.0.1).

### Features

[Section titled “Features”](#features)

#### Fix bugs in `to_clickhouse` and improve diagnostics

[Section titled “Fix bugs in to\_clickhouse and improve diagnostics”](#fix-bugs-in-to_clickhouse-and-improve-diagnostics)

The `to_clickhouse` operator now supports `blob`s, which will be sent as an `Array(UInt8)`.

By [@IyeOnline](https://github.com/IyeOnline) in [#5122](https://github.com/tenzir/tenzir/pull/5122).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix bugs in `to_clickhouse` and improve diagnostics

[Section titled “Fix bugs in to\_clickhouse and improve diagnostics”](#fix-bugs-in-to_clickhouse-and-improve-diagnostics-1)

We fixed multiple bugs that caused unexpected internal failures and the creation of potentially empty `Tuple`s in ClickHouse.

By [@IyeOnline](https://github.com/IyeOnline) in [#5122](https://github.com/tenzir/tenzir/pull/5122).

#### Keep the y-axis order for `chart_*` as specified

[Section titled “Keep the y-axis order for chart\_\* as specified”](#keep-the-y-axis-order-for-chart_-as-specified)

The `chart_*` operators no longer sort y-axes by their names. Instead, the user-provided order is used. For example, in `metrics "pipeline" | chart_bar x=timestamp, resolution=1d, y={"Ingress": ingress.bytes.sum(), "Egress": egress.bytes.sum()}` the field order is now “Ingress” followed by “Egress” as specified instead of the other way around.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5131](https://github.com/tenzir/tenzir/pull/5131).

# Moving Fields

Tenzir Node v5.1 adds more flexible ways to access and move fields, bitwise functions, and a native integration with Google Cloud Logging.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.0).

### Features

[Section titled “Features”](#features)

#### Experiment with a trailing `?` for field access

[Section titled “Experiment with a trailing ? for field access”](#experiment-with-a-trailing--for-field-access)

The `.?field` operator for field access with suppressed warnings is now deprecated in favor of `.field?`. We added the `.?` operator just recently, and it quickly gained a lot of popularity. However, suppressing warnings in top-level fields required writing `this.?field`, which is a mouthful. Now, with the trailing questionmark, this is just `field?` instead. Additionally, the trailing `?` operator works for index-based access, e.g., `field[index]?`. The `.?` operator will be removed in the near future. We’re sorry for the inconvenience.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5128](https://github.com/tenzir/tenzir/pull/5128).

#### Implement the `move` keyword

[Section titled “Implement the move keyword”](#implement-the-move-keyword)

The `move` keyword may be used in front of fields anywhere in assignments to automatically drop fields after the assignment. For example, `foo = {bar: move bar, baz: move baz}` moves the top-level fields `bar` and `baz` into a new record under the top-level field `foo`.

The `move`, `drop`, and `unroll` operators now support the `?` field access notation to suppress warnings when the accessed field does not exist or the parent record is `null`. For example, `drop foo?` only drops the field `foo` if it exists, and does not warn if it doesn’t. This also works with the newly introduced `move` keyword.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5127](https://github.com/tenzir/tenzir/pull/5127).

#### Add a `split_at_regex` option to `read_lines`

[Section titled “Add a split\_at\_regex option to read\_lines”](#add-a-split_at_regex-option-to-read_lines)

We added a `split_at_regex` option allows for the use of regular expressions to split events with the `read_lines` operator.

By [@tobim](https://github.com/tobim) in [#5123](https://github.com/tenzir/tenzir/pull/5123).

#### Implement `to_google_cloud_logging`

[Section titled “Implement to\_google\_cloud\_logging”](#implement-to_google_cloud_logging)

We added a `to_google_cloud_logging` operator that can send events to [Google Cloud Logging](https://cloud.google.com/logging).

By [@raxyte](https://github.com/raxyte) in [#5135](https://github.com/tenzir/tenzir/pull/5135).

#### Try and switch the Docker Image to `debian::trixie`

[Section titled “Try and switch the Docker Image to debian::trixie”](#try-and-switch-the-docker-image-to-debiantrixie)

The `to_snowflake` operator is now available in the arm64 Docker image.

By [@IyeOnline](https://github.com/IyeOnline) in [#5113](https://github.com/tenzir/tenzir/pull/5113).

#### Expose Arrow’s bit-wise compute functions

[Section titled “Expose Arrow’s bit-wise compute functions”](#expose-arrows-bit-wise-compute-functions)

We added bit-wise functions to TQL, including `bit_and`, `bit_or`, `bit_xor`, `bit_not`, `shift_left`, and `shift_right`. These functions enable performing bit-level operations on numeric values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5129](https://github.com/tenzir/tenzir/pull/5129).

### Changes

[Section titled “Changes”](#changes)

#### Assume UTF8 in `file_contents`

[Section titled “Assume UTF8 in file\_contents”](#assume-utf8-in-file_contents)

The `file_contents` function now returns contents as `string` by default. Non-UTF-8 files can be read by specifying the `binary=true` option.

By [@raxyte](https://github.com/raxyte) in [#5135](https://github.com/tenzir/tenzir/pull/5135).

#### Try and switch the Docker Image to `debian::trixie`

[Section titled “Try and switch the Docker Image to debian::trixie”](#try-and-switch-the-docker-image-to-debiantrixie-1)

The `tenzir/tenzir` and `tenzir/tenzir-node` Docker images now use `debian:trixie-slim` instead of `debian:bookworm-slim` as a base image.

By [@IyeOnline](https://github.com/IyeOnline) in [#5113](https://github.com/tenzir/tenzir/pull/5113).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Use a proper subpipeline for `fork`

[Section titled “Use a proper subpipeline for fork”](#use-a-proper-subpipeline-for-fork)

Operators that interact with state in the node that is not local to the pipeline, e.g., `context::update`, now properly work when used inside the nested pipeline of the `fork` operator. Previously, pipelines of the form `fork { context::update … }` failed at runtime.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5133](https://github.com/tenzir/tenzir/pull/5133).

#### Fix edge case when parsing nullable lists with type conflicts

[Section titled “Fix edge case when parsing nullable lists with type conflicts”](#fix-edge-case-when-parsing-nullable-lists-with-type-conflicts)

Parsing of nullable lists with type conflicts could previously lead to an error under very rare circumstances. This now works as expected.

By [@jachris](https://github.com/jachris) in [#5134](https://github.com/tenzir/tenzir/pull/5134).

#### Fix TLS options in `from_http`

[Section titled “Fix TLS options in from\_http”](#fix-tls-options-in-from_http)

We fixed a bug in parsing the TLS options for the `from_http` operator, preventing disabling of TLS.

By [@raxyte](https://github.com/raxyte) in [#5135](https://github.com/tenzir/tenzir/pull/5135).

# Fix Move Edge Cases

Fix edge cases for moving fields when using the `move` keyword in the `where` and `map` functions.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `map(move foo, name, expr)`

[Section titled “Fix map(move foo, name, expr)”](#fix-mapmove-foo-name-expr)

The `move` keyword now works as expected for the first positional argument of the `map` and `where` functions.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5151](https://github.com/tenzir/tenzir/pull/5151).

#### Enable `to_gogle_cloud_logging` for the Nix builds

[Section titled “Enable to\_gogle\_cloud\_logging for the Nix builds”](#enable-to_gogle_cloud_logging-for-the-nix-builds)

The `to_google_cloud_logging` operator was not available in static binary builds due to an oversight. This is now fixed.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5154](https://github.com/tenzir/tenzir/pull/5154).

# Google Cloud Logging Improvements

This release improves the integration with Google Cloud Logging.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.2).

### Changes

[Section titled “Changes”](#changes)

#### Split out `name` option and use metadata server when unset

[Section titled “Split out name option and use metadata server when unset”](#split-out-name-option-and-use-metadata-server-when-unset)

We split the `name` option of `to_google_cloud_logging`, no longer requiring user to construct the ID manually.

By [@raxyte](https://github.com/raxyte) in [#5160](https://github.com/tenzir/tenzir/pull/5160).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Delay catalog lookups until catalog is ready

[Section titled “Delay catalog lookups until catalog is ready”](#delay-catalog-lookups-until-catalog-is-ready)

The `export`, `metrics`, `diagnostics` and `partitions` operators returned an empty result when used before the node had successfully loaded its persisted data. They now wait correctly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5156](https://github.com/tenzir/tenzir/pull/5156).

#### Always shut down the pipeline manager first

[Section titled “Always shut down the pipeline manager first”](#always-shut-down-the-pipeline-manager-first)

We fixed a very rare bug that on shutdown could mark running pipelines as stopped, completed, or failed, causing the pipelines not to restart alongside the node.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5163](https://github.com/tenzir/tenzir/pull/5163).

#### Do not refuse startup when `pid.lock` is invalid

[Section titled “Do not refuse startup when pid.lock is invalid”](#do-not-refuse-startup-when-pidlock-is-invalid)

The node no longer refuses to start when its last shutdown happened in the brief period on startup after its PID file was created and before it was flushed.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5164](https://github.com/tenzir/tenzir/pull/5164).

#### Split out `name` option and use metadata server when unset

[Section titled “Split out name option and use metadata server when unset”](#split-out-name-option-and-use-metadata-server-when-unset-1)

The `to_google_cloud_logging` operator is now available in both Docker and static builds. The operator had earlier been missing due to a configuration issue.

By [@raxyte](https://github.com/raxyte) in [#5160](https://github.com/tenzir/tenzir/pull/5160).

# URL Encoding

This release adds support for URL encoding and decoding, and fixes the handling of user-defined operators in TQL2.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.3).

### Features

[Section titled “Features”](#features)

#### Implement `strict { ... }`

[Section titled “Implement strict { ... }”](#implement-strict--)

We added a new `strict` operator that takes a pipeline and treats all warnings emitted by that pipeline as errors, i.e., effectively stopping the pipeline at the first diagnostic. This is useful when you to ensure want a critical piece of your pipeline does not continue in unexpected scenarios.

By [@raxyte](https://github.com/raxyte) in [#5174](https://github.com/tenzir/tenzir/pull/5174).

#### Add `encode_url` and `decode_url` functions

[Section titled “Add encode\_url and decode\_url functions”](#add-encode_url-and-decode_url-functions)

The `encode_url` and `decode_url` functions encode and decode URLs. For example, `"Hello%20World".decode_url()` returns `b"Hello World"`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5168](https://github.com/tenzir/tenzir/pull/5168).

### Changes

[Section titled “Changes”](#changes)

#### Remove `pipeline::internal_parse()` from `to_azure_log_analytics`

[Section titled “Remove pipeline::internal\_parse() from to\_azure\_log\_analytics”](#remove-pipelineinternal_parse-from-to_azure_log_analytics)

The `table` option of the `to_azure_log_analytics` has been renamed to `stream` to better reflect the expected value. Additionally, a new option `batch_timeout` has been added to configure the max duration to wait before finishing a batch.

By [@raxyte](https://github.com/raxyte) in [#5166](https://github.com/tenzir/tenzir/pull/5166).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix user-defined operators for TQL2

[Section titled “Fix user-defined operators for TQL2”](#fix-user-defined-operators-for-tql2)

User-defined operators still required the `// tql2` comment at the start or the `tenzir.tql2` option to be set, despite TQL2 being the default since Tenzir Node v5.0. They now work as expected.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5169](https://github.com/tenzir/tenzir/pull/5169).

# Compression Fixes

This PR restores the ability to customize the `import` operator’s compression level.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.4).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Do not ignore the `tenzir.zstd-compression-level` option

[Section titled “Do not ignore the tenzir.zstd-compression-level option”](#do-not-ignore-the-tenzirzstd-compression-level-option)

The `tenzir.zstd-compression-level` option now works again as advertised for setting the Zstd compression level for the partitions written by the `import` operator. For the past few releases, newly written partitions unconditionally used the default compression level.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5183](https://github.com/tenzir/tenzir/pull/5183).

#### Fix lifetime issues and small bugs in `write_syslog`

[Section titled “Fix lifetime issues and small bugs in write\_syslog”](#fix-lifetime-issues-and-small-bugs-in-write_syslog)

We fixed a crash in `write_syslog` when receiving unexpected inputs and improved some diagnostics.

By [@raxyte](https://github.com/raxyte) in [#5180](https://github.com/tenzir/tenzir/pull/5180).

# Escape Sequences

This release adds support for escape sequences in string literals.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.5).

### Features

[Section titled “Features”](#features)

#### Support escape sequences in string literals

[Section titled “Support escape sequences in string literals”](#support-escape-sequences-in-string-literals)

String literals in TQL now support Unicode escape sequences like `\xHH`, `\uHHHH`, `\UHHHHHHHH`, and `\u{...}`. Additionally, the named escape sequences `\r`, `\b`, `\f`, `\v`, `\a`, and `\e`, are now supported in addition to the previously supported `\n`, `\t`, and `\0`.

TQL now supports literals for blobs through the `b"..."` syntax (or alternatively `br#"..."#`). Blobs are a sequence of bytes that unlike strings do not need to hold valid UTF-8, and can as such represent any binary data.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5187](https://github.com/tenzir/tenzir/pull/5187).

# Timestamp Components

This release adds the ability to extract hour, minute, and day components from timestamps.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.6).

### Features

[Section titled “Features”](#features)

#### Add `hour`, `minute`, and `second` time component extraction

[Section titled “Add hour, minute, and second time component extraction”](#add-hour-minute-and-second-time-component-extraction)

The `hour`, `minute`, and `second` functions extract the respective components of a `time` value, and compliment the existing `year`, `month`, and `day` functions. The `second` function includes subsecond precision.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5190](https://github.com/tenzir/tenzir/pull/5190).

#### Add prototype for static platform config

[Section titled “Add prototype for static platform config”](#add-prototype-for-static-platform-config)

Users of the Sovereign Edition of Tenzir can now use workspace tokens to connect their Tenzir Nodes to the Tenzir Platform.

By [@lava](https://github.com/lava) in [#5149](https://github.com/tenzir/tenzir/pull/5149).

### Changes

[Section titled “Changes”](#changes)

#### Remove deprecated `as_secs()` function

[Section titled “Remove deprecated as\_secs() function”](#remove-deprecated-as_secs-function)

We removed the deprecated `<duration>.as_secs()` function which has been superseded by `<duration>.count_seconds()` quite some time ago. The `count_seconds()` function provides the same functionality with a more consistent naming convention that aligns with other duration-related functions like `count_minutes()`, `count_hours()`, etc.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5190](https://github.com/tenzir/tenzir/pull/5190).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix a hang in `if` for stalled inputs

[Section titled “Fix a hang in if for stalled inputs”](#fix-a-hang-in-if-for-stalled-inputs)

We fixed a regression in the `if` statement that caused it to indefinitely withhold the last batch of events when its input stalled.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5196](https://github.com/tenzir/tenzir/pull/5196).

#### Handle assert gracefully in `write_syslog`

[Section titled “Handle assert gracefully in write\_syslog”](#handle-assert-gracefully-in-write_syslog)

We now gracefully handle a panic in `write_syslog`, when `structured_data` does not have the expected shape.

By [@raxyte](https://github.com/raxyte) in [#5191](https://github.com/tenzir/tenzir/pull/5191).

# Disabling Packages

This release adds the ability to disable entire packages.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.7).

### Features

[Section titled “Features”](#features)

#### Allow packages to be disabled properly

[Section titled “Allow packages to be disabled properly”](#allow-packages-to-be-disabled-properly)

Packages gained a new `config.disabled` option that causes all pipelines and contexts in the package to be disabled.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5161](https://github.com/tenzir/tenzir/pull/5161).

#### Add option to `load_tcp` for storing sender information

[Section titled “Add option to load\_tcp for storing sender information”](#add-option-to-load_tcp-for-storing-sender-information)

The `peer_field` option for `load_tcp` allows for setting a field for the IP address, port, and hostname of the connected peer.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5173](https://github.com/tenzir/tenzir/pull/5173).

#### Implement `<record>.keys()`

[Section titled “Implement \<record>.keys()”](#implement-recordkeys)

The `keys` function returns a list of strings representing the field names of a record.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5202](https://github.com/tenzir/tenzir/pull/5202).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Make the run loop of exec nodes cheaper

[Section titled “Make the run loop of exec nodes cheaper”](#make-the-run-loop-of-exec-nodes-cheaper)

Fixed a bug in the `load_tcp` operator that would cause it to require server certificates for incoming connections.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5193](https://github.com/tenzir/tenzir/pull/5193).

#### Allow packages to be disabled properly

[Section titled “Allow packages to be disabled properly”](#allow-packages-to-be-disabled-properly-1)

For configured pipelines, the `tenzir.pipelines.<pipeline>.disabled` configuration option was silently ignored unless the pipeline was part of a package. This no longer happens, and disabling the pipelines through the option now works correctly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5161](https://github.com/tenzir/tenzir/pull/5161).

#### Fix URL check in `to_hive`

[Section titled “Fix URL check in to\_hive”](#fix-url-check-in-to_hive)

The `to_hive` operator no longer incorrectly rejects URLs, which was due to a bug introduced by Tenzir v5.1.6.

By [@jachris](https://github.com/jachris) in [#5204](https://github.com/tenzir/tenzir/pull/5204).

# Fixed AMQP Integration

This release fixes a bug that sometimes prevented the `save_amqp` operator from sending messages.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.1.8).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Make amqp saver/loader detached

[Section titled “Make amqp saver/loader detached”](#make-amqp-saverloader-detached)

We fixed a bug in `save_amqp` that caused the operator to not send any messages.

By [@IyeOnline](https://github.com/IyeOnline) in [#5206](https://github.com/tenzir/tenzir/pull/5206).

#### Fix config parsing bug

[Section titled “Fix config parsing bug”](#fix-config-parsing-bug)

We fixed a bug that caused the 101st entry in objects by alphabetical order in `tenzir.yaml` configuration files to be ignored.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5210](https://github.com/tenzir/tenzir/pull/5210).

# OCSF Trim & Derive

This release introduces two new powerful OCSF operators that automate enum derivation and provide intelligent field trimming. The update also includes string padding functions, better HTTP requests, IP categorization and much more!

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.10.0).

### Features

[Section titled “Features”](#features)

#### Improved node robustness

[Section titled “Improved node robustness”](#improved-node-robustness)

We added an experimental feature to run node-independent operators of a pipeline in dedicated subprocesses. This brings improved error resilience and resource utilization. You can opt-in to this feature with the setting `tenzir.disable-pipeline-subprocesses: false` in `tenzir.yaml`. We plan to enable this feature by default in the future.

By [@tobim](https://github.com/tobim) in [#5233](https://github.com/tenzir/tenzir/pull/5233).

#### Operations on concatenated secrets

[Section titled “Operations on concatenated secrets”](#operations-on-concatenated-secrets)

You can now arbitrarily nest operations on secrets. This is useful for APIs that expect authentication is an encoded blob:

```tql
let $headers = {
  auth: f"{secret("user")}:{secret("password")}".encode_base64()
}
```

By [@IyeOnline](https://github.com/IyeOnline) in [#5324](https://github.com/tenzir/tenzir/pull/5324).

#### New string padding functions

[Section titled “New string padding functions”](#new-string-padding-functions)

Ever tried aligning threat actor names in your incident reports? Or formatting CVE IDs with consistent spacing for your vulnerability dashboard? We’ve all been there, fighting with inconsistent string lengths that make our security tools output look like alphabet soup. 🍲

Meet your new formatting friends: `pad_start()` and `pad_end()`!

#### Live Threat Feed Dashboard

[Section titled “Live Threat Feed Dashboard”](#live-threat-feed-dashboard)

Create a real-time threat indicator board with perfectly aligned columns:

```tql
from {time: "14:32", actor: "APT29", target: "energy", severity: 9},
     {time: "14:35", actor: "Lazarus", target: "finance", severity: 10},
     {time: "14:41", actor: "APT1", target: "defense", severity: 8}
select threat_line = time + " │ " + actor.pad_end(12) + " │ " +
                     target.pad_end(10) + " │ " + severity.string().pad_start(2, "0")
write_lines
```

```plaintext
14:32 │ APT29        │ energy     │ 09
14:35 │ Lazarus      │ finance    │ 10
14:41 │ APT1         │ defense    │ 08
```

#### CVE Priority Matrix

[Section titled “CVE Priority Matrix”](#cve-priority-matrix)

Format CVE IDs and CVSS scores for your vulnerability management system:

```tql
from {cve: "CVE-2024-1337", score: 9.8, vector: "network", status: "🔴"},
     {cve: "CVE-2024-42", score: 7.2, vector: "local", status: "🟡"},
     {cve: "CVE-2024-31415", score: 5.4, vector: "physical", status: "🟢"}
select priority = status + " " + cve.pad_end(16) + " [" +
                  score.string().pad_start(4) + "] " + vector.pad_start(10, "·")
write_lines
```

```plaintext
🔴 CVE-2024-1337    [ 9.8] ···network
🟡 CVE-2024-42      [ 7.2] ·····local
🟢 CVE-2024-31415   [ 5.4] ··physical
```

#### Network Flow Analysis

[Section titled “Network Flow Analysis”](#network-flow-analysis)

Build clean firewall logs with aligned source/destination pairs:

```tql
from {src: "10.0.0.5", dst: "8.8.8.8", proto: "DNS", bytes: 234},
     {src: "192.168.1.100", dst: "13.107.42.14", proto: "HTTPS", bytes: 8924},
     {src: "172.16.0.50", dst: "185.199.108.153", proto: "SSH", bytes: 45812}
select flow = src.pad_start(15) + " → " + dst.pad_start(15) +
              " [" + proto.pad_end(5) + "] " + bytes.string().pad_start(7) + " B"
write_lines
```

```plaintext
       10.0.0.5 →         8.8.8.8 [DNS  ]     234 B
  192.168.1.100 →    13.107.42.14 [HTTPS]    8924 B
    172.16.0.50 → 185.199.108.153 [SSH  ]   45812 B
```

Both padding functions accept three parameters:

* **String to pad** (required)
* **Target length** (required)
* **Padding character** (optional, defaults to space)

If your string is already longer than the target length, it returns unchanged. Multi-character padding? That’s a paddlin’ (returns an error).

Your SOC dashboards never looked so clean! 🎯

By [@mavam](https://github.com/mavam) in [#5344](https://github.com/tenzir/tenzir/pull/5344).

#### Sinks in HTTP Parsing Pipelines

[Section titled “Sinks in HTTP Parsing Pipelines”](#sinks-in-http-parsing-pipelines)

Parsing pipeline in the `from_http` and `http` operators now support sinks. This worked already in `from_file` parsing pipelines and now works, as expected, also in the HTTP parsing pipelines. For example, you can now write:

```tql
from_http "https://cra.circl.lu/opendata/geo-open/mmdb-country-asn/latest.mmdb" {
  context::load "geo-open-country-asn"
}
```

By [@mavam](https://github.com/mavam) in [#5343](https://github.com/tenzir/tenzir/pull/5343).

#### HTTP request body encoding

[Section titled “HTTP request body encoding”](#http-request-body-encoding)

The `from_http` and `http` operators now support using `record` values for the request `body` parameter. By default, the record is serialized as JSON. You can also specify `encode="form"` to send the body as URL-encoded form data. When using `form` encoding, nested fields are flattened using dot notation (e.g., `foo: {bar: "baz"}` => `foo.bar=baz`). This supersedes the `payload` parameter, which therefore is now deprecated.

###### Examples

[Section titled “Examples”](#examples)

By default, setting `body` to a record will JSON-encode it:

```tql
http "https://api.example.com/data", body={foo: "bar", count: 42}
```

```http
POST /data HTTP/1.1
Host: api.example.com
Content-Type: application/json
Content-Length: 33


{
  "foo": "bar",
  "count": 42
}
```

To change the encoding, you can use the `encode` option:

```tql
http "https://api.example.com/data",
  body={foo: {bar: "baz"}, count: 42},
  encode="form"
```

```http
POST /data HTTP/1.1
Host: api.example.com
Content-Type: application/x-www-form-urlencoded
Content-Length: 20


foo.bar=baz&count=42
```

Arbitrary body contents can be sent by using a string or blob:

```tql
http "https://api.example.com/data", body="hello world!"
```

```http
POST /data HTTP/1.1
Host: api.example.com
Content-Length: 12


hello world!
```

By [@raxyte](https://github.com/raxyte) in [#5305](https://github.com/tenzir/tenzir/pull/5305).

#### IP address categorization functions

[Section titled “IP address categorization functions”](#ip-address-categorization-functions)

Ever wondered if that suspicious traffic is coming from inside the corporate network? 🏢 We’ve got you covered with a new suite of IP address classification functions that make network analysis a breeze.

**`is_private()`** - Quickly spot internal RFC 1918 addresses in your logs. Perfect for identifying lateral movement or distinguishing between internal and external threats:

```tql
where src_ip.is_private() and dst_ip.is_global()
// Catch data exfiltration attempts from your internal network
```

**`is_global()`** - Find publicly routable addresses. Essential for tracking external attackers or monitoring outbound connections:

```tql
where src_ip.is_global() and failed_login_count > 5
// Detect brute force attempts from the internet
```

**`is_multicast()`** - Identify multicast traffic (224.0.0.0/4, ff00::/8). Great for spotting mDNS, SSDP, and other broadcast protocols that shouldn’t cross network boundaries:

```tql
where dst_ip.is_multicast() and src_ip.is_global()
// Flag suspicious multicast from external sources
```

**`is_link_local()`** - Detect link-local addresses (169.254.0.0/16, fe80::/10). Useful for identifying misconfigurations or APIPA fallback:

```tql
where server_ip.is_link_local()
// Find services accidentally binding to link-local addresses
```

**`is_loopback()`** - Spot loopback addresses (127.0.0.0/8, ::1). Hunt for suspicious local connections or tunneled traffic:

```tql
where src_ip != dst_ip and dst_ip.is_loopback()
// Unusual loopback connections might indicate malware
```

**`ip_category()`** - Get the complete classification in one shot. Returns: “global”, “private”, “multicast”, “link\_local”, “loopback”, “broadcast”, or “unspecified”:

```tql
where src_ip.ip_category() == "private" and dst_ip.ip_category() == "multicast"
// Analyze traffic patterns by IP category
```

These functions work seamlessly with both IPv4 and IPv6 addresses, making them future-proof for your dual-stack environments. Happy hunting! 🔍

By [@mavam](https://github.com/mavam) in [#5336](https://github.com/tenzir/tenzir/pull/5336).

#### `ocsf::trim` and `ocsf::derive`

[Section titled “ocsf::trim and ocsf::derive”](#ocsftrim-and-ocsfderive)

Tenzir now provides two new operators for processing OCSF events:

**`ocsf::derive`** automatically assigns enum strings from their integer counterparts and vice versa. It performs bidirectional enum derivation for OCSF events and validates consistency between existing enum values.

```tql
from {
  activity_id: 1,
  class_uid: 1001,
  metadata: {version: "1.5.0"},
}
ocsf::derive
```

This transforms the event to include the derived `activity_name: "Create"` and `class_name: "File System Activity"` fields.

**`ocsf::trim`** intelligently removes fields from OCSF events to reduce data size while preserving essential information. You can also have explicit control over optional and recommended field removal.

```tql
from {
  class_uid: 3002,
  class_name: "Authentication",
  user: {
    name: "alice",
    display_name: "Alice",
  },
  status: "Success",
}
ocsf::trim
```

This removes non-essential fields like `class_name` and `user.display_name` while keeping critical information intact.

By [@jachris](https://github.com/jachris) in [#5330](https://github.com/tenzir/tenzir/pull/5330).

#### Compression for `write_bitz`

[Section titled “Compression for write\_bitz”](#compression-for-write_bitz)

Tenzir’s internal wire format, which is accessible through the `read_bitz` and `write_bitz` operators, now uses Zstd compression internally, resulting in a significantly smaller output size. This change is backwards-compatible.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5335](https://github.com/tenzir/tenzir/pull/5335).

### Changes

[Section titled “Changes”](#changes)

#### Better query optimization

[Section titled “Better query optimization”](#better-query-optimization)

Previously, queries that used `export` followed by a `where` that used fields such as `this["field name"]` were not optimized. Now, the same optimizations apply as with normal fields, improving the performance of such queries.

By [@jachris](https://github.com/jachris) in [#5362](https://github.com/tenzir/tenzir/pull/5362).

#### Improved `join` behavior

[Section titled “Improved join behavior”](#improved-join-behavior)

The `join` function now also works with empty lists that are typed as `list<null>`. Furthermore, it now emits more helpful warnings.

By [@jachris](https://github.com/jachris) in [#5356](https://github.com/tenzir/tenzir/pull/5356).

#### Respecting error responses from Azure Log Analytics

[Section titled “Respecting error responses from Azure Log Analytics”](#respecting-error-responses-from-azure-log-analytics)

The `to_azure_log_analytics` operator now emits an error when it receives any response considering an internal error. Those normally indicate configuration errors and the pipeline will now stop with an error instead of continuing to send data that will not be received correctly.

By [@tobim](https://github.com/tobim) in [#5314](https://github.com/tenzir/tenzir/pull/5314).

#### Renamed `to_asl`

[Section titled “Renamed to\_asl”](#renamed-to_asl)

We renamed our Amazon Security Lake integration operator from `to_asl` to `to_amazon_security_lake`. The old name is now deprecated and will be removed in the future.

By [@IyeOnline](https://github.com/IyeOnline) in [#5340](https://github.com/tenzir/tenzir/pull/5340).

#### `kv` parser no longer produces empty fields

[Section titled “kv parser no longer produces empty fields”](#kv-parser-no-longer-produces-empty-fields)

Our Key-Value parsers (the `read_kv` operator and `parse_kv` function) previously produced empty values if the `value_split` was not found.

With this change, a “field” missing a `value_split` is considered an extension of the previous fields value instead:

```tql
from \
  {input: "x=1 y=2 z=3 4 5 a=6"},
this = { ...input.parse_kv() }
```

Previous result:

```tql
{x:1, y:2, z:"3", "4":"", "5":"", a:6}
```

New result:

```tql
{x:1, y:2, z:"3 4 5", a:6}
```

By [@IyeOnline](https://github.com/IyeOnline) in [#5313](https://github.com/tenzir/tenzir/pull/5313).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Non-default databases in `to_clickhouse`

[Section titled “Non-default databases in to\_clickhouse”](#non-default-databases-in-to_clickhouse)

The `to_clickhouse` operator erroneously rejected `table` arguments of the form `database_name.table_name`. This is now fixed, allowing you to write to non-default databases.

By [@IyeOnline](https://github.com/IyeOnline) in [#5355](https://github.com/tenzir/tenzir/pull/5355).

#### Remove file size limit from Amazon Security Lake Integration

[Section titled “Remove file size limit from Amazon Security Lake Integration”](#remove-file-size-limit-from-amazon-security-lake-integration)

We removed the 256MB file size limit from the Amazon Security Lake integration.

By [@IyeOnline](https://github.com/IyeOnline) in [#5340](https://github.com/tenzir/tenzir/pull/5340).

#### Newlines before `else`

[Section titled “Newlines before else”](#newlines-before-else)

Previously, the `if … { … } else { … }` construct required that there was no newline before `else`. This restriction is now lifted, which allows placing `else` at the beginning of the line:

```tql
if x { … }
else if y { … }
else { … }
```

By [@jachris](https://github.com/jachris) in [#5348](https://github.com/tenzir/tenzir/pull/5348).

#### Fixed `encrypt_cryptopan` function

[Section titled “Fixed encrypt\_cryptopan function”](#fixed-encrypt_cryptopan-function)

We fixed a bug that sometimes caused the `encrypt_cryptopan` function to fail with the error “got `ip`, expected `ip`”, which was caused by an incorrect type check. The function now works as expected again.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5345](https://github.com/tenzir/tenzir/pull/5345).

#### Fixed `list_separator` option name in `print_csv`

[Section titled “Fixed list\_separator option name in print\_csv”](#fixed-list_separator-option-name-in-print_csv)

The `print_csv`, `print_ssv` and `print_tsv` functions had an option incorrectly named `field_separator`. Instead, these functions have an option `list_separator` now, allowing you to change the list separator. You cannot set a custom `field_separator` on these functions. If you want to print with custom `field_separator`s, use `print_xsv` instead.

By [@IyeOnline](https://github.com/IyeOnline) in [#5357](https://github.com/tenzir/tenzir/pull/5357).

#### Fix `context::create_geoip` without `db_path`

[Section titled “Fix context::create\_geoip without db\_path”](#fix-contextcreate_geoip-without-db_path)

The `context::create_geoip` operator failed with a `message_mismatch` error when no `db_path` option was provided. This was caused by an internal serialization error, which we now fixed. This is the only known place where this error occurred.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5342](https://github.com/tenzir/tenzir/pull/5342).

#### Fix `http` operator pagination

[Section titled “Fix http operator pagination”](#fix-http-operator-pagination)

The `http` operator dropped all provided HTTP headers after the first request when performing paginated requests. The operator now preserves the headers for all requests.

By [@mavam](https://github.com/mavam) in [#5332](https://github.com/tenzir/tenzir/pull/5332).

# Better Performance

This release delivers significant performance improvements for situations with many concurrent pipelines, making Tenzir more robust under high-load scenarios. New features include AWS role assumption support, enhanced string trimming functionality, and improved HTTP error handling capabilities. Additionally, this release adds several new operators and comes with various bug fixes.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.11.0).

### Features

[Section titled “Features”](#features)

#### Roles in `save_s3` and `to_amazon_security_lake`

[Section titled “Roles in save\_s3 and to\_amazon\_security\_lake”](#roles-in-save_s3-and-to_amazon_security_lake)

We have added new options to assume a role to the `save_s3` and `to_amazon_security_lake` operators. You can specify an AWS `role` and the operator(s) will assume this role for authorization and optionally. Additionally you can specify an `external_id` to use alongside the role.

By [@raxyte](https://github.com/raxyte), [@IyeOnline](https://github.com/IyeOnline) in [#5391](https://github.com/tenzir/tenzir/pull/5391).

#### Trimming custom characters

[Section titled “Trimming custom characters”](#trimming-custom-characters)

The `trim()`, `trim_start()`, and `trim_end()` functions can now remove specific characters from strings, not just whitespace. Pass a second argument containing a string where each character represents a character to remove:

```tql
from {
  path: "/path/to/file/".trim("/"),
  decorated: "--hello--world--".trim("-"),
  complex: "/-/data/-/".trim("/-")
}
```

```tql
{
  path: "path/to/file",
  decorated: "hello--world",
  complex: "data"
}
```

Each character in the second argument is treated individually, not as a complete string to match:

```tql
from {
  // Removes 'a', 'e', and 'g' from both ends
  chars: "abcdefg".trim("aeg"),
  // Removes any 'o', 'l', 'e', or 'h' from both ends
  word: "helloworldhello".trim("olleh")
}
```

```tql
{
  chars: "bcdf",
  word: "wr"
}
```

This also works with `trim_start()` and `trim_end()` for one-sided trimming:

```tql
from {
  start: "///api/v1/users".trim_start("/"),
  end: "data.csv.tmp.....".trim_end(".")
}
```

```tql
{
  start: "api/v1/users",
  end: "data.csv"
}
```

By [@mavam](https://github.com/mavam) in [#5389](https://github.com/tenzir/tenzir/pull/5389).

#### Handling HTTP error status codes

[Section titled “Handling HTTP error status codes”](#handling-http-error-status-codes)

The `from_http` and `http` operators now provide an `error_field` option that lets you specify a field to receive the error response as a `blob`. When you set this option, the operators keep events with status codes outside the 200–399 range so you can handle them manually.

By [@raxyte](https://github.com/raxyte) in [#5358](https://github.com/tenzir/tenzir/pull/5358).

#### Versioned sources in `to_amazon_security_lake` operator

[Section titled “Versioned sources in to\_amazon\_security\_lake operator”](#versioned-sources-in-to_amazon_security_lake-operator)

The `to_amazon_security_lake` operator now supports versioned custom sources, such as

```tql
let $lake_url = "s3://aws-security-data-lake-eu-west-2-lake-abcdefghijklmnopqrstuvwxyz1234/ext/tnz-ocsf-dns/1.0/"
to_amazon_security_lake $lake_url, …
```

By [@IyeOnline](https://github.com/IyeOnline) in [#5369](https://github.com/tenzir/tenzir/pull/5369).

#### Dropping null fields

[Section titled “Dropping null fields”](#dropping-null-fields)

The new `drop_null_fields` operator removes fields containing null values from events. Without arguments, it drops all fields with null values. With field arguments, it drops the specified fields if they contain null values, and for record fields, it also recursively drops all null fields within them.

Drop all null fields:

```tql
from {
  id: 42,
  user: {name: "alice", email: null},
  status: null,
  tags: ["security", "audit"]
}
drop_null_fields
```

```tql
{
  id: 42,
  user: {
    name: "alice",
  },
  tags: [
    "security",
    "audit",
  ],
}
```

Drop specific null fields:

```tql
from {
  id: 42,
  user: {name: "alice", email: null},
  status: null,
  tags: ["security", "audit"]
}
drop_null_fields user.email
```

```tql
{
  id: 42,
  user: {
    name: "alice",
  },
  status: null,
  tags: [
    "security",
    "audit",
  ],
}
```

Note that `status` remains because it wasn’t specified in the field list.

When specifying a record field, all null fields within it are removed:

```tql
from {
  user: {name: "alice", email: null, role: null},
  settings: {theme: "dark", notifications: null}
}
drop_null_fields user
```

```tql
{
  user: {
    name: "alice",
  },
  settings: {
    theme: "dark",
    notifications: null,
  },
}
```

The `user.email` and `user.role` fields are removed because they are null fields within the specified `user` record. The `settings.notifications` field remains because `settings` was not specified.

By [@mavam](https://github.com/mavam) in [#5370](https://github.com/tenzir/tenzir/pull/5370).

#### More supported types in `read_parquet`

[Section titled “More supported types in read\_parquet”](#more-supported-types-in-read_parquet)

Tenzir’s does not support all types that Parquet supports. We have enabled the `read_parquet` operator to accept more types that are convertible to supported types. It will convert integer, floating point, and time types to the appropriate (wider) Tenzir type. For example, if your Parquet file contains a column of type `int32`, it will now be read in as `int64` instead of rejecting the entire file.

By [@IyeOnline](https://github.com/IyeOnline) in [#5373](https://github.com/tenzir/tenzir/pull/5373).

#### Dynamic `log_type` for `to_google_secops`

[Section titled “Dynamic log\_type for to\_google\_secops”](#dynamic-log_type-for-to_google_secops)

The `to_google_secops` operator now supports dynamic `log_type`s. You can set the option to any expression evaluating to a string, e.g.:

```tql
from {type: "CUSTOM_DNS", text: "..."},
     {type: "BIND_DNS", text: "..."}
to_google_secops log_type=type, log_text=text, ...
```

By [@raxyte](https://github.com/raxyte) in [#5365](https://github.com/tenzir/tenzir/pull/5365).

#### New `read_all` operator

[Section titled “New read\_all operator”](#new-read_all-operator)

The `read_all` operator produces a single event for its entire input stream.

By [@jachris](https://github.com/jachris) in [#5368](https://github.com/tenzir/tenzir/pull/5368).

#### Account key authentication for Azure Blob Storage

[Section titled “Account key authentication for Azure Blob Storage”](#account-key-authentication-for-azure-blob-storage)

The `load_azure_blob_storage` and `save_azure_blob_storage` operators now support account key (shared key) authentication via a new `account_key` option. This provides an additional method for accessing Azure Blob Storage, alongside existing authentication options.

By [@raxyte](https://github.com/raxyte) in [#5380](https://github.com/tenzir/tenzir/pull/5380).

### Changes

[Section titled “Changes”](#changes)

#### Performance improvements

[Section titled “Performance improvements”](#performance-improvements)

Tenzir can now handle significantly more concurrent pipelines without becoming unresponsive. These improvements make the system significantly more robust under high load, with response times remaining stable even with thousands of concurrent pipelines.

By [@jachris](https://github.com/jachris) in [#5382](https://github.com/tenzir/tenzir/pull/5382).

#### Improvements to `context::enrich`

[Section titled “Improvements to context::enrich”](#improvements-to-contextenrich)

The `context::enrich` operator now allows using `mode="append"` even if the enrichment does not have the exact same type as the existing type, as long as they are compatible.

Furthermore, `mode="ocsf"` now returns `null` if no enrichment took place instead of a record with a `null` data field.

By [@jachris](https://github.com/jachris) in [#5388](https://github.com/tenzir/tenzir/pull/5388).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Context operator metrics

[Section titled “Context operator metrics”](#context-operator-metrics)

The data flowing through the `context::` family of operators is no longer counted as actual ingress and egress.

By [@jachris](https://github.com/jachris) in [#5383](https://github.com/tenzir/tenzir/pull/5383).

#### Fixed secrets in `headers` argument in `from_http`

[Section titled “Fixed secrets in headers argument in from\_http”](#fixed-secrets-in-headers-argument-in-from_http)

We fixed a crash when using a secret in the `headers` argument of the `from_http` operator.

By [@IyeOnline](https://github.com/IyeOnline) in [#5376](https://github.com/tenzir/tenzir/pull/5376).

#### Fixed crash in `read_parquet`

[Section titled “Fixed crash in read\_parquet”](#fixed-crash-in-read_parquet)

Tenzir and the `read_parquet` operator only support a subset of all Parquet types. Reading an unsupported Parquet file could previously crash Tenzir in some situations. This is now fixed and the operator instead raises an error.

By [@IyeOnline](https://github.com/IyeOnline) in [#5373](https://github.com/tenzir/tenzir/pull/5373).

#### Fixed issue with table creation in `to_clickhouse`

[Section titled “Fixed issue with table creation in to\_clickhouse”](#fixed-issue-with-table-creation-in-to_clickhouse)

Multiple `to_clickhouse` operators can now attempt to create the same ClickHouse table at the same time without an error.

By [@IyeOnline](https://github.com/IyeOnline) in [#5360](https://github.com/tenzir/tenzir/pull/5360).

#### Fixed `to_amazon_security_lake` partitioning

[Section titled “Fixed to\_amazon\_security\_lake partitioning”](#fixed-to_amazon_security_lake-partitioning)

The `to_amazon_security_lake` incorrectly partitioned as `…/accountID=…`. It now uses the correct `…/accountId=…`.

By [@IyeOnline](https://github.com/IyeOnline) in [#5369](https://github.com/tenzir/tenzir/pull/5369).

#### Return type of `map` for empty lists

[Section titled “Return type of map for empty lists”](#return-type-of-map-for-empty-lists)

Previously, the `map` function would return the input list when the input was empty, possibly producing type warnings downstream. It now correctly returns `list<null>` instead.

By [@jachris](https://github.com/jachris) in [#5385](https://github.com/tenzir/tenzir/pull/5385).

#### Formatting `ip` and `subnet` values in `to_amazon_security_lake`

[Section titled “Formatting ip and subnet values in to\_amazon\_security\_lake”](#formatting-ip-and-subnet-values-in-to_amazon_security_lake)

The `to_amazon_security_lake` operator now correctly formats `ip` and `subnet` values as strings and formats timestamps using millisecond precision, similar to the Security Lake built-in sources.

By [@raxyte](https://github.com/raxyte) in [#5387](https://github.com/tenzir/tenzir/pull/5387).

# HTTP Fix and Compression for Azure Log Analytics

This release introduces payload compression for Azure Log Analytics to reduce bandwidth usage, as well as an important fix for a `from_http` bug that was introduced with the previous release.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.11.1).

### Changes

[Section titled “Changes”](#changes)

#### `to_azure_log_analytics` compression

[Section titled “to\_azure\_log\_analytics compression”](#to_azure_log_analytics-compression)

The `to_azure_log_analytics` operator now compresses the sent payload, significantly reducing the bandwidth consumed.

By [@raxyte](https://github.com/raxyte) in [#5394](https://github.com/tenzir/tenzir/pull/5394).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed `from_http` default port

[Section titled “Fixed from\_http default port”](#fixed-from_http-default-port)

Using the `from_http` operator as a client without explicitly specifying a port resulted in an error complaining that the port cannot be zero. This now works as expected, meaning that the default port is derived correctly from the URL scheme, i.e., 80 for HTTP and 443 for HTTPS.

By [@jachris](https://github.com/jachris) in [#5398](https://github.com/tenzir/tenzir/pull/5398).

# OCSF 1.6.0

This release adds support for OCSF 1.6.0 and introduces the `replace` operator.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.12.0).

### Features

[Section titled “Features”](#features)

#### Check if your data is truly empty

[Section titled “Check if your data is truly empty”](#check-if-your-data-is-truly-empty)

Ever stared at your security logs wondering if that suspicious-looking field is actually empty or just pretending? We’ve all been there. That’s why we added `is_empty()` - a universal emptiness detector that works on strings, lists, and records.

```tql
from {
  id: "evt-12345",
  tags: ["malware", "c2"],
  metadata: {},
  description: "",
  iocs: []
}
has_metadata = not metadata.is_empty()
has_description = not description.is_empty()
has_iocs = not iocs.is_empty()
```

```tql
{
  id: "evt-12345",
  tags: [
    "malware",
    "c2",
  ],
  metadata: {},
  description: "",
  iocs: [],
  has_metadata: false,
  has_description: false,
  has_iocs: false,
}
```

No more checking `length() == 0` or wondering if that field exists but is empty. Just ask `is_empty()` and move on with your threat hunting!

By [@mavam](https://github.com/mavam) in [#5403](https://github.com/tenzir/tenzir/pull/5403).

#### Replacing values

[Section titled “Replacing values”](#replacing-values)

The new `replace` operator allows you to find and replace all occurrences of a specific value across all fields (but not in lists) in your data with another value. This is particularly useful for data sanitization, redacting sensitive information, or normalizing values across datasets.

The operator scans every field in each input event and replaces any value that equals the `what` parameter with the value specified by `with`.

Replace all occurrences of the string `"-"` with null:

```tql
from {
  status: "-",
  data: {result: "-", count: 42},
  items: ["-", "valid", "-"]
}
replace what="-", with=null
```

```tql
{
  status: null,
  data: {result: null, count: 42},
  items: ["-", "valid", "-"]
}
```

Redact a specific IP address across all fields:

```tql
from {
  src_ip: 192.168.1.1,
  dst_ip: 10.0.0.1,
  metadata: {source: 192.168.1.1}
}
replace what=192.168.1.1, with="REDACTED"
```

```tql
{
  src_ip: "REDACTED",
  dst_ip: 10.0.0.1,
  metadata: {source: "REDACTED"}
}
```

By [@raxyte](https://github.com/raxyte) in [#5372](https://github.com/tenzir/tenzir/pull/5372).

#### `role` and `external_id` options for \`load\_s3

[Section titled “role and external\_id options for \`load\_s3”](#role-and-external_id-options-for-load_s3)

We added `role` and `external_id` options to the `load_s3` operator, bringing it in line with `save_s3`, which already features these options.

By [@IyeOnline](https://github.com/IyeOnline) in [#5406](https://github.com/tenzir/tenzir/pull/5406).

#### Support for OCSF 1.6.0

[Section titled “Support for OCSF 1.6.0”](#support-for-ocsf-160)

The `ocsf::` family of operators now supports OCSF 1.6.0.

Furthermore, the version `1.7.0-dev` is also supported now.

By [@jachris](https://github.com/jachris) in [#5407](https://github.com/tenzir/tenzir/pull/5407).

### Changes

[Section titled “Changes”](#changes)

#### Handling of type conflicts when reading data

[Section titled “Handling of type conflicts when reading data”](#handling-of-type-conflicts-when-reading-data)

Tenzir requires all items of a list to have the same type. As a result, items in lists that contain different types (such as `[1, "test"]`) are cast to the common type `string`. Previously, all items were stored with their JSON representation, leading to the result `["1", "\"test\""]`. Now, only lists and record are stored as JSON, and strings are preserved without extra quotes. Thus, the new output is `["1", "test"]`.

By [@jachris](https://github.com/jachris) in [#5405](https://github.com/tenzir/tenzir/pull/5405).

#### Automatic integer casting in `ocsf::apply`

[Section titled “Automatic integer casting in ocsf::apply”](#automatic-integer-casting-in-ocsfapply)

The `ocsf::apply` operator now automatically casts `uint64` values to `int64` when the OCSF schema expects an integer field. This is important because the exact integer type is mostly considered an implementation detail. Unsigned integers are mainly produced when reading events for which a schema has been defined. This change makes sure that OCSF mappings that use the resulting events can successfully pass through `ocsf::apply`.

**Example:**

```tql
from {
  class_uid: 4001,
  metadata: {
    version: "1.5.0"
  },
  severity_id: uint(3)
}
ocsf::apply
```

Previously, this would result in a type mismatch warning and the `severity_id` field would be set to null. Now the `uint64` value 3 is automatically cast to `int64`, preserving the data. Values that exceed the maximum `int64` value will still generate a warning and be set to null.

By [@jachris](https://github.com/jachris) in [#5401](https://github.com/tenzir/tenzir/pull/5401).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Metadata handling of `ocsf::derive` and `ocsf::trim`

[Section titled “Metadata handling of ocsf::derive and ocsf::trim”](#metadata-handling-of-ocsfderive-and-ocsftrim)

The `ocsf::derive` and `ocsf::trim` operators now correctly preserve the metadata (such as `@name`) of the incoming event instead of overwriting it with the internal metadata used to encode OCSF schemas.

By [@jachris](https://github.com/jachris) in [#5402](https://github.com/tenzir/tenzir/pull/5402).

#### Optimization of the `delay` operator

[Section titled “Optimization of the delay operator”](#optimization-of-the-delay-operator)

The `delay` operator optimization routine incorrectly declared that the behavior of the operator does not depend on the order of its input. As a result, chains such as `sort -> delay -> publish` did not correctly delay events.

By [@jachris](https://github.com/jachris) in [#5399](https://github.com/tenzir/tenzir/pull/5399).

# SecOps & SecurityLake Fixes

We fixed two bugs in the `to_google_secops` and `to_amazon_security_lake` operators.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.12.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Assertion failures in `to_google_secops`

[Section titled “Assertion failures in to\_google\_secops”](#assertion-failures-in-to_google_secops)

The `to_google_secops` operator failed assertions when a batch of data was missing `log_type` or if no input was received for longer than `batch_timeout`.

By [@raxyte](https://github.com/raxyte) in [#5411](https://github.com/tenzir/tenzir/pull/5411).

#### Fixed handling of `time` in `to_amazon_security_lake`

[Section titled “Fixed handling of time in to\_amazon\_security\_lake”](#fixed-handling-of-time-in-to_amazon_security_lake)

Previously events with a `null` value for the OCSF `time` field would incorrectly be written to some partition in the lake. In rare circumstances, this could also cause a crash.

The operator now correctly skips events without a valid `time`.

By [@IyeOnline](https://github.com/IyeOnline) in [#5409](https://github.com/tenzir/tenzir/pull/5409).

# Enhanced UDP Source

This release enhances UDP ingestion with the new `from_udp` operator that produces structured events with sender metadata. We also improved the execution model for `every` and `cron` subpipelines, added DNS lookup capabilities, and made the Syslog parser more flexible.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.13.0).

### Features

[Section titled “Features”](#features)

#### Receive UDP datagrams as events

[Section titled “Receive UDP datagrams as events”](#receive-udp-datagrams-as-events)

The new `from_udp` operator receives UDP datagrams and outputs structured events containing both the data and peer information.

Unlike `load_udp` which outputs raw bytes, `from_udp` produces events with metadata about the sender, making it ideal for security monitoring and network analysis where knowing the source of each datagram is important.

Each received datagram becomes an event with this structure:

```tql
from_udp "0.0.0.0:1234"
```

```tql
{
  data: "Hello, UDP!\n",
  peer: {
    ip: 192.168.1.100,
    port: 54321,
  },
}
```

Enable hostname resolution for DNS lookups (disabled by default for performance):

```tql
from_udp "0.0.0.0:1234", resolve_hostnames=true
```

```tql
{
  data: "Hello, UDP!\n",
  peer: {
    ip: 192.168.1.100,
    port: 54321,
    hostname: "client.example.com",
  },
}
```

By [@mavam](https://github.com/mavam) in [#5375](https://github.com/tenzir/tenzir/pull/5375).

#### Perform inline DNS lookups

[Section titled “Perform inline DNS lookups”](#perform-inline-dns-lookups)

The new `dns_lookup` operator enables DNS resolution for both IP addresses and domain names. It performs reverse PTR lookups for IP addresses and forward A/AAAA lookups for hostnames, returning structured results with hostnames or IP addresses with their types and TTLs.

Resolve a domain name to IP addresses:

```tql
from {
  host: "example.com"
}
dns_lookup host
```

```tql
{
  host: "example.com",
  dns_lookup: {
    records: [
      {
        address: 2600:1406:3a00:21::173e:2e65,
        type: "AAAA",
        ttl: 58s,
      },
      {
        address: 23.215.0.136,
        type: "A",
        ttl: 2.433333333333333min,
      },
      // ... more records
    ],
  },
}
```

Resolve an IP address to a hostname:

```tql
from {
  ip: 8.8.8.8
}
dns_lookup ip
```

```tql
{
  ip: 8.8.8.8,
  dns_lookup: {
    hostname: "dns.google",
  },
}
```

By [@mavam](https://github.com/mavam), [@IyeOnline](https://github.com/IyeOnline) in [#5379](https://github.com/tenzir/tenzir/pull/5379).

#### `contains_null(x:any)`

[Section titled “contains\_null(x:any)”](#contains_nullxany)

We added a new `contains_null` function that checks if the input value contains any `null` values.

By [@raxyte](https://github.com/raxyte) in [#5419](https://github.com/tenzir/tenzir/pull/5419).

#### Context for `assert` operator

[Section titled “Context for assert operator”](#context-for-assert-operator)

The `assert` operator now has a `message` option that can be used to provide context about the event failing the assertion.

By [@raxyte](https://github.com/raxyte) in [#5433](https://github.com/tenzir/tenzir/pull/5433).

#### More lenient RFC 3164 Syslog parsing

[Section titled “More lenient RFC 3164 Syslog parsing”](#more-lenient-rfc-3164-syslog-parsing)

Our syslog parser now allows for a `.` character in the tag/app\_name field and any character in the `process_id` field. This allows you to parse the log:

```plaintext
<21>Aug 18 12:00:00 hostname_redacted .NetRuntime[-]: content...
```

```tql
{
  facility: 2,
  severity: 5,
  timestamp: "Aug 18 12:00:00",
  hostname: "hostname_redacted",
  app_name: ".NetRuntime",
  process_id: "-",
  content: "content...",
}
```

By [@IyeOnline](https://github.com/IyeOnline) in [#5426](https://github.com/tenzir/tenzir/pull/5426).

### Changes

[Section titled “Changes”](#changes)

#### `every` and `cron` subpipelines

[Section titled “every and cron subpipelines”](#every-and-cron-subpipelines)

We changed the execution model for `every` and `cron` subpipelines, resulting in:

* operators such as `context::load` now execute properly.
* subpipelines can contain both `remote` and `local` operators.
* subpipelines must not accept or output bytes.

By [@raxyte](https://github.com/raxyte) in [#5410](https://github.com/tenzir/tenzir/pull/5410).

#### Deprecation of `split_at_null` option of `read_lines`

[Section titled “Deprecation of split\_at\_null option of read\_lines”](#deprecation-of-split_at_null-option-of-read_lines)

The `split_at_null` option of the `read_lines` operator is now deprecated. Use `read_delimited "\0"` instead.

By [@jachris](https://github.com/jachris) in [#5431](https://github.com/tenzir/tenzir/pull/5431).

#### Amazon Security Lake

[Section titled “Amazon Security Lake”](#amazon-security-lake)

We have made two convenience changes to the `to_amazon_security_lake` operator:

* The `role` parameter now defaults to the automatically generated role for the custom source in Security Lake. If you are using a different role, you can still specify it.
* The operator now uses UUIDv7 for the names of the files written into the Security Lake’s blob storage. Since UUIDv7 is time ordered, inspecting the files in the lake becomes slightly easier.

By [@mavam](https://github.com/mavam), [@IyeOnline](https://github.com/IyeOnline) in [#5412](https://github.com/tenzir/tenzir/pull/5412).

#### Sorting Improvements

[Section titled “Sorting Improvements”](#sorting-improvements)

We have re-done the internals of the `sort` operator. You will now be able to more reliably sort events using lists or records as keys. Lists are compared lexicographically between their values, while records are compared by their sorted key-value pairs.

By [@IyeOnline](https://github.com/IyeOnline) in [#5425](https://github.com/tenzir/tenzir/pull/5425).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Rare crash in `save_tcp` operator

[Section titled “Rare crash in save\_tcp operator”](#rare-crash-in-save_tcp-operator)

We fixed a rare shutdown crash in the `save_tcp` operator.

By [@jachris](https://github.com/jachris) in [#5420](https://github.com/tenzir/tenzir/pull/5420).

# Azure Blob Storage & Stability Improvements

This release adds a new Azure Blob Storage operator with account key authentication and improves Google Security Operations retry handling. It also contains various small fixes and improvements.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.13.1).

### Features

[Section titled “Features”](#features)

#### `from_azure_blob_storage` operator

[Section titled “from\_azure\_blob\_storage operator”](#from_azure_blob_storage-operator)

The new `from_azure_blob_storage` operator works similarly to `from_file` but supports additional Azure Blob Storage specific options.

For example, you can set the `account_key`:

```tql
from_azure_blob_storage "abfs://container/data/*.csv", account_key="your-account-key"
```

By [@raxyte](https://github.com/raxyte) in [#5429](https://github.com/tenzir/tenzir/pull/5429).

#### Dynamic `namespace` and retry logic for `to_google_secops`

[Section titled “Dynamic namespace and retry logic for to\_google\_secops”](#dynamic-namespace-and-retry-logic-for-to_google_secops)

The `to_google_secops` operator now retries requests which fail with a `5XX` or a `429` status code. Additionally, the `namespace` option of the operator now supports all expressions that evaluate to a `string`.

By [@raxyte](https://github.com/raxyte) in [#5446](https://github.com/tenzir/tenzir/pull/5446).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Misleading `from_file remove=true` warning

[Section titled “Misleading from\_file remove=true warning”](#misleading-from_file-removetrue-warning)

The `from_file` operator emits a warning when using `remove=true` if the file could not be removed. When deleting the last file inside an S3 directory, we keep that directory around by inserting a zero-sized object. However, this failed when the necessary `PutObject` permissions were not granted, thus emitting a warning even though the file was removed successfully. For this specific case, we thus no longer emit a warning. Other issues during file deletion are still reported.

By [@jachris](https://github.com/jachris) in [#5438](https://github.com/tenzir/tenzir/pull/5438).

#### Timezone troubles from `parse_time()`

[Section titled “Timezone troubles from parse\_time()”](#timezone-troubles-from-parse_time)

We fixed assertion failures when using the `parse_time` function with the `%z` or `%Z` specifiers.

By [@raxyte](https://github.com/raxyte) in [#5435](https://github.com/tenzir/tenzir/pull/5435).

#### Fixed crash when writing out enumerations

[Section titled “Fixed crash when writing out enumerations”](#fixed-crash-when-writing-out-enumerations)

We fixed a rare crash that could occur when writing/printing enumeration values in various formats.

By [@IyeOnline](https://github.com/IyeOnline) in [#5434](https://github.com/tenzir/tenzir/pull/5434).

#### Buffering in the `fork` operator

[Section titled “Buffering in the fork operator”](#buffering-in-the-fork-operator)

We fixed an issue in the `fork` operator where the last event would get stuck.

By [@raxyte](https://github.com/raxyte) in [#5436](https://github.com/tenzir/tenzir/pull/5436).

# New S3 Operator & Fork Operator Bugfix

This release adds a new S3 operator and fixes a bug within the `fork` operator.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.13.2).

### Features

[Section titled “Features”](#features)

#### `from_s3` operator

[Section titled “from\_s3 operator”](#from_s3-operator)

The `from_s3` operator reads files from Amazon S3 with support for glob patterns, automatic format detection, and file monitoring.

```tql
from_s3 "s3://my-bucket/data/**.json"
```

The operator supports multiple authentication methods including default AWS credentials, explicit access keys, IAM role assumption, and anonymous access for public buckets:

```tql
from_s3 "s3://my-bucket/data.csv",
  access_key=secret("AWS_ACCESS_KEY"),
  secret_key=secret("AWS_SECRET_KEY")
```

For S3-compatible services, specify custom endpoints via URL parameters:

```tql
from_s3 "s3://my-bucket/data/**.json?endpoint_override=minio.example.com:9000&scheme=http"
```

Additional features include file watching for continuous ingestion, automatic file removal or renaming after processing, and path field injection to track source files in events.

By [@raxyte](https://github.com/raxyte) in [#5449](https://github.com/tenzir/tenzir/pull/5449).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `fork` operator stopping after initial events

[Section titled “Fix fork operator stopping after initial events”](#fix-fork-operator-stopping-after-initial-events)

We fixed a bug where the `fork` operator would stop processing events after handling only the first few events, causing data loss in downstream pipeline stages.

By [@raxyte](https://github.com/raxyte) in [#5450](https://github.com/tenzir/tenzir/pull/5450).

# SentinelOne Data Lake

This release introduces an integration fo SentinelOne Singularity™ Data Lake and a new message based `to_kafka` operator that features a one to one event to message relation.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.14.0).

### Features

[Section titled “Features”](#features)

#### Send data to Kafka topics with `to_kafka`

[Section titled “Send data to Kafka topics with to\_kafka”](#send-data-to-kafka-topics-with-to_kafka)

The new `to_kafka` operator allows you to send one Kafka message per event, making it easier to integrate Tenzir with tools that rely on the 1:1 correlation between messages and events.

**Examples**

Use `to_kafka` to send JSON events to a topic:

```tql
subscribe "logs"
to_kafka "events", message=this.print_json()
```

Send specific field values with custom keys for partitioning:

```tql
subscribe "alerts"
to_kafka "metrics", message=alert_msg, key="server-01"
```

By [@raxyte](https://github.com/raxyte) in [#5460](https://github.com/tenzir/tenzir/pull/5460).

#### SentinelOne Singularity Data Lake Integration

[Section titled “SentinelOne Singularity Data Lake Integration”](#sentinelone-singularity-data-lake-integration)

We have added an integration for the SentinelOne Singularity™ Data Lake!

The new `to_sentinelone_data_lake` operator allows you to easily send structured and unstructured events to the data lake:

```tql
subscribe "sentinelone-data-lake"
to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net",
  token=secret("sentinelone-token")
```

By [@IyeOnline](https://github.com/IyeOnline) in [#5455](https://github.com/tenzir/tenzir/pull/5455).

#### `insert_separator` option for `load_zmq`

[Section titled “insert\_separator option for load\_zmq”](#insert_separator-option-for-load_zmq)

The `load_zmq` operator now supports an optional `insert_separator` parameter to append a custom string to each received ZeroMQ message. This enables better message separation and parsing for downstream operators.

By [@raxyte](https://github.com/raxyte) in [#5456](https://github.com/tenzir/tenzir/pull/5456).

### Changes

[Section titled “Changes”](#changes)

#### Use UUIDv7 for file naming in `to_hive` operator

[Section titled “Use UUIDv7 for file naming in to\_hive operator”](#use-uuidv7-for-file-naming-in-to_hive-operator)

The `to_hive` operator now uses UUIDv7 instead of consecutive numbers for file naming within partitions. This change provides guaranteed uniqueness across concurrent processes and natural time-based ordering of files, preventing filename conflicts when multiple processes write to the same partition simultaneously.

Example output paths changed from:

* `/partition/1.json`
* `/partition/2.json` To:
* `/partition/01234567-89ab-cdef-0123-456789abcdef.json`
* `/partition/01234568-cd01-2345-6789-abcdef012345.json`

UUIDv7 combines the benefits of timestamp-based ordering with collision resistance, making it ideal for distributed data processing scenarios.

By [@jachris](https://github.com/jachris) in [#5464](https://github.com/tenzir/tenzir/pull/5464).

# Lambda Captures

This release enhances TQL’s data transformation capabilities with lambda expressions that can capture surrounding fields in `map` and `where` functions, plus grouped enumeration for separate event counting. We’ve also improved operator composability with enhanced `to_splunk` parameters, added octet counting support for syslog messages, and fixed critical issues in Kafka message handling and HTTP request processing.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.15.0).

### Features

[Section titled “Features”](#features)

#### Grouped enumeration

[Section titled “Grouped enumeration”](#grouped-enumeration)

The `enumerate` operator now supports a `group` option to enumerate events separately based on a value.

For example, to have a field act as a counter for a value, use the following pipeline:

```tql
from {x: 1}, {x: 2}, {x: "1"}, {x: 2}
enumerate count, group=x
count = count + 1
```

```tql
{
  count: 1,
  x: 1,
}
{
  count: 1,
  x: 2,
}
{
  count: 1,
  x: "1",
}
{
  count: 2,
  x: 2,
}
```

By [@raxyte](https://github.com/raxyte) in [#5475](https://github.com/tenzir/tenzir/pull/5475).

#### Flag for preventing automatic pipeline starts

[Section titled “Flag for preventing automatic pipeline starts”](#flag-for-preventing-automatic-pipeline-starts)

When the node starts, pipelines that were previously running are immediately started. The new `--no-autostart` flag can be used to disable this behavior.

By [@jachris](https://github.com/jachris) in [#5470](https://github.com/tenzir/tenzir/pull/5470).

#### Lambdas in `map` and `where` can capture surrounding fields

[Section titled “Lambdas in map and where can capture surrounding fields”](#lambdas-in-map-and-where-can-capture-surrounding-fields)

Lambda expressions in the `map` and `where` functions can now capture and access fields from the surrounding context, enabling more powerful data transformations.

For example:

```tql
from {
  host: "server1",
  ports: [80, 443, 8080]
}
ports = ports.map(p => {host: host, port: p})
```

```tql
{
  host: "server1",
  ports: [
    {
      host: "server1",
      port: 80,
    },
    {
      host: "server1",
      port: 443,
    },
    {
      host: "server1",
      port: 8080,
    },
  ],
}
```

By [@raxyte](https://github.com/raxyte) in [#5457](https://github.com/tenzir/tenzir/pull/5457).

#### Improve `to_splunk` composability

[Section titled “Improve to\_splunk composability”](#improve-to_splunk-composability)

We have improved the composability of the `to_splunk` operator. The `host` and `source` parameters now accept a `string`-expression instead of only a constant. Further, there is a new `event` parameter that can be used to specify what should be send as the event to the Splunk HTTP Event Collector.

The combination of these options improves the composability of the operator, allowing you to set event-specific Splunk parameters, while not also transmitting them as part of the actual event:

```tql
from {
  host: "my-host",
  a: 42,
  b: 0
}


// move the entire event into `event`
this = { event: this }


// hoist the splunk specific field back out
move host = event.host


to_splunk "https://localhost:8088",
  hec_token=secret("splunk-hec-token"),
  host=host,
  event=event
```

By [@IyeOnline](https://github.com/IyeOnline) in [#5478](https://github.com/tenzir/tenzir/pull/5478).

#### Octet Counting in `read_syslog`

[Section titled “Octet Counting in read\_syslog”](#octet-counting-in-read_syslog)

We have added a new option `octet_counting` to the `read_syslog` operator. Enabling this option will determine messages boundaries according to [RFC6587](https://datatracker.ietf.org/doc/html/rfc6587#section-3.4.1) instead of our heuristic.

By [@IyeOnline](https://github.com/IyeOnline) in [#5472](https://github.com/tenzir/tenzir/pull/5472).

### Changes

[Section titled “Changes”](#changes)

#### Dedicated Syslog Schema Names

[Section titled “Dedicated Syslog Schema Names”](#dedicated-syslog-schema-names)

The `read_syslog` operator now produces dedicated schemas `syslog.rfc5425`, `syslog.rfc3164` and `syslog.unknown` instead of an unspecific `tenzir.syslog`.

By [@IyeOnline](https://github.com/IyeOnline) in [#5472](https://github.com/tenzir/tenzir/pull/5472).

#### Keep zeek TSV logs as-is in `read_zeek_tsv`

[Section titled “Keep zeek TSV logs as-is in read\_zeek\_tsv”](#keep-zeek-tsv-logs-as-is-in-read_zeek_tsv)

Parsing Zeek TSV logs no longer attempts to cast the parsed events to a shipped Zeek schema.

By [@tobim](https://github.com/tobim) in [#5461](https://github.com/tenzir/tenzir/pull/5461).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Explicit Commits in `load_kafka`

[Section titled “Explicit Commits in load\_kafka”](#explicit-commits-in-load_kafka)

The `load_kafka` operator now explicitly commits messages it has consumed. By default, it will commit every 1000 messages or every 10 seconds, with the behavior being customizable via two new operator arguments.

Previously, the operator would commit every message asynchronously loaded by the backing library automatically, which may have included messages that were never accepted by the pipeline.

By [@IyeOnline](https://github.com/IyeOnline) in [#5465](https://github.com/tenzir/tenzir/pull/5465).

#### `http` operator stalling

[Section titled “http operator stalling”](#http-operator-stalling)

The `http` operator now correctly handles its internal waiting state, fixing an intermittent issue where HTTP requests could hang unexpectedly.

By [@raxyte](https://github.com/raxyte) in [#5479](https://github.com/tenzir/tenzir/pull/5479).

#### Improved Syslog Output Schema

[Section titled “Improved Syslog Output Schema”](#improved-syslog-output-schema)

We have improved our `read_syslog` operator and `parse_syslog` function. They no longer re-order fields if the syslog format changes mid-stream and produce correctly typed null values for the special `-` value.

By [@IyeOnline](https://github.com/IyeOnline) in [#5472](https://github.com/tenzir/tenzir/pull/5472).

#### Fixed `to_kafka` crash

[Section titled “Fixed to\_kafka crash”](#fixed-to_kafka-crash)

The recently released `to_kafka` operator would fail with an internal error when used without specifying the `message` argument.

The operator now works as expected, sending the entire event if the argument is not specified.

By [@IyeOnline](https://github.com/IyeOnline) in [#5465](https://github.com/tenzir/tenzir/pull/5465).

# Stability Improvements

This release brings forth stability improvements under high load that could cause platform unresponsiveness, fixes API request isolation problems, better kafka diagnostics and more.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.16.0).

### Features

[Section titled “Features”](#features)

#### Subscribe to multiple topics at once

[Section titled “Subscribe to multiple topics at once”](#subscribe-to-multiple-topics-at-once)

The `subscribe` operator now accepts multiple topics to subscribe to. For example, `subscribe "notices", "alerts"` subscribes to both the `notices`, and the `alerts` topic. This makes it easier to build pipelines that join multiple topics back together.

By [@jachris](https://github.com/jachris) in [#5494](https://github.com/tenzir/tenzir/pull/5494).

#### `from_gcs` operator

[Section titled “from\_gcs operator”](#from_gcs-operator)

The new `from_gcs` operator reads files from Google Cloud Storage with support for glob patterns, authentication via Application Default Credentials, and all standard file processing features like monitoring and path tracking.

By [@raxyte](https://github.com/raxyte) in [#5491](https://github.com/tenzir/tenzir/pull/5491).

### Changes

[Section titled “Changes”](#changes)

#### Better kafka diagnostics

[Section titled “Better kafka diagnostics”](#better-kafka-diagnostics)

The `kafka` related operators now emit more diagnostics in unexpected situations, providing more information to diagnose problems.

By [@raxyte](https://github.com/raxyte) in [#5490](https://github.com/tenzir/tenzir/pull/5490).

#### Better defaults for `load_kafka`

[Section titled “Better defaults for load\_kafka”](#better-defaults-for-load_kafka)

The `load_kafka` operators previously used `offset="end"` as the default, which meant that it always started from the end of the topic. This default was now changed to `"stored"`, such that the previously commited offset is used instead.

By [@jachris](https://github.com/jachris) in [#5485](https://github.com/tenzir/tenzir/pull/5485).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Pipeline execution under high loads

[Section titled “Pipeline execution under high loads”](#pipeline-execution-under-high-loads)

Previously, the execution of certain pipelines under high load scenarios could lead to general unresponsiveness. In extreme cases, this meant that the platform wasn’t able to reach the node. This issue has now been resolved, leading to a more reliable and responsive experience.

By [@jachris](https://github.com/jachris) in [#5486](https://github.com/tenzir/tenzir/pull/5486).

#### API request isolation

[Section titled “API request isolation”](#api-request-isolation)

Requests to the `/pipeline` API are now properly isolated and sequentialized. Before, it could happen that certain requests that should not be executed concurrently were interleaved. This could lead to unpredictable results when interacting with pipelines through the platform.

By [@jachris](https://github.com/jachris) in [#5486](https://github.com/tenzir/tenzir/pull/5486).

#### `session_name` and `external_id` in `aws_iam` options

[Section titled “session\_name and external\_id in aws\_iam options”](#session_name-and-external_id-in-aws_iam-options)

The `load_kafka`, `save_kafka` and `to_kafka` operators now accept configuring `session_name` and `external_id` for `aws_iam` options.

By [@raxyte](https://github.com/raxyte) in [#5481](https://github.com/tenzir/tenzir/pull/5481).

#### Behavior of the `throttle` operator

[Section titled “Behavior of the throttle operator”](#behavior-of-the-throttle-operator)

The `throttle` operator now correctly forwards its data in a timely manner.

By [@jachris](https://github.com/jachris) in [#5488](https://github.com/tenzir/tenzir/pull/5488).

#### Hang in `every` and `cron`

[Section titled “Hang in every and cron”](#hang-in-every-and-cron)

We fixed a bug in `every` and `cron` operators that could cause them to hang and panic with assertions failures.

By [@raxyte](https://github.com/raxyte) in [#5483](https://github.com/tenzir/tenzir/pull/5483).

# Package UDOs

This release introduces user-defined operators in packages, allowing you to extend Tenzir with custom operators defined in TQL files. It also adds list manipulation functions, a recursive search function, and improved memory management.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.17.0).

### Features

[Section titled “Features”](#features)

#### Checking if a value exists in another value

[Section titled “Checking if a value exists in another value”](#checking-if-a-value-exists-in-another-value)

The new `contains()` function recursively searches for a value within data structures and returns `true` if found, `false` otherwise.

By [@raxyte](https://github.com/raxyte) in [#5493](https://github.com/tenzir/tenzir/pull/5493).

#### Improved list manipulation

[Section titled “Improved list manipulation”](#improved-list-manipulation)

We have added two new functions that make managing set-like lists easier.

The `add` function ensures uniqueness when building lists. Perfect for maintaining deduplicated threat intel feeds or collecting unique user sessions:

```tql
from {xs: [1]},
     {xs: [2]},
     {xs: []}
select result = xs.add(2)
```

```tql
{result: [1,2]}
{result: [2]}
{result: [2]}
```

The `remove` function cleans up your lists by eliminating all occurrences of unwanted elements. Ideal for filtering out known-good domains from suspicious activity logs or removing false positives from alert lists:

```tql
from {xs: [1, 2, 1, 3], y: 1},
     {xs: [4, 5], y: 1},
select result = xs.remove(y)
```

```tql
{result: [2, 3]}
{result: [4, 5]}
```

By [@mavam](https://github.com/mavam), [@IyeOnline](https://github.com/IyeOnline) in [#5471](https://github.com/tenzir/tenzir/pull/5471).

#### User-defined operators in packages

[Section titled “User-defined operators in packages”](#user-defined-operators-in-packages)

This extends the package format with user-defined operators. A packaged operator can be used from a pipeline after the package is installed on a node. Package operators are defined in `.tql` files the `operators` subdirectory of a package. Once installed, the operators can be called by its ID, which is constructed from the filesystem path.

Here is an example from a hypothetical MISP package. This is the directory structure with an operator:

```plaintext
└── misp
    └── operators
        └── event
            └── to_ocsf.tql
```

And you can use the operator in TQL:

```dart
misp::event::to_ocsf
```

By [@tobim](https://github.com/tobim) in [#5496](https://github.com/tenzir/tenzir/pull/5496).

### Changes

[Section titled “Changes”](#changes)

#### Memory usage when importing many different schemas at once

[Section titled “Memory usage when importing many different schemas at once”](#memory-usage-when-importing-many-different-schemas-at-once)

Previously, importing a high volume of highly heterogeneous events could lead to memory usage issues because of internal buffering that was only limited on a per-schema basis. With the introduction of a global limit across all schemas, this issue has now been fixed. The configuration option `tenzir.max-buffered-events` can be used to tune the new buffering limits.

By [@tobim](https://github.com/tobim), [@jachris](https://github.com/jachris) in [#5508](https://github.com/tenzir/tenzir/pull/5508).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed spawning of demo nodes

[Section titled “Fixed spawning of demo nodes”](#fixed-spawning-of-demo-nodes)

Fixed an issue that would cause demo nodes on <https://app.tenzir.com> to fail when spawning.

By [@lava](https://github.com/lava) in [#5504](https://github.com/tenzir/tenzir/pull/5504).

#### Handle spaces in filesystem paths

[Section titled “Handle spaces in filesystem paths”](#handle-spaces-in-filesystem-paths)

File paths containing spaces are now properly handled by operators.

By [@raxyte](https://github.com/raxyte) in [#5499](https://github.com/tenzir/tenzir/pull/5499).

# Performance Improvements

This release focuses on improving performance and memory usage. Pipelines are now faster, especially when using if conditions or parsing highly heterogeneous events. Memory usage has also been substantially reduced.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.18.0).

### Changes

[Section titled “Changes”](#changes)

#### Periodically release unused memory in server mode

[Section titled “Periodically release unused memory in server mode”](#periodically-release-unused-memory-in-server-mode)

Tenzir Node now calls malloc\_trim every 10 minutes to release unused memory back to the operating system, reducing memory fragmentation in long-running instances.

By [@lava](https://github.com/lava) in [#5524](https://github.com/tenzir/tenzir/pull/5524).

#### Improved `export` memory management

[Section titled “Improved export memory management”](#improved-export-memory-management)

The database partitions opened by the `export` operator previously read and forwarded their entire contents to `export` without waiting for the operator to forward them. This circumvented the usual backpressure mechanism and could lead to unexpectedly high memory usage. Now, the backpressure is propagated to the underlying storage layer.

By [@jachris](https://github.com/jachris) in [#5520](https://github.com/tenzir/tenzir/pull/5520).

#### Improved pipeline execution

[Section titled “Improved pipeline execution”](#improved-pipeline-execution)

We fine-tuned the scheduling logic responsible for the execution of pipelines. In particular, certain pipelines that invoke parsing functions now take significantly less memory to run. Furthermore, `if` runs much faster in situations with many small batches, preventing pipeline congestion and therefore also lower memory usage.

By [@jachris](https://github.com/jachris) in [#5519](https://github.com/tenzir/tenzir/pull/5519), [#5525](https://github.com/tenzir/tenzir/pull/5525).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed record sorting

[Section titled “Fixed record sorting”](#fixed-record-sorting)

Calling `sort` on records may have caused a crash for more involved objects. This no longer happens.

By [@mavam](https://github.com/mavam) in [#5526](https://github.com/tenzir/tenzir/pull/5526).

# Watching Directories

Tenzir Node v5.2 brings our most requested feature to life, adding the ability to watch a local filesystem directory or a blob storage bucket for new files within a pipeline.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.2.0).

### Features

[Section titled “Features”](#features)

#### Add `from_file` operator

[Section titled “Add from\_file operator”](#add-from_file-operator)

The new `from_file` operator can be used to read multiple files from a potentially remote filesystem using globbing expressions. It also supports watching for new files and deletion after a file has been read.

`read_lines` now has an additional `binary=true` option which should be used if the incoming byte stream is not valid UTF-8.

By [@jachris](https://github.com/jachris) in [#5203](https://github.com/tenzir/tenzir/pull/5203).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Handle empty files in `read_xsv` & friends

[Section titled “Handle empty files in read\_xsv & friends”](#handle-empty-files-in-read_xsv--friends)

We fixed a bug that caused `read_xsv` & friends to crash when trying to read an empty file.

By [@IyeOnline](https://github.com/IyeOnline) in [#5215](https://github.com/tenzir/tenzir/pull/5215).

#### Fix hang for operators with infinite idle timeout

[Section titled “Fix hang for operators with infinite idle timeout”](#fix-hang-for-operators-with-infinite-idle-timeout)

We fixed a hang in the `cache` and `buffer` operators when their input finished. This also prevented the node from shutting down cleanly.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5219](https://github.com/tenzir/tenzir/pull/5219).

#### Fix a crash in `to_clickhouse` and bump `clickhouse-cpp`

[Section titled “Fix a crash in to\_clickhouse and bump clickhouse-cpp”](#fix-a-crash-in-to_clickhouse-and-bump-clickhouse-cpp)

We fixed a bug in `to_clickhouse` that caused the operator to crash when encountering lists.

By [@IyeOnline](https://github.com/IyeOnline) in [#5221](https://github.com/tenzir/tenzir/pull/5221).

#### Add `from_file` operator

[Section titled “Add from\_file operator”](#add-from_file-operator-1)

The `read_lines` operator now validates that the incoming lines are UTF-8.

By [@jachris](https://github.com/jachris) in [#5203](https://github.com/tenzir/tenzir/pull/5203).

# Talking HTTP

This release brings forth improvements to HTTP support in Tenzir, supporting requests as transformations and paginating APIs.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.3.0).

### Features

[Section titled “Features”](#features)

#### Implement `http` operator

[Section titled “Implement http operator”](#implement-http-operator)

We implemented the `http` operator that allows making HTTP/1.1 requests to a URL. The operator also allows paginate APIs based on the responses.

By [@raxyte](https://github.com/raxyte) in [#5188](https://github.com/tenzir/tenzir/pull/5188).

#### Introduce lambda functions

[Section titled “Introduce lambda functions”](#introduce-lambda-functions)

TQL now supports lambda expressions. They are supported in the `where` and `map` functions on list, and on the newly added `count_if` aggregation function. Instead of `[1, 2, 3].map(x, x + 1)`, use `[1, 2, 3].map(x => x + 1)`. This subtle change makes it obvious that the expression is not evaluated on the entire list, but rather on each element individually.

The `count_if` aggregation function counts the number of elements in a list that satisfy a given predicate. For example, `[1, 2, 3].count_if(x => x > 1)` returns `2`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5150](https://github.com/tenzir/tenzir/pull/5150).

#### Implement `from_http` client

[Section titled “Implement from\_http client”](#implement-from_http-client)

The `from_http` operator now supports HTTP client functionality. This allows sending HTTP/1.1 requests, including support for custom methods, headers, payloads, pagination, retries, and connection timeouts. The operator can be used to fetch data from HTTP APIs and ingest it directly into pipelines.

Make a simple GET request auto-selecting the parser:

```tql
from_http "https://api.example.com/data"
```

Post data to some API:

```tql
from_http "https://api.example.com/submit", payload={foo: "bar"}.print_json(),
          headers={"Content-Type": "application/json"}
```

Paginating APIs:

```tql
from_http "https://api.example.com/items",
          paginate=(x => x.next_url if x.has_more? == true)
```

By [@raxyte](https://github.com/raxyte) in [#5177](https://github.com/tenzir/tenzir/pull/5177).

### Changes

[Section titled “Changes”](#changes)

#### `load_http` deprecated

[Section titled “load\_http deprecated”](#load_http-deprecated)

The `from` operator now dispatches to `from_http` for `http[s]` URLs.

The `load_http` operator is now deprecated in favor of `from_http`.

By [@raxyte](https://github.com/raxyte) in [#5177](https://github.com/tenzir/tenzir/pull/5177).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix a segfault in `save_amqp` on connection loss

[Section titled “Fix a segfault in save\_amqp on connection loss”](#fix-a-segfault-in-save_amqp-on-connection-loss)

We fixed a crash in `save_amqp` when trying to send a message after the connection was lost.

By [@IyeOnline](https://github.com/IyeOnline) in [#5226](https://github.com/tenzir/tenzir/pull/5226).

#### Fix overflow warning for `-9223372036854775808`

[Section titled “Fix overflow warning for -9223372036854775808”](#fix-overflow-warning-for--9223372036854775808)

The lowest 64-bit integer, `-9223372036854775808`, no longer causes an overflow warning.

By [@jachris](https://github.com/jachris) in [#5223](https://github.com/tenzir/tenzir/pull/5223).

#### Fix a crash in `to_clickhouse`

[Section titled “Fix a crash in to\_clickhouse”](#fix-a-crash-in-to_clickhouse)

We fixed an issue when trying to send lists in `to_clickhouse` that would cause the ClickHouse server to drop the connection.

By [@IyeOnline](https://github.com/IyeOnline) in [#5231](https://github.com/tenzir/tenzir/pull/5231).

#### Fix evaluation of `null if true else …`

[Section titled “Fix evaluation of null if true else …”](#fix-evaluation-of-null-if-true-else)

The expression `null if true else 42` previously returned `42`. It now correctly returns `null`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5150](https://github.com/tenzir/tenzir/pull/5150).

# Fixed Python Operator

This release fixes a regression in the Python operator, which was not working correctly in v5.3.0.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.3.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `python` operator

[Section titled “Fix python operator”](#fix-python-operator)

Tenzir Node v5.3.0 contained a mismatched version of the Python operator, causing the operator to fail to start. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5258](https://github.com/tenzir/tenzir/pull/5258).

# Fix Python Operator

Tenzir Node v5.3.1 updated the pyproject version but did not actually commit it, causing the Python operator to fail to start. This release fixes the issue.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.3.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `python` operator

[Section titled “Fix python operator”](#fix-python-operator)

Tenzir Node v5.3.0 contained a mismatched version of the Python operator, causing the operator to fail to start. This no longer happens.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5258](https://github.com/tenzir/tenzir/pull/5258).

# HTTP fixes

The from\_http and http operators now support response sizes upto 2GiB

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.3.3).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Handle large HTTP responses

[Section titled “Handle large HTTP responses”](#handle-large-http-responses)

The HTTP client operators `from_http` and `http` now support response sizes upto 2 GiB.

By [@raxyte](https://github.com/raxyte) in [#5269](https://github.com/tenzir/tenzir/pull/5269).

# Fix Demo Node Packages

This release fixes a bug that caused package installation outside of the Tenzir Library to fail, which caused Demo Nodes in the Tenzir Platform to not have any packages installed.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.3.4).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `package::add`

[Section titled “Fix package::add”](#fix-packageadd)

The `package::add` operator did not correctly handle the switch to `from_http` in the previous release and as a result errored when installing packages manually. This has now been fixed. However, package installation via Tenzir Platform was still functional. This was also the cause of the demo node not having any pipelines or pre-installed packages when launched.

By [@raxyte](https://github.com/raxyte) in [#5271](https://github.com/tenzir/tenzir/pull/5271).

# Format Strings

With the introduction of format strings to TQL, this release makes string construction from multiple parts easier than ever before.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.4.0).

### Features

[Section titled “Features”](#features)

#### Format strings

[Section titled “Format strings”](#format-strings)

TQL now supports format strings as you might know them from other languages like Python. Format strings allow you to flexibly construct strings in a very succinct way by using a pair of braces within an `f"…"` string.

For example, assume that you have events with two integer fields, `found` and `total`. We can construct a message from this as follows:

```tql
percent = round(found / total * 100).string()
message = "Found " + found.string() + "/" + total.string() + " => " + percent + "%"
```

Using the new format strings, this simply becomes

```tql
percent = round(found / total * 100)
message = f"Found {found}/{total} => {percent}%"
```

You can also use arbitrary expressions inside `{` to simplify this even further:

```tql
message = f"Found {found}/{total} => {round(found / total * 100)}%"
```

If you ever need an actual `{` in your format string, you can use `{{`. The same goes for the closing brace `}`, which needs to be written as `}}` within format strings.

By [@jachris](https://github.com/jachris), [@IyeOnline](https://github.com/IyeOnline) in [#5254](https://github.com/tenzir/tenzir/pull/5254).

### Changes

[Section titled “Changes”](#changes)

#### Remove `meta` keyword

[Section titled “Remove meta keyword”](#remove-meta-keyword)

The identifier `meta` is no longer a keyword and can thus now be used as a normal field name.

By [@jachris](https://github.com/jachris) in [#5275](https://github.com/tenzir/tenzir/pull/5275), [#5276](https://github.com/tenzir/tenzir/pull/5276).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Gracefully handle null values when charting with resolution

[Section titled “Gracefully handle null values when charting with resolution”](#gracefully-handle-null-values-when-charting-with-resolution)

The `chart_bar` and `chart_pie` operators had a bug when the x-axis had a `null` value and the `resolution` option was specified. The unfortunate panic due to this bug has now been fixed.

By [@raxyte](https://github.com/raxyte) in [#5273](https://github.com/tenzir/tenzir/pull/5273).

#### Pipeline activity refresh without running pipelines

[Section titled “Pipeline activity refresh without running pipelines”](#pipeline-activity-refresh-without-running-pipelines)

The `pipeline::activity` operator now always yields new events, even when all running pipelines are hidden.

By [@jachris](https://github.com/jachris) in [#5278](https://github.com/tenzir/tenzir/pull/5278).

#### Invalid scientific notation when using `write_json`

[Section titled “Invalid scientific notation when using write\_json”](#invalid-scientific-notation-when-using-write_json)

When using `write_json` with large floating-point numbers, the resulting JSON was ill-formed. For example, the number `5483819555176798000.0` was previously printed as `5.483819555176798e+18.0`. The extra `.0` at the end is not valid JSON. Thus, the output was rejected by some parsers. Now, `write_json` renders this number as `5.483819555176798e+18` instead.

This bug was also observable on the Tenzir Platform, where it could lead to request timeouts. Now, large numbers are shown correctly.

By [@jachris](https://github.com/jachris) in [#5274](https://github.com/tenzir/tenzir/pull/5274).

#### Unreliable `where` diagnostics

[Section titled “Unreliable where diagnostics”](#unreliable-where-diagnostics)

The `where` operator now correctly produces diagnostics also for simple expressions, which was previously not the case in some situations.

By [@jachris](https://github.com/jachris) in [#5277](https://github.com/tenzir/tenzir/pull/5277).

# JSON Printer Fix

This release fixes a bug within the JSON printer that could lead to invalid JSON being produced, and also led to response timeouts when using the Tenzir Platform.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.4.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Incorrect values when charting

[Section titled “Incorrect values when charting”](#incorrect-values-when-charting)

The charting operators did not update aggregations correctly, which resulted in out-of-sync or `null` values.

By [@raxyte](https://github.com/raxyte) in [#5281](https://github.com/tenzir/tenzir/pull/5281).

#### Fixed invalid JSON for small numbers

[Section titled “Fixed invalid JSON for small numbers”](#fixed-invalid-json-for-small-numbers)

Operators such as `write_json` previously emitted invalid JSON for small numbers. This also affected the Tenzir Platform as it invalidated some responses, which could lead to no data showing up in the Explorer.

By [@jachris](https://github.com/jachris) in [#5282](https://github.com/tenzir/tenzir/pull/5282).

# OCSF Support

Built-in support for normalizing OCSF events to their upstream schema makes normalizations easier than ever with Tenzir Node v5.5.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.5.0).

### Features

[Section titled “Features”](#features)

#### Entropy Calculation

[Section titled “Entropy Calculation”](#entropy-calculation)

TQL now supports calculating the Shannon entropy of data using the new `entropy` aggregation function. This function measures the amount of uncertainty or randomness in your data, which is particularly useful for analyzing data distributions and information content.

The entropy function calculates Shannon entropy using the formula `H(x) = -sum(p(x[i]) \* log(p(x[i])))`, where `p(x[i])` is the probability of each unique value. Higher entropy values indicate more randomness, while lower values indicate more predictability in your data.

For example, if you have a dataset with different categories and want to measure how evenly distributed they are:

```tql
from {category: "A"}, {category: "A"}, {category: "B"}, {category: "C"}
summarize entropy_value = category.entropy()
```

This will return an entropy value of approximately 1.04, indicating moderate randomness in the distribution.

The function also supports normalization via an optional `normalize` parameter. When set to `true`, the entropy is normalized between 0 and 1 by dividing by the logarithm of the number of unique values:

```tql
from {category: "A"}, {category: "A"}, {category: "B"}, {category: "C"}
summarize normalized_entropy = category.entropy(normalize=true)
```

This returns a normalized entropy value of approximately 0.95, making it easier to compare entropy across datasets with different numbers of unique values.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#4852](https://github.com/tenzir/tenzir/pull/4852).

#### Rename files after reading them

[Section titled “Rename files after reading them”](#rename-files-after-reading-them)

The `from_file` operator now supports moving files after reading them.

For example, `from_file "logs/*.log", rename=path => f"{path}.done"` reads all `.log` files in the `logs` directory, and after reading them renames the files to have the extension `.log.done`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5285](https://github.com/tenzir/tenzir/pull/5285).

#### Dedicated OCSF operator

[Section titled “Dedicated OCSF operator”](#dedicated-ocsf-operator)

The new operator `ocsf::apply` converts events to the OCSF schema, making sure that all events have the same type. It supports all OCSF versions (including `-dev` versions), all OCSF classes and all OCSF profiles. The schema to use is determined by `class_uid`, `metadata.version` and `metadata.profiles` (if it exists). The operator emits warnings if it finds unexpected fields or mismatched types. Expect more OCSF-native functionality coming to Tenzir soon!

By [@jachris](https://github.com/jachris) in [#5220](https://github.com/tenzir/tenzir/pull/5220).

#### Writing CEF and LEEF

[Section titled “Writing CEF and LEEF”](#writing-cef-and-leef)

We have added two new functions `print_leef` and `print_cef`. With these and the already existing `write_syslog`, you are now able to write nested CEF or LEEF in a syslog frame. In combination with the already existing ability to read nested CEF and LEEF, this enables you to transparently forward firewall logs.

For example, you can read in CEF messages, enrich them, and send them out again:

```tql
// Accept syslog over TCP
load_tcp "127.0.0.1:1234" {
  read_syslog
}
// Parse the nested message as structured CEF data
message = message.parse_cef()
// Enrich the message, if its a high severity message
if message.severity in ["High", "Very High", "7", "8", "9"] {
  context::enrich "my-context",
    key=message.extension.source_ip,
    into=message.extension
}
// Re-write the message as CEF
message = message.extension.print_cef(
  cef_version=message.cef_version,
  device_vendor=message.device_vendor, device_product=message.device_product,
  device_version=message.device_version, signature_id=signature_id,
  severity=message.severity,
  name=r#"enriched via "my-context": "# + message.name
)
// Write as syslog again
write_syslog
// Send the bytestream to some destination
```

By [@IyeOnline](https://github.com/IyeOnline) in [#5280](https://github.com/tenzir/tenzir/pull/5280).

### Changes

[Section titled “Changes”](#changes)

#### Updated OCSF functions

[Section titled “Updated OCSF functions”](#updated-ocsf-functions)

The functions available under `ocsf::` were updated to fully reflect the newest OCSF schema. Additionally, the functions `ocsf::type_uid` and `ocsf::type_name` were added.

By [@jachris](https://github.com/jachris) in [#5220](https://github.com/tenzir/tenzir/pull/5220).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed panic in `parent_dir`

[Section titled “Fixed panic in parent\_dir”](#fixed-panic-in-parent_dir)

The `parent_dir` function no longer panics on some inputs.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5285](https://github.com/tenzir/tenzir/pull/5285).

#### SNI support in `from_http`

[Section titled “SNI support in from\_http”](#sni-support-in-from_http)

The `from_http` operator now correctly sets the domain for TLS SNI (Server Name Indication).

By [@tobim](https://github.com/tobim) in [#5288](https://github.com/tenzir/tenzir/pull/5288).

#### CPU limits in containers

[Section titled “CPU limits in containers”](#cpu-limits-in-containers)

Nodes now correctly respect cgroup CPU limits on Linux. Previously, such limits were ignored, and the node always used the physical number of cores available, unless a lower number was explicitly configured through the `caf.scheduler.max-threads` option. This bug fix may improve performance and resource utilization for nodes running in environments with such limitations.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5288](https://github.com/tenzir/tenzir/pull/5288).

#### Fixed stack traces in Docker images

[Section titled “Fixed stack traces in Docker images”](#fixed-stack-traces-in-docker-images)

Backtraces no longer miss function identifiers when running the official Docker images.

By [@tobim](https://github.com/tobim) in [#5283](https://github.com/tenzir/tenzir/pull/5283).

# Dynamic Publish

The operator now supports event-dependent topics, making routing between pipelines more flexible. Additionally, new and operators make taking apart custom logs easier than before.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.6.0).

### Features

[Section titled “Features”](#features)

#### `read_delimited` and `read_delimited_regex`

[Section titled “read\_delimited and read\_delimited\_regex”](#read_delimited-and-read_delimited_regex)

TQL now supports two new operators for parsing data streams with custom delimiters: `read_delimited` and `read_delimited_regex`. These operators provide a more intuitive and discoverable way to split data on custom separators compared to the deprecated `split_at_regex` option in `read_lines`.

The `read_delimited` operator splits input on exact string or blob matches:

```tql
load_file "data.txt"
read_delimited "||"
```

The `read_delimited_regex` operator splits input using regular expression patterns:

```tql
load_tcp "0.0.0.0:514" {
  read_delimited_regex "(?=<[0-9]+>)"
}
```

Both operators support binary data processing and optionally including the separator in the output. The `split_at_regex` option in `read_lines` is now deprecated in favor of these dedicated operators.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5291](https://github.com/tenzir/tenzir/pull/5291).

#### Publishing to dynamic topics

[Section titled “Publishing to dynamic topics”](#publishing-to-dynamic-topics)

The `publish` operator now allows for dynamic topics to be derived from each individual event.

For example, assuming Suricata logs, `publish f"suricata.{event_type}"` now publishes to the topic `suricata.alert` for alert events and `suricata.flow` for flow events. This works with any expression that evaluates to a string, including `publish @name` to use the schema name of the event.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5294](https://github.com/tenzir/tenzir/pull/5294).

#### HTTP request metadata

[Section titled “HTTP request metadata”](#http-request-metadata)

The `from_http` operator now supports the `metadata_field` option when using the server mode and not just client mode. The request metadata has the following schema:

| Field      | Type     | Description                          |
| :--------- | :------- | :----------------------------------- |
| `headers`  | `record` | The request headers.                 |
| `query`    | `record` | The query parameters of the request. |
| `path`     | `string` | The path requested.                  |
| `fragment` | `string` | The URI fragment of the request.     |
| `method`   | `string` | The HTTP method of the request.      |
| `version`  | `string` | The HTTP version of the request.     |

By [@raxyte](https://github.com/raxyte) in [#5295](https://github.com/tenzir/tenzir/pull/5295).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### OCSF `-dev` versions

[Section titled “OCSF -dev versions”](#ocsf--dev-versions)

The `ocsf::apply` operator can now be used with the newest development version of OCSF (v1.6.0-dev). Previously, it claimed that this version does not exist.

By [@jachris](https://github.com/jachris) in [#5296](https://github.com/tenzir/tenzir/pull/5296).

#### Fixed panic in `write_parquet`

[Section titled “Fixed panic in write\_parquet”](#fixed-panic-in-write_parquet)

The `write_parquet` operator no longer panics when specifying `compression_type="snappy"` without a `compression_level`.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5293](https://github.com/tenzir/tenzir/pull/5293).

# Fixed Distinct Counts

This release restores an aggregation function that was accidentally made unavailable in Tenzir Node v5.6.0.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.6.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix `count_distinct` being missing

[Section titled “Fix count\_distinct being missing”](#fix-count_distinct-being-missing)

We fixed a bug that caused `count_distinct` to be unavailable in Tenzir Node v5.6.0.

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5298](https://github.com/tenzir/tenzir/pull/5298).

# Secret Secrets

Tenzir Node v5.7.0 introduces a new secret type that keeps its sensitive content hidden while enabling flexible secret retrieval. This release also adds support for OCSF extensions and brings several improvements to the operator.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.7.0).

### Features

[Section titled “Features”](#features)

#### `save_tcp` now reconnects on network outages

[Section titled “save\_tcp now reconnects on network outages”](#save_tcp-now-reconnects-on-network-outages)

The `save_tcp` (`from "tcp://..."`) operator now tries to reconnect in case of recoverable errors such as network outages and in case the remote end disconnects.

You can use the new options `retry_delay: duration` and `max_retry_count: int` to tune the behavior to your needs. The default values are set to 30 seconds and 10 times respectively.

By [@tobim](https://github.com/tobim) in [#5230](https://github.com/tenzir/tenzir/pull/5230).

#### Add an option to add extra headers to the platform request

[Section titled “Add an option to add extra headers to the platform request”](#add-an-option-to-add-extra-headers-to-the-platform-request)

The new option `tenzir.platform-extra-headers` causes the Tenzir Node to add the given extra HTTP headers when establishing the connection to the Tenzir Platform, for example to pass additional authentication headers when traversing proxies.

You can set this variable either via configuration file:

```yaml
tenzir:
  platform-extra-headers:
    Authentication: Bearer XXXX
    Proxy-Authentication: Bearer YYYY
```

or as environment variable: (note the double underscore before the name of the header)

```sh
TENZIR_PLATFORM_EXTRA_HEADERS__AUTHENTICATION="Bearer XXXX"
TENZIR_PLATFORM_EXTRA_HEADERS__PROXY_AUTHENTICATION="Bearer YYYY"
```

When using the environment variable version, the Tenzir Node always converts the name of the header to lowercase and converts underscores to dashes, so a header specified as `TENZIR_PLATFORM_EXTRA_HEADERS__EXTRA_HEADER=extra` will be sent as `extra-header: extra` in the HTTP request.

By [@lava](https://github.com/lava) in [#5287](https://github.com/tenzir/tenzir/pull/5287).

#### Support for OCSF extensions

[Section titled “Support for OCSF extensions”](#support-for-ocsf-extensions)

The `ocsf::apply` operator now supports OCSF extensions. This means that `metadata.extensions` is now also taken into account for casting and validation. At the moment, only the extensions versioned together with OCSF are supported. This includes the `win` and `linux` extensions.

By [@jachris](https://github.com/jachris) in [#5306](https://github.com/tenzir/tenzir/pull/5306).

#### Enhanced file renaming in `from_file` operator

[Section titled “Enhanced file renaming in from\_file operator”](#enhanced-file-renaming-in-from_file-operator)

The `from_file` operator now provides enhanced file renaming capabilities when using the `rename` parameter. These improvements make file operations more robust and user-friendly.

**Directory creation**: The operator now automatically creates intermediate directories when renaming files to paths that don’t exist yet. For example, if you rename a file to `/new/deep/directory/structure/file.txt`, all necessary parent directories (`/new`, `/new/deep`, `/new/deep/directory`, `/new/deep/directory/structure`) will be created automatically.

```tql
from_file "/data/*.json", rename=path => f"/processed/by-date/2024/01/{path.file_name()}"
```

**Trailing slash handling**: When the rename target ends with a trailing slash, the operator now automatically appends the original filename. This makes it easy to move files to different directories while preserving their names.

```tql
// This will rename "/input/data.json" to "/output/data.json"
from_file "/input/*.json", rename=path => "/output/"
```

Previously, you would have needed to manually extract and append the filename:

```tql
// Old approach - no longer necessary
from_file "/input/*.json", rename=path => f"/output/{path.file_name()}"
```

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5303](https://github.com/tenzir/tenzir/pull/5303).

#### Preserving variants when using `ocsf::apply`

[Section titled “Preserving variants when using ocsf::apply”](#preserving-variants-when-using-ocsfapply)

The `ocsf::apply` operator now has an additional `preserve_variants` option, which makes it so that free-form objects are preserved as-is, instead of being JSON-encoded. Most notably, this applies to the `unmapped` field. For example, if `unmapped` is `{x: 42}`, then `ocsf::apply` would normally JSON-encode it so that it ends up with the value `"{\"x\": 42}"`. If `ocsf::apply preserve_variants=true` is used instead, then `unmapped` simply stays a record. Note that this means that the event schema changes whenever the type of `unmapped` changes.

By [@jachris](https://github.com/jachris) in [#5312](https://github.com/tenzir/tenzir/pull/5312).

#### Secrets

[Section titled “Secrets”](#secrets)

Tenzir now features a new first class type: `secret`. As the name suggests, this type contains a secret value that cannot be accessed by a user:

```tql
from { s: secret("my-secret") }
```

```tql
{
  s: "***", // Does not render the secret value
}
```

A secret is created by the `secret` function, which changes its behavior with this release.

Operators now accept secrets where appropriate, most notably for username and password arguments, but also for URLs:

```tql
let $url = "https://" + secret("splunk-host") + ":8088"
to_splunk $url, hec_token=secret("splunk-hec-token")
```

However, a `string` is implicitly convertible to a `secret` in an operator argument, meaning that you do not have to configure a secret if you are fine with just a string literal:

```tql
to_splunk "https://localhost:8088", hec_token="my-plaintext-token"
```

Along with this feature in the Tenzir Node, we introduced secret stores to the Tenzir Platform. You can now centrally manage secrets in the platform, which will usable by all nodes within the workspace. Read more about this in the release notes for the Tenzir Platform and our Explanations page on secrets.

By [@IyeOnline](https://github.com/IyeOnline) in [#5065](https://github.com/tenzir/tenzir/pull/5065), [#5197](https://github.com/tenzir/tenzir/pull/5197).

### Changes

[Section titled “Changes”](#changes)

#### The `secret` function returns secrets

[Section titled “The secret function returns secrets”](#the-secret-function-returns-secrets)

The `secret` function now returns a `secret`, the strong type introduced in this release. Previously it returned a plaintext `string`. This change protects secrets from being leaked, as only operators can resolve secrets now.

If you want to retain the old behavior , you can enable the configuration option `tenzir.legacy-secret-model`. In this mode, the `secret` function can only resolve secrets from the Tenzir Node’s configuration file and not access any external secret store.

By [@IyeOnline](https://github.com/IyeOnline) in [#5065](https://github.com/tenzir/tenzir/pull/5065), [#5197](https://github.com/tenzir/tenzir/pull/5197).

#### Kafka operators now automatically configure SASL mechanism for AWS IAM

[Section titled “Kafka operators now automatically configure SASL mechanism for AWS IAM”](#kafka-operators-now-automatically-configure-sasl-mechanism-for-aws-iam)

The `load_kafka` and `save_kafka` operators now automatically set `sasl.mechanism` option to the expected `OAUTHBEARER` when using the `aws_iam` option. If the mechanism has already been set to a different value, an error is emitted.

By [@raxyte](https://github.com/raxyte) in [#5307](https://github.com/tenzir/tenzir/pull/5307).

#### TQL2 support in compaction plugin

[Section titled “TQL2 support in compaction plugin”](#tql2-support-in-compaction-plugin)

The pipelines defined as part of the compaction configuration can now use TQL2. For backwards-compatibility, TQL1 pipelines still work, but they are deprecated and emit a warning on start-up.

By [@jachris](https://github.com/jachris) in [#5302](https://github.com/tenzir/tenzir/pull/5302).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed shutdown hang during storage optimization

[Section titled “Fixed shutdown hang during storage optimization”](#fixed-shutdown-hang-during-storage-optimization)

Nodes periodically merge and optimize their storage over time. We fixed a hang on shutdown for nodes while this process was ongoing.

By [@IyeOnline](https://github.com/IyeOnline) in [#5301](https://github.com/tenzir/tenzir/pull/5301).

#### `from_file` with a per-file sink

[Section titled “from\_file with a per-file sink”](#from_file-with-a-per-file-sink)

The `from_file` operator no longer fails when its per-file pipeline argument is a sink. Before this fix, the following pipeline which opens a new TCP connection per file would not work:

```tql
from_file "./*.csv" {
  read_csv
  write_ndjson
  save_tcp "localhost:8080"
}
```

By [@dominiklohmann](https://github.com/dominiklohmann) in [#5303](https://github.com/tenzir/tenzir/pull/5303).

# Smarter HTTP Ingestion

This release introduces format and compression inference from URLs for HTTP data sources, streamlining data loading workflows. It also includes bug fixes for secret resolution and HTTP server mode.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.8.0).

### Features

[Section titled “Features”](#features)

#### HTTP format and compression inference

[Section titled “HTTP format and compression inference”](#http-format-and-compression-inference)

The `from_http` and `http` operators now automatically infer the file format (such as JSON, CSV, Parquet, etc.) and compression type (such as gzip, zstd, etc.) directly from the URL’s file extension, just like the generic `from` operator. This makes it easier to load data from HTTP sources without manually specifying the format or decompression step.

If the format or compression cannot be determined from the URL, the operators will fall back to using the HTTP `Content-Type` and `Content-Encoding` response headers to determine how to parse and decompress the data.

**Examples**

**Inference Succeeds**

```tql
from_http "https://example.org/data/events.csv.zst"
```

The operator infers both the `zstd` compression and the `CSV` format from the file extension, decompresses, and parses accordingly.

**Inference Fails, Fallback to Headers**

```tql
from_http "https://example.org/download"
```

If the URL does not contain a recognizable file extension, the operator will use the HTTP `Content-Type` and `Content-Encoding` headers from the response to determine the format and compression.

**Manual Specification Required**

```tql
from_http "https://example.org/archive" {
  decompress_gzip
  read_json
}
```

If neither the URL nor the HTTP headers provide enough information, you can explicitly specify the decompression and parsing steps using a pipeline argument.

By [@raxyte](https://github.com/raxyte) in [#5300](https://github.com/tenzir/tenzir/pull/5300).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix crash in `from secret`

[Section titled “Fix crash in from secret”](#fix-crash-in-from-secret)

We fixed a crash in `from secret("key")`. This is now gracefully rejected, as generic `from` cannot resolve secrets.

By [@IyeOnline](https://github.com/IyeOnline) in [#5321](https://github.com/tenzir/tenzir/pull/5321).

#### `from_http server=true` assertion failures

[Section titled “from\_http server=true assertion failures”](#from_http-servertrue-assertion-failures)

`from_http server=true` failed with internal assertions and stopped the pipeline on receiving requests when `metadata_field` was specified.

By [@raxyte](https://github.com/raxyte) in [#5325](https://github.com/tenzir/tenzir/pull/5325).

# UUID Functions

This release brings a family of UUID functions to TQL, making it easier to generate random numbers for a variety of use cases.

Download the release on [GitHub](https://github.com/tenzir/tenzir/releases/tag/v5.9.0).

### Features

[Section titled “Features”](#features)

#### Add `uuid()` function for generating UUIDs

[Section titled “Add uuid() function for generating UUIDs”](#add-uuid-function-for-generating-uuids)

Need a unique identifier? Look no further! The new `uuid()` function brings the power of Universally Unique Identifiers to Tenzir, supporting multiple UUID versions for different use cases.

**Generate tracking IDs for security events:**

```tql
from {
  event_id: uuid(),
  timestamp: now(),
  action: "login_attempt"
}
```

```tql
{
  event_id: "62c9b810-1ecc-4511-9707-977b72c2a9dc",
  timestamp: 2025-07-04T13:47:15.473012Z,
  action: "login_attempt",
}
```

**Create time-ordered database keys with v7:**

```tql
// v7 UUIDs are perfect for database primary keys - they're time-sortable!
from {
  id: uuid(version="v7"),
  created_at: now(),
  user: "alice"
}
```

```tql
{
  id: "0197d5b1-1dc1-7070-804f-d6d749f15f56",
  created_at: 2025-07-04T13:47:23.969114Z,
  user: "alice",
}
```

**Build distributed system identifiers with v1:**

```tql
// v1 includes MAC address for true uniqueness across nodes
from {
  node_id: uuid(version="v1"),
  cluster: "production"
}
```

```tql
{
  node_id: "6eac5cce-58dd-11f0-a47d-33e666d9ff94",
  cluster: "production",
}
```

**Generate secure random tokens with v4 (default):**

```tql
// Perfect for session tokens or API keys
from {
  session_token: uuid(),  // defaults to v4
  expires_at: now() + 1h
}
```

```tql
{
  session_token: "f43e6460-23e2-45a3-87af-f8b7d10c4e35",
  expires_at: 2025-07-04T14:47:44.632335Z,
}
```

**Use v6 for better database performance:**

```tql
// v6 reorders v1 fields for improved database index locality
from {
  record_id: uuid(version="v6"),
  data: "important stuff"
}
```

```tql
{
  record_id: "1f058dd7-aa81-6496-a3ef-bd8da76352a4",
  data: "important stuff",
}
```

**Even generate the special nil UUID:**

```tql
// Sometimes you need all zeros
from {
  placeholder: uuid(version="nil")
}
```

```tql
{
  placeholder: "00000000-0000-0000-0000-000000000000",
}
```

The function supports UUID versions 1, 4 (default), 6, 7, and nil—covering everything from time-based identifiers to cryptographically secure random IDs. Whether you’re tracking security events, building distributed systems, or just need a unique identifier, `uuid()` has you covered!

By [@mavam](https://github.com/mavam) in [#5097](https://github.com/tenzir/tenzir/pull/5097).

# Tenzir Platform Changelog

This page lists the changelog for Tenzir Platform.

## Versions

[Section titled “Versions”](#versions)

* [Next (Unreleased)](/changelog/platform/next)
* [Version 1.19.1](/changelog/platform/v1-19-1)
* [Version 1.19.0](/changelog/platform/v1-19-0)
* [Version 1.18.0](/changelog/platform/v1-18-0)
* [Version 1.17.4](/changelog/platform/v1-17-4)
* [Version 1.17.3](/changelog/platform/v1-17-3)
* [Version 1.17.2](/changelog/platform/v1-17-2)
* [Version 1.17.1](/changelog/platform/v1-17-1)
* [Version 1.17.0](/changelog/platform/v1-17-0)
* [Version 1.16.1](/changelog/platform/v1-16-1)
* [Version 1.16.0](/changelog/platform/v1-16-0)
* [Version 1.15.0](/changelog/platform/v1-15-0)
* [Version 1.14.1](/changelog/platform/v1-14-1)
* [Version 1.14.0](/changelog/platform/v1-14-0)
* [Version 1.13.0](/changelog/platform/v1-13-0)
* [Version 1.12.0](/changelog/platform/v1-12-0)
* [Version 1.11.1](/changelog/platform/v1-11-1)
* [Version 1.10.4](/changelog/platform/v1-10-4)
* [Version 1.10.3](/changelog/platform/v1-10-3)
* [Version 1.10.2](/changelog/platform/v1-10-2)
* [Version 1.10.1](/changelog/platform/v1-10-1)
* [Version 1.10.0](/changelog/platform/v1-10-0)
* [Version 1.9.7](/changelog/platform/v1-9-7)
* [Version 1.9.6](/changelog/platform/v1-9-6)
* [Version 1.9.5](/changelog/platform/v1-9-5)
* [Version 1.9.4](/changelog/platform/v1-9-4)
* [Version 1.9.3](/changelog/platform/v1-9-3)
* [Version 1.9.2](/changelog/platform/v1-9-2)
* [Version 1.9.1](/changelog/platform/v1-9-1)
* [Version 1.9.0](/changelog/platform/v1-9-0)
* [Version 1.8.5](/changelog/platform/v1-8-5)
* [Version 1.8.4](/changelog/platform/v1-8-4)
* [Version 1.8.3](/changelog/platform/v1-8-3)
* [Version 1.8.2](/changelog/platform/v1-8-2)
* [Version 1.8.1](/changelog/platform/v1-8-1)
* [Version 1.8.0](/changelog/platform/v1-8-0)
* [Version 1.7.2](/changelog/platform/v1-7-2)
* [Version 1.7.1](/changelog/platform/v1-7-1)
* [Version 1.7.0](/changelog/platform/v1-7-0)
* [Version 1.6.1](/changelog/platform/v1-6-1)
* [Version 1.6.0](/changelog/platform/v1-6-0)
* [Version 1.5.0](/changelog/platform/v1-5-0)
* [Version 1.4.1](/changelog/platform/v1-4-1)
* [Version 1.4.0](/changelog/platform/v1-4-0)
* [Version 1.3.0](/changelog/platform/v1-3-0)
* [Version 1.2.1](/changelog/platform/v1-2-1)
* [Version 1.2.0](/changelog/platform/v1-2-0)
* [Version 1.1.2](/changelog/platform/v1-1-2)
* [Version 1.1.1](/changelog/platform/v1-1-1)
* [Version 1.1.0](/changelog/platform/v1-1-0)
* [Version 1.0.8](/changelog/platform/v1-0-8)
* [Version 1.0.7](/changelog/platform/v1-0-7)
* [Version 1.0.6](/changelog/platform/v1-0-6)
* [Version 1.0.5](/changelog/platform/v1-0-5)
* [Version 1.0.4](/changelog/platform/v1-0-4)
* [Version 1.0.3](/changelog/platform/v1-0-3)
* [Version 1.0.2](/changelog/platform/v1-0-2)
* [Version 1.0.1](/changelog/platform/v1-0-1)
* [Version 1.0.0](/changelog/platform/v1-0-0)
* [Version 0.20.2](/changelog/platform/v0-20-2)
* [Version 0.20.1](/changelog/platform/v0-20-1)
* [Version 0.20.0](/changelog/platform/v0-20-0)
* [Version 0.19.1](/changelog/platform/v0-19-1)
* [Version 0.19.0](/changelog/platform/v0-19-0)
* [Version 0.18.2](/changelog/platform/v0-18-2)
* [Version 0.18.1](/changelog/platform/v0-18-1)
* [Version 0.18.0](/changelog/platform/v0-18-0)
* [Version 0.17.2](/changelog/platform/v0-17-2)
* [Version 0.17.1](/changelog/platform/v0-17-1)
* [Version 0.17.0](/changelog/platform/v0-17-0)
* [Version 0.16.0](/changelog/platform/v0-16-0)

# Next

Unreleased changes.

### Features

[Section titled “Features”](#features)

#### Added keyboard shortcut indicators

[Section titled “Added keyboard shortcut indicators”](#added-keyboard-shortcut-indicators)

We added visual indicators for keyboard shortcuts to all buttons in modals and other places that support them, making it easier to discover and use shortcuts.

By [@gitryder](https://github.com/gitryder).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Big numbers in objects and lists

[Section titled “Big numbers in objects and lists”](#big-numbers-in-objects-and-lists)

Big numbers (those exceeding `9007199254740991`) in records and lists no longer cause the data table to fail rendering.

By [@jachris](https://github.com/jachris).

# Tenzir Platform v0.16

This release introduces the initial public version of the on-premise Tenzir Platform for Sovereign Edition customers.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.16.0).

# Tenzir Platform v0.17

This release introduces the ability to change pipelines on [app.tenzir.com](https://app.tenzir.com/) more quickly. Users can click on any pipeline on the overview page to open a detailed view, directly edit the definition or options, and use the new action menu to quickly start, pause, stop, duplicate, or delete pipelines.

This release brings the following improvements for Sovereign Edition users: The `tenzir-platform` CLI supports changing user and workspace icons, printing node and workspace lists in JSON format, and logging in with the client credentials flow for non-interactive authentication.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.17.0).

# Tenzir Platform v0.17.1

This patch release fixes a bug where very long-running instances of the tenant-manager issue expired user keys, making it impossible for users to log in successfully.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.17.1).

# Tenzir Platform v0.17.2

This release fixes a bug in the `tenzir-platform auth` subcommand.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.17.2).

# Tenzir Platform v0.18

This release introduces diagnostics on the overview page on [app.tenzir.com](https://app.tenzir.com/), making it easier to spot mistakes in pipelines. The overview page becomes more responsive when viewing a node with many running pipelines.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.18.0).

# Tenzir Platform v0.18.1

* This release fixes a memory leak in the overview page
* This release updates the docker compose examples by automatically pinning them to the corresponding platform version

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.18.1).

# Tenzir Platform v0.18.2

This bugfix release:

* Improves the fix for the memory leak on the overview page
* Fixes an argument parsing bug in the `tenzir-platform admin delete-auth-rule` CLI command

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.18.2).

# Tenzir Platform v0.19

* This release moves pipeline filters into the pipeline table’s header, preparing for further upcoming changes to the table
* This release adds a detailed activity view to the detailed pipeline view that opens when clicking on a pipeline
* Clicking on the diagnostics column in the pipelines table now opens the detailed pipeline view with the diagnostics tab active
* This release fixes scrolling in the detailed pipeline view

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.19.0).

# Tenzir Platform v0.19.1

* This release fixes the CI not triggering for the Tenzir Platform v0.19 release, which caused the release artifacts not to be created.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.19.1).

# Tenzir Platform v0.20

This release brings the following improvements and changes:

* The new `tenzir-platform admin spawn-node` CLI command allows running nodes with arbitrary Docker images
* The CLI no longer requests the ‘email’ scope by default when using the non-interactive login, fixing a compatibility issue with the keycloak identity provider
* This release reduces the CPU usage of activity spark bars in the pipelines list, making them update more smoothly
* Spark bars no longer stop updating for stopped pipelines
* This release fixes a bug that causes increased network usage from fetching diagnostics and metrics in the app
* This release fixes scrolling in the detailed pipeline view for pipelines with long definitions

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.20.0).

# Tenzir Platform v0.20.1

This release brings the following improvements and changes:

* This release fixes a bug that causes spark bars for recently completed or stopped pipelines to show incorrect data
* This release fixes a bug that causes the event inspector in the explorer not to be scrollable

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.20.1).

# Tenzir Platform v0.20.2

This bugfix release contains various small fixes and reliability improvements for the Tenzir Platform:

* This release fixes request ID forwarding and malformed JSON responses
* This release fixes the `admin update-workspace` subcommand for CLI
* This release improves the Websocket Gateway

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v0.20.2).

# Tenzir Platform v1.0

Tenzir Platform becomes generally available.

Since going live with app.tenzir.com last year, this release makes quite a few changes. This release announces an all-new open source library for packages, early availability of TQL2 (the next-generation language for pipelines), and a complete redesign of app.tenzir.com, which now has a sleek, modern look.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.0).

# Tenzir Platform v1.0.1

This patch release contains the following bug fixes and improvements over [Tenzir Platform v1.0.0](https://github.com/tenzir/platform/releases/tag/v1.0.0):

* This release displays errors when fetching packages in a less obtrusive way.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.1).

# Tenzir Platform v1.0.2

This patch release contains the following bug fixes and improvements over [Tenzir Platform v1.0.1](https://github.com/tenzir/platform/releases/tag/v1.0.1):

* This release fixes a potential hang when opening the history when it contains some pipelines whose definition must be URL-encoded.
* Sorting schemas now has a deterministic order when both a schema and a category with the same name exist, e.g., for a schema `foo` and `foo.bar`.
* This release fixes a bug that potentially leads to corruption of dashboard configurations after loading the dashboard fails.
* This release fixes a hang when adding a chart to a dashboard without assigning a name explicitly.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.2).

# Tenzir Platform v1.0.3

This patch release contains the following bug fixes and improvements over [Tenzir Platform v1.0.2](https://github.com/tenzir/platform/releases/tag/v1.0.2):

* This release fixes the download button to work correctly for pipelines written in TQL2.
* This release fixes charts on the dashboard to work correctly for pipelines written in TQL2.
* This release improves the run button in the explorer to check more reliably whether a pipeline is deployable.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.3).

# Tenzir Platform v1.0.4

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.0.3](https://github.com/tenzir/platform/releases/tag/v1.0.3):

* This release fixes and improves alignment and styling throughout the application.
* The run button in the explorer now indicates whether it is activated.
* Open dropdown menus now prevent accidental interaction with other elements on the page when clicking on them.
* Pipeline names can no longer be edited inline in the pipelines table, and instead must be edited through the sidebar.
* The *installed* tab becomes the default tab in the library.
* The nodes dropdown on the pipelines page with a collapsed nodes sidebar now includes all functionality from the sidebar.
* Activity metrics no longer show `undefined B/s` for the unit when the ingress or egress averages between 0 and 1 byte per second.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.4).

# Tenzir Platform v1.0.5

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.0.4](https://github.com/tenzir/platform/releases/tag/v1.0.4):

* This release open-sources the **Tenzir Platform CLI**, which can be used to show and manage nodes and workspaces. Try it yourself with `uvx tenzir-platform --help`.
* This release implements support for package updates in the Library tab.
* This release completely redesigns and reimplements the toast components.
* Additionally, this release contains many fixes and reliability improvements, in particular for diagnostics and in the pipeline inspector.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.5).

# Tenzir Platform v1.0.6

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.0.5](https://github.com/tenzir/platform/releases/tag/v1.0.5):

* This release fixes the JSON formatting in the Tenzir Platform CLI output.
* This release significantly reduces latency and network traffic when interacting with nodes in the app by making requests go through the platform control endpoint directly.
* The library becomes searchable and allows additional pipeline configuration when installing packages.
* Package descriptions now support Markdown, e.g., to insert links or code blocks.
* This release fixes some minor UI bugs and adds a few quality-of-life improvements to the new redesigned app.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.6).

# Tenzir Platform v1.0.7

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.0.6](https://github.com/tenzir/platform/releases/tag/v1.0.6):

* This release fixes a regression that causes the pipelines page not to display any pipelines for nodes with older versions than Tenzir Node v4.20.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.7).

# Tenzir Platform v1.0.8

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.0.7](https://github.com/tenzir/platform/releases/tag/v1.0.7):

* This release fixes the positioning of the “Load More” button in the Explorer results table.
* This release improves the pipeline table UI with clearer filtering indicators and a “New” badge for newly created pipelines.
* This release resolves an issue where toasts occasionally render under drawers.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.0.8).

# Tenzir Platform v1.1

This release brings key enhancements, including improved diagnostics, authentication updates, and various bug fixes for a smoother user experience.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.1.0).

# Tenzir Platform v1.1.1

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.1](https://github.com/tenzir/platform/releases/tag/v1.1.0):

* The new **Add pipeline** button on the pipeline page brings users back to the Explorer.
* Scrolling in the schemas dropdown in the Explorer now works as expected again.
* This release applies a few cosmetic touchups throughout the user interface.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.1.1).

# Tenzir Platform v1.1.2

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.1.1](https://github.com/tenzir/platform/releases/tag/v1.1.1):

* This release fixes a bug in the handling of OIDC discovery URLs, which leads to custom OIDC providers being accessed with an incorrect authorization URL.
* This release updates the `localdev` example setup to work correctly in the case where the built-in `dex` auth provider is accessed through plain HTTP.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.1.2).

# Layout Restructure & Workspaces

This release restructures the page layout for better usability and adds the ability to statically define workspaces in on-prem environments.

This release also contains a large number of additional bug fixes and improvements:

**Infrastructure**

* This release improves the `localdev` example setup for the Tenzir Platform. It now contains a Tenzir Node that is automatically connected to the local platform instance and a CLI container with admin permissions. Additionally, users of the `localdev` setup are now granted admin permissions by default.
* This release fixes a bug in the alerts API that causes configured alerts to be only activated after the next restart of the websocket gateway.
* The new `TENZIR_PLATFORM_METRICS_STORE_S3_BUCKET_NAME` variable can be set to a valid S3 bucket name. If set, the platform stores all pipeline metrics it receives from connected Tenzir Nodes into this bucket.

**UI**

* The URL format was changed and now contains the workspace id, making it easier to share Tenzir Platform URLs with other users. NOTE: There is no automatic redirect, so old pipeline share url will not work anymore.
* This release fixes the y-axis ticks for stacked area and bar charts.
* This release fixes rendering of blobs in the `Data.Blob` component.
* This release adds BITZ and TQL as downloadable formats for events.
* This release fixes an issue where pie chart colors are sometimes incorrect in the dashboard.

**CLI**

* This release replaces the `--dry-run` option for the `tenzir-platform admin add-auth-rule` commands with the new `tenzir-platform tools print-auth-rule` commands.
* The Tenzir Platform CLI now automatically authenticates using the client credentials flow when a client secret is provided as environment variable.
* The Tenzir Platform CLI now supports device code flow authentication for IdP’s without a complete verification URL (like Microsoft Entra)

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.10.0).

# Static Workspaces & Health

This patch release fixes a number of issues found since the release of [Tenzir Platform v1.10](https://github.com/tenzir/platform/releases/tag/v1.10.0):

* The platform now generates valid Tenzir tokens for non-ephemeral nodes in statically defined workspaces.
* This release adds a new `/health` endpoint to the user API that can be used to check for network connectivity.
* This release fixes TQL syntax highlighting.
* This release fixes the display of very old demo nodes in the “Connect Node” view.
* This release adjusts the background color of the library page.
* This release stabilizes the order of items in the contexts table.
* This release applies several fixes to the nodes page.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.10.1).

# Chart & Node Fixes

This patch release contains a number of additional bugfixes since [Tenzir Platform v1.10.1](https://github.com/tenzir/platform/releases/tag/v1.10.1):

* This release fixes a bug that causes nodes to be shown as offline in the websocket gateway while still being connected.
* Tooltips in line and grouped area charts are now sorted by value.
* This release fixes a bug where charts with logarithmic scales do not show any ticks when the domain includes 0.
* This release fixes a bug where maximum and minimum values are not being passed to explorer charts.
* This release fixes repeated values appearing in logarithmic scales on charts.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.10.2).

# Platform Changelog

As of this release, there is a detailed changelog for the Tenzir Platform on the revamped docs.tenzir.com.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.10.3).

### Features

[Section titled “Features”](#features)

#### New documentation site

[Section titled “New documentation site”](#new-documentation-site)

The embedded documentation on app.tenzir.com now points to the revamped documentation page. This page features:

* A clean, new look with a clearer page structure.
* More guides and an easier-to-navigate TQL reference.
* A detailed changelog for Tenzir Platform (in addition to the existing changelog for Tenzir Node).

By [@dominiklohmann](https://github.com/dominiklohmann).

# Ephemeral Node Icon

This release adds a custom icon for ephemeral nodes, making them easier to distinguish from regular ones.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.10.4).

### Features

[Section titled “Features”](#features)

#### Custom ephemeral node icon

[Section titled “Custom ephemeral node icon”](#custom-ephemeral-node-icon)

Ephemeral nodes now show a custom icon, indicating that they disappear when disconnecting.

By [@gitryder](https://github.com/gitryder).

# Pipeline Widgets

The all-new pipeline widgets make it easy to see at a glance which the total ingress and egress of all pipelines, and to easily figure out which pipelines had warnings and errors.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.11.0).

### Features

[Section titled “Features”](#features)

#### Support HTTP Proxies in the Sovereign Edition

[Section titled “Support HTTP Proxies in the Sovereign Edition”](#support-http-proxies-in-the-sovereign-edition)

The `ghcr.io/tenzir/app` container image for the Sovereign Edition now respects the `HTTP_PROXY` and `HTTPS_PROXY` environment variables.

By [@tobim](https://github.com/tobim).

#### Page load progress bar

[Section titled “Page load progress bar”](#page-load-progress-bar)

Switching between pages in the app now shows a loading bar at the top of the page.

By [@dit7ya](https://github.com/dit7ya).

#### Pipeline page widgets

[Section titled “Pipeline page widgets”](#pipeline-page-widgets)

**Introduction**

The pipeline page now features four widgets placed above the table of pipelines. These widgets provide information about pipelines shown below, as well as allow you to further filter down the list of pipelines.

**The Status Widget**

The first widget, placed in the top left of the screen, shows the total number of pipelines found after search and filtering. It also shows a breakdown of how many pipelines exist for each of 4 statuses:

* `Running`,
* `Completed`,
* `Failed` or
* `Stopped`.

Clicking on one of these status buttons will filter to pipelines with that particular status. Holding the `Shift` key while clicking will add the selected status to your current filter instead of replacing it.

The status widget replaces the pipeline state filtering feature found in the pipelines table header, which has now been removed.

**The Daily Ingress/Egress Widget**

Just below the Status Widget is a widget that shows the total ingress and egress traffic for all pipelines after search and filtering, and the percentage difference between the two.

**The Ingress/Egress Chart Widget**

To the right of the other two widgets, the Ingress/Egress Chart shows a graph of all combined ingress and egress for the pipelines after searching and filtering. It supports a few different periods which can be selected from a drop-down.

**The Diagnostics Widget**

On the rightmost side of the widgets row you’ll find a heatmap visualizing how many pipelines experienced warnings or errors, with four-hour cells. Searching or otherwise filtering the list of pipelines will update this widget to reflect the new set of pipelines.

Clicking on a cell will filter the list of pipelines to contain only those pipelines that encountered warnings or errors during the corresponding time period. Clicking again will remove the filter.

The time range selected for the diagnostics is also used for the diagnostics column in the pipeline list below.

By [@gitryder](https://github.com/gitryder), [@avaq](https://github.com/avaq), [@dit7ya](https://github.com/dit7ya).

### Changes

[Section titled “Changes”](#changes)

#### Removal of pipeline pausing

[Section titled “Removal of pipeline pausing”](#removal-of-pipeline-pausing)

Pipeline pausing is a rarely used feature, and its support will soon be removed from nodes, so we’ve updated the UI, and no longer show the pause buttons in the pipeline detail page, and the multi-select pipeline actions.

If a pipeline is already paused, it can still be found in the pipeline list, and is categorized by the status widget as a “stopped” pipeline.

By [@dit7ya](https://github.com/dit7ya), [@gitryder](https://github.com/gitryder).

#### Removal of pipeline labels

[Section titled “Removal of pipeline labels”](#removal-of-pipeline-labels)

Labels are a feature that has long been scheduled for removal, and the rework of the pipelines page that comes with this release was a good opportunity to do so.

By [@avaq](https://github.com/avaq), [@dit7ya](https://github.com/dit7ya).

#### Node ID and tab in the URL

[Section titled “Node ID and tab in the URL”](#node-id-and-tab-in-the-url)

The selected node, and the tab you’re viewing, are now part of the page URL, which has the following consequences:

1. You should now stay where you were when reloading the page
2. Copying links, bookmarking, etc. is now possible.
3. Some links into the app might have broken, but most will redirect to the right place.
4. Previously created pipeline share URLs will not work anymore.

By [@avaq](https://github.com/avaq).

#### Lower wait times when data loading fails

[Section titled “Lower wait times when data loading fails”](#lower-wait-times-when-data-loading-fails)

When there’s an error that prevents the app from being able to load data, the app would previously keep trying for 30 seconds while showing a loading spinner. This has been reduced to 5 seconds so that in case of a problem, the UI will display the error sooner.

By [@avaq](https://github.com/avaq).

#### Improved page titles

[Section titled “Improved page titles”](#improved-page-titles)

The page you’re on in the app is now shown in the tab title, making it easier to find the right tab when working with multiple tabs, and making it easier to find things in the browser history.

By [@avaq](https://github.com/avaq), [@gitryder](https://github.com/gitryder).

#### Optimized pipeline listing

[Section titled “Optimized pipeline listing”](#optimized-pipeline-listing)

On the pipelines page, the list of pipelines is now more realtime while using less network traffic.

By [@avaq](https://github.com/avaq).

#### Move pipeline actions into table header

[Section titled “Move pipeline actions into table header”](#move-pipeline-actions-into-table-header)

We have moved the pipeline actions (start/ stop/ delete) into a floating action bar to the bottom. The table layout has also slightly changed - to select pipelines (for actions), click on the toggle button in the header.

By [@gitryder](https://github.com/gitryder).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed version identifier in Sovereign Edition

[Section titled “Fixed version identifier in Sovereign Edition”](#fixed-version-identifier-in-sovereign-edition)

The Sovereign Edition of the Tenzir Platform now correctly prints its current Git commit hash in the browser console upon loading.

By [@lava](https://github.com/lava) in [#87](https://github.com/tenzir/platform/pull/87).

#### Fix glitch when saving a pipeline

[Section titled “Fix glitch when saving a pipeline”](#fix-glitch-when-saving-a-pipeline)

Fixed an issue where when updating a pipeline via the pipeline detail pane, the “Restart on error” checkbox would briefly reset to its previous state before switching to the correct one.

By [@avaq](https://github.com/avaq).

#### Fix navigation sometimes being non-responsive

[Section titled “Fix navigation sometimes being non-responsive”](#fix-navigation-sometimes-being-non-responsive)

We resolved an issue where ocasionally and seemingly at random, when clicking any button in the app that would lead to another page, nothing would happen, and you’d have to click again.

By [@avaq](https://github.com/avaq).

# Pipeline Widgets

The all-new pipeline widgets make it easy to see at a glance which the total ingress and egress of all pipelines, and to easily figure out which pipelines had warnings and errors.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.11.1).

### Features

[Section titled “Features”](#features)

#### Support HTTP Proxies in the Sovereign Edition

[Section titled “Support HTTP Proxies in the Sovereign Edition”](#support-http-proxies-in-the-sovereign-edition)

The `ghcr.io/tenzir/app` container image for the Sovereign Edition now respects the `HTTP_PROXY` and `HTTPS_PROXY` environment variables.

By [@tobim](https://github.com/tobim).

#### Page load progress bar

[Section titled “Page load progress bar”](#page-load-progress-bar)

Switching between pages in the app now shows a loading bar at the top of the page.

By [@dit7ya](https://github.com/dit7ya).

#### Pipeline page widgets

[Section titled “Pipeline page widgets”](#pipeline-page-widgets)

**Introduction**

The pipeline page now features four widgets placed above the table of pipelines. These widgets provide information about pipelines shown below, as well as allow you to further filter down the list of pipelines.

**The Status Widget**

The first widget, placed in the top left of the screen, shows the total number of pipelines found after search and filtering. It also shows a breakdown of how many pipelines exist for each of 4 statuses:

* `Running`,
* `Completed`,
* `Failed` or
* `Stopped`.

Clicking on one of these status buttons will filter to pipelines with that particular status. Holding the `Shift` key while clicking will add the selected status to your current filter instead of replacing it.

The status widget replaces the pipeline state filtering feature found in the pipelines table header, which has now been removed.

**The Daily Ingress/Egress Widget**

Just below the Status Widget is a widget that shows the total ingress and egress traffic for all pipelines after search and filtering, and the percentage difference between the two.

**The Ingress/Egress Chart Widget**

To the right of the other two widgets, the Ingress/Egress Chart shows a graph of all combined ingress and egress for the pipelines after searching and filtering. It supports a few different periods which can be selected from a drop-down.

**The Diagnostics Widget**

On the rightmost side of the widgets row you’ll find a heatmap visualizing how many pipelines experienced warnings or errors, with four-hour cells. Searching or otherwise filtering the list of pipelines will update this widget to reflect the new set of pipelines.

Clicking on a cell will filter the list of pipelines to contain only those pipelines that encountered warnings or errors during the corresponding time period. Clicking again will remove the filter.

The time range selected for the diagnostics is also used for the diagnostics column in the pipeline list below.

By [@gitryder](https://github.com/gitryder), [@avaq](https://github.com/avaq), [@dit7ya](https://github.com/dit7ya).

### Changes

[Section titled “Changes”](#changes)

#### Removal of pipeline pausing

[Section titled “Removal of pipeline pausing”](#removal-of-pipeline-pausing)

Pipeline pausing is a rarely used feature, and its support will soon be removed from nodes, so we’ve updated the UI, and no longer show the pause buttons in the pipeline detail page, and the multi-select pipeline actions.

If a pipeline is already paused, it can still be found in the pipeline list, and is categorized by the status widget as a “stopped” pipeline.

By [@dit7ya](https://github.com/dit7ya), [@gitryder](https://github.com/gitryder).

#### Removal of pipeline labels

[Section titled “Removal of pipeline labels”](#removal-of-pipeline-labels)

Labels are a feature that has long been scheduled for removal, and the rework of the pipelines page that comes with this release was a good opportunity to do so.

By [@avaq](https://github.com/avaq), [@dit7ya](https://github.com/dit7ya).

#### Node ID and tab in the URL

[Section titled “Node ID and tab in the URL”](#node-id-and-tab-in-the-url)

The selected node, and the tab you’re viewing, are now part of the page URL, which has the following consequences:

1. You should now stay where you were when reloading the page
2. Copying links, bookmarking, etc. is now possible.
3. Some links into the app might have broken, but most will redirect to the right place.
4. Previously created pipeline share URLs will not work anymore.

By [@avaq](https://github.com/avaq).

#### Lower wait times when data loading fails

[Section titled “Lower wait times when data loading fails”](#lower-wait-times-when-data-loading-fails)

When there’s an error that prevents the app from being able to load data, the app would previously keep trying for 30 seconds while showing a loading spinner. This has been reduced to 5 seconds so that in case of a problem, the UI will display the error sooner.

By [@avaq](https://github.com/avaq).

#### Improved page titles

[Section titled “Improved page titles”](#improved-page-titles)

The page you’re on in the app is now shown in the tab title, making it easier to find the right tab when working with multiple tabs, and making it easier to find things in the browser history.

By [@avaq](https://github.com/avaq), [@gitryder](https://github.com/gitryder).

#### Optimized pipeline listing

[Section titled “Optimized pipeline listing”](#optimized-pipeline-listing)

On the pipelines page, the list of pipelines is now more realtime while using less network traffic.

By [@avaq](https://github.com/avaq).

#### Move pipeline actions into table header

[Section titled “Move pipeline actions into table header”](#move-pipeline-actions-into-table-header)

We have moved the pipeline actions (start/ stop/ delete) into a floating action bar to the bottom. The table layout has also slightly changed - to select pipelines (for actions), click on the toggle button in the header.

By [@gitryder](https://github.com/gitryder).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed version identifier in Sovereign Edition

[Section titled “Fixed version identifier in Sovereign Edition”](#fixed-version-identifier-in-sovereign-edition)

The Sovereign Edition of the Tenzir Platform now correctly prints its current Git commit hash in the browser console upon loading.

By [@lava](https://github.com/lava) in [#87](https://github.com/tenzir/platform/pull/87).

#### Fix glitch when saving a pipeline

[Section titled “Fix glitch when saving a pipeline”](#fix-glitch-when-saving-a-pipeline)

Fixed an issue where when updating a pipeline via the pipeline detail pane, the “Restart on error” checkbox would briefly reset to its previous state before switching to the correct one.

By [@avaq](https://github.com/avaq).

#### Fix navigation sometimes being non-responsive

[Section titled “Fix navigation sometimes being non-responsive”](#fix-navigation-sometimes-being-non-responsive)

We resolved an issue where ocasionally and seemingly at random, when clicking any button in the app that would lead to another page, nothing would happen, and you’d have to click again.

By [@avaq](https://github.com/avaq).

# Action Bar

Tenzir Platform v1.12 introduces an action bar at the bottom of the Explorer, providing easier access to view settings. Additionally, the widget row on the nodes page has been enhanced with numerous improvements and bug fixes.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.12.0).

### Features

[Section titled “Features”](#features)

#### Support GCloud OAuth Clients

[Section titled “Support GCloud OAuth Clients”](#support-gcloud-oauth-clients)

The platform now supports tokens created by Google Cloud IAP.

By [@lava](https://github.com/lava).

### Changes

[Section titled “Changes”](#changes)

#### Improved pipeline list activity metrics loading state

[Section titled “Improved pipeline list activity metrics loading state”](#improved-pipeline-list-activity-metrics-loading-state)

* The loading state for the sparkbars in the pipeline table no longer shows forever if none of the pipelines are running.
* The loading spinner for the sparkbars in the pipeline table has been replaced by a pulsating skeleton of the sparkbars, which is a little easier on the eyes.

By [@avaq](https://github.com/avaq).

#### Result controls moved to a floating bar

[Section titled “Result controls moved to a floating bar”](#result-controls-moved-to-a-floating-bar)

The result controls in the Explorer are now shown in a floating bar anchored at the bottom center of the page, instead of above the results table, for easier access and a cleaner layout.

By [@gitryder](https://github.com/gitryder).

#### Schemas pane on the right

[Section titled “Schemas pane on the right”](#schemas-pane-on-the-right)

The schemas pane in the Explorer is now docked at the right edge consistently, removing confusion between the icons for expanding schemas pane and the nodes pane, and putting more attention on the results table.

By [@dominiklohmann](https://github.com/dominiklohmann).

#### Add realtime updates to pipeline list diagnostics

[Section titled “Add realtime updates to pipeline list diagnostics”](#add-realtime-updates-to-pipeline-list-diagnostics)

The diagnostics widget and numbers of diagnostics in the pipeline list now update in real time.

By [@avaq](https://github.com/avaq).

#### Improvements to the Ingress/Egress data visualization

[Section titled “Improvements to the Ingress/Egress data visualization”](#improvements-to-the-ingressegress-data-visualization)

* The daily throughput widget now shows the number of bytes for the window that exactly matches the 24h range from the last seen data point, including smooth interpolation.
* The ingress/egress data shown now gets more realtime updates.
* The ingress/egress graph’s X domain was made more stable, meaning there’s no sudden jumps in the graph’s X axis when new data is shown.

By [@avaq](https://github.com/avaq).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix editor strip overlapping last lines

[Section titled “Fix editor strip overlapping last lines”](#fix-editor-strip-overlapping-last-lines)

The editor’s control strip, which includes the Run button, no longer overlaps the last few lines of your code. You can now see and work with your entire script without anything getting in the way.

By [@gitryder](https://github.com/gitryder).

#### Preserved dashboard cell order when adding new cells

[Section titled “Preserved dashboard cell order when adding new cells”](#preserved-dashboard-cell-order-when-adding-new-cells)

Fixed an issue where existing dashboard cells were sometimes being reordered when a new cell was added.

By [@dit7ya](https://github.com/dit7ya).

# Schema Search & GCP Support

This release contains improved integration for running the Tenzir Platform inside GCP, a new Schema Search functionality, and an option for showing the total diagnostic count in heatmap cells.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.13.0).

### Features

[Section titled “Features”](#features)

#### Option to allow passing extra headers to the API endpoint

[Section titled “Option to allow passing extra headers to the API endpoint”](#option-to-allow-passing-extra-headers-to-the-api-endpoint)

The Tenzir Platform CLI now respects the `TENZIR_PLATFORM_CLI_EXTRA_HEADERS` environment variable to add extra headers to any request made against the platform API. The value of this variable must be set to a map of strings, eg.:

```plaintext
TENZIR_PLATFORM_CLI_EXTRA_HEADERS='{"Proxy-Authentication": "Bearer XXXXXXXXXXXXXXXXX"}'
```

By [@lava](https://github.com/lava) in [#107](https://github.com/tenzir/platform/pull/107).

#### Explorer schema search and filter

[Section titled “Explorer schema search and filter”](#explorer-schema-search-and-filter)

The schemas pane/ dropdown in the explorer page now has a search functionality.

By [@dit7ya](https://github.com/dit7ya).

#### Add support for Google OAuth clients to Platform CLI

[Section titled “Add support for Google OAuth clients to Platform CLI”](#add-support-for-google-oauth-clients-to-platform-cli)

The Tenzir Platform CLI now supports logging into internal Google OAuth clients created in GCP using the device code flow.

By [@lava](https://github.com/lava) in [#107](https://github.com/tenzir/platform/pull/107).

#### Add option for showing total diagnostics count in heatmap cells

[Section titled “Add option for showing total diagnostics count in heatmap cells”](#add-option-for-showing-total-diagnostics-count-in-heatmap-cells)

Users can now switch between counting unique pipelines with diagnostics (default) or total diagnostic message counts in heatmap cells. The new “Count by” option is available in the time range dropdown of the diagnostics heatmap in the pipelines tab of the nodes page.

By [@dit7ya](https://github.com/dit7ya).

#### Better error handling in the Tenzir Platform CLI

[Section titled “Better error handling in the Tenzir Platform CLI”](#better-error-handling-in-the-tenzir-platform-cli)

When encountering authentication errors, the Tenzir Platform CLI now exits with a nice error message instead of printing a raw stacktrace:

```sh
$ TENZIR_PLATFORM_CLI_ID_TOKEN=xxxx tenzir-platform workspace list
Error: Invalid JWT
  while validating TENZIR_PLATFORM_CLI_ID_TOKEN
(hint) upstream error: Not enough segments
```

By [@lava](https://github.com/lava) in [#107](https://github.com/tenzir/platform/pull/107).

### Changes

[Section titled “Changes”](#changes)

#### Update Tenzir Platform examples

[Section titled “Update Tenzir Platform examples”](#update-tenzir-platform-examples)

We updated all examples in the Tenzir Platform repository to use the latest best practices and to better integrate with the new docs page at <https://docs.tenzir.com>

We also removed the outdated `tenzir-developers` example, and added a new `native-tls` example instead showing a complete setup with a private certificate authority.

Note that in order to get more consistent terminology in our examples, we updated the following variable names. If you are planning to use an old `.env` file with the new platform version, you will need to update these names as well.

The internal environment variables used by the individual docker services have not been changed, so if you use your own `docker-compose.yaml` file updating the platform version is safe without renaming these variables in your `.env` file.

```plaintext
TENZIR_PLATFORM_DOMAIN -> TENZIR_PLATFORM_UI_ENDPOINT
TENZIR_PLATFORM_CONTROL_ENDPOINT -> TENZIR_PLATFORM_NODES_ENDPOINT
TENZIR_PLATFORM_BLOBS_ENDPOINT -> TENZIR_PLATFORM_DOWNLOADS_ENDPOINT


TENZIR_PLATFORM_OIDC_ADMIN_RULES -> TENZIR_PLATFORM_ADMIN_RULES
```

By [@lava](https://github.com/lava) in [#106](https://github.com/tenzir/platform/pull/106).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix total activity change getting capped at 99.9

[Section titled “Fix total activity change getting capped at 99.9”](#fix-total-activity-change-getting-capped-at-999)

The percentage change in the ingress/egress widget was wrongly capped at 99.9 even when the actual value was much higher.

By [@dit7ya](https://github.com/dit7ya).

#### Fix pages reloading or resetting out of nowhere

[Section titled “Fix pages reloading or resetting out of nowhere”](#fix-pages-reloading-or-resetting-out-of-nowhere)

We addressed a bug that caused many components in the app to reset or reload whenever the user session was automatically extended in the background (primarily affecting sovereign edition users with short session lifespans).

By [@avaq](https://github.com/avaq).

# Platform Secrets

This release adds CLI support for adding, removing and updating secrets. It also adds a new three-dot menu on the pipelines page, as well as partial pipeline execution from the history.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.14.0).

### Features

[Section titled “Features”](#features)

#### Three-dot menu for pipeline items

[Section titled “Three-dot menu for pipeline items”](#three-dot-menu-for-pipeline-items)

You can now quickly start, stop, copy the ID, or delete a pipeline right from the pipelines table. Just click the three-dot menu at the far right of any pipeline row. It’s a simple way to manage one pipeline at a time, without needing to open the bulk actions menu.

By [@gitryder](https://github.com/gitryder).

#### Add secrets support for the Platform

[Section titled “Add secrets support for the Platform”](#add-secrets-support-for-the-platform)

With this release, the Tenzir Platform now supports storing secrets on a per-workspace level.

In the Tenzir UI, you can click on the new gears icon in the workspace switcher to get to the Workspace settings, where you can add, modify or delete secrets for you workspace.

In the Tenzir CLI, you can use the new `tenzir-platform secret` subcommand for the same purpose:

```plain
tenzir-platform secret add <name> [--file=<file>] [--value=<value>] [--env]
tenzir-platform secret update <secret> [--file=<file>] [--value=<value>] [--env]
tenzir-platform secret delete <secret>
tenzir-platform secret list [--json]
```

By [@lava](https://github.com/lava) in [#73](https://github.com/tenzir/platform/pull/73).

#### Support using different blob storage URLs for UI and Nodes

[Section titled “Support using different blob storage URLs for UI and Nodes”](#support-using-different-blob-storage-urls-for-ui-and-nodes)

The Tenzir Platform now supports the `BLOB_STORAGE__NODES_PUBLIC_ENDPOINT_URL` environment variable that allows overriding the URL used by the nodes to reach the configured S3-compatible blob storage. This is useful if the Tenzir Nodes run in a separate network from the Tenzir UI, and the blob storage is exposed under different domains in both networks.

By [@lava](https://github.com/lava).

#### Partial pipeline execution

[Section titled “Partial pipeline execution”](#partial-pipeline-execution)

You can now run a selected portion of the pipeline by highlighting it with your mouse or keyboard. When a selection is active, the **Run** button changes to **Run selected**, allowing you to execute just the highlighted section for quicker iteration and testing.

By [@gitryder](https://github.com/gitryder).

#### Secret data type display

[Section titled “Secret data type display”](#secret-data-type-display)

Secret data types now display as `<secret>` in the platform, for example, in the explorer table. A tooltip explains that secret values are not transported to the platform for security reasons.

In TQL exports (such as copy to clipboard), secrets are copied as `null`.

By [@dit7ya](https://github.com/dit7ya).

### Changes

[Section titled “Changes”](#changes)

#### Make the list of nodes reload more often again

[Section titled “Make the list of nodes reload more often again”](#make-the-list-of-nodes-reload-more-often-again)

We recently raised the refresh time on the list of nodes to 30 seconds. After your feedback, it became apparent that we should have kept it as it was.

By [@avaq](https://github.com/avaq).

#### Better error messages for HTTP errors

[Section titled “Better error messages for HTTP errors”](#better-error-messages-for-http-errors)

The Tenzir Platform CLI now prints better error messages for HTTP errors encountered when communicating with the Tenzir Platform, including the contents of the `detail` field if provided.

By [@lava](https://github.com/lava) in [#73](https://github.com/tenzir/platform/pull/73).

#### Chronological diagnostics sorting in the explorer

[Section titled “Chronological diagnostics sorting in the explorer”](#chronological-diagnostics-sorting-in-the-explorer)

Diagnostics listed next to the editor are now sorted chronologically, making it easier to follow the order in which issues occur.

By [@gitryder](https://github.com/gitryder).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix false ‘deployed pipeline failed’ toasts

[Section titled “Fix false ‘deployed pipeline failed’ toasts”](#fix-false-deployed-pipeline-failed-toasts)

We no longer show ‘deployed pipeline failed’ toasts for pipelines that weren’t actually deployed, but were running via the Explorer, Dashboard, or other sources like that.

#### Fix ‘View’-button in failed pipeline Toast

[Section titled “Fix ‘View’-button in failed pipeline Toast”](#fix-view-button-in-failed-pipeline-toast)

When a deployed pipeline fails, a “Toast” popup is shown in the bottom right that allows you to view the error, except, that button didn’t work. Now it does!

And while we were at it, we also improved the text for these toasts a little to make it clearer that it’s specifically about a *deployed* pipeline.

By [@avaq](https://github.com/avaq).

#### Correctly set workspace category when creating new workspaces

[Section titled “Correctly set workspace category when creating new workspaces”](#correctly-set-workspace-category-when-creating-new-workspaces)

The Tenzir Platform CLI now correctly sets the workspace category when creating a new workspace.

By [@lava](https://github.com/lava) in [#73](https://github.com/tenzir/platform/pull/73).

# Fix Pipelines Table

We resolved an issue where some rows in the pipelines table were being cut off. The table now scrolls properly when there are many entries.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.14.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix table scroll to show all pipelines

[Section titled “Fix table scroll to show all pipelines”](#fix-table-scroll-to-show-all-pipelines)

We fixed a bug that prevented the table from scrolling when pipelines overflowed, so you can now access all rows without any being cut off.

By [@gitryder](https://github.com/gitryder).

# External JWT Support

This release adds support for reading externally-supplied JWT tokens from a header, instead of manually clicking on the *Log In* button.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.15.0).

### Features

[Section titled “Features”](#features)

#### Add support for login via externally-supplied JWTs

[Section titled “Add support for login via externally-supplied JWTs”](#add-support-for-login-via-externally-supplied-jwts)

You can now configure the Tenzir Platform to accept externally-supplied JWTs, instead of presenting a *Login* button and performing the OIDC flow itself. This is done by setting the `PRIVATE_JWT_FROM_HEADER` environment variable of the Tenzir UI service to the name of a header containing the external JWT.

This is useful for situations where access to the Tenzir Platform is already protected by an external authentication proxy that provides identity information to the application. In this case, the provided information can now be used directly instead of going through a second round of logins.

For example, to use this feature in combination with Google Cloud IAP, you would set `PRIVATE_JWT_FROM_HEADER=X-Goog-IAP-JWT-Assertion` and set the trusted issuer in the platform to `{"issuer":"https://cloud.google.com/iap","audiences":["<your_iap_audience>"]}`, where the audience string depends on your IAP configuration but would typically look like `"/projects/<project_number>/global/backendServices/<oauth_client_id>"`.

By [@lava](https://github.com/lava).

#### Add support for passing multiple issuer URLs

[Section titled “Add support for passing multiple issuer URLs”](#add-support-for-passing-multiple-issuer-urls)

The Tenzir Platform can now accept JWTs from multiple independent issuers. This is useful in situations where the CLI and UI users are served by two different OIDC providers.

To configure multiple OIDC providers, set the `TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES` environment variable in your `.env` file to a list of issuer configurations:

```dotenv
TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='[{"issuer": "https://accounts.google.com", "audiences": ["audience1"]}, {"issuer": "https://cloud.google.com/iap", "audiences": ["audience2", "audience3"]}]'
```

Note that the previous way of setting a single JSON object with `issuer` and `audiences` keys for this variable is still supported, so no change is required if you only want to use a single issuer.

By [@lava](https://github.com/lava).

#### Persistent widget time range

[Section titled “Persistent widget time range”](#persistent-widget-time-range)

The selected time range for the widgets now persists across page reloads and navigation, so you don’t have to reselect it each time.

By [@gitryder](https://github.com/gitryder), [@avaq](https://github.com/avaq).

### Changes

[Section titled “Changes”](#changes)

#### Dashboard error handling improvements

[Section titled “Dashboard error handling improvements”](#dashboard-error-handling-improvements)

Improve error handling related to loading dashboard data, adding cells to the dashboard, and saving/deleting dashboards.

By [@avaq](https://github.com/avaq).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix nodes not changing/disappearing after editing/deleting them

[Section titled “Fix nodes not changing/disappearing after editing/deleting them”](#fix-nodes-not-changingdisappearing-after-editingdeleting-them)

Fix an issue where the node wouldn’t disappear from the list of nodes after the node was deleted, or would update itself after being renamed, from the dropdown menu (while the side bar was collapsed).

By [@avaq](https://github.com/avaq).

# Diagnostic Discovery

This release adds two mechanism for a better diagnostics experience. Diagnostics are now shown directly in the editor. Additionally, the diagnostics heatmap in the pipeline overview can now be interacted with.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.16.0).

### Features

[Section titled “Features”](#features)

#### A new way to view pipeline diagnostics

[Section titled “A new way to view pipeline diagnostics”](#a-new-way-to-view-pipeline-diagnostics)

You can now view the exact diagnostics that occurred during a window of time by clicking on the corresponding cell in the heatmap.

This opens up a new panel below the heatmap where diagnostics are shown grouped by pipeline. From here, it’s also possible to search for diagnostics by text.

By [@dit7ya](https://github.com/dit7ya), [@avaq](https://github.com/avaq), [@gitryder](https://github.com/gitryder).

#### Diagnostics in the editor

[Section titled “Diagnostics in the editor”](#diagnostics-in-the-editor)

Diagnostics are now shown directly in the editor, with the associated code snippet being highlighted. Users can hover over the code to see detailed error and warning information in a floating popup. Additionally, the diagnostics pane opener button has been moved to the editor’s results strip for improved accessibility.

By [@dit7ya](https://github.com/dit7ya).

### Changes

[Section titled “Changes”](#changes)

#### Removed TQL2 switch from editor

[Section titled “Removed TQL2 switch from editor”](#removed-tql2-switch-from-editor)

With the minimum required node version now at 5.0.0, the editor no longer shows the TQL2 switch. You can now run TQL2 pipelines natively without needing to toggle anything.

By [@gitryder](https://github.com/gitryder).

# Fix Scrolling Issues

This release fixes two issues in the Tenzir UI that were found since the last release.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.16.1).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix unscrollable pipelines table

[Section titled “Fix unscrollable pipelines table”](#fix-unscrollable-pipelines-table)

We fixed a bug that prevented the pipelines table from scrolling to the bottom, which caused some pipelines to be hidden and inaccessible.

By [@gitryder](https://github.com/gitryder).

#### Fixed stray line in widget axis

[Section titled “Fixed stray line in widget axis”](#fixed-stray-line-in-widget-axis)

We fixed a visual bug where a thin white line appeared above the “Egress” label in the first widget on the Nodes page due to an empty axis label.

By [@gitryder](https://github.com/gitryder).

# Working Package Examples

This release fixes the display of example pipelines in packages.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.17.0).

### Changes

[Section titled “Changes”](#changes)

#### Print better error message for audience mismatch

[Section titled “Print better error message for audience mismatch”](#print-better-error-message-for-audience-mismatch)

The error message emitted by the platform on an audience mismatch in a supplied JWT now mentions the expected and provided audiences. (unless the provided audience contains non-url-safe characters)

By [@lava](https://github.com/lava).

#### Custom OIDC sign-in request params

[Section titled “Custom OIDC sign-in request params”](#custom-oidc-sign-in-request-params)

You can now use the environment variable `EXTRA_OIDC_REQUEST_PARAMS` to set custom query parameters with the sign-in requests that get sent to the OIDC provider. This is useful for configuring the endpoint’s behavior through provider specific options.

By [@tobim](https://github.com/tobim).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed 0 showing as empty in package inputs

[Section titled “Fixed 0 showing as empty in package inputs”](#fixed-0-showing-as-empty-in-package-inputs)

We fixed an issue where the default value 0 in package inputs was incorrectly shown as an empty field.

By [@gitryder](https://github.com/gitryder).

#### Fixed the examples not loading in the Explorer

[Section titled “Fixed the examples not loading in the Explorer”](#fixed-the-examples-not-loading-in-the-explorer)

We fixed an issue that prevented examples from loading in the examples pane of the Explorer tab. They now appear as expected.

By [@gitryder](https://github.com/gitryder).

# Visible zero durations

The app now renders durations of length `0` correctly in the detailed event view.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.17.1).

### Changes

[Section titled “Changes”](#changes)

#### Allow renaming of nodes before connection

[Section titled “Allow renaming of nodes before connection”](#allow-renaming-of-nodes-before-connection)

Previously, a node had to be connected before renaming was possible. You can now rename provisioned nodes even before they’re connected.

By [@gitryder](https://github.com/gitryder).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed 0s durations showing as empty

[Section titled “Fixed 0s durations showing as empty”](#fixed-0s-durations-showing-as-empty)

We fixed a formatting issue where exact 0s durations appeared as empty strings in the Inspector. These now display correctly.

By [@gitryder](https://github.com/gitryder).

#### Fixed selection in read-only pipeline code blocks

[Section titled “Fixed selection in read-only pipeline code blocks”](#fixed-selection-in-read-only-pipeline-code-blocks)

We fixed an issue where code in code blocks couldn’t be selected in package or configured pipelines. You can now easily copy specific parts of a pipeline.

By [@gitryder](https://github.com/gitryder).

# Improved History Pane Controls

You can now select parts of a pipeline from the history pane without closing it and the bottom bar in charts does not overlap with the contant any more.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.17.2).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed history item clicks blocking text selection

[Section titled “Fixed history item clicks blocking text selection”](#fixed-history-item-clicks-blocking-text-selection)

Clicking a history item no longer adds its pipeline to the editor. That action now appears as a button in the floating controls on each item, so you can freely select and copy text without accidentally modifying the editor or closing the history pane.

By [@gitryder](https://github.com/gitryder).

#### Added bottom margin to charts

[Section titled “Added bottom margin to charts”](#added-bottom-margin-to-charts)

We added extra space below charts to prevent them from overlapping with the floating controls bar in the Explorer.

By [@gitryder](https://github.com/gitryder).

# Fixed Download Urls

This bugfix release fixes an issue where the Tenzir Platform would generate download URLs with an incorrect signature.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.17.3).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fix download links for Sovereign Edition platform

[Section titled “Fix download links for Sovereign Edition platform”](#fix-download-links-for-sovereign-edition-platform)

We switched to use the AWS v4 signature algorithm for generating presigned URLs, rather than the deprecated v2 version. This fixes an incompatibility between recent versions of boto3 and seaweedfs.

By [@lava](https://github.com/lava) in [#123](https://github.com/tenzir/platform/pull/123).

# Updated CORS settings

This patch release contains no public-facing bug-fixes or features..

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.17.4).

# UI Reorganization

With this release of the Tenzir Platform, we reorganized the UI to make the most important pages more accessible.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.18.0).

### Features

[Section titled “Features”](#features)

#### Allow custom OIDC scope and audience params

[Section titled “Allow custom OIDC scope and audience params”](#allow-custom-oidc-scope-and-audience-params)

Added support for customizing OIDC scope and audience parameters in the CLI to provide more flexibility in authentication configuration.

By [@tobim](https://github.com/tobim) in [#127](https://github.com/tenzir/platform/pull/127).

#### Added search to key dropdowns

[Section titled “Added search to key dropdowns”](#added-search-to-key-dropdowns)

You can now easily search and select items in key dropdowns such as the node selector, dashboard filters, and the workspace switcher, making it quicker to find what you need and reducing the effort of scrolling through long lists.

By [@gitryder](https://github.com/gitryder).

#### Live activity redesign with Sparklines

[Section titled “Live activity redesign with Sparklines”](#live-activity-redesign-with-sparklines)

The platform now shows activity for deployed piplines with sparklines (instead of sparkbars).

The data behind the sparkline is also improved and more accurate now.

By [@dit7ya](https://github.com/dit7ya).

#### Copy token action for nodes

[Section titled “Copy token action for nodes”](#copy-token-action-for-nodes)

We added a “Copy token” option to the node selector’s actions menu, so you can quickly copy a Tenzir node’s token when needed.

By [@gitryder](https://github.com/gitryder).

### Changes

[Section titled “Changes”](#changes)

#### Reorganized navigation for easier access

[Section titled “Reorganized navigation for easier access”](#reorganized-navigation-for-easier-access)

We reorganized the navigation to make it easier to use. The Nodes menu item is now replaced by dedicated items for Pipelines, Explorer, Contexts, and Library. The Packages tab has also moved into the Library, where you can select it from the dropdown.

By [@gitryder](https://github.com/gitryder).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed performance issue in charts

[Section titled “Fixed performance issue in charts”](#fixed-performance-issue-in-charts)

We resolved an issue that could cause the app to use more memory than necessary when displaying charts.

By [@gitryder](https://github.com/gitryder), [@dit7ya](https://github.com/dit7ya).

# Pipeline Detail Page

This release adds a new detail page, as well as many UI fixes and improvements.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.19.0).

### Features

[Section titled “Features”](#features)

#### Pipeline detail page

[Section titled “Pipeline detail page”](#pipeline-detail-page)

Pipelines now have their own dedicated detail pages that you can access by clicking on any pipeline name. This gives you a comprehensive view of your pipeline in a full-screen interface designed for deep inspection and configuration.

By [@gitryder](https://github.com/gitryder).

### Changes

[Section titled “Changes”](#changes)

#### Show current and latest versions in node update info

[Section titled “Show current and latest versions in node update info”](#show-current-and-latest-versions-in-node-update-info)

We now show both the current version and the latest available version when an update is available or required. For example, in the node selector you see your current version alongside the latest release version.

By [@gitryder](https://github.com/gitryder).

#### Update postgres in examples to v17.6

[Section titled “Update postgres in examples to v17.6”](#update-postgres-in-examples-to-v176)

Update the postgres version to 17.6 in the example docker-compose.yaml files.

By [@weh](https://github.com/weh) in [#129](https://github.com/tenzir/platform/pull/129).

#### Prepare upcoming library format changes

[Section titled “Prepare upcoming library format changes”](#prepare-upcoming-library-format-changes)

The Tenzir Platform now supports the upcoming package format changes in the Tenzir Library, which will be introduced in the near future.

By \[@gitryder, tobim]\(<https://github.com/gitryder>, tobim).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Various UI Improvements

[Section titled “Various UI Improvements”](#various-ui-improvements)

This release also contains many additional bugfixes and improvements to the Tenzir UI:

* Fixed an issue that prevented cookie deletion.
* Improved error logging for OpenID configuration decoding failures.
* Fixed a bug where column resizing would break when new data with the same schema arrived.
* Big numbers are now handled precisely in the data table and inspector.
* A toast message is now displayed while the data download is being prepared.
* Fixed a bug that caused the page to freeze when clicking outside the pipeline editor during editing.
* The timestamp displayed in the tooltip of ingress/egress charts was fixed.

By \[@gitryder, dit7ya]\(<https://github.com/gitryder>, dit7ya).

#### Detailed pipeline activity spikes

[Section titled “Detailed pipeline activity spikes”](#detailed-pipeline-activity-spikes)

The detailed pipeline activity charts for the 15min range previously showed spikes even when the underlying data flow was steady. Now, the activity chart should correctly follow the actual data flow.

By [@jachris](https://github.com/jachris).

#### Fix inifinite loading for dashboard cells that produce no data

[Section titled “Fix inifinite loading for dashboard cells that produce no data”](#fix-inifinite-loading-for-dashboard-cells-that-produce-no-data)

Dashboard cells that produce no data no longer show an infinite loading animation.

By [@dit7ya](https://github.com/dit7ya).

# Various Bugfixes

This release fixes several bugs in the Tenzir Platform; from the Secret Store API to the way ephemeral node tokens are generated.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.19.1).

### Changes

[Section titled “Changes”](#changes)

#### Enable secrets in localdev example

[Section titled “Enable secrets in localdev example”](#enable-secrets-in-localdev-example)

Secrets are now enabled by default in the ‘localdev’ example deployment of the Tenzir Platform.

By [@lava](https://github.com/lava) in [#131](https://github.com/tenzir/platform/pull/131).

### Bug Fixes

[Section titled “Bug Fixes”](#bug-fixes)

#### Fixed internal server error when changing the default secret store

[Section titled “Fixed internal server error when changing the default secret store”](#fixed-internal-server-error-when-changing-the-default-secret-store)

Fixed an issue that could cause the Tenzir Platform API to return a 500 internal server error for certain parameters.

By [@lava](https://github.com/lava) in [#131](https://github.com/tenzir/platform/pull/131).

#### Update the secret store CLI interface to align with the Platform API

[Section titled “Update the secret store CLI interface to align with the Platform API”](#update-the-secret-store-cli-interface-to-align-with-the-platform-api)

The Tenzir Platform CLI was calling an outdated version of the secret store API, leading to unintended 404 errors. The CLI has been updated to use the latest version now.

By [@lava](https://github.com/lava) in [#131](https://github.com/tenzir/platform/pull/131).

#### Fix token generation for static workspaces

[Section titled “Fix token generation for static workspaces”](#fix-token-generation-for-static-workspaces)

Fixed a bug that caused static workspace tokens to include stray b’…’ characters, resulting in invalid node tokens.

By [@lava](https://github.com/lava).

# Tenzir Platform v1.2

This release brings improvements to diagnostics in the Explorer, adds the ability to download charts, and includes many stability improvements.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.2.0).

# Tenzir Platform v1.2.1

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.2](https://github.com/tenzir/platform/releases/tag/v1.2.0):

* This release handles OIDC issuer URLs without a trailing slash correctly.
* This release fixes an issue where the `PUBLIC_OIDC_SCOPES` variable is ignored.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.2.1).

# Tenzir Platform v1.3

This release introduces a new **vertical layout** option to make better use of the screen space available for event data and longer pipelines:

This release also contains many additional improvements and bugfixes:

* The explorer event inspector now automatically selects the first event if it is open.
* This release fixes an issue in the detailed activity charts in the pipelines page where the ingress and egress activities are mistakenly swapped.
* Charts in the dashboard show up to 10,000 events now.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.3.0).

# Tenzir Platform v1.4

This release introduces **alerts** for the Tenzir Platform, allowing users to get notified when a node is unexpectedly offline.

This release also contains a lot of other noteworthy improvements and bug fixes:

* The workspace switcher becomes significantly faster
* A new diagnostics drawer on the dashboard page makes warnings and errors more accessible
* The image download of charts now works as expected

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.4.0).

# Tenzir Platform v1.4.1

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.4](https://github.com/tenzir/platform/releases/tag/v1.4.0):

* This release fixes a bug where the app shows an infinite loader when creating a new account after logging out of an existing account.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.4.1).

# Tenzir Platform v1.5

This release brings a major upgrade to Dashboards making them independent of nodes.

This release also introduces a **dedicated Contexts page** that allows for managing contexts directly in the Tenzir Platform.

This release includes several additional enhancements and fixes:

* This release fixes an issue preventing package uninstallation.
* This release resolves a bug causing infinite loading when logging into a new account after logging out.
* This release fixes a problem that breaks account deletion.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.5.0).

# Tenzir Platform v1.6

This release features a new UI for example pipelines and adds support for the new TQL2 mode for nodes.

Note: The **minimum node version** for Tenzir Platform v1.6 updates to Tenzir Node **v4.24**. Older nodes need to be upgraded before they can reconnect to the newest platform version.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.6.0).

# Tenzir Platform v1.6.1

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.6](https://github.com/tenzir/platform/releases/tag/v1.6.0):

* This release fixes an issue that prevents the display of activity spark bars
* This release fixes a race condition that leads to a parse error in the detailed pipeline activity view
* This release adds a new option to specify CORS settings for the on-prem version of the websocket gateway

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.6.1).

# Tenzir Platform v1.7

This release introduces a new Drag’n’Drop feature to easily work with data from local files, and adds additional configuration knobs for Sovereign Edition users.

Sovereign Edition users get a few more options to customize their deployment:

* `ANALYTICS_WEBHOOK_URL`: Allows configuring an analytics sink for sending events
* `TLS_KEYFILE`/`TLS_CERTFILE`: Allow the platform and websocket gateway services to provide native TLS support, when not running behind a reverse proxy

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.7.0).

# Tenzir Platform v1.7.1

This patch release contains the following bug fixes and improvements over [Tenzir Platform v1.7](https://github.com/tenzir/platform/releases/tag/v1.7.0):

* This release fixes an issue that causes Demo Nodes to periodically lose their connection to the platform
* This release fixes node switching in the Library package install tab
* This release adds new placeholder screens
* This release fixes table column order mismatch for numerical keys
* This release prevents schema tree from getting cut-off

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.7.1).

# Tenzir Platform v1.7.2

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.7.1](https://github.com/tenzir/platform/releases/tag/v1.7.1):

* This release handles incoming timestamps with a ‘Z’ timezone suffix in pipeline data

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.7.2).

# New Charting & Examples

This release adds support for the new and improved charting operators of Tenzir Node v4.27, and revamps example deployments in the platform repository.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.8.0).

# Explorer & TLS Support

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.8](https://github.com/tenzir/platform/releases/tag/v1.8):

* This release fixes explorer results being hidden when pausing the pipeline
* This release fixes Run button scrolling out of view
* This release adds a Discord link to the app
* This release supports `TLS_KEYFILE` and `TLS_CERTFILE` in the app

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.8.1).

# Dashboard & Error Handling

This patch release contains the following changes, bug fixes and improvements over [Tenzir Platform v1.8.1](https://github.com/tenzir/platform/releases/tag/v1.8.1):

* Numbers in the platform become thousands-separated
* Titles become required before adding charts to dashboards
* Dashboard charts now load data fully correctly
* Failure of various data-fetching tasks now shows errors in the UI
* This release includes styling fixes, updates, and small features

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.8.2).

# Node Selection & UI Fixes

This patch release includes the following bug fixes over [Tenzir Platform v1.8.2](https://github.com/tenzir/platform/releases/tag/v1.8.2):

* This release fixes a bug where a newly added node is not automatically selected on the Pipelines page.
* This release ensures that the node selector on different pages does not overflow off the screen.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.8.3).

# Pipeline Page & Load Time

This patch release includes the following bug fixes over [Tenzir Platform v1.8.3](https://github.com/tenzir/platform/releases/tag/v1.8.3):

* This release fixes a bug where the pipeline page crashes when navigating away from it and back to a newly loaded page.
* This release fixes the broken hover state of the dropdown in DetailedActivity.
* This release reduces 5s-7s spikes in the initial load time of the platform page.
* This release improves the styling and user experience of the download menu to be consistent with the node submenu used for charts.”

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.8.4).

# CI Workflow Improvements

This release does not contain any user-facing changes, only improvements to the internal CI release workflow.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.8.5).

# Explorer for Large Datasets

This release revamps the Explorer to better support large datasets.

Additionally, this release contains the following bug fixes and improvements:

* The *Add to Dashboard* button in the Explorer is no longer disabled when the corresponding history entry is removed.
* This release fixes a bug that prevents the *Save* button on dashboards from working. The save button now also shows a loading state when applicable.
* This release makes pie chart legends scrollable.
* All log scales in charts change to symlog scales, which handle zero and negative values more gracefully.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.9.0).

# Auth Rules & CLI Tokens

This patch release includes the following bug fixes over [Tenzir Platform v1.9.0](https://github.com/tenzir/platform/releases/tag/v1.9.0), all of them geared towards Sovereign Edition users:

* This release improves safety against accidentally logging secrets from the configuration
* This release adds support for a new ‘allow-all’ auth rule that allows configuring global access to workspaces
* This release adds support for using access tokens as a fallback for id tokens in the CLI
* This release adds support for a new `TENZIR_PLATFORM_CLI_ID_TOKEN` override to manually set a valid id token for the CLI, instead of going through an OIDC flow

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.9.1).

# CLI Verbose Fix

This patch release includes the following bug fixes over [Tenzir Platform v1.9.1](https://github.com/tenzir/platform/releases/tag/v1.9.1):

* This release fixes the CLI to not ignore the `TENZIR_PLATFORM_CLI_VERBOSE` setting

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.9.2).

# Performance & Metrics

This patch release includes the following bug fixes over [Tenzir Platform v1.9.2](https://github.com/tenzir/platform/releases/tag/v1.9.2):

* This release fixes some small issues in the platform, and improves performance at various places.
* The platform gains opt-in support for storing pipeline metrics from connected nodes.
* OIDC requests now use form serialization correctly.
* This release fixes variable descriptions in the onprem example of the Sovereign Edition platform.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.9.3).

# Table & UI Improvements

This patch release includes the following improvements over [Tenzir Platform v1.9.3](https://github.com/tenzir/platform/releases/tag/v1.9.3):

* This release fixes the pipelines and context tables to use the full available width of the screen.
* The edit button for pipeline names in the detailed pipelines pane no longer remains dysfunctional.
* The history in the Explorer now works correctly for going back to the very last and going forward to the most recent entry.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.9.4).

# Pipeline Table & Bandwidth

This patch release includes the following improvements over [Tenzir Platform v1.9.4](https://github.com/tenzir/platform/releases/tag/v1.9.4):

* The pipelines table sorting and filtering by column works correctly again.
* This release reduces bandwidth usage between the node and the platform.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.9.5).

# TQL2 Switch Improvements

This patch release includes the following improvements over [Tenzir Platform v1.9.5](https://github.com/tenzir/platform/releases/tag/v1.9.5):

* The TQL2 switch no longer shows in the Explorer for nodes that have the TQL2-only mode enabled through the `TENZIR_TQL2=true` option. This prepares for the upcoming Tenzir Node v5.0 release, which enables TQL2 by default.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.9.6).

# Data Inspector Overhaul

This patch release comes with a number of frontend improvements since [Tenzir Platform v1.9.6](https://github.com/tenzir/platform/releases/tag/v1.9.5):

* This release completely overhauls the data inspector to give a more detailed tree view of the data, and makes it possible to collapse and expand the tree.
* The copy button in the data inspector now copies the data as TQL instead of JSON.

Download the release on [GitHub](https://github.com/tenzir/platform/releases/tag/v1.9.7).