---
title: Deduplicate events
---

The [`deduplicate`](/reference/operators/deduplicate) operator
provides a powerful mechanism to remove duplicate events in a pipeline.

There are numerous use cases for deduplication, such as reducing noise,
optimizing costs and making threat detection and response more efficient.

## Basic deduplication

Let's start with a simple example to understand how deduplication works. Imagine
you're monitoring user logins and want to see only unique users, regardless of
how many times they log in:

```tql
from {user: "alice", action: "login", time: 1},
     {user: "bob", action: "login", time: 2},
     {user: "alice", action: "login", time: 3},
     {user: "alice", action: "logout", time: 4}
deduplicate user
```

```tql
{user: "alice", action: "login", time: 1}
{user: "bob", action: "login", time: 2}
```

The operator keeps only the first occurrence of each unique value for the
specified field(s). In this example:

- Alice's first login (time: 1) is kept
- Bob's login (time: 2) is kept
- Alice's second login (time: 3) is dropped because we already saw
  `user: "alice"`
- Note that Alice's logout (time: 4) would also be dropped with this simple
  deduplication

## Deduplicate by multiple fields

Often you need more nuanced deduplication. For example, you might want to track
unique user-action pairs to see each distinct activity per user:

```tql
from {user: "alice", action: "login", time: 1},
     {user: "bob", action: "login", time: 2},
     {user: "alice", action: "login", time: 3},
     {user: "alice", action: "logout", time: 4}
deduplicate {user: user, action: action}
```

```tql
{user: "alice", action: "login", time: 1}
{user: "bob", action: "login", time: 2}
{user: "alice", action: "logout", time: 4}
```

Now we keep unique combinations of user and action:

- Alice's first login is kept (unique: alice+login)
- Bob's login is kept (unique: bob+login)
- Alice's second login is dropped (duplicate: alice+login already seen)
- Alice's logout is kept (unique: alice+logout is a new combination)

This approach is useful for tracking distinct user activities rather than just
unique users.

## Analyze unique host pairs

When investigating network incidents, you often want to identify all unique
communication patterns between hosts. This example shows network connections
with nested ID fields containing origin and response hosts:

```tql
from {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 1024},
     {id: {orig_h: "10.0.0.2", resp_h: "192.168.1.1"}, bytes: 2048},
     {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 512},
     {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.2"}, bytes: 256}
deduplicate {orig_h: id.orig_h, resp_h: id.resp_h}
```

```tql
{id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 1024}
{id: {orig_h: "10.0.0.2", resp_h: "192.168.1.1"}, bytes: 2048}
{id: {orig_h: "10.0.0.1", resp_h: "192.168.1.2"}, bytes: 256}
```

The deduplication works on the extracted host pairs:

- First connection (10.0.0.1 → 192.168.1.1) is kept
- Second connection (10.0.0.2 → 192.168.1.1) is kept (different origin)
- Third connection (10.0.0.1 → 192.168.1.1) is dropped (duplicate of first)
- Fourth connection (10.0.0.1 → 192.168.1.2) is kept (different destination)

Note that flipped connections (A→B vs B→A) are considered different pairs. This
helps identify bidirectional communication patterns.

## Remove duplicate alerts

Security monitoring often generates duplicate alerts that create noise and fatigue.
Here's how to suppress repeated alerts for the same threat pattern:

```tql
from {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 1},
     {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 2},
     {src_ip: "10.0.0.2", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 3},
     {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Port Scan", time: 4}
deduplicate {src: src_ip, dst: dest_ip, sig: signature}
```

```tql
{src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 1}
{src_ip: "10.0.0.2", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 3}
{src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Port Scan", time: 4}
```

The deduplication creates a composite key from source, destination, and signature:

- First "Suspicious DNS" from 10.0.0.1 is kept
- Second identical alert (time: 2) is suppressed as a duplicate
- "Suspicious DNS" from different source 10.0.0.2 is kept (different pattern)
- "Port Scan" from 10.0.0.1 is kept (different signature)

This approach reduces alert volume while preserving visibility into distinct threat patterns.

### Using timeout for time-based deduplication

In production environments, you often want to suppress duplicates only within a
certain time window. This ensures you don't miss recurring issues that happen
over longer periods.

The `create_timeout` parameter resets the deduplication state after the
specified duration:

```tql
deduplicate {src: src_ip, dst: dest_ip, sig: signature}, create_timeout=1h
```

This configuration:

- Suppresses duplicate alerts for the same source/destination/signature
  combination
- Resets after 1 hour, allowing the same alert pattern through again
- Helps balance noise reduction with visibility into persistent threats

For example, if a host is repeatedly targeted:

- 9:00 AM: First "Port Scan" alert is shown
- 9:15 AM: Duplicate suppressed
- 9:30 AM: Duplicate suppressed
- 10:05 AM: Same alert shown again (timeout expired)

## Tracking dropped events

Sometimes you need visibility into how many duplicates were suppressed. The
`count_field` option adds a field to each output event showing the number of
dropped events for that deduplication key.

### Monitoring alert suppression

When deduplicating security alerts, you may want to know how many duplicates
were suppressed to understand the volume of repeated activity:

```tql
from {src_ip: "10.0.0.1", signature: "Port Scan", time: 1},
     {src_ip: "10.0.0.1", signature: "Port Scan", time: 2},
     {src_ip: "10.0.0.1", signature: "Port Scan", time: 3},
     {src_ip: "10.0.0.2", signature: "Port Scan", time: 4},
     {src_ip: "10.0.0.2", signature: "Port Scan", time: 5},
     {src_ip: "10.0.0.1", signature: "Port Scan", time: 6}
deduplicate {src: src_ip, sig: signature}, distance=2, count_field=suppressed
```

```tql
{src_ip: "10.0.0.1", signature: "Port Scan", time: 1, suppressed: 0}
{src_ip: "10.0.0.2", signature: "Port Scan", time: 4, suppressed: 0}
{src_ip: "10.0.0.1", signature: "Port Scan", time: 6, suppressed: 2}
```

The `suppressed` field shows:

- First occurrence from each source has `suppressed: 0`
- When 10.0.0.1 appears again at time 6, it shows `suppressed: 2` because
  events at times 2 and 3 were dropped due to the `distance=2` constraint

This information helps you understand the scale of duplicate activity. A high
suppression count might indicate:

- Persistent scanning or attack attempts from a specific source
- Noisy sensors generating excessive duplicate events
- Configuration issues causing event duplication

### Understanding suppression counts

The `count_field` value represents events dropped since the last output for that
key:

- **First occurrence**: Always has a count of `0` since no previous events were
  dropped
- **After suppression**: Shows how many events were dropped before this one was
  allowed through
- **After timeout expiration**: Resets to `0` since the deduplication state was
  cleared

This makes it easy to identify which events represent the start of a new
pattern versus continuation of existing activity.

## Best practices

1. **Choose fields carefully**: Deduplicate on fields that truly identify unique
   events for your use case. Too few fields may drop important events; too many
   may not deduplicate effectively.

2. **Consider order**: The [`deduplicate`](/reference/operators/deduplicate)
   operator keeps the _first_ occurrence. If you need the latest, consider using
   [`reverse`](/reference/operators/reverse) first:

   ```tql
   reverse | deduplicate user | reverse
   ```

3. **Use timeout wisely**: For streaming data, `create_timeout` prevents memory
   from growing indefinitely while still reducing noise. Choose durations based
   on your threat detection windows.

4. **Combine with other operators**: Often you'll want to filter
   ([`where`](/reference/operators/where)) or transform
   ([`set`](/reference/operators/set)) data before deduplication to normalize
   keys:

   ```tql
   normalized_ip = src_ip.string()
   deduplicate normalized_ip
   ```

5. **Track suppression for context**: Use `count_field` when you need visibility
   into duplicate volume, especially for security monitoring where the number of
   repeated events can indicate severity or persistence of threats.
