---
title: to_databricks
category: Outputs/Events
example: 'to_databricks workspace="https://…'
---

import Integration from '@components/see-also/Integration.astro';

Sends events to a Databricks Unity Catalog table.

```tql
to_databricks workspace=string, catalog=string, schema=string, table=string,
              client_id=string, client_secret=string, [warehouse_id=string],
              [staging_volume=string], [partition_by=list], [sort_by=list],
              [flush_interval=duration], [file_size=int],
              [method=string], [ingest_mode=string]
```

## Description

The `to_databricks` operator writes events to a table in
[Databricks Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/index.html).
It produces optimized Parquet files and commits them atomically to the target
table, making data immediately queryable in Databricks SQL, notebooks, and
downstream tools.

The operator supports multiple ingestion methods with different cost and
governance trade-offs. See the [Databricks integration
overview](/integrations/databricks) for guidance on choosing a method.

### `workspace = string`

The URL of your Databricks workspace. This is the base URL you see when logged
into Databricks.

| Cloud | Example                                              |
| ----- | ---------------------------------------------------- |
| AWS   | `https://dbc-a1b2c3d4-e5f6.cloud.databricks.com`     |
| Azure | `https://adb-1234567890123456.7.azuredatabricks.net` |
| GCP   | `https://1234567890123456.7.gcp.databricks.com`      |

The operator derives all API endpoints from this URL automatically, including
the Unity Catalog REST API (`/api/2.1/unity-catalog/`) and the SQL Statement
API (`/api/2.0/sql/statements`).

### `catalog = string`

The Unity Catalog catalog name. Catalogs are the top-level namespace and
typically represent environments or data domains.

Common patterns include environment-based naming (`prod`, `dev`, `staging`) or
domain-based naming (`analytics`, `security`, `finance`). New workspaces have a
default catalog named `main`.

### `schema = string`

The schema (also called database) within the catalog. Schemas group related
tables and control access at a finer granularity than catalogs.

For data lake architectures, consider medallion-style naming: `bronze` for raw
ingested data, `silver` for cleaned and conformed data, `gold` for
business-ready aggregates. Alternatively, use domain-specific names like
`network_logs`, `application_events`, or `audit_trail`.

The schema must exist before ingestion. Create it with:

```sql
CREATE SCHEMA IF NOT EXISTS my_catalog.bronze;
```

### `table = string`

The name of the target table. The operator creates the table if it does not
exist, inferring the schema from the first batch of events.

Table naming conventions vary by use case. For raw ingestion, names often
reflect the data source: `zeek_conn`, `cloudtrail_events`, `firewall_logs`.
For normalized data, names may reflect the event type or entity:
`network_connections`, `user_authentications`, `file_operations`.

### `client_id = string`

The Application (client) ID of a Databricks service principal for OAuth
machine-to-machine authentication.

Create a service principal in **Account Console → User management → Service
principals**. The Application ID displayed after creation is your `client_id`.

### `client_secret = string`

The OAuth secret for the service principal. Generate this in the service
principal's **Secrets** tab. The secret is shown only once at creation time.

The service principal requires these Unity Catalog privileges:

```sql
GRANT USE CATALOG ON CATALOG my_catalog TO `my-service-principal`;
GRANT USE SCHEMA ON SCHEMA my_catalog.bronze TO `my-service-principal`;
GRANT CREATE TABLE ON SCHEMA my_catalog.bronze TO `my-service-principal`;
GRANT READ VOLUME, WRITE VOLUME ON VOLUME my_catalog.bronze.staging
  TO `my-service-principal`;
```

### `warehouse_id = string (optional)`

The ID of the SQL Warehouse that executes `COPY INTO` statements. Required when
`method` is `delta`.

Find this in **SQL Warehouses → [Your Warehouse] → Connection details**. The
warehouse ID is the final segment of the HTTP Path, after `/sql/1.0/warehouses/`.

Serverless SQL Warehouses are recommended for ingestion workloads due to fast
startup times and automatic scaling. Each `COPY INTO` execution consumes DBUs
at your warehouse's rate (typically $0.07–0.70/DBU depending on tier).

### `staging_volume = string (optional)`

The Unity Catalog Volume where Parquet files are staged before being committed
to the table. If not specified, the operator creates a temporary staging
location.

Volumes provide managed storage within Unity Catalog:

```sql
CREATE VOLUME IF NOT EXISTS my_catalog.bronze.ingestion_staging;
```

Staged files are automatically removed after successful commits.

### `partition_by = list (optional)`

The partitioning scheme for the target table. Partitioning improves query
performance by enabling partition pruning on filtered queries.

Defaults to no partitioning. Specify partition columns when creating tables
that will be queried with predictable filter patterns. This option only applies
when the operator creates a new table; it is ignored when appending to existing
tables whose partitioning is already defined.

Common patterns:

| Pattern                                | Use Case                                       |
| -------------------------------------- | ---------------------------------------------- |
| `[time_bucket("1d", time)]`            | Time-series data with date-range queries       |
| `[time_bucket("1h", time)]`            | High-volume streams needing hourly granularity |
| `[time_bucket("1d", time), source]`    | Multi-source ingestion with source filtering   |
| `[time_bucket("1d", time), class_uid]` | OCSF events filtered by event class            |

Avoid over-partitioning. Each partition should contain at least 1GB of data
for optimal file sizes and query performance.

### `sort_by = list (optional)`

Columns to sort rows by within each output file. Sorting improves query
performance by enabling data skipping: Databricks reads Parquet column
statistics (min/max values per row group) to skip files that cannot contain
matching rows.

When `partition_by` is specified, rows are always sorted by partition columns
first to ensure partition alignment (each file contains data for exactly one
partition). The `sort_by` columns are applied as secondary sort keys.

For best results, sort by columns that appear frequently in `WHERE` clauses:

```tql
to_databricks ...,
  partition_by=[time_bucket("1d", time)],
  sort_by=[src_ip, dst_ip]
```

This produces files where queries like `WHERE src_ip = '10.0.0.1'` can skip
entire files based on min/max statistics.

### `flush_interval = duration (optional)`

Maximum time to buffer events before committing to the table. This controls
data freshness—the maximum delay before events become queryable.

Defaults to `5m`. When this interval elapses, the operator commits whatever
data has accumulated, even if `file_size` has not been reached.

| Interval | Data Latency | Recommended For                    |
| -------- | ------------ | ---------------------------------- |
| `1m`     | ~1 minute    | Real-time dashboards, alerting     |
| `5m`     | ~5 minutes   | General analytics, SOC workflows   |
| `15m`    | ~15 minutes  | Batch analytics, cost optimization |
| `1h`     | ~1 hour      | Archival, compliance retention     |

Shorter intervals improve freshness but may produce smaller files. The
operator optimizes file layout regardless of flush timing.

### `file_size = int (optional)`

Target size for output Parquet files.

Defaults to `1Gi`. When the buffer reaches this size, the operator flushes
immediately regardless of `flush_interval`. This produces optimally-sized
files that minimize the need for later compaction via `OPTIMIZE`.

Databricks recommends files between 256MB and 1GB for best query performance.
Larger files reduce metadata overhead and improve scan efficiency; smaller
files enable finer-grained data skipping.

For high-volume streams, the `file_size` threshold triggers most flushes. For
low-volume streams, `flush_interval` ensures timely commits despite smaller
file sizes.

### `method = string (optional)`

The ingestion method for writing data to Databricks:

- `delta`: Stages optimized Parquet files in a Unity Catalog Volume and commits
  them via `COPY INTO` using the SQL Statement API. Requires a SQL Warehouse
  and consumes DBUs proportional to data volume. Produces fully managed Delta
  tables with complete Unity Catalog governance.

- `iceberg` _(planned)_: Uses the Unity Catalog Iceberg REST Catalog API to
  commit directly to managed Iceberg tables. Zero DBU cost—only API calls.
  Requires tables created with Iceberg format.

- `external-delta` _(planned)_: Writes Delta tables directly to customer-managed
  cloud storage (S3, ADLS, GCS) using the Delta Lake protocol. Zero DBU cost.
  Tables are registered as external tables in Unity Catalog with reduced
  governance features compared to managed tables.

Defaults to `delta`.

See the [Databricks integration overview](/integrations/databricks) for detailed
guidance on choosing a method based on cost, latency, and governance requirements.

### `ingest_mode = string (optional)`

Controls table creation and append behavior:

- `create_append`: Creates the table if it does not exist, otherwise appends.
- `create`: Creates the table, failing if it already exists.
- `append`: Appends to an existing table, failing if it does not exist.

Defaults to `create_append`.

When creating tables, the operator infers the schema from the first batch of
events. Subsequent batches with additional fields trigger automatic schema
evolution via `mergeSchema`. Fields present in the table but missing from
events are filled with `null`.

## Write-Time Optimizations

The operator applies several optimizations to produce query-efficient files
that minimize the need for post-write maintenance:

**Partition alignment**: Each output file contains data for exactly one
partition value. This ensures perfect partition pruning—queries filtering on
partition columns never read irrelevant files.

**Sorted rows**: Rows are sorted by partition columns (if specified) followed
by `sort_by` columns. Sorting produces tight min/max statistics in Parquet row
groups, enabling aggressive data skipping during queries.

**Optimal file sizing**: The `file_size` parameter (default 1GB) produces
files in Databricks' recommended size range, reducing metadata overhead and
the urgency of running `OPTIMIZE` for compaction.

**Efficient encoding**: The operator leverages Apache Arrow's Parquet writer
with dictionary encoding, run-length encoding, and Zstd compression for
compact, fast-to-read files.

These optimizations are applied regardless of the `method` chosen. For managed
Delta tables (`method=delta`), `COPY INTO` preserves the file organization.
For Iceberg and external Delta methods, the files are committed directly with
their optimized layout intact.

## Examples

### Send events to a bronze layer table

Ingest raw Zeek connection logs into a medallion-architecture data lake:

```tql
from_file "/var/log/zeek/conn.log", read=read_zeek_tsv
to_databricks
  workspace="https://adb-1234567890.azuredatabricks.net",
  catalog="security",
  schema="bronze",
  table="zeek_conn",
  client_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  client_secret=<client_secret>,
  warehouse_id="abc123def456",
  partition_by=[time_bucket("1d", ts)]
```

### Stream OCSF events with partitioning

Write normalized security events with OCSF-optimized partitioning:

```tql
subscribe "ocsf"
to_databricks
  workspace="https://dbc-a1b2c3d4.cloud.databricks.com",
  catalog="prod",
  schema="silver",
  table="security_events",
  client_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  client_secret=<client_secret>,
  warehouse_id="abc123def456",
  staging_volume="tenzir_staging",
  partition_by=[time_bucket("1d", time), class_uid]
```

### Low-latency ingestion for alerting

Minimize data latency for real-time security monitoring:

```tql
subscribe "alerts"
to_databricks
  workspace="https://1234567890123456.7.gcp.databricks.com",
  catalog="security",
  schema="realtime",
  table="high_priority_alerts",
  client_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  client_secret=<client_secret>,
  warehouse_id="abc123def456",
  flush_interval=1m,
  file_size=256Mi
```

### Optimized ingestion for network analytics

High-volume network telemetry with sorting for common query patterns:

```tql
subscribe "network"
to_databricks
  workspace="https://adb-1234567890.azuredatabricks.net",
  catalog="analytics",
  schema="silver",
  table="network_flows",
  client_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  client_secret=<client_secret>,
  warehouse_id="abc123def456",
  partition_by=[time_bucket("1d", time)],
  sort_by=[src_ip, dst_ip, dst_port],
  flush_interval=5m,
  file_size=1Gi
```

### Cost-optimized batch ingestion

Reduce DBU consumption for high-volume archival workloads:

```tql
export
to_databricks
  workspace="https://dbc-a1b2c3d4-e5f6.cloud.databricks.com",
  catalog="archive",
  schema="compliance",
  table="audit_logs",
  client_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  client_secret=<client_secret>,
  warehouse_id="abc123def456",
  flush_interval=15m,
  file_size=1Gi,
  partition_by=[time_bucket("1d", time), source_type]
```

## See Also

- <Integration>databricks</Integration>
