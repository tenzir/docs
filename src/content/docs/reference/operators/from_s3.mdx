---
title: from_s3
category: Inputs/Events
example: 'from_s3 "s3://my-bucket/data/**.json"'
---

import Op from '@components/see-also/Op.astro';
import Integration from '@components/see-also/Integration.astro';
import FromFileCommonParams from '@partials/operators/FromFileCommonParams.mdx';
import AWSIAMOptions from '@partials/operators/AWSIAMOptions.mdx';

Reads one or multiple files from Amazon S3.

```tql
from_s3 url:string, [anonymous=bool, aws_iam=record, watch=bool,
  remove=bool, rename=string->string, path_field=field, max_age=duration] { â€¦ }
```

## Description

The `from_s3` operator reads files from Amazon S3, with support for glob
patterns, automatic format detection, and file monitoring.

By default, authentication is handled by AWS's default credentials provider
chain, which may read from multiple environment variables and credential files:

- `~/.aws/credentials` and `~/.aws/config`
- `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN`
- EC2 instance metadata service
- ECS container credentials

### `url: string`

URL identifying the S3 location where data should be read from.

The characters `*` and `**` have a special meaning. `*` matches everything
except `/`. `**` matches everything including `/`. The sequence `/**/` can also
match nothing. For example, `bucket/**/data` matches `bucket/data`.

Supported URI format:
`s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)`

Options can be appended to the path as query parameters:

- `region`: AWS region (e.g., `us-east-1`)
- `scheme`: Connection scheme (`http` or `https`)
- `endpoint_override`: Custom S3-compatible endpoint
- `allow_bucket_creation`: Allow creating buckets if they don't exist
- `allow_bucket_deletion`: Allow deleting buckets

### `anonymous = bool (optional)`

Use anonymous credentials instead of any configured authentication.

Defaults to `false`.

<AWSIAMOptions />

<FromFileCommonParams removeNote={true} />

## Examples

### Read every JSON file from a bucket

```tql
from_s3 "s3://my-bucket/data/**.json"
```

### Read CSV files using explicit credentials

```tql
from_s3 "s3://my-bucket/data.csv", aws_iam={
  access_key_id: "AKIAIOSFODNN7EXAMPLE",
  secret_access_key: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
}
```

### Read from S3-compatible service with custom endpoint

```tql
from_s3 "s3://my-bucket/data/**.json?endpoint_override=minio.example.com:9000&scheme=http"
```

### Read files continuously and assume IAM role

```tql
from_s3 "s3://logs/application/**.json", watch=true, aws_iam={
  assume_role: "arn:aws:iam::123456789012:role/LogReaderRole"
}
```

### Process files and move them to an archive bucket

```tql
from_s3 "s3://input-bucket/**.json",
  rename=(path => "archive/" + path)
```

### Add source path to events

```tql
from_s3 "s3://data-bucket/**.json", path_field=source_file
```

### Read Zeek logs with anonymous access

```tql
from_s3 "s3://public-bucket/zeek/**.log", anonymous=true {
  read_zeek_tsv
}
```

## See Also

- <Op>from_file</Op>
- <Op>load_s3</Op>
- <Op>save_s3</Op>
- <Integration>amazon/s3</Integration>
