<SYSTEM>Guides</SYSTEM>

# Account Creation

Get started with Tenzir by creating an account: 1. Go to [app.tenzir.com](https://app.tenzir.com). ![Landing page](/_astro/signin.D-Kjtq1Z_12o64U.webp) 2. Log in with your identity provider. This creates an account implicitly. ![IdP](/_astro/signin-choice.CDMmwGmU_1kGjAa.webp) 3. Can't use an identity provider? Click **Sign up** below the **Continue** button. We recommend using a [passkey](https://www.passkeys.com/what-are-passkeys.html) as a safer alternative to traditional passwords. ![Create passkey](/_astro/signin-passkey.AZS33VhO_1Cr2tF.webp) 4. If you created an account with us, you'll receive an email with a link to verify your email address. Click the link to complete the registration process. üéâ Congratulations, you now have an account and can can freely use the Tenzir [Community Edition](https://tenzir.com/pricing). ## Delete an Account [Section titled ‚ÄúDelete an Account‚Äù](#delete-an-account) Delete your account as follows: 1. Go to the [Account](https://app.tenzir.com/account) page. 2. Click *Delete Account*. 3. (Optionally) Leave a note explaining why you delete your account. Caution Deleting your account will remove all data about you from our cloud platform. You will also lose the ability to manage pipelines on your node. If you decide to come back, just re-create an account as described above.

# Collect metrics

Tenzir keeps track of metrics about node resource usage, pipeline state, and runtime performance. Metrics are stored as internal events in the node‚Äôs storage engine, allowing you to work with metrics just like regular data. Use the [`metrics`](/reference/operators/metrics) input operator to access the metrics. The operator documentation lists [all available metrics](/reference/operators/metrics#schemas) in detail. The `metrics` operator provides a *copy* of existing metrics. You can use it multiple time to reference the same metrics feed. ## Write metrics to a file [Section titled ‚ÄúWrite metrics to a file‚Äù](#write-metrics-to-a-file) Export metrics continuously to a file via `metrics --live`: ```tql metrics live=true write_ndjson save_file "metrics.json", append=true ``` This attaches to incoming metrics feed, renders them as NDJSON, and then writes the output to a file. Without the `live` option, the `metrics` operator returns the snapshot of all historical metrics. ## Summarize metrics [Section titled ‚ÄúSummarize metrics‚Äù](#summarize-metrics) You can [shape](/guides/data-shaping/shape-data) metrics like ordinary data, e.g., write aggregations over metrics to compute runtime statistics suitable for reporting or dashboarding: ```tql metrics "operator" where sink == true summarize runtime=sum(duration), pipeline_id sort -runtime ``` The above example computes the total runtime over all pipelines grouped by their unique ID.

# Install a package

[Packages](/explanations/packages) provide a flexible approach for combining operators, pipelines, contexts, and examples into a unified deployable unit. ## Install from the Tenzir Library [Section titled ‚ÄúInstall from the Tenzir Library‚Äù](#install-from-the-tenzir-library) The most convenient way to install a package is through the [Tenzir Library](https://app.tenzir.com/library): 1. Click on a package 2. Select the *Install* tab 3. Define your inputs (optional) 4. Click the *Install* button in the bottom right ## Install with the package operator [Section titled ‚ÄúInstall with the package operator‚Äù](#install-with-the-package-operator) To install a package interactively in TQL, use the [`package::add`](/reference/operators/package/add) operator: ```tql package::add "/path/to/pkg" ``` This installs the package from the directory `/path/to/pkg`. Pass an `inputs` record to adjust the package configuration and replace the package‚Äôs templates with concrete values: ```tql package::add "package.yaml", inputs={ endpoint: "localhost:42000", policy: "block", } ``` Your package now appears when you list all installed packages: ```tql package::list ``` ```tql { id: "your-package", install_status: "installed", // ‚Ä¶ } ``` To uninstall a package interactively, use [`package::remove`](/reference/operators/package/remove) and pass the package ID. ```tql package::remove "your-package" ``` ## Install with Infrastructure as Code (IaC) [Section titled ‚ÄúInstall with Infrastructure as Code (IaC)‚Äù](#install-with-infrastructure-as-code-iac) For IaC-style deployments, you can install packages *as code* by putting them next to your `tenzir.yaml` configuration file: Inside the `packages` directory, each installed package has its own directory. The directory name matches the package ID. The node searches for packages in the following locations: 1. The `packages` directory in all [configuration directories](/explanations/configuration). 2. All directories specified in the `tenzir.package-dirs` configuration option. To provide inputs in IaC-mode, place a `config.yaml` file next to the `package.yaml` file. For example, this configuration sets the inputs `endpoint` and `policy`: config.yaml ```yaml inputs: endpoint: localhost:42000 policy: block ```

# Manage a pipeline

A pipeline can be in one of the following **states** after you [run it](/guides/basic-usage/run-pipelines): * **Created**: the pipeline has just been deployed. * **Running**: the pipeline is actively processing data. * **Completed**: there is no more data to process. * **Failed**: an error occurred. * **Paused**: the user interrupted execution, keeping in-memory state. * **Stopped**: the user interrupted execution, resetting all in-memory state. The [app](https://app.tenzir.com/) or [API](/reference/node/api) allow you to manage the pipeline lifecycles. ## Change the state of a pipeline [Section titled ‚ÄúChange the state of a pipeline‚Äù](#change-the-state-of-a-pipeline) In the [app](https://app.tenzir.com/overview), an icon visualizes the current pipeline state. Change a state as follows: 1. Click the checkbox on the left next to the pipeline, or the checkbox in the column header to select all pipelines. 2. Click the button corresponding to the desired action, i.e., *Start*, *Pause*, *Stop*, or *Delete*. 3. Confirm your selection. For the [API](/reference/node/api), use the following endpoints based on the desired actions: * Start, pause, and stop: `/pipeline/update` * Delete: `/pipeline/delete` ## Understand pipeline state transitions [Section titled ‚ÄúUnderstand pipeline state transitions‚Äù](#understand-pipeline-state-transitions) The diagram below illustrates the various states, where circles correspond to states and arrows to state transitions: ![Pipeline States](/_astro/pipeline-states.KFiJ59Ps_19DKCs.svg) The grey buttons indicate the actions you, as a user, can take to transition into a different state. The orange arrows are transitions that take place automatically based on system events.

# Run pipelines

You can run a [pipeline](/explanations/architecture/pipeline) via the [platform](https://app.tenzir.com), on the command line using the `tenzir` binary, or as code via the configuration file. ## In the platform [Section titled ‚ÄúIn the platform‚Äù](#in-the-platform) Run a pipeline by writing typing it in the editor and hitting the *Run* button. The following invariants apply: 1. You must start with an input operator 2. The browser is always the output operator The diagram below illustrates these mechanics: ![Pipeline in the Browser](/_astro/pipeline-browser.B2LCkA6F_19DKCs.svg) For example, write `from {x: 42}` and click *Run* to see a single event show up. ## On the command line [Section titled ‚ÄúOn the command line‚Äù](#on-the-command-line) On the command line, run `tenzir <pipeline>` where `<pipeline>` is the definition of the pipeline. If the pipeline expects events as its input, an implicit `load_stdin | read_json` will be prepended. If it expects bytes instead, only `load_stdin` is prepended. Likewise, if the pipeline outputs events, an implicit `write_json | save_stdout` will be appended. If it outputs bytes instead, only `save_stdout` is appended. The diagram below illustrates these mechanics: ![Pipeline on the command line](/_astro/pipeline-cli.BkLsRsJs_19DKCs.svg) For example, run `tenzir 'version | drop dependencies'` to see a single event in the terminal: ```tql { version: "5.0.1+g847fcc6334", tag: "g847fcc6334", major: 5, minor: 0, patch: 1, features: [ "chart_limit", "modules", "tql2_from", "exact_schema", "tql2_only", ], build: { type: "Release", tree_hash: "ef28a81eb124cc46a646250d1fb17390", assertions: false, sanitizers: { address: false, undefined_behavior: false, }, }, } ``` You could also render the output differently by choosing a different format: ```sh tenzir 'version | drop dependencies | write_csv' tenzir 'version | drop dependencies | write_ssv' tenzir 'version | drop dependencies | write_parquet | save_file "version.parquet' ``` Instead of passing the pipeline description to the `tenzir` executable, you can also load the definition from a file via `-f`: ```sh tenzir -f pipeline.tql ``` This will interpret the file contents as pipeline and run it. ## As Code [Section titled ‚ÄúAs Code‚Äù](#as-code) In addition to running pipelines interactively, you can also deploy *pipelines as code (PaC)*. This infrastructure-as-code-like method differs from the app-based deployment in two ways: 1. Pipelines deployed as code always start with the Tenzir node, ensuring continuous operation. 2. To safeguard them, deletion via the user interface is disallowed. Here‚Äôs a an example of deploying a pipeline through your configuration: \<prefix>/etc/tenzir/tenzir.yaml ```yaml tenzir: pipelines: # A unique identifier for the pipeline that's used for metrics, diagnostics, # and API calls interacting with the pipeline. suricata-over-tcp: # An optional user-facing name for the pipeline. Defaults to the id. name: Onboard Suricata from TCP # An optional user-facing description of the pipeline. description: | Onboards Suricata EVE JSON from TCP port 34343. # The definition of the pipeline. Configured pipelines that fail to start # cause the node to fail to start. definition: | load_tcp "0.0.0.0:34343" read_suricata publish "suricata" # Pipelines that encounter an error stop running and show an error state. # This option causes pipelines to automatically restart when they # encounter an error instead. The first restart happens immediately, and # subsequent restarts after the configured delay, defaulting to 1 minute. # The following values are valid for this option: # - Omit the option, or set it to null or false to disable. # - Set the option to true to enable with the default delay of 1 minute. # - Set the option to a valid duration to enable with a custom delay. restart-on-error: 1 minute # Add a list of labels that are shown in the pipeline overview page at # app.tenzir.com. labels: - Suricata - Onboarding # Disable the pipeline. disabled: false # Pipelines that are unstoppable will run automatically and indefinitely. # They are not able to pause or stop. # If they do complete, they will end up in a failed state. # If `restart-on-error` is enabled, they will restart after the specified # duration. unstoppable: true ```

# Build Environment

## Use Nix as reproducible development environment [Section titled ‚ÄúUse Nix as reproducible development environment‚Äù](#use-nix-as-reproducible-development-environment) We use [Nix](https://nixos.org) for reproducible Tenzir Node builds. Fetch the dependencies for a dynamic build by running `nix develop` from the topmost directory in the `tenzir/tenzir` source tree. You can automatically add the dependencies to your shell environment when you cd into the source directory via [direnv](https://direnv.net). Create an `.envrc` with the content: ```plaintext use flake ``` If you want to silence the messages about binary caches you can use a variation of `.envrc` that invokes `nix` with a lower verbosity setting: ```sh use_flake2() { watch_file flake.nix watch_file flake.lock mkdir -p "$(direnv_layout_dir)" eval "$(nix --quiet --quiet print-dev-env --profile "$(direnv_layout_dir)/flake-profile" "$@")" } use_flake2 ``` The `tenzir/tenzir` repository comes with a set of CMake configure and build presets that can be used in this environment: * `nix-clang-debug` * `nix-clang-redeb` * `nix-clang-release` * `nix-gcc-debug` * `nix-gcc-redeb` * `nix-gcc-release` ### Compile static binaries [Section titled ‚ÄúCompile static binaries‚Äù](#compile-static-binaries) Static binaries require a that the dependencies were built in static mode as well. That means we need to use a different environment; you can enter it with: ```sh nix develop .#tenzir-static ``` The CMake presets for that mode are: * `nix-gcc-static-debug` * `nix-gcc-static-redeb` * `nix-gcc-static-release`

# Code of Conduct

## Our Pledge [Section titled ‚ÄúOur Pledge‚Äù](#our-pledge) In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. ## Our Standards [Section titled ‚ÄúOur Standards‚Äù](#our-standards) Examples of behavior that contributes to creating a positive environment include: * Using welcoming and inclusive language * Being respectful of differing viewpoints and experiences * Gracefully accepting constructive criticism * Focusing on what is best for the community * Showing empathy towards other community members Examples of unacceptable behavior by participants include: * The use of sexualized language or imagery and unwelcome sexual attention or advances * Trolling, insulting/derogatory comments, and personal or political attacks * Public or private harassment * Publishing others‚Äô private information, such as a physical or electronic address, without explicit permission * Other conduct which could reasonably be considered inappropriate in a professional setting ## Our Responsibilities [Section titled ‚ÄúOur Responsibilities‚Äù](#our-responsibilities) Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. ## Scope [Section titled ‚ÄúScope‚Äù](#scope) This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. ## Enforcement [Section titled ‚ÄúEnforcement‚Äù](#enforcement) Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by [contacting the project team](https://docs.tenzir.com/discord). All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project‚Äôs leadership. ## Attribution [Section titled ‚ÄúAttribution‚Äù](#attribution) This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org), version 1.4, available at <https://www.contributor-covenant.org/version/1/4/code-of-conduct.html>. For answers to common questions about this code of conduct, see <https://www.contributor-covenant.org/faq>.

# Coding Style

This page documents the coding style for the languages we use. ## Documentation [Section titled ‚ÄúDocumentation‚Äù](#documentation) When documenting bugs, deficiencies, future tasks, or noteworthy things in the code, we use two keywords that most editors and tools recognize: `FIXME:` and `TODO:`. We use `FIXME` for a *known bug* and `TODO` for everything else. The subsequent `:` is important for tooling, such as syntax highlighters. Here are two examples: ```cpp // FIXME: this currently fails on FreeBSD. // FIXME: this algorithms is broken for i < 0. ``` A typical `TODO` could be: ```python # TODO: refactor this code to separate mechanism from policy. # TODO: add another argument to process user-defined tags. ``` ## EditorConfig [Section titled ‚ÄúEditorConfig‚Äù](#editorconfig) * Some projects in the Tenzir organization provide `.editorconfig` files. Please respect the settings defined in these. For many editors, plugins exist to automatically apply EditorConfig files. ## Scripting Languages [Section titled ‚ÄúScripting Languages‚Äù](#scripting-languages) * Scripts are executables (`chmod +x path/to/your-script`) and words in their names are separated using dashes (`your-script` over `your_script`). * The first line of a script should be a shebang, e.g., `'#!/bin/sh'` or `#!/usr/bin/env python3`. * The second line is empty. * Starting at the third line, write a comment detailing usage instructions, and a short and concise description of the script. ### Shell Scripts [Section titled ‚ÄúShell Scripts‚Äù](#shell-scripts) * Prefer to use POSIX sh when possible. * Tenzir uses [ShellCheck](https://github.com/koalaman/shellcheck) for linting. Pull request review feedback for shell scripts is in parts based on ShellCheck. ### Python [Section titled ‚ÄúPython‚Äù](#python) * We use Python 3, with no special restrictions for newer features. Specify the minimum required version in the shebang, e.g. `#!/usr/bin/env python3.6`. * Use [black](https://github.com/psf/black) for linting. Black is a heavily opinionated tool for both formatting and linting, and we found its opinion to be a good standard for us to use. ## Web Development [Section titled ‚ÄúWeb Development‚Äù](#web-development) * All web-based projects in the Tenzir organization define style checkers and linters in their respective configuration files, so they are automatically applied. ## CMake [Section titled ‚ÄúCMake‚Äù](#cmake) CMake is the build scaffold of Tenzir. ### General [Section titled ‚ÄúGeneral‚Äù](#general) * Prefer targets and properties over variables. * Don‚Äôt use global *include\_directories*. * Export consumable targets to both build and install directories. * Assign sensible export names for your targets, the `tenzir::` namespace is implicitly prefixed. ### Formatting [Section titled ‚ÄúFormatting‚Äù](#formatting) * The cmake files are formatted with [cmake-format](https://github.com/cheshirekow/cmake_format). ## C++ [Section titled ‚ÄúC++‚Äù](#c) Tenzir‚Äôs core is written in C++. We follow a style based on STL, [Google style](http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml), and [CAF style](https://github.com/actor-framework/actor-framework/blob/master/CONTRIBUTING.md) guidelines. ### General [Section titled ‚ÄúGeneral‚Äù](#general-1) * Minimize vertical whitespace within functions. Use comments to separate logical code blocks. * The `const` keyword precedes the type, e.g., `const T&` as opposed to `T const&`. * `*` and `&` bind to the *type*, e.g., `T* arg` instead of `T *arg`. * When declaring variables and functions, provide the [storage class specifier](https://en.cppreference.com/w/cpp/language/storage_duration) (`extern`, `static`, `thread_local`, `mutable`) first, followed by the [declaration specifiers](https://en.cppreference.com/w/cpp/language/declarations#Specifiers) in order of `friend`, `inline`, `virtual`, `explicit`, `constexpr`, `consteval`, and `constinit`. - Always use `auto` to declare a variable unless you cannot initialize it immediately or if you actually want a type conversion. In the latter case, provide a comment why this conversion is necessary. - Never use unwrapped, manual resource management such as `new` and `delete`. - Never use `typedef`; always write `using T = X` in favor of `typedef X T`. - Keywords are always followed by a whitespace: `if (...)`, `template <...>`, `while (...)`, etc. - Do not add whitespace when negating an expression with `!`: ```cpp if (!sunny()) stay_home() ``` - Opening braces belong onto the same line: ```cpp struct foo { void f() { } }; ``` - Use inline functions for trivial code, such as getters/setters or straight-forward logic that does not span more than 3 lines. ### Header [Section titled ‚ÄúHeader‚Äù](#header) * Header filenames end in `.hpp` and implementation filenames in `.cpp`. * All header files should use `#pragma once` to prevent multiple inclusion. * Don‚Äôt use `#include` when a forward declarations suffices. It can make sense to outsource forward declarations into a separate file per module. The file name should be `<MODULE>/fwd.h`. * Include order is from high-level to low-level headers, e.g., ```cpp // iff a matching header exists #include "tenzir/matching_header.hpp" #include "tenzir/logger.hpp" #include <3rd/party.hpp> #include <memory> #include <sys/types.h> ``` `clang-format` is configured to automatically change the include order accordingly. Includes separated by preprocessor directives need to be sorted manually. Within each section, the order should be alphabetical. Tenzir includes should always be in double quotes and relative to the source directory, whereas system-wide includes in angle brackets. See below for an example on how to structure includes in unit tests. * As in the standard library, the order of parameters when declaring a function is: inputs, then outputs. API coherence and symmetry trumps this rule, e.g., when the first argument of related functions model the same concept. ### Classes [Section titled ‚ÄúClasses‚Äù](#classes) * Use the order `public`, `protected`, `private` for functions and members in classes. * Mark single-argument constructors as `explicit` to avoid implicit conversions; use `explicit(false)` to indicate that a non-explicit constructor is intentional. * The order of member functions within a class is: constructors, operators, mutating members, accessors. * Friends first: put friend declaration immediate after opening the class. * Put declarations (and/or definitions) of assignment operators right after the constructors, and all other operators at the bottom of the public section. * Use structs for state-less classes or when the API is the struct‚Äôs state. * Prefer types with value semantics over reference semantics. * Use the [rule of zero or rule of five](http://en.cppreference.com/w/cpp/language/rule_of_three). * When providing a move constructor and move-assignment operator, declare them as `noexcept`. * Use brace-initialization for member construction when possible. Only use parenthesis-initialization to avoid calling a `std::initializer_list` overload. ### Naming [Section titled ‚ÄúNaming‚Äù](#naming) * Class names, constants, and function names are lowercase with underscores. * Template parameter types should be written in CamelCase. * Types and variables should be nouns, while functions performing an action should be ‚Äúcommand‚Äù verbs. Getter and setter functions should be nouns. We do not use an explicit `get_` or `set_` prefix. Classes used to implement metaprogramming functions also should use verbs, e.g., `remove_const`. * All library macros should start with `TENZIR_` to avoid potential clashes with external libraries. * Names of (i) classes/structs, (ii) functions, and (iii) enums should be lower case and delimited by underscores. * Put non-API implementation into namespace `detail`. * Member variables have an underscore (`_`) as suffix, unless they constitute the public interface. Getters and setters use the same member name without the suffix. * Put static non-const variables in an anonymous namespace. * Name generic temporary or input variables `x`, `y`, and `z`. If such variables represent a collection of elements, use their plural form `xs`, `ys`, and `zs`. * Prefix counter variables with `num_`. * If a function has a return value, use `result` as variable name. ### Breaking [Section titled ‚ÄúBreaking‚Äù](#breaking) * Break constructor initializers after the comma, use two spaces for indentation, and place each initializer on its own line (unless you don‚Äôt need to break at all): ```cpp my_class::my_class() : my_base_class{some_function()}, greeting_{"Hello there! This is my_class!"}, some_bool_flag_{false} { // ok } other_class::other_class() : name_{"tommy"}, buddy_{"michael"} { // ok } ``` * Break function arguments after the comma for both declaration and invocation: ```cpp a_rather_long_return_type f(const std::string& x, const std::string& y) { // ... } ``` If that turns out intractable, break directly after the opening parenthesis: ```cpp template <typename T> black_hole_space_time_warp f( typename const T::gravitational_field_manager& manager, typename const T::antimatter_clustear& cluster) { // ... } ``` * Break template parameters without indentation: ```cpp template <class T> auto identity(T x) { return x; } ``` * Break trailining return types without indentation if they cannot fit on the same line: ```cpp template <class T> auto compute_upper_bound_on_compressed_data(T x) -> std::enable_if_t<std::is_integral_v<T>, T> { return detail::bound(x); } ``` * Break before binary and ternary operators: ```cpp if (today_is_a_sunny_day() && it_is_not_too_hot_to_go_swimming()) { // ... } ``` ### Template Metaprogramming [Section titled ‚ÄúTemplate Metaprogramming‚Äù](#template-metaprogramming) * Use the `typename` keyword only to access dependent types. For general template parameters, use `class` instead: ```cpp template <class T> struct foo { using type = typename T::type; }; ``` * Use `T` for generic, unconstrained template parameters and `x` for generic function arguments. Suffix both with `s` for template parameter packs: ```cpp template <class T, class... Ts> auto f(T x, Ts... xs) { // ... } ``` * Break `using name = ...` statements always directly after `=` if they do not fit in one line. * Use one level of indentation per ‚Äúopen‚Äù template and place the closing `>`, `>::type` or `>::value` on its own line. For example: ```cpp using optional_result_type = typename std::conditional< std::is_same<result_type, void>::value, bool, optional<result_type> >::type; ``` * When dealing with ‚Äúordinary‚Äù templates, use indentation based on the position of the last opening `<`: ```cpp using type = quite_a_long_template_which_needs_a_break<std::string, double>; ``` * When adding new type traits, also provide `*_t` and/or `*_v` helpers: ```cpp template <class T> using my_trait_t = typename my_trait<T>::type; template <class T> constexpr auto my_trait_v = my_trait<T>::value; ``` ### Logging [Section titled ‚ÄúLogging‚Äù](#logging) * Available log levels are `ERROR`, `WARN`, `INFO`, `VERBOSE`, `DEBUG` and `TRACE`. * Messages can be sent by using the `TENZIR_<level>` macros. * Try to restrict usage of the `TENZIR_INFO` message type to the main actors. Info is the chattiest level that most users will see, so it should require no or only little understanding of Tenzir‚Äôs system architecture for the reader to understand. * Use the `TENZIR_TRACE_SCOPE` macro to elicit an additional message at the exit of the current scope. The trace level can be used to create a trace of the call stack with fine grained control over its depth. Since the size of trace messages can quickly go out of hand, omit trace messages from helper functions and generic algorithm implementations. ### Comments [Section titled ‚ÄúComments‚Äù](#comments) * Doxygen comments start with `///`. * Use Markdown instead of Doxygen formatters. * Use `@cmd` rather than `\cmd`. * Document pre- and post-conditions with `@pre` and `@post` (where appropriate). * Reference other parameters with emphasis: ```cpp /// @param x A number between 0 and 1. /// @param y Scales *x* by a constant factor. ``` * Use `@tparam` to document template parameters. * For simple getters or obvious functions returning a value, use a one-line `@returns` statement: ```cpp /// @returns The answer. int f(); ``` * Use `//` or `/*` and `*/` to define basic comments that should not be swallowed by Doxygen. ### External Files [Section titled ‚ÄúExternal Files‚Äù](#external-files) When integrating 3rd-party code into the code base, use the following scaffold: ```cpp // _ _____ __________ // | | / / _ | / __/_ __/ Visibility // | |/ / __ |_\ \ / / Across // |___/_/ |_/___/ /_/ Space and Time // // SPDX-FileCopyrightText: (c) 2022 The Tenzir Contributors // SPDX-License-Identifier: BSD-3-Clause // // This file comes from a 3rd party and has been adapted to fit into the Tenzir // code base. Details about the original file: // // - Repository: https://github.com/Microsoft/GSL // - Commit: d6b26b367b294aca43ff2d28c50293886ad1d5d4 // - Path: GSL/include/gsl/gsl_byte // - Author: Microsoft // - Copyright: (c) 2015 Microsoft Corporation. All rights reserved. // - License: MIT (code here) ``` ### Unit Tests [Section titled ‚ÄúUnit Tests‚Äù](#unit-tests) * Every new feature must come with unit tests. * The filename and path should mirror the component under test. For example, the component `tenzir/detail/feature.hpp` should have a test file called `test/detail/feature.cpp`. * The include order in unit tests resembles the order for standard headers, except that unit test includes and the suite definition comes at the top. * Make judicious use of *fixtures* for prepping your test environment. * The snippet below illustrates a simple example for a new component `tenzir/foo.hpp` that would go into `test/foo.cpp`. ```cpp // _ _____ __________ // | | / / _ | / __/_ __/ Visibility // | |/ / __ |_\ \ / / Across // |___/_/ |_/___/ /_/ Space and Time // // SPDX-FileCopyrightText: (c) 2022 The Tenzir Contributors // SPDX-License-Identifier: BSD-3-Clause #define SUITE foo #include "tenzir/foo.hpp" // Unit under test #include "test.hpp" // Unit test framework and scaffolding #include <iostream> // standard library includes #include <caf/...> // CAF includes #include "tenzir/..." // Tenzir includes using namespace tenzir; namespace { struct fixture { fixture() { // Setup context = 42; } ~fixture() { // Teardown context = 0; } int context; }; } // namespace <anonymous> FIXTURE_SCOPE(foo_tests, fixture) TEST(construction) { MESSAGE("default construction"); foo x; MESSAGE("assignment"); x = 42; CHECK_EQUAL(x, context); } FIXTURE_SCOPE_END() ``` ### Continuous Integration [Section titled ‚ÄúContinuous Integration‚Äù](#continuous-integration) We use GitHub Actions to build and test each commit. Merging a pull request requires that all checks pass for the latest commit in the branch. GitHub displays the status of the individual checks in the pull request. ### Code Coverage [Section titled ‚ÄúCode Coverage‚Äù](#code-coverage) The GitHub Actions workflow [Analysis](https://github.com/tenzir/tenzir/actions/workflows/analysis.yaml) contains a *Code Coverage* job that runs unit tests for libtenzir and bundled plugins, and integration tests for Tenzir with bundled plugins to create a detailed line coverage report. The CI creates and uploads reports as an artifact in the Analysis workflow as part of every pull request and for every merge to master. In addition to the local report, the workflow uploads the coverage report to [Codecov](https://app.codecov.io/gh/tenzir/tenzir), which offers a visual interface for seeing coverage changes introduced by code changes: [![Codecov Report](https://codecov.io/gh/tenzir/tenzir/branch/master/graphs/tree.svg?token=T9JgpY4KHO)](https://app.codecov.io/gh/tenzir/tenzir) Each block represents a single file in the project. The size and color of each block is represented by the number of statements and the coverage, respectively. Codecov offers also a [sunburst](https://codecov.io/gh/tenzir/tenzir/branch/master/graphs/sunburst.svg?token=T9JgpY4KHO) and [icicle](https://codecov.io/gh/tenzir/tenzir/branch/master/graphs/sunburst.svg?token=T9JgpY4KHO) graph, visualizing the same data with a different approach. To generate a coverage report locally, create a new Debug build of Tenzir with the CMake option `-D TENZIR_ENABLE_CODE_COVERAGE=ON` and run the `ccov` build target. This creates a coverage report in `<path/to/build-dir>/ccov`.

# Documentation

The source code of the Tenzir documentation is at <https://github.com/tenzir/docs>. We use [Astro](https://astro.build) with [Starlight](https://starlight.astro.build) as our site framework. ## Quick Reference [Section titled ‚ÄúQuick Reference‚Äù](#quick-reference) ### Essential Commands [Section titled ‚ÄúEssential Commands‚Äù](#essential-commands) | Command | Action | | :------------- | :----------------------------------------- | | `pnpm install` | Install dependencies | | `pnpm dev` | Start local dev server at `localhost:4321` | | `pnpm build` | Build production site to `./dist/` | | `pnpm preview` | Preview build locally before deploying | ### Development Commands [Section titled ‚ÄúDevelopment Commands‚Äù](#development-commands) | Command | Action | | :--------------------- | :----------------------------------------------- | | `pnpm lint:markdown` | Lint all Markdown files | | `pnpm lint:linkcheck` | Validate all internal and external links | | `pnpm lint:prettier` | Check code formatting | | `pnpm astro ...` | Run CLI commands like `astro add`, `astro check` | | `pnpm astro -- --help` | Get help using the Astro CLI | For anything beyond editing Markdown content, check out [Starlight‚Äôs docs](https://starlight.astro.build/) or read [Astro‚Äôs documentation](https://docs.astro.build). For git workflow, branching strategy, and commit message conventions, see our [Git and GitHub Workflow](/guides/contribution/workflow) guide. ## Local Development [Section titled ‚ÄúLocal Development‚Äù](#local-development) ### First-Time Setup [Section titled ‚ÄúFirst-Time Setup‚Äù](#first-time-setup) 1. **Clone the repository**: ```bash git clone https://github.com/tenzir/docs.git cd docs ``` 2. **Install dependencies**: ```bash pnpm install ``` 3. **Start development server**: ```bash pnpm dev ``` 4. **View the site**: Browse to `http://localhost:4321/` The development server includes: * üî• Hot module reloading for instant updates * üìù Live Markdown rendering * üîç Error reporting in the browser ### Production Build [Section titled ‚ÄúProduction Build‚Äù](#production-build) Create and preview a production build: ```bash # Build the site pnpm build # Preview the build pnpm preview ``` Then browse to `http://localhost:4321/` to view the production site. ## Link Checking [Section titled ‚ÄúLink Checking‚Äù](#link-checking) The documentation includes automated link validation to ensure all internal and external links work correctly: ### Local Link Checking [Section titled ‚ÄúLocal Link Checking‚Äù](#local-link-checking) Before submitting changes, run link checking locally to catch broken links: ```bash pnpm lint:linkcheck ``` This will build the site with link validation enabled and report any broken links found. ### CI Integration [Section titled ‚ÄúCI Integration‚Äù](#ci-integration) Link checking runs automatically in our CI pipeline: * **All builds**: Link validation is enabled for production builds * **Pull requests**: Link checking runs as part of the lint workflow * **Scheduled maintenance**: Weekly link checks run every Sunday to catch broken external links ### How It Works [Section titled ‚ÄúHow It Works‚Äù](#how-it-works) The link checker validates: * Internal page references (e.g., `/guides/quickstart`) * Anchor links within pages (e.g., `/reference/operators#aggregate`) * External URLs (with appropriate timeout and retry logic) * Relative links between documentation files ### Fixing Broken Links [Section titled ‚ÄúFixing Broken Links‚Äù](#fixing-broken-links) When the link checker finds issues: 1. **Invalid internal links**: Update the link to point to the correct page path 2. **Missing anchor references**: Ensure the target heading or element exists 3. **Broken external links**: Update URLs or remove outdated references 4. **False positives**: Add exclusions to the `starlightLinksValidator` configuration in `astro.config.mjs` The link checker will cause builds to fail if broken links are detected, ensuring the documentation maintains high quality. ## Optimize Images [Section titled ‚ÄúOptimize Images‚Äù](#optimize-images) To keep the repository size manageable, always optimize image files *before* committing them. This is especially important for formats like PNG, which can contain unnecessary metadata or use inefficient compression. ### PNG Images [Section titled ‚ÄúPNG Images‚Äù](#png-images) We recommend using [pngquant](https://pngquant.org/), a command-line utility for lossy compression of PNG files. It significantly reduces file size while preserving image quality. To compress a PNG file in-place: ```bash pngquant --ext .png --force --quality=65-80 image.png ``` ### JPEG Images [Section titled ‚ÄúJPEG Images‚Äù](#jpeg-images) Use [jpegoptim](https://github.com/tjko/jpegoptim), a utility for optimizing JPEGs without perceptible quality loss: ```bash jpegoptim --strip-all --max=80 image.jpg ``` Alternatively, you can use [mozjpeg](https://github.com/mozilla/mozjpeg) for even better compression ratios. ### SVG Images [Section titled ‚ÄúSVG Images‚Äù](#svg-images) Use [svgo](https://github.com/svg/svgo), a Node-based tool to optimize SVG files by removing unnecessary data: ```bash svgo image.svg ``` This automatically rewrites the file in-place with redundant code removed and optimized structure. ## Auto-Updated Files [Section titled ‚ÄúAuto-Updated Files‚Äù](#auto-updated-files) **Important**: Some files in this repository are automatically updated by CI and should **never** be manually edited. Manual changes to these files will be overwritten the next time the [update workflow](https://github.com/tenzir/docs/blob/main/.github/workflows/update.yaml) runs. ### Automatically Updated Files [Section titled ‚ÄúAutomatically Updated Files‚Äù](#automatically-updated-files) The following files are synchronized from upstream repositories: **Tenzir Node Documentation:** * `src/content/docs/changelog/node` is generated from individual changelog entries * `src/content/docs/reference/functions` is synced from [tenzir/tenzir](https://github.com/tenzir/tenzir) at `docs/functions/` * `src/content/docs/reference/operators` is synced from [tenzir/tenzir](https://github.com/tenzir/tenzir) at `docs/operators/` * `src/content/docs/reference/functions.mdx` is generated from individual function files * `src/content/docs/reference/operators.mdx` is generated from individual operator files * `src/content/apis/openapi.node.yaml` is the API specification for Tenzir Node and generated from the node‚Äôs source code * `tenzir.yaml.example` is the node‚Äôs example configuration file **Tenzir Platform Documentation:** * `src/content/docs/changelog/platform` is generated from individual changelog entries * `src/content/docs/reference/platform-cli.mdx` is synced from [tenzir/platform](https://github.com/tenzir/platform) at `docs/platform-cli.mdx` ### Making Changes to Auto-Updated Content [Section titled ‚ÄúMaking Changes to Auto-Updated Content‚Äù](#making-changes-to-auto-updated-content) If you need to update content in these files: 1. **For Functions/Operators**: Make changes in the [tenzir/tenzir](https://github.com/tenzir/tenzir) repository 2. **For Platform CLI**: Make changes in the [tenzir/platform](https://github.com/tenzir/platform) repository 3. **For generation logic**: Modify the scripts in `scripts/` or the update workflow Our CI will automatically prevent pull requests that modify auto-updated files. If you encounter this error, revert your changes and make them in the appropriate upstream repository instead. ## Edit Diagrams [Section titled ‚ÄúEdit Diagrams‚Äù](#edit-diagrams) We use [Excalidraw](https://excalidraw.com) as primary tool to create sketches of architectural diagrams. When exporting Excalidraw scenes, always **use light mode** and **choose SVG** as export format, as we have some CSS magic in place that automatically inverts colors SVGs so that they also look nicely when using dark mode. Tenzir developers have access to our Excalidraw [Documentation collection](https://app.excalidraw.com/o/6dBWEFf9h1l/9RErQkL9e2v). For everyone else, please reach out to us on our [Discord server](/discord) to request changes to existing SVGs. ## Writing Style and Formatting [Section titled ‚ÄúWriting Style and Formatting‚Äù](#writing-style-and-formatting) This section covers the editorial and technical standards for contributing to the Tenzir documentation. ### Writing Guidelines [Section titled ‚ÄúWriting Guidelines‚Äù](#writing-guidelines) We follow the [Google Style Guide](https://developers.google.com/style) for clear and consistent technical documentation. Most notably: * Use **active voice** in general. * Avoid anthropomorphic language‚Äîdon‚Äôt attribute human qualities to software or hardware. * Include definite and indefinite articles (a, an, and the) in your writing. Don‚Äôt skip articles for brevity, including in headings and titles. * In document titles and headings, use **sentence case**. Capitalize only the first word in the title, the first word in a subheading after a colon, and any proper nouns or other terms that are always capitalized a certain way. * Capitalize product names. * Write documentation in an **informal tone**‚Äîuse common two-word contractions such as ‚Äúyou‚Äôre,‚Äù ‚Äúdon‚Äôt,‚Äù and ‚Äúthere‚Äôs.‚Äù * **Define technical terms and acronyms** on first use. Don‚Äôt assume readers know specialized vocabulary. * **Put the most important information first**. Don‚Äôt bury key details in the middle of paragraphs. * **Use consistent terminology** throughout. Don‚Äôt use different words for the same concept (e.g., don‚Äôt alternate between ‚Äúnode‚Äù and ‚Äúinstance‚Äù). * **Avoid unclear pronouns** like ‚Äúit,‚Äù ‚Äúthis,‚Äù or ‚Äúthat‚Äù without clear antecedents. Be explicit about what you‚Äôre referring to. * **Choose strong, specific verbs** over weak ones. Use ‚Äúuse‚Äù instead of ‚Äúutilize,‚Äù ‚Äúhelp‚Äù instead of ‚Äúfacilitate,‚Äù and ‚Äúshow‚Äù instead of ‚Äúdemonstrate.‚Äù * **Eliminate redundant phrases**. Write ‚Äúto‚Äù instead of ‚Äúin order to,‚Äù ‚Äúnow‚Äù instead of ‚Äúat this point in time,‚Äù and ‚Äúbecause‚Äù instead of ‚Äúdue to the fact that.‚Äù * **Avoid vague qualifiers** like ‚Äúsimply,‚Äù ‚Äújust,‚Äù ‚Äúeasily,‚Äù or ‚Äúobviously.‚Äù These don‚Äôt add clarity and may frustrate readers who find the task difficult. * **Provide context** for why something matters. Don‚Äôt just explain what to do‚Äî explain when and why to do it. * **Use hyperlinks judiciously**. Link to external tools, products, or resources when first mentioned, but avoid overlinking within the same document. ### Formatting Standards [Section titled ‚ÄúFormatting Standards‚Äù](#formatting-standards) Follow these conventions to maintain consistency across all documentation files. #### General [Section titled ‚ÄúGeneral‚Äù](#general) * Every file must end with a newline character, but avoid empty lines at the end of a file. #### Markdown Content [Section titled ‚ÄúMarkdown Content‚Äù](#markdown-content) * Break lines at **80 characters**. * When editing Markdown, run `pnpm lint:markdown:fix` and `pnpm lint:prettier:fix` when you‚Äôre done. #### Code [Section titled ‚ÄúCode‚Äù](#code) * Avoid empty lines within functions. * When editing source code (`.js`, `.jsx`, `.ts`, `.tsx`, `.astro` files), run `pnpm lint:eslint:fix` when you‚Äôre done.

# Security Policy

Security is a serious matter for us. We want to ensure and maintain a secure environment for our customers and the open-source community. ## Reporting a Vulnerability [Section titled ‚ÄúReporting a Vulnerability‚Äù](#reporting-a-vulnerability) We are eager to work with the community to resolve security vulnerabilities within our tech stack in a timely manner and to properly acknowledge the contributor(s). Please do not publicly disclose a vulnerability until we have an opportunity to review and address the issue. Follow these steps to report a vulneratbility: 1. [Open a security advisory](https://help.github.com/en/articles/creating-a-maintainer-security-advisory), which is visible to project maintainers only. Please do *not* submit a normal issue or pull request in our public repositories. 2. We will confirm the receipt of the report within two business days. (It make take additional time time to resolve the issue.) 3. If you already have a patch, we will review it and approve it privately; once merged it will be publicly disclosed. We will acknowledge you in our changelog. 4. In case we need additional information during the investigation, we will be actively reaching out. Please do not publicly mention the security issue until after we have updated the public repository so that other downstream users have an opportunity to patch their software. ## Contact [Section titled ‚ÄúContact‚Äù](#contact) If you have any questions, please contact us directly at [security@tenzir.com](mailto://security@tenzir.com).

# Git and GitHub Workflow

The following diagram visualizes our branching model: ![Git Branching Model](/_astro/git-branching-model.DUtyVqtp_19DKCs.svg) Our git workflow looks as follows: * The `main` branch reflects the latest state of development, and should always compile. * In case we need to release a hotfix, we use dedicated patch release branches. * The `latest` branch always points to the latest release that is not a release candidate. It exists so support a streamlined workflow for some packaging tools (e.g., Nix). * For new features or fixes, use *topic branches* that branch off `main` with a naming convention of `topic/description`. After completing work in a topic branch, check the following steps to prepare for a merge back into `main`: * Squash your commits such that each commit reflects a self-contained change. You can interactively rebase all commits in your current pull request with `git rebase --interactive $(git merge-base origin/main HEAD)`. * Create a pull request to `main` on GitHub. * Wait for the results of continuous integration tools and fix any reported issues. * Ask a maintainer to review your work when your changes merge cleanly. If you don‚Äôt want a specific maintainer‚Äôs feedback, ask for a team review from [tenzir/engineering](https://github.com/orgs/tenzir/teams/engineering). * Address the feedback articulated during the review. * A maintainer will merge the topic branch into `main` after it passes the code review. * Similarly, for features or fixes relating to a specific GitHub issue, use *topic branches* that branch off `main` with a naming convention of `topic/XXX`, replacing XXX with a short description of the issue. ## Commit Messages [Section titled ‚ÄúCommit Messages‚Äù](#commit-messages) Commit messages are formatted according to [this git style guide](https://github.com/agis/git-style-guide). * The first line succinctly summarizes the changes in no more than 50 characters. It is capitalized and written in and imperative present tense: e.g., ‚ÄúFix a bug‚Äù as opposed to ‚ÄúFixes a bug‚Äù or ‚ÄúFixed a bug‚Äù. As a mnemonic, prepend ‚ÄúWhen applied, this commit will‚Äù to the commit summary and check if it builds a full sentence. * The first line does not contain a dot at the end. (Think of it as the header of the following description). * The second line is empty. * Optional long descriptions as full sentences begin on the third line, indented at 72 characters per line, explaining *why* the change is needed, *how* it addresses the underlying issue, and what *side-effects* it might have.

# Fetch data from APIs

This guide covers how to fetch data from HTTP APIs using the [`from_http`](/reference/operators/from_http) and [`http`](/reference/operators/http) operators. Whether you make simple GET requests, handle authentication, or implement pagination, the operators provide flexible HTTP client capabilities for API integration. ## Basic API Requests [Section titled ‚ÄúBasic API Requests‚Äù](#basic-api-requests) Start with these fundamental patterns for making HTTP requests to APIs. ### Simple GET Requests [Section titled ‚ÄúSimple GET Requests‚Äù](#simple-get-requests) To fetch data from an API endpoint, pass the URL as the first parameter to the `from_http` operator: ```tql from_http "https://api.example.com/data" ``` The operator makes a GET request by default and forwards the response as an event. The `from_http` operator is an input operator, i.e., it starts a pipeline. The companion operator `http` is a transformation, allowing you to specify the URL as a field by referencing an event field that contains the URL: ```tql from {url: "https://api.example.com/data"} http url ``` This pattern is useful when processing multiple URLs or when URLs are generated dynamically. Most of our subsequent examples use `from_http`, as the operator options are very similar. ### Parsing the HTTP Response Body [Section titled ‚ÄúParsing the HTTP Response Body‚Äù](#parsing-the-http-response-body) The `from_http` and `http` operators automatically determine how to parse the HTTP response body using multiple methods: 1. **URL-based inference**: The operators first check the URL‚Äôs file extension to infer both the format (JSON, CSV, Parquet, etc.) and compression type (gzip, zstd, etc.). This works just like the generic `from` operator. 2. **Header-based inference**: If the format cannot be determined from the URL, the operators fall back to using the HTTP `Content-Type` and `Content-Encoding` response headers. 3. **Manual specification**: You can always override automatic inference by providing a parsing pipeline. #### Automatic Format and Compression Inference [Section titled ‚ÄúAutomatic Format and Compression Inference‚Äù](#automatic-format-and-compression-inference) When the URL contains a recognizable file extension, the operators automatically handle decompression and parsing: ```tql from_http "https://example.org/data/events.csv.zst" ``` This automatically infers `zstd` compression and `CSV` format from the file extension, decompresses, and parses accordingly. For URLs without clear extensions, the operators use HTTP headers: ```tql from_http "https://example.org/download" ``` If the server responds with `Content-Type: application/json` and `Content-Encoding: gzip`, the operator will decompress and parse as JSON. #### Manual Format Specification [Section titled ‚ÄúManual Format Specification‚Äù](#manual-format-specification) You can manually override the parser for the response body by specifying a parsing pipeline, i.e., a pipeline that transforms bytes to events. For example, if an API returns CSV data without a proper `Content-Type`, you can specify the parsing pipeline as follows: ```tql from_http "https://api.example.com/users" { read_csv } ``` This parses the response from CSV into structured events that you can process further. Similarly, if you need to handle specific compression and format combinations that aren‚Äôt automatically detected: ```tql from_http "https://example.org/archive" { decompress_gzip read_json } ``` This explicitly specifies to decompress gzip and then parse as JSON, regardless of the URL or HTTP headers. ### POST Requests with Data [Section titled ‚ÄúPOST Requests with Data‚Äù](#post-requests-with-data) Send data to APIs by specifying the `method` parameter as ‚Äúpost‚Äù and providing the request body in the `body` parameter: ```tql from_http "https://api.example.com/users", method="post", body={"name": "John", "email": "john@example.com"} ``` Similarly, with the `http` operator you can also parameterize the entire HTTP request using event fields by referencing field values for each parameter: ```tql from { url: "https://api.example.com/users", method: "post", data: { name: "John", email: "john@example.com" } } http url, method=method, body=data ``` The operators automatically use POST method when you specify a body. ## Request Configuration [Section titled ‚ÄúRequest Configuration‚Äù](#request-configuration) Configure requests with headers, authentication, and other options for different API requirements. ### Adding Headers [Section titled ‚ÄúAdding Headers‚Äù](#adding-headers) Include custom headers by providing the `headers` parameter as a record containing key-value pairs: ```tql from_http "https://api.example.com/data", headers={ "Authorization": "Bearer " + secret("YOUR_BEARER_TOKEN") } ``` Headers help you authenticate with APIs and specify request formats. Use the [`secret`](/reference/functions/secret) function to retrieve sensitive API tokens, as in the above example. ### TLS and Security [Section titled ‚ÄúTLS and Security‚Äù](#tls-and-security) Enable TLS by setting the `tls` parameter to `true` and configure client certificates using the `certfile` and `keyfile` parameters: ```tql from_http "https://secure-api.example.com/data", tls=true, certfile="/path/to/client.crt", keyfile="/path/to/client.key" ``` Use these options when APIs require client certificate authentication. ### Timeout and Retry Configuration [Section titled ‚ÄúTimeout and Retry Configuration‚Äù](#timeout-and-retry-configuration) Configure timeouts and retry behavior by setting the `connection_timeout`, `max_retry_count`, and `retry_delay` parameters: ```tql from_http "https://api.example.com/data", timeout=10s, max_retries=3, retry_delay=2s ``` These settings help handle network issues and API rate limiting gracefully. ## Data Enrichment [Section titled ‚ÄúData Enrichment‚Äù](#data-enrichment) Use HTTP requests to enrich existing data with information from external APIs. ### Preserving Input Context [Section titled ‚ÄúPreserving Input Context‚Äù](#preserving-input-context) Keep original event data while adding API responses by specifying the `response_field` parameter to control where the response is stored: ```tql from { domain: "example.com", severity: "HIGH", api_url: "https://threat-intel.example.com/lookup", response_field: "threat_data", } http f"{api_url}?domain={domain}", response_field=response_field ``` This approach preserves your original data and adds API responses in a specific field. ### Adding Metadata [Section titled ‚ÄúAdding Metadata‚Äù](#adding-metadata) Capture HTTP response metadata by specifying the `metadata_field` parameter to store status codes and headers separately from the response body: ```tql from_http "https://api.example.com/status", metadata_field=http_meta ``` The metadata includes status codes and response headers for debugging and monitoring. ## Pagination and Bulk Processing [Section titled ‚ÄúPagination and Bulk Processing‚Äù](#pagination-and-bulk-processing) Handle APIs that return large datasets across multiple pages. ### Simple Pagination [Section titled ‚ÄúSimple Pagination‚Äù](#simple-pagination) Implement automatic pagination by providing a lambda function to the `paginate` parameter that extracts the next page URL from the response: ```tql from_http "https://api.example.com/search?q=query", paginate=(response => "next_page_url" if response.has_more) ``` The operator continues making requests as long as the pagination lambda function returns a valid URL. ### Complex Pagination Logic [Section titled ‚ÄúComplex Pagination Logic‚Äù](#complex-pagination-logic) Handle APIs with custom pagination schemes by building pagination URLs dynamically using expressions that reference response data: ```tql let $base_url = "https://api.example.com/items" from_http f"{$base_url}?page=1", paginate=(x => f"{$base_url}?page={x.page + 1}" if x.page < x.total_pages), ``` This example builds pagination URLs dynamically based on response data. ### Rate Limiting [Section titled ‚ÄúRate Limiting‚Äù](#rate-limiting) Control request frequency by configuring the `paginate_delay` parameter to add delays between requests and the `parallel` parameter to limit concurrent requests: ```tql from { url: "https://api.example.com/data", paginate_delay: 500ms, parallel: 2 } http url, paginate="next_url" if has_next, paginate_delay=paginate_delay, parallel=parallel ``` Use `paginate_delay` and `parallel` to manage request rates appropriately. ## Practical Examples [Section titled ‚ÄúPractical Examples‚Äù](#practical-examples) These examples demonstrate typical use cases for API integration in real-world scenarios. ### API Monitoring [Section titled ‚ÄúAPI Monitoring‚Äù](#api-monitoring) Monitor API health and response times: ```tql from_http "https://api.example.com/health", metadata_field=metadata select date=metadata.headers.Date.parse_time("%a, %d %b %Y %H:%M:%S %Z") latency = now() - date ``` The above example parses the `Date` header from the HTTP response via [`parse_time`](/reference/functions/parse_time) into a timestamp and then compares it to the current wallclock time using the [`now`](/reference/functions/now) function. Nit: `%T` is a shortcut for `%H:%M:%S`. ## Error Handling [Section titled ‚ÄúError Handling‚Äù](#error-handling) Handle API errors and failures gracefully in your data pipelines. ### Retry Configuration [Section titled ‚ÄúRetry Configuration‚Äù](#retry-configuration) Configure automatic retries by setting the `max_retry_count` parameter to specify the number of retry attempts and `retry_delay` to control the time between retries: ```tql from_http "https://unreliable-api.example.com/data", max_retries=5, retry_delay=2s ``` ### Status Code Handling [Section titled ‚ÄúStatus Code Handling‚Äù](#status-code-handling) Check HTTP status codes by capturing metadata and filtering based on the `code` field to handle different response types: ```tql from_http "https://api.example.com/data", metadata_field=metadata where metadata.code >= 200 and metadata.code < 300 ``` ## Best Practices [Section titled ‚ÄúBest Practices‚Äù](#best-practices) Follow these practices for reliable and efficient API integration: 1. **Use appropriate timeouts**. Set a reasonable `connection_timeout` for your use case. 2. **Implement retry logic**. Configure `max_retry_count` and `retry_delay` for handling transient failures. 3. **Respect rate limits**. Use `parallel` and `paginate_delay` to control request rates. 4. **Handle errors gracefully**. Check status codes in metadata (`metadata_field`) and implement fallback logic. 5. **Secure credentials**. Access API keys and tokens via [secrets](/explanations/secrets), not in code. 6. **Monitor API usage**. Track response times and error rates for performance. 7. **Leverage automatic format inference**. Use descriptive file extensions in URLs when possible to enable automatic format and compression detection.

# Read and watch files

This guide covers how to read files and monitor directories for new files using the [`from_file`](/reference/operators/from_file) operator. Whether you process individual files, batch process directories, or set up real-time file monitoring, [`from_file`](/reference/operators/from_file) provides a unified approach to file-based data ingestion. ## Basic File Reading [Section titled ‚ÄúBasic File Reading‚Äù](#basic-file-reading) The [`from_file`](/reference/operators/from_file) operator handles various file types and formats. Start with these fundamental patterns for reading individual files. ### Single Files [Section titled ‚ÄúSingle Files‚Äù](#single-files) To read a single file, specify the path to the [`from_file`](/reference/operators/from_file) operator: ```tql from_file "/path/to/file.json" ``` The operator automatically detects the file format from the file extension. This works for all supported formats including JSON, CSV, Parquet, and others. ### Compressed Files [Section titled ‚ÄúCompressed Files‚Äù](#compressed-files) The operator handles compressed files automatically. You need no additional configuration: ```tql from_file "/path/to/file.csv.gz" ``` Supported compression formats include gzip, bzip2, and Zstd. ### Custom Parsing [Section titled ‚ÄúCustom Parsing‚Äù](#custom-parsing) When automatic format detection doesn‚Äôt suffice, specify a custom [parsing](/reference/operators/#parsing) pipeline: ```tql from_file "/path/to/file.log" { read_syslog } ``` The parsing pipeline runs on the file content and must return events. ## Directory Processing [Section titled ‚ÄúDirectory Processing‚Äù](#directory-processing) You can process multiple files efficiently using glob patterns. This section covers batch processing and recursive directory operations. ### Processing Multiple Files [Section titled ‚ÄúProcessing Multiple Files‚Äù](#processing-multiple-files) Use glob patterns to process multiple files at once: ```tql from_file "/path/to/directory/*.csv.zst" ``` This example processes all Zstd-compressed CSV files in the specified directory. You can also use glob patterns to consume files regardless of their format: ```tql from_file "~/data/**" ``` This processes all files in the `~/data` directory and its subdirectories, automatically detecting and parsing each file format. ### Recursive Directory Processing [Section titled ‚ÄúRecursive Directory Processing‚Äù](#recursive-directory-processing) Use `**` to match files recursively through subdirectories: ```tql from_file "/path/to/directory/**.csv" ``` ### Custom Parsing for Multiple Files [Section titled ‚ÄúCustom Parsing for Multiple Files‚Äù](#custom-parsing-for-multiple-files) When you process multiple files with custom parsing, the pipeline runs separately for each file: ```tql from_file "/path/to/directory/*.log" { read_lines } ``` ## File Monitoring [Section titled ‚ÄúFile Monitoring‚Äù](#file-monitoring) Set up real-time file processing by monitoring directories for changes. These features enable continuous data ingestion workflows. ### Watch for New Files [Section titled ‚ÄúWatch for New Files‚Äù](#watch-for-new-files) Use the `watch` parameter to monitor a directory for new files: ```tql from_file "/path/to/directory/*.csv", watch=true ``` This sets up continuous monitoring, processing new files as they appear in the directory. ### Remove Files After Processing [Section titled ‚ÄúRemove Files After Processing‚Äù](#remove-files-after-processing) Combine watching with automatic file removal using the `remove` parameter: ```tql from_file "/path/to/directory/*.csv", watch=true, remove=true ``` This approach helps you implement file-based queues where the system should automatically clean up processed files. ## Cloud Storage Integration [Section titled ‚ÄúCloud Storage Integration‚Äù](#cloud-storage-integration) Access files directly from cloud storage providers using their native URLs. The operator supports major cloud platforms transparently. ### Amazon S3 [Section titled ‚ÄúAmazon S3‚Äù](#amazon-s3) Access S3 buckets directly using `s3://` URLs: ```tql from_file "s3://bucket/path/to/file.csv" ``` Glob patterns work with S3 as well: ```tql from_file "s3://bucket/data/**/*.parquet" ``` ### Google Cloud Storage [Section titled ‚ÄúGoogle Cloud Storage‚Äù](#google-cloud-storage) Access GCS buckets using `gs://` URLs: ```tql from_file "gs://bucket/path/to/file.csv" ``` Cloud storage integration uses Apache Arrow‚Äôs filesystem APIs and supports the same glob patterns and options as local files, including recursive globbing across cloud storage hierarchies. ## Common Patterns [Section titled ‚ÄúCommon Patterns‚Äù](#common-patterns) These examples demonstrate typical use cases that combine multiple features of the [`from_file`](/reference/operators/from_file) operator for real-world scenarios. ### Real-time Log Processing [Section titled ‚ÄúReal-time Log Processing‚Äù](#real-time-log-processing) Monitor a log directory and process files as they arrive: ```tql from_file "/var/log/application/*.log", watch=true { read_lines parse_json } ``` ### Batch Data Processing [Section titled ‚ÄúBatch Data Processing‚Äù](#batch-data-processing) Process all files in a data directory: ```tql from_file "/data/exports/**.parquet" ``` ### Archive Processing with Cleanup [Section titled ‚ÄúArchive Processing with Cleanup‚Äù](#archive-processing-with-cleanup) Process archived data and remove files after successful ingestion: ```tql from_file "/archive/*.csv.gz", remove=true ``` ## Migration Notes [Section titled ‚ÄúMigration Notes‚Äù](#migration-notes) Transitioning from Legacy Operators We designed the [`from_file`](/reference/operators/from_file) operator to replace the existing [`load_file`](/reference/operators/load_file), [`load_s3`](/reference/operators/load_s3), and [`load_gcs`](/reference/operators/load_gcs) operators. While we still support these legacy operators, [`from_file`](/reference/operators/from_file) provides a more unified and feature-rich approach to file ingestion. We plan to add some advanced features from the legacy operators (such as file tailing, anonymous S3 access, and Unix domain socket support) in future releases of [`from_file`](/reference/operators/from_file).

# Aggregate and summarize data

Aggregation transforms streams of events into meaningful summaries. Whether you‚Äôre calculating statistics, counting occurrences, or finding extremes, the [`summarize`](/reference/operators/summarize) operator combined with aggregation functions provides powerful data analysis capabilities. ## Understanding the summarize operator [Section titled ‚ÄúUnderstanding the summarize operator‚Äù](#understanding-the-summarize-operator) The `summarize` operator groups events and applies aggregation functions. Its syntax is: ```plaintext summarize <aggregation>, <aggregation>, ..., <group>, <group>, ... ``` Where: * Aggregations are expressions like [`sum()`](/reference/functions/sum), [`count()`](/reference/functions/count), [`mean()`](/reference/functions/mean), etc. * Groups are field names to group by ## Basic aggregations [Section titled ‚ÄúBasic aggregations‚Äù](#basic-aggregations) Start with fundamental aggregation functions on event streams. ### Count events [Section titled ‚ÄúCount events‚Äù](#count-events) Count total events and unique values with [`count()`](/reference/functions/count) and [`count_distinct()`](/reference/functions/count_distinct): ```tql from {product: "apple", price: 100, category: "fruit"}, {product: "banana", price: 250, category: "fruit"}, {product: "carrot", price: 175, category: "vegetable"}, {product: "apple", price: 120, category: "fruit"}, {product: "banana", price: 225, category: "fruit"} summarize total_count = count(), unique_products = count_distinct(product) ``` ```tql { total_count: 5, unique_products: 3 } ``` ### Sum and average [Section titled ‚ÄúSum and average‚Äù](#sum-and-average) Calculate totals and averages: ```tql from {product: "apple", price: 100, quantity: 2}, {product: "banana", price: 250, quantity: 1}, {product: "carrot", price: 175, quantity: 3}, {product: "apple", price: 120, quantity: 2}, {product: "banana", price: 225, quantity: 1} summarize total_revenue = sum(price * quantity), avg_price = mean(price), total_quantity = sum(quantity) ``` ```tql { total_revenue: 1440, avg_price: 174.0, total_quantity: 9 } ``` ### Min and max [Section titled ‚ÄúMin and max‚Äù](#min-and-max) Find extreme values with [`min()`](/reference/functions/min) and [`max()`](/reference/functions/max): ```tql from {sensor: "A", temperature: 72, timestamp: 2024-01-15T10:00:00}, {sensor: "B", temperature: 68, timestamp: 2024-01-15T10:05:00}, {sensor: "A", temperature: 75, timestamp: 2024-01-15T10:10:00}, {sensor: "B", temperature: 82, timestamp: 2024-01-15T10:15:00}, {sensor: "A", temperature: 71, timestamp: 2024-01-15T10:20:00} summarize min_temp = min(temperature), max_temp = max(temperature), earliest = min(timestamp), latest = max(timestamp) ``` ```tql { min_temp: 68, max_temp: 82, earliest: 2024-01-15T10:00:00.000000, latest: 2024-01-15T10:20:00.000000 } ``` ## Grouping data [Section titled ‚ÄúGrouping data‚Äù](#grouping-data) Group events by one or more fields to calculate aggregations per group. ### Group by single field [Section titled ‚ÄúGroup by single field‚Äù](#group-by-single-field) Calculate statistics per category: ```tql from {product: "apple", price: 100, category: "fruit"}, {product: "banana", price: 250, category: "fruit"}, {product: "carrot", price: 175, category: "vegetable"}, {product: "lettuce", price: 125, category: "vegetable"}, {product: "orange", price: 225, category: "fruit"} summarize avg_price = mean(price), item_count = count(), category ``` ```tql { avg_price: 191.66666666666666, item_count: 3, category: "fruit", } { avg_price: 150.0, item_count: 2, category: "vegetable", } ``` ### Group by multiple fields [Section titled ‚ÄúGroup by multiple fields‚Äù](#group-by-multiple-fields) Group by multiple dimensions: ```tql from {user: "alice", action: "login", duration: 45, date: "2024-01-15"}, {user: "bob", action: "login", duration: 38, date: "2024-01-15"}, {user: "alice", action: "view", duration: 12, date: "2024-01-15"}, {user: "alice", action: "login", duration: 52, date: "2024-01-16"}, {user: "bob", action: "edit", duration: 89, date: "2024-01-16"} summarize avg_duration = mean(duration), action_count = count(), user, action ``` ```tql { avg_duration: 38.0, action_count: 1, user: "bob", action: "login", } { avg_duration: 89.0, action_count: 1, user: "bob", action: "edit", } { avg_duration: 48.5, action_count: 2, user: "alice", action: "login", } { avg_duration: 12.0, action_count: 1, user: "alice", action: "view", } ``` ## Statistical functions [Section titled ‚ÄúStatistical functions‚Äù](#statistical-functions) Use statistical aggregation functions for deeper analysis. ### Percentiles and median [Section titled ‚ÄúPercentiles and median‚Äù](#percentiles-and-median) Calculate distribution statistics with [`quantile()`](/reference/functions/quantile): ```tql from {endpoint: "/api/users", latency: 120}, {endpoint: "/api/users", latency: 135}, {endpoint: "/api/users", latency: 110}, {endpoint: "/api/orders", latency: 245}, {endpoint: "/api/orders", latency: 225}, {endpoint: "/api/orders", latency: 280} summarize p50 = quantile(latency, q=0.5), p90 = quantile(latency, q=0.9), p95 = quantile(latency, q=0.95), endpoint ``` ```tql { p50: 245.0, p90: 280.0, p95: 280.0, endpoint: "/api/orders", } { p50: 120.0, p90: 135.0, p95: 135.0, endpoint: "/api/users", } ``` ### Standard deviation and variance [Section titled ‚ÄúStandard deviation and variance‚Äù](#standard-deviation-and-variance) Measure data spread with [`stddev()`](/reference/functions/stddev) and [`variance()`](/reference/functions/variance): ```tql from {server: "web1", cpu: 45}, {server: "web1", cpu: 52}, {server: "web1", cpu: 48}, {server: "web2", cpu: 85}, {server: "web2", cpu: 92}, {server: "web2", cpu: 88} summarize avg_cpu = mean(cpu), cpu_stddev = stddev(cpu), cpu_variance = variance(cpu), server ``` ```tql { avg_cpu: 48.333333333333336, cpu_stddev: 2.8674417556808622, cpu_variance: 8.222222222222145, server: "web1", } { avg_cpu: 88.33333333333333, cpu_stddev: 2.8674417556810217, cpu_variance: 8.22222222222306, server: "web2", } ``` ### Mode and distinct values [Section titled ‚ÄúMode and distinct values‚Äù](#mode-and-distinct-values) Find most common values and collect unique values with [`mode()`](/reference/functions/mode), [`distinct()`](/reference/functions/distinct), and [`count_if()`](/reference/functions/count_if): ```tql from {user: "alice", browser: "chrome", action: "login"}, {user: "bob", browser: "firefox", action: "view"}, {user: "alice", browser: "chrome", action: "edit"}, {user: "charlie", browser: "chrome", action: "login"}, {user: "alice", browser: "safari", action: "login"} summarize most_common_browser = mode(browser), unique_browsers = distinct(browser), login_count = count_if(action, x => x == "login") ``` ```tql { most_common_browser: "chrome", unique_browsers: [ "chrome", "firefox", "safari", ], login_count: 3, } ``` ### Value frequencies and entropy [Section titled ‚ÄúValue frequencies and entropy‚Äù](#value-frequencies-and-entropy) Analyze value distributions with [`value_counts()`](/reference/functions/value_counts) and [`entropy()`](/reference/functions/entropy): ```tql from {category: "A", value: 10}, {category: "B", value: 20}, {category: "A", value: 15}, {category: "B", value: 25}, {category: "C", value: 30} summarize frequencies = value_counts(category), info_entropy = entropy(category) ``` ```tql { frequencies: [ { value: "A", count: 2, }, { value: "B", count: 2, }, { value: "C", count: 1, }, ], info_entropy: 1.0549201679861442, } ``` ## Collecting values [Section titled ‚ÄúCollecting values‚Äù](#collecting-values) Use [`collect()`](/reference/functions/collect) and [`distinct()`](/reference/functions/distinct) to gather values: ```tql from {user: "alice", action: "login", timestamp: 2024-01-15T10:00:00}, {user: "bob", action: "view", timestamp: 2024-01-15T10:01:00}, {user: "alice", action: "edit", timestamp: 2024-01-15T10:02:00}, {user: "charlie", action: "login", timestamp: 2024-01-15T10:03:00}, {user: "alice", action: "logout", timestamp: 2024-01-15T10:04:00} summarize all_actions = collect(action), unique_users = distinct(user), event_count = count() ``` ```tql { all_actions: [ "login", "view", "edit", "login", "logout", ], unique_users: [ "alice", "charlie", "bob", ], event_count: 5, } ``` ### First and last values [Section titled ‚ÄúFirst and last values‚Äù](#first-and-last-values) Get boundary values with [`first()`](/reference/functions/first) and [`last()`](/reference/functions/last): ```tql from {sensor: "temp1", reading: 72, time: 2024-01-15T09:00:00}, {sensor: "temp1", reading: 75, time: 2024-01-15T10:00:00}, {sensor: "temp1", reading: 78, time: 2024-01-15T11:00:00}, {sensor: "temp2", reading: 68, time: 2024-01-15T09:00:00}, {sensor: "temp2", reading: 71, time: 2024-01-15T10:00:00} summarize first_reading = first(reading), last_reading = last(reading), avg_reading = mean(reading), sensor ``` ```tql { first_reading: 72, last_reading: 78, avg_reading: 75.0, sensor: "temp1", } { first_reading: 68, last_reading: 71, avg_reading: 69.5, sensor: "temp2", } ``` ## Boolean aggregations [Section titled ‚ÄúBoolean aggregations‚Äù](#boolean-aggregations) Use [`all()`](/reference/functions/all) and [`any()`](/reference/functions/any) for boolean checks: ```tql from {test: "unit", passed: true, duration: 45}, {test: "integration", passed: true, duration: 120}, {test: "e2e", passed: false, duration: 300}, {test: "performance", passed: true, duration: 180} summarize all_passed = all(passed), any_failed = any(not passed), total_duration = sum(duration) ``` ```tql { all_passed: false, any_failed: true, total_duration: 645 } ``` ## Practical examples [Section titled ‚ÄúPractical examples‚Äù](#practical-examples) ### Analyze API response times [Section titled ‚ÄúAnalyze API response times‚Äù](#analyze-api-response-times) ```tql from { requests: [ {endpoint: "/api/users", method: "GET", duration: 45, status: 200}, {endpoint: "/api/users", method: "POST", duration: 120, status: 201}, {endpoint: "/api/orders", method: "GET", duration: 89, status: 200}, {endpoint: "/api/users", method: "GET", duration: 38, status: 200}, {endpoint: "/api/orders", method: "GET", duration: 156, status: 500} ] } unroll requests summarize endpoint=requests.endpoint, count=count(), avg_duration=mean(requests.duration) ``` ```tql { endpoint: "/api/orders", count: 2, avg_duration: 122.5, } { endpoint: "/api/users", count: 3, avg_duration: 67.66666666666667, } ``` ### Calculate sales metrics [Section titled ‚ÄúCalculate sales metrics‚Äù](#calculate-sales-metrics) ```tql from { sales: [ {date: "2024-01-01", amount: 1200, region: "North"}, {date: "2024-01-01", amount: 800, region: "South"}, {date: "2024-01-02", amount: 1500, region: "North"}, {date: "2024-01-02", amount: 950, region: "South"}, {date: "2024-01-03", amount: 1100, region: "North"} ] } // Calculate totals by date unroll sales summarize date=sales.date, total=sum(sales.amount) ``` ```tql {date: "2024-01-01", total: 2000} {date: "2024-01-02", total: 2450} {date: "2024-01-03", total: 1100} ``` ### Monitor system health [Section titled ‚ÄúMonitor system health‚Äù](#monitor-system-health) ```tql from { metrics: [ {timestamp: "10:00", cpu: 45, memory: 62, disk: 78}, {timestamp: "10:01", cpu: 52, memory: 64, disk: 78}, {timestamp: "10:02", cpu: 89, memory: 71, disk: 79}, {timestamp: "10:03", cpu: 67, memory: 68, disk: 79}, {timestamp: "10:04", cpu: 48, memory: 65, disk: 80} ] } cpu_alert = metrics.map(m => m.cpu > 80).any() avg_memory = metrics.map(m => m.memory).mean() disk_trend = metrics.last().disk - metrics.first().disk health_summary = { cpu_max: metrics.map(m => m.cpu).max(), memory_avg: avg_memory, disk_growth: disk_trend, critical: cpu_alert } ``` ```tql { metrics: [ ... ], cpu_alert: true, avg_memory: 66.0, disk_trend: 2, health_summary: { cpu_max: 89, memory_avg: 66.0, disk_growth: 2, critical: true, }, } ``` ## Complex aggregations [Section titled ‚ÄúComplex aggregations‚Äù](#complex-aggregations) Combine multiple aggregation functions for comprehensive analysis: ```tql from {method: "GET", endpoint: "/api/users", status: 200, duration: 45}, {method: "POST", endpoint: "/api/users", status: 201, duration: 120}, {method: "GET", endpoint: "/api/orders", status: 200, duration: 89}, {method: "GET", endpoint: "/api/users", status: 200, duration: 38}, {method: "GET", endpoint: "/api/orders", status: 500, duration: 156}, {method: "DELETE", endpoint: "/api/users/123", status: 204, duration: 67} summarize request_count = count(), avg_duration = mean(duration), error_count = count_if(status, s => s >= 400), unique_endpoints = count_distinct(endpoint), method error_rate = error_count / request_count ``` ```tql { request_count: 1, avg_duration: 120.0, error_count: 0, unique_endpoints: 1, method: "POST", error_rate: 0.0, } { request_count: 1, avg_duration: 67.0, error_count: 0, unique_endpoints: 1, method: "DELETE", error_rate: 0.0, } { request_count: 4, avg_duration: 82.0, error_count: 1, unique_endpoints: 2, method: "GET", error_rate: 0.25, } ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Choose appropriate functions**: Use `mean()` for averages, `median()` for skewed data 2. **Handle empty collections**: Check if lists are empty before aggregating 3. **Consider memory usage**: Large collections can consume significant memory 4. **Combine aggregations**: Calculate multiple statistics in one pass for efficiency ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Transform collections](/guides/data-shaping/transform-collections) - Work with lists and records * [Filter and select data](/guides/data-shaping/filter-and-select-data) - Filter before aggregating * [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations

# Convert data formats

Data comes in many formats. Converting between formats is essential for integration, export, and interoperability. This guide shows you how to transform data between JSON, CSV, YAML, and other common formats using TQL‚Äôs print functions. ## Print to JSON [Section titled ‚ÄúPrint to JSON‚Äù](#print-to-json) JSON is the most common data exchange format. Use [`print_json()`](/reference/functions/print_json) to convert any data to JSON strings: ```tql from { user: { name: "Alice", age: 30, roles: ["admin", "user"], email: null, metadata: {} } } json_string = user.print_json() ``` ```tql { user: { name: "Alice", age: 30, roles: ["admin", "user"], email: null, metadata: {} }, json_string: "{\n \"name\": \"Alice\",\n \"age\": 30,\n \"roles\": [\n \"admin\",\n \"user\"\n ],\n \"email\": null,\n \"metadata\": {}\n}", json_stripped: "{\n \"name\": \"Alice\",\n \"age\": 30,\n \"roles\": [\n \"admin\",\n \"user\"\n ]\n}" } ``` The [`print_json()`](/reference/functions/print_json) function has [`write_json`](/reference/operators/write_json) as sibling operator to format the entire event stream as JSON: ```plaintext from { user: { name: "Alice", age: 30, roles: ["admin", "user"], email: null, metadata: {} } } write_json ``` ```json { "user": { "name": "Alice", "age": 30, "roles": [ "admin", "user" ], "email": null, "metadata": {} } } ``` ### Print newline-delimited JSON [Section titled ‚ÄúPrint newline-delimited JSON‚Äù](#print-newline-delimited-json) For streaming data, use [`print_ndjson()`](/reference/functions/print_ndjson): ```tql from { events: [ {type: "login", user: "alice"}, {type: "view", user: "bob"}, {type: "logout", user: "alice"} ] } ndjson = events.print_ndjson() ``` ```tql { events: [ { type: "login", user: "alice", }, { type: "view", user: "bob", }, { type: "logout", user: "alice", }, ], ndjson: "[{\"type\":\"login\",\"user\":\"alice\"},{\"type\":\"view\",\"user\":\"bob\"},{\"type\":\"logout\",\"user\":\"alice\"}]", } ``` And with the [`write_ndjson`](/reference/operators/write_ndjson) dual: ```tql from { events: [ {type: "login", user: "alice"}, {type: "view", user: "bob"}, {type: "logout", user: "alice"} ] } write_ndjson ``` ```json {"events":[{"type":"login","user":"alice"},{"type":"view","user":"bob"},{"type":"logout","user":"alice"}]} ``` ## Print to CSV [Section titled ‚ÄúPrint to CSV‚Äù](#print-to-csv) Convert tabular data to CSV with [`print_csv()`](/reference/functions/print_csv): ```tql from {name: "Alice", age: 30, city: "NYC"}, {name: "Bob", age: 25, city: "SF"}, {name: "Charlie", age: 35, city: "LA"} csv_data = this.print_csv() ``` ```tql { name: "Alice", age: 30, city: "NYC", csv_data: "Alice,30,NYC", } { name: "Bob", age: 25, city: "SF", csv_data: "Bob,25,SF", } { name: "Charlie", age: 35, city: "LA", csv_data: "Charlie,35,LA", } ``` You get an extra header with [`write_csv`](/reference/operators/write_csv) dual: ```tql from {name: "Alice", age: 30, city: "NYC"}, {name: "Bob", age: 25, city: "SF"}, {name: "Charlie", age: 35, city: "LA"} csv_data = this.print_csv() ``` ```csv name,age,city Alice,30,NYC Bob,25,SF Charlie,35,LA ``` ## Print to TSV and SSV [Section titled ‚ÄúPrint to TSV and SSV‚Äù](#print-to-tsv-and-ssv) For tab-separated and space-separated values, use [`print_tsv()`](/reference/functions/print_tsv) and [`print_ssv()`](/reference/functions/print_ssv): ```tql from { record: {id: 1, name: "Alice Smith", status: "active"} } tsv = record.print_tsv() ssv = record.print_ssv() ``` ```tql { record: { id: 1, name: "Alice Smith", status: "active", }, tsv: "1\tAlice Smith\tactive", ssv: "1 \"Alice Smith\" active", } ``` With [`write_tsv`](/reference/operators/write_tsv): ```tql from { record: {id: 1, name: "Alice Smith", status: "active"} } write_tsv ``` ```txt record.id record.name record.status 1 Alice Smith active ``` Note the additional double quotes with [`write_ssv`](/reference/operators/write_ssv) because space is overloaded as field separator. ```tql from { record: {id: 1, name: "Alice Smith", status: "active"} } write_ssv ``` ```txt record.id record.name record.status 1 "Alice Smith" active ``` ## Print to custom-separated values [Section titled ‚ÄúPrint to custom-separated values‚Äù](#print-to-custom-separated-values) If none of the existing \*SV formats meet your needs, the [`print_xsv()`](/reference/functions/print_xsv) function to customize field separator, list separator, and what to render for absent values: Use [`print_xsv()`](/reference/functions/print_xsv) for custom separators: ```tql from { item: {sku: "A001", desc: "Widget", price: 9.99, scores: [42, 84], details: null} } pipe_separated = item.print_xsv(field_separator=" ‚è∏Ô∏é ", list_separator=" ‚åò ", null_value="‚àÖ") ``` And for the entire event stream via [`write_xsv`](/reference/operators/write_xsv): ```tql { item: { sku: "A001", desc: "Widget", price: 9.99, scores: [ 42, 84, ], details: null, }, pipe_separated: "A001 ‚è∏Ô∏é Widget ‚è∏Ô∏é 9.99 ‚è∏Ô∏é 42 ‚åò 84 ‚è∏Ô∏é ‚àÖ", } ``` ```txt item.sku ‚è∏Ô∏é item.desc ‚è∏Ô∏é item.price ‚è∏Ô∏é item.scores ‚è∏Ô∏é item.details A001 ‚è∏Ô∏é Widget ‚è∏Ô∏é 9.99 ‚è∏Ô∏é 42 ‚åò 84 ‚è∏Ô∏é ‚àÖ ``` ## Print to YAML [Section titled ‚ÄúPrint to YAML‚Äù](#print-to-yaml) Convert data to YAML format with [`print_yaml()`](/reference/functions/print_yaml): ```tql from { config: { server: { host: "localhost", port: 8080, ssl: true }, features: ["auth", "api", "websocket"] } } yaml = config.print_yaml() ``` ```tql { config: {...}, yaml: "server:\n host: localhost\n port: 8080\n ssl: true\nfeatures:\n - auth\n - api\n - websocket" } ``` Turn the entire event stream into a YAML document stream via [`write_yaml`](/reference/operators/write_yaml): ```tql from { config: { server: { host: "localhost", port: 8080, ssl: true }, features: ["auth", "api", "websocket"] } } write_yaml ``` ```yaml --- config: server: host: localhost port: 8080 ssl: true features: - auth - api - websocket ... ``` ## Print key-value pairs [Section titled ‚ÄúPrint key-value pairs‚Äù](#print-key-value-pairs) Convert records to key-value format with [`print_kv()`](/reference/functions/print_kv): ```tql from { event: { timestamp: "2024-01-15T10:30:00", level: "ERROR", message: "Connection failed", code: 500 } } kv_default = event.print_kv() kv_custom = event.print_kv(value_separator=": ", field_separator=" | ") ``` ```tql { event: { timestamp: "2024-01-15T10:30:00", level: "ERROR", message: "Connection failed", code: 500, }, kv_default: "timestamp=2024-01-15T10:30:00 level=ERROR message=\"Connection failed\" code=500", kv_custom: "timestamp: 2024-01-15T10:30:00 | level: ERROR | message: Connection failed | code: 500", } ``` ## Print security formats [Section titled ‚ÄúPrint security formats‚Äù](#print-security-formats) ### CEF (Common Event Format) [Section titled ‚ÄúCEF (Common Event Format)‚Äù](#cef-common-event-format) Print security events in CEF format with [`print_cef()`](/reference/functions/print_cef): ```tql from { extension: { src: "10.0.0.1", dst: "192.168.1.1", spt: 12345, dpt: 22 } } cef = extension.print_cef( cef_version="0", device_vendor="Security Corp", device_product="Firewall", device_version="1.0", signature_id="100", name="Port Scan Detected", severity="7" ) ``` ```tql { extension: {src: "10.0.0.1", dst: "192.168.1.1", spt: 12345, dpt: 22}, cef: "CEF:0|Security Corp|Firewall|1.0|100|Port Scan Detected|7|src=10.0.0.1 dst=192.168.1.1 spt=12345 dpt=22" } ``` ### LEEF (Log Event Extended Format) [Section titled ‚ÄúLEEF (Log Event Extended Format)‚Äù](#leef-log-event-extended-format) Print in IBM QRadar‚Äôs LEEF format with [`print_leef()`](/reference/functions/print_leef): ```tql from { attributes: { srcIP: "10.0.0.5", dstIP: "192.168.1.10", action: "BLOCK" } } leef = attributes.print_leef( vendor="Security Corp", product_name="IDS", product_version="2.0", event_class_id="200" ) ``` ```tql { attributes: {srcIP: "10.0.0.5", dstIP: "192.168.1.10", action: "BLOCK"}, leef: "LEEF:2.0|Security Corp|IDS|2.0|200|srcIP=10.0.0.5|dstIP=192.168.1.10|action=BLOCK" } ``` As above, add `select leef | write_lines` to create line-based LEEF output. ## Convert between formats [Section titled ‚ÄúConvert between formats‚Äù](#convert-between-formats) Chain parsing and printing to convert between formats: ```tql from { json_data: "{\"name\":\"Alice\",\"age\":30,\"city\":\"NYC\"}" } // JSON to CSV set parsed = json_data.parse_json() set as_csv = parsed.print_csv() // JSON to YAML set as_yaml = parsed.print_yaml() // JSON to Key-Value set as_kv = parsed.print_kv() ``` ```tql { json_data: "{\"name\":\"Alice\",\"age\":30,\"city\":\"NYC\"}", parsed: { name: "Alice", age: 30, city: "NYC", }, as_csv: "Alice,30,NYC", as_yaml: "name: Alice\nage: 30\ncity: NYC", as_kv: "name=Alice age=30 city=NYC", } ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Choose appropriate formats**: * JSON for APIs and modern systems * CSV for spreadsheets and analysis * YAML for configuration files * Key-value for logging systems 2. **Handle special characters**: Be aware of how each format handles quotes, newlines, and separators 3. **Consider file size**: JSON is verbose, CSV is compact 4. **Preserve data types**: Some formats (CSV) lose type information 5. **Use proper escaping**: Let print functions handle escaping automatically ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse different formats * [Transform collections](/guides/data-shaping/transform-collections) - Prepare data for printing * [Manipulate strings](/guides/data-shaping/manipulate-strings) - Work with printed output

# Deduplicate events

The [`deduplicate`](/reference/operators/deduplicate) operator provides a powerful mechanism to remove duplicate events in a pipeline. There are numerous use cases for deduplication, such as reducing noise, optimizing costs and making threat detection and response more efficient. ## Basic deduplication [Section titled ‚ÄúBasic deduplication‚Äù](#basic-deduplication) Let‚Äôs start with a simple example to understand how deduplication works. Imagine you‚Äôre monitoring user logins and want to see only unique users, regardless of how many times they log in: ```tql from {user: "alice", action: "login", time: 1}, {user: "bob", action: "login", time: 2}, {user: "alice", action: "login", time: 3}, {user: "alice", action: "logout", time: 4} deduplicate user ``` ```tql {user: "alice", action: "login", time: 1} {user: "bob", action: "login", time: 2} ``` The operator keeps only the first occurrence of each unique value for the specified field(s). In this example: * Alice‚Äôs first login (time: 1) is kept * Bob‚Äôs login (time: 2) is kept * Alice‚Äôs second login (time: 3) is dropped because we already saw `user: "alice"` * Note that Alice‚Äôs logout (time: 4) would also be dropped with this simple deduplication ## Deduplicate by multiple fields [Section titled ‚ÄúDeduplicate by multiple fields‚Äù](#deduplicate-by-multiple-fields) Often you need more nuanced deduplication. For example, you might want to track unique user-action pairs to see each distinct activity per user: ```tql from {user: "alice", action: "login", time: 1}, {user: "bob", action: "login", time: 2}, {user: "alice", action: "login", time: 3}, {user: "alice", action: "logout", time: 4} deduplicate {user: user, action: action} ``` ```tql {user: "alice", action: "login", time: 1} {user: "bob", action: "login", time: 2} {user: "alice", action: "logout", time: 4} ``` Now we keep unique combinations of user and action: * Alice‚Äôs first login is kept (unique: alice+login) * Bob‚Äôs login is kept (unique: bob+login) * Alice‚Äôs second login is dropped (duplicate: alice+login already seen) * Alice‚Äôs logout is kept (unique: alice+logout is a new combination) This approach is useful for tracking distinct user activities rather than just unique users. ## Analyze unique host pairs [Section titled ‚ÄúAnalyze unique host pairs‚Äù](#analyze-unique-host-pairs) When investigating network incidents, you often want to identify all unique communication patterns between hosts. This example shows network connections with nested ID fields containing origin and response hosts: ```tql from {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 1024}, {id: {orig_h: "10.0.0.2", resp_h: "192.168.1.1"}, bytes: 2048}, {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 512}, {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.2"}, bytes: 256} deduplicate {orig_h: id.orig_h, resp_h: id.resp_h} ``` ```tql {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.1"}, bytes: 1024} {id: {orig_h: "10.0.0.2", resp_h: "192.168.1.1"}, bytes: 2048} {id: {orig_h: "10.0.0.1", resp_h: "192.168.1.2"}, bytes: 256} ``` The deduplication works on the extracted host pairs: * First connection (10.0.0.1 ‚Üí 192.168.1.1) is kept * Second connection (10.0.0.2 ‚Üí 192.168.1.1) is kept (different origin) * Third connection (10.0.0.1 ‚Üí 192.168.1.1) is dropped (duplicate of first) * Fourth connection (10.0.0.1 ‚Üí 192.168.1.2) is kept (different destination) Note that flipped connections (A‚ÜíB vs B‚ÜíA) are considered different pairs. This helps identify bidirectional communication patterns. ## Remove duplicate alerts [Section titled ‚ÄúRemove duplicate alerts‚Äù](#remove-duplicate-alerts) Security monitoring often generates duplicate alerts that create noise and fatigue. Here‚Äôs how to suppress repeated alerts for the same threat pattern: ```tql from {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 1}, {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 2}, {src_ip: "10.0.0.2", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 3}, {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Port Scan", time: 4} deduplicate {src: src_ip, dst: dest_ip, sig: signature} ``` ```tql {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 1} {src_ip: "10.0.0.2", dest_ip: "8.8.8.8", signature: "Suspicious DNS", time: 3} {src_ip: "10.0.0.1", dest_ip: "8.8.8.8", signature: "Port Scan", time: 4} ``` The deduplication creates a composite key from source, destination, and signature: * First ‚ÄúSuspicious DNS‚Äù from 10.0.0.1 is kept * Second identical alert (time: 2) is suppressed as a duplicate * ‚ÄúSuspicious DNS‚Äù from different source 10.0.0.2 is kept (different pattern) * ‚ÄúPort Scan‚Äù from 10.0.0.1 is kept (different signature) This approach reduces alert volume while preserving visibility into distinct threat patterns. ### Using timeout for time-based deduplication [Section titled ‚ÄúUsing timeout for time-based deduplication‚Äù](#using-timeout-for-time-based-deduplication) In production environments, you often want to suppress duplicates only within a certain time window. This ensures you don‚Äôt miss recurring issues that happen over longer periods. The `create_timeout` parameter resets the deduplication state after the specified duration: ```tql deduplicate {src: src_ip, dst: dest_ip, sig: signature}, create_timeout=1h ``` This configuration: * Suppresses duplicate alerts for the same source/destination/signature combination * Resets after 1 hour, allowing the same alert pattern through again * Helps balance noise reduction with visibility into persistent threats For example, if a host is repeatedly targeted: * 9:00 AM: First ‚ÄúPort Scan‚Äù alert is shown * 9:15 AM: Duplicate suppressed * 9:30 AM: Duplicate suppressed * 10:05 AM: Same alert shown again (timeout expired) ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Choose fields carefully**: Deduplicate on fields that truly identify unique events for your use case. Too few fields may drop important events; too many may not deduplicate effectively. 2. **Consider order**: The [`deduplicate`](/reference/operators/deduplicate) operator keeps the *first* occurrence. If you need the latest, consider using [`reverse`](/reference/operators/reverse) first: ```tql reverse | deduplicate user | reverse ``` 3. **Use timeout wisely**: For streaming data, `create_timeout` prevents memory from growing indefinitely while still reducing noise. Choose durations based on your threat detection windows. 4. **Combine with other operators**: Often you‚Äôll want to filter ([`where`](/reference/operators/where)) or transform ([`set`](/reference/operators/set)) data before deduplication to normalize keys: ```tql normalized_ip = src_ip.string() deduplicate normalized_ip ```

# Extract structured data from text

Real-world data is messy. Log lines contain embedded JSON. CSV fields hide key-value pairs. Network packets wrap multiple protocols. This guide shows you how to extract structured data from text using TQL‚Äôs parsing functions, starting simple and building up to complex scenarios. ## Parse JSON embedded in strings [Section titled ‚ÄúParse JSON embedded in strings‚Äù](#parse-json-embedded-in-strings) The most common parsing task is extracting JSON data from string fields. Let‚Äôs start with a simple example: ```tql from {message: "{\"user\": \"alice\", \"action\": \"login\", \"timestamp\": 1234567890}"} data = message.parse_json() ``` ```tql { message: "{\"user\": \"alice\", \"action\": \"login\", \"timestamp\": 1234567890}", data: { user: "alice", action: "login", timestamp: 1234567890, }, } ``` The [`parse_json()`](/reference/functions/parse_json) function converts the JSON string into a structured record. You can now access nested fields directly: ```tql from {message: "{\"user\": \"alice\", \"action\": \"login\", \"timestamp\": 1234567890}"} data = message.parse_json() user = data.user action = data.action ``` ```tql { message: "{\"user\": \"alice\", \"action\": \"login\", \"timestamp\": 1234567890}", data: {user: "alice", action: "login", timestamp: 1234567890}, user: "alice", action: "login", } ``` ## Extract key-value pairs [Section titled ‚ÄúExtract key-value pairs‚Äù](#extract-key-value-pairs) Many logs use simple key-value formats. The [`parse_kv()`](/reference/functions/parse_kv) function handles these automatically: ```tql from {log: "status=200 method=GET path=/api/users duration=45ms"} fields = log.parse_kv() ``` ```tql { log: "status=200 method=GET path=/api/users duration=45ms", fields: { status: 200, method: "GET", path: "/api/users", duration: 45ms, }, } ``` Notice how `parse_kv()` automatically: * Detects the `=` separator * Converts numeric values (status becomes 200, not ‚Äú200‚Äù) * Parses duration values (duration becomes `45ms`, not ‚Äú45ms‚Äù) ### Customize separators [Section titled ‚ÄúCustomize separators‚Äù](#customize-separators) Not all key-value pairs use `=`. Specify custom separators: ```tql from {log: "user:alice action:login time:2024-01-15"} fields = log.parse_kv(field_split=" ", value_split=":") ``` ```tql { log: "user:alice action:login time:2024-01-15", fields: { user: "alice", action: "login", time: 2024-01-15T00:00:00Z, }, } ``` ## Parse tabular data formats [Section titled ‚ÄúParse tabular data formats‚Äù](#parse-tabular-data-formats) TQL provides several functions for parsing tabular data: ### CSV (Comma-Separated Values) [Section titled ‚ÄúCSV (Comma-Separated Values)‚Äù](#csv-comma-separated-values) Use [`parse_csv()`](/reference/functions/parse_csv) for standard CSV: ```tql from {line: "alice,30,engineer,SF"} fields = line.parse_csv(header=["name", "age", "role", "location"]) ``` ```tql { line: "alice,30,engineer,SF", fields: { name: "alice", age: 30, role: "engineer", location: "SF", }, } ``` To get an array without field names, use `split()`: ```tql from {line: "alice,30,engineer,SF"} values = line.split(",") ``` ```tql { line: "alice,30,engineer,SF", values: [ "alice", "30", "engineer", "SF", ], } ``` ### TSV (Tab-Separated Values) [Section titled ‚ÄúTSV (Tab-Separated Values)‚Äù](#tsv-tab-separated-values) For tab-separated data, use [`parse_tsv()`](/reference/functions/parse_tsv): ```tql from {line: "alice\t30\tengineer"} fields = line.parse_tsv(header=["name", "age", "role"]) ``` ```tql { line: "alice\t30\tengineer", fields: { name: "alice", age: 30, role: "engineer", }, } ``` ### SSV (Space-Separated Values) [Section titled ‚ÄúSSV (Space-Separated Values)‚Äù](#ssv-space-separated-values) For space-separated data, use [`parse_ssv()`](/reference/functions/parse_ssv): ```tql from {line: "alice 30 engineer"} fields = line.parse_ssv(header=["name", "age", "role"]) ``` ```tql { line: "alice 30 engineer", fields: { name: "alice", age: 30, role: "engineer", }, } ``` ### XSV (Custom-Separated Values) [Section titled ‚ÄúXSV (Custom-Separated Values)‚Äù](#xsv-custom-separated-values) For custom separators, use [`parse_xsv()`](/reference/functions/parse_xsv): ```tql from {line: "alice|30|engineer|SF"} fields = line.parse_xsv(field_separator="|", list_separator=",", null_value="", header=["name", "age", "role", "location"]) ``` ```tql { line: "alice|30|engineer|SF", fields: { name: "alice", age: 30, role: "engineer", location: "SF", }, } ``` ## Parse YAML data [Section titled ‚ÄúParse YAML data‚Äù](#parse-yaml-data) YAML is common in configuration files. Use [`parse_yaml()`](/reference/functions/parse_yaml): ```tql from {config: "user: alice\nrole: admin\npermissions:\n - read\n - write"} data = config.parse_yaml() ``` ```tql { config: "user: alice\nrole: admin\npermissions:\n - read\n - write", data: { user: "alice", role: "admin", permissions: [ "read", "write", ], }, } ``` ## Use Grok patterns for complex formats [Section titled ‚ÄúUse Grok patterns for complex formats‚Äù](#use-grok-patterns-for-complex-formats) When data doesn‚Äôt follow simple patterns, [`parse_grok()`](/reference/functions/parse_grok) provides powerful pattern matching: ```tql from {log: "2024-01-15 10:30:45 ERROR [UserService] Failed to authenticate user alice"} parsed = log.parse_grok("%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \\[%{DATA:service}\\] %{GREEDYDATA:message}") ``` ```tql { log: "2024-01-15 10:30:45 ERROR [UserService] Failed to authenticate user alice", parsed: { timestamp: 2024-01-15T10:30:45Z, level: "ERROR", service: "UserService", message: "Failed to authenticate user alice", }, } ``` Common Grok patterns include: * `%{DATA:fieldname}` - Match any characters (non-greedy) * `%{GREEDYDATA:fieldname}` - Match any characters (greedy) * `%{NUMBER:fieldname}` - Match numbers * `%{IP:fieldname}` - Match IP addresses * `%{TIMESTAMP_ISO8601:fieldname}` - Match ISO timestamps * `%{LOGLEVEL:fieldname}` - Match log levels (ERROR, WARN, INFO, etc.) * `%{WORD:fieldname}` - Match a single word * `%{QUOTEDSTRING:fieldname}` - Match quoted strings ## Parse standard log formats [Section titled ‚ÄúParse standard log formats‚Äù](#parse-standard-log-formats) ### Syslog messages [Section titled ‚ÄúSyslog messages‚Äù](#syslog-messages) Syslog is ubiquitous in system logging. Use [`parse_syslog()`](/reference/functions/parse_syslog): ```tql from {line: "2024-01-15T10:30:45.123Z myhost myapp[1234]: User login failed"} syslog = line.parse_syslog() ``` ```tql { line: "2024-01-15T10:30:45.123Z myhost myapp[1234]: User login failed", syslog: { facility: null, severity: null, timestamp: 2024-01-15T10:30:45.123Z, hostname: "myhost", app_name: "myapp", process_id: "1234", content: "User login failed", }, } ``` ### CEF (Common Event Format) [Section titled ‚ÄúCEF (Common Event Format)‚Äù](#cef-common-event-format) For security tools using CEF, use [`parse_cef()`](/reference/functions/parse_cef): ```tql from {log: "CEF:0|Security|Firewall|1.0|100|Connection Blocked|5|src=10.0.0.1 dst=192.168.1.1 spt=12345 dpt=443"} event = log.parse_cef() ``` ```tql { log: "CEF:0|Security|Firewall|1.0|100|Connection Blocked|5|src=10.0.0.1 dst=192.168.1.1 spt=12345 dpt=443", event: { cef_version: 0, device_vendor: "Security", device_product: "Firewall", device_version: "1.0", signature_id: "100", name: "Connection Blocked", severity: "5", extension: { src: 10.0.0.1, dst: 192.168.1.1, spt: 12345, dpt: 443, }, }, } ``` ### LEEF (Log Event Extended Format) [Section titled ‚ÄúLEEF (Log Event Extended Format)‚Äù](#leef-log-event-extended-format) For IBM QRadar‚Äôs LEEF format, use [`parse_leef()`](/reference/functions/parse_leef): ```tql from {log: "LEEF:1.0|Security|Firewall|1.0|100|src=10.0.0.1|dst=192.168.1.1|spt=12345|dpt=443"} event = log.parse_leef() ``` ```tql { log: "LEEF:1.0|Security|Firewall|1.0|100|src=10.0.0.1|dst=192.168.1.1|spt=12345|dpt=443", event: { leef_version: "1.0", vendor: "Security", product_name: "Firewall", product_version: "1.0", event_class_id: "100", attributes: { src: "10.0.0.1|dst=192.168.1.1|spt=12345|dpt=443", }, }, } ``` ## Parse timestamps [Section titled ‚ÄúParse timestamps‚Äù](#parse-timestamps) Convert time strings to proper timestamp values with [`parse_time()`](/reference/functions/parse_time): ```tql from { log1: "Event at 2024-01-15", log2: "Event at 15/Jan/2024:10:30:45", log3: "Event at Mon Jan 15 10:30:45 2024" } time1 = log1.split(" at ")[1].parse_time("%Y-%m-%d") time2 = log2.split(" at ")[1].parse_time("%d/%b/%Y:%H:%M:%S") time3 = log3.split(" at ")[1].parse_time("%a %b %d %H:%M:%S %Y") ``` ```tql { log1: "Event at 2024-01-15", log2: "Event at 15/Jan/2024:10:30:45", log3: "Event at Mon Jan 15 10:30:45 2024", time1: 2024-01-15T00:00:00Z, time2: 2024-01-15T10:30:45Z, time3: 2024-01-15T10:30:45Z, } ``` ## Layer multiple parsers [Section titled ‚ÄúLayer multiple parsers‚Äù](#layer-multiple-parsers) Real-world logs often require multiple parsing steps. Let‚Äôs parse a web server log that contains syslog formatting with embedded JSON: ```tql from { line: "2024-01-15T10:30:45Z web nginx[5678]: {\"method\":\"POST\",\"path\":\"/api/login\",\"status\":401,\"duration\":\"125ms\",\"client\":\"192.168.1.100\"}" } // First, parse the syslog wrapper syslog = line.parse_syslog() // Then parse the JSON content request = syslog.content.parse_json() // Extract specific fields we care about method = request.method path = request.path status = request.status client_ip = request.client.ip() ``` ```tql { line: "2024-01-15T10:30:45Z web nginx[5678]: {\"method\":\"POST\",\"path\":\"/api/login\",\"status\":401,\"duration\":\"125ms\",\"client\":\"192.168.1.100\"}", syslog: { facility: null, severity: null, timestamp: 2024-01-15T10:30:45Z, hostname: "web", app_name: "nginx", process_id: "5678", content: "{\"method\":\"POST\",\"path\":\"/api/login\",\"status\":401,\"duration\":\"125ms\",\"client\":\"192.168.1.100\"}", }, request: { method: "POST", path: "/api/login", status: 401, duration: 125ms, client: 192.168.1.100, }, method: "POST", path: "/api/login", status: 401, client_ip: 192.168.1.100, } ``` ## Parse and transform incrementally [Section titled ‚ÄúParse and transform incrementally‚Äù](#parse-and-transform-incrementally) When dealing with complex nested data, work incrementally. Here‚Äôs a practical example with firewall logs: ```tql from { log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN" } // Step 1: Extract the basic structure parts = log.parse_grok("%{TIMESTAMP_ISO8601:time} %{DATA:device} %{DATA:action} %{GREEDYDATA:details}") ``` ```tql { log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN", parts: { time: 2024-01-15T10:30:45Z, device: "FW01", action: "BLOCK", details: "src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN", }, } ``` Now parse the details field: ```tql from { log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN" } parts = log.parse_grok("%{TIMESTAMP_ISO8601:time} %{DATA:device} %{DATA:action} %{GREEDYDATA:details}") // Step 2: Parse the key-value pairs parts.details = parts.details.parse_kv() ``` ```tql { log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN", parts: { time: 2024-01-15T10:30:45Z, device: "FW01", action: "BLOCK", details: { src: "10.0.0.5:54321", dst: "93.184.216.34:443", proto: "TCP", flags: "SYN", }, } } ``` Finally, parse the IP:port combinations: ```tql from { log: "2024-01-15 10:30:45 FW01 BLOCK src=10.0.0.5:54321 dst=93.184.216.34:443 proto=TCP flags=SYN" } parts = log.parse_grok("%{TIMESTAMP_ISO8601:time} %{DATA:device} %{DATA:action} %{GREEDYDATA:details}") parts.details = parts.details.parse_kv() // Step 3: Split IP:port combinations src_parts = parts.details.src.split(":") dst_parts = parts.details.dst.split(":") // Step 4: Create clean output this = { timestamp: parts.time, device: parts.device, action: parts.action, src_ip: src_parts[0].ip(), src_port: src_parts[1].int(), dst_ip: dst_parts[0].ip(), dst_port: dst_parts[1].int(), protocol: parts.details.proto, flags: parts.details.flags } ``` ```tql { timestamp: 2024-01-15T10:30:45Z, device: "FW01", action: "BLOCK", src_ip: 10.0.0.5, src_port: 54321, dst_ip: 93.184.216.34, dst_port: 443, protocol: "TCP", flags: "SYN", } ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Work incrementally**: Parse complex data in stages, testing each step 2. **Check intermediate results**: Examine data after each parsing step 3. **Handle errors gracefully**: Parsing functions return null on failure 4. **Use appropriate parsers**: * JSON/YAML for structured data * Key-value for simple pairs * CSV/TSV/SSV for tabular data * Grok for complex patterns * Specific parsers (syslog, CEF, LEEF) for standard formats 5. **Transform types**: After parsing, convert strings to appropriate types (timestamps, IPs, numbers) 6. **Consider performance**: Simpler parsers (JSON, KV) are faster than complex ones (Grok) ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Filter and select data](/guides/data-shaping/filter-and-select-data) - Work with parsed fields * [Transform basic values](/guides/data-shaping/transform-basic-values) - Convert parsed strings to proper types * [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations

# Filter and select data

Filtering and selecting are fundamental operations when working with data streams. This guide shows you how to filter events based on conditions and select specific fields from your data. ## Understanding operators vs functions [Section titled ‚ÄúUnderstanding operators vs functions‚Äù](#understanding-operators-vs-functions) Before we dive in, it‚Äôs important to understand a key distinction in TQL: * **Operators** like [`where`](/reference/operators/where), [`select`](/reference/operators/select), and [`drop`](/reference/operators/drop) work on entire event streams * **Functions** like [`starts_with()`](/reference/functions/starts_with) or mathematical comparisons work on individual values within events You‚Äôll see both in action throughout this guide. ## Filter events with conditions [Section titled ‚ÄúFilter events with conditions‚Äù](#filter-events-with-conditions) Use the [`where`](/reference/operators/where) operator to keep only events that match specific conditions. ### Basic filtering [Section titled ‚ÄúBasic filtering‚Äù](#basic-filtering) Filter events based on a simple condition. This example keeps only successful HTTP requests (status code 200): ```tql from {status: 200, path: "/api/users"}, {status: 404, path: "/api/missing"}, {status: 200, path: "/home"} where status == 200 ``` ```tql {status: 200, path: "/api/users"} {status: 200, path: "/home"} ``` The event with `status: 404` is filtered out because it doesn‚Äôt match our condition. ### Combining conditions [Section titled ‚ÄúCombining conditions‚Äù](#combining-conditions) Use logical operators (`and`, `or`, `not`) to combine multiple conditions: ```tql from {status: 200, path: "/api/users", size: 1024}, {status: 404, path: "/api/missing", size: 512}, {status: 200, path: "/home", size: 2048} where status == 200 and size > 1000 ``` ```tql {status: 200, path: "/api/users", size: 1024} {status: 200, path: "/home", size: 2048} ``` You can also use `or` and `not` with functions like [`starts_with()`](/reference/functions/starts_with): ```tql from {status: 200, path: "/api/users"}, {status: 404, path: "/api/missing"}, {status: 500, path: "/api/error"} where status == 200 or not path.starts_with("/api/m") ``` ```tql {status: 200, path: "/api/users"} {status: 500, path: "/api/error"} ``` ### Using functions in filters [Section titled ‚ÄúUsing functions in filters‚Äù](#using-functions-in-filters) Functions work on values to create more sophisticated filters. For example, [`ends_with()`](/reference/functions/ends_with) checks string suffixes: ```tql from {user: "alice", email: "alice@example.com"}, {user: "bob", email: "bob@gmail.com"}, {user: "charlie", email: "charlie@example.com"} where email.ends_with("example.com") ``` ```tql {user: "alice", email: "alice@example.com"} {user: "charlie", email: "charlie@example.com"} ``` ### Filtering with patterns [Section titled ‚ÄúFiltering with patterns‚Äù](#filtering-with-patterns) Match patterns using regular expressions with [`match_regex()`](/reference/functions/match_regex): ```tql from {log: "Error: Connection timeout"}, {log: "Info: Request processed"}, {log: "Error: Invalid input"} where log.match_regex("Error:") ``` ```tql {log: "Error: Connection timeout"} {log: "Error: Invalid input"} ``` ## Select specific fields [Section titled ‚ÄúSelect specific fields‚Äù](#select-specific-fields) The [`select`](/reference/operators/select) operator lets you pick which fields to keep in your output. ### Basic field selection [Section titled ‚ÄúBasic field selection‚Äù](#basic-field-selection) Select only the fields you need: ```tql from {name: "alice", age: 30, city: "NYC"}, {name: "bob", age: 25, city: "SF"} select name, age ``` ```tql {name: "alice", age: 30} {name: "bob", age: 25} ``` ### Renaming fields [Section titled ‚ÄúRenaming fields‚Äù](#renaming-fields) You can rename fields while selecting them. This is useful when standardizing field names from different data sources: ```tql from {first_name: "alice", years: 30}, {first_name: "bob", years: 25} select name=first_name, age=years ``` ```tql {name: "alice", age: 30} {name: "bob", age: 25} ``` Here `first_name` becomes `name` and `years` becomes `age` in the output. ### Computing new fields [Section titled ‚ÄúComputing new fields‚Äù](#computing-new-fields) Create new fields with expressions during selection: ```tql from {price: 100, tax_rate: 0.08}, {price: 50, tax_rate: 0.08} select price, tax=price * tax_rate, total=price * (1 + tax_rate) ``` ```tql {price: 100, tax: 8.0, total: 108.0} {price: 50, tax: 4.0, total: 54.0} ``` ## Remove unwanted fields [Section titled ‚ÄúRemove unwanted fields‚Äù](#remove-unwanted-fields) The [`drop`](/reference/operators/drop) operator removes specified fields, keeping everything else. ### Basic field removal [Section titled ‚ÄúBasic field removal‚Äù](#basic-field-removal) Remove fields you don‚Äôt need: ```tql from {user: "alice", password: "secret", email: "alice@example.com"}, {user: "bob", password: "hidden", email: "bob@example.com"} drop password ``` ```tql {user: "alice", email: "alice@example.com"} {user: "bob", email: "bob@example.com"} ``` ### Dropping multiple fields [Section titled ‚ÄúDropping multiple fields‚Äù](#dropping-multiple-fields) Remove several fields at once: ```tql from {id: 1, internal_id: "xyz", debug: true, name: "alice"}, {id: 2, internal_id: "abc", debug: false, name: "bob"} drop internal_id, debug ``` ```tql {id: 1, name: "alice"} {id: 2, name: "bob"} ``` ## Add computed fields [Section titled ‚ÄúAdd computed fields‚Äù](#add-computed-fields) Use the [`set`](/reference/operators/set) operator to override existing fields and add new fields without removing existing ones. ### Adding simple fields [Section titled ‚ÄúAdding simple fields‚Äù](#adding-simple-fields) Add a constant field to all events: ```tql from {user: "alice"}, {user: "bob"} set source = "api" ``` ```tql {user: "alice", source: "api"} {user: "bob", source: "api"} ``` ### Computing field values [Section titled ‚ÄúComputing field values‚Äù](#computing-field-values) Add fields based on calculations: ```tql from {bytes_sent: 1024, bytes_received: 2048}, {bytes_sent: 512, bytes_received: 1024} set total_bytes = bytes_sent + bytes_received ``` ```tql {bytes_sent: 1024, bytes_received: 2048, total_bytes: 3072} {bytes_sent: 512, bytes_received: 1024, total_bytes: 1536} ``` ### Using the `else` keyword [Section titled ‚ÄúUsing the else keyword‚Äù](#using-the-else-keyword) Use the `else` word to provide default values for null fields: ```tql from {name: "alice", score: 85}, {name: "bob"}, {name: "charlie", score: 95} score = score? else 0 ``` ```tql {name: "alice", score: 85} {name: "bob", score: 0} {name: "charlie", score: 95} ``` Accessing non-existent fields Because the second event had no `score` field, the `else` keyword filled in a default value of `0`. However, accessing non-existent fields generates a warning. We use the trailing question mark (`?`) to suppress warnings when accessing fields that may not exist, i.e., `score? else 0` in the above example. This is distinctively different from an existing field with a null value, which does not elicit a warning. ## Combining operations [Section titled ‚ÄúCombining operations‚Äù](#combining-operations) Real-world pipelines often combine multiple operations: ```tql from {method: "GET", path: "/api/users", status: 200, duration_ms: 45}, {method: "POST", path: "/api/users", status: 201, duration_ms: 120}, {method: "GET", path: "/api/users/123", status: 404, duration_ms: 15} where status >= 200 and status < 300 select method, path, duration = duration_ms.milliseconds() ``` ```tql {method: "GET", path: "/api/users", duration: 45ms} {method: "POST", path: "/api/users", duration: 120ms} ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Filter early**: Apply [`where`](/reference/operators/where) conditions as early as possible to reduce the amount of data flowing through your pipeline. 2. **Select only what you need**: Use [`select`](/reference/operators/select) to keep only necessary fields, especially when dealing with large events. 3. **Choose the right operator**: * Use [`select`](/reference/operators/select) when you want to restrict the output to specific fields. * Use [`drop`](/reference/operators/drop) when you want to remove a few fields from many. * Use [`set`](/reference/operators/set) when you want to add/override fields without changing existing ones. 4. **Understand null handling**: The [`where`](/reference/operators/where) operator skips events where the condition evaluates to null or false. Use the question mark operator (`?`) to suppress warnings when accessing fields that may not exist. ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations * [Transform basic values](/guides/data-shaping/transform-basic-values) - Learn about value transformations * [Slice and sample data](/guides/data-shaping/slice-and-sample-data) - Control the flow of events

# Manipulate strings

String manipulation is essential for cleaning, formatting, and transforming text data. This guide covers TQL‚Äôs comprehensive string functions, from simple case changes to complex pattern matching and encoding operations. ## Change text case [Section titled ‚ÄúChange text case‚Äù](#change-text-case) Transform strings to different cases for consistency and formatting: ```tql from {name: "john smith", title: "data ENGINEER", code: "xyz-123"} lower_name = name.to_lower() upper_code = code.to_upper() title_case = title.to_title() cap_name = name.capitalize() ``` ```tql { name: "john smith", title: "data ENGINEER", code: "xyz-123", lower_name: "john smith", upper_code: "XYZ-123", title_case: "Data Engineer", cap_name: "John smith", } ``` Functions explained: * [`to_lower()`](/reference/functions/to_lower) - Converts all characters to lowercase * [`to_upper()`](/reference/functions/to_upper) - Converts all characters to uppercase * [`to_title()`](/reference/functions/to_title) - Capitalizes first letter of each word * [`capitalize()`](/reference/functions/capitalize) - Capitalizes only the first letter ## Trim whitespace [Section titled ‚ÄúTrim whitespace‚Äù](#trim-whitespace) Clean up strings by removing unwanted whitespace: ```tql from { raw: " hello world ", prefix: "\t\tdata", suffix: "value \n" } trimmed = raw.trim() no_prefix = prefix.trim_start() no_suffix = suffix.trim_end() ``` ```tql { raw: " hello world ", prefix: "\t\tdata", suffix: "value \n", trimmed: "hello world", no_prefix: "data", no_suffix: "value" } ``` Functions: * [`trim()`](/reference/functions/trim) - Removes whitespace from both ends * [`trim_start()`](/reference/functions/trim_start) - Removes whitespace from beginning * [`trim_end()`](/reference/functions/trim_end) - Removes whitespace from end ## Split and join strings [Section titled ‚ÄúSplit and join strings‚Äù](#split-and-join-strings) Break strings apart and combine them back together: ```tql from { path: "/home/user/documents/report.pdf", tags: "security,network,alert" } parts = path.split("/") tag_list = tags.split(",") rejoined = parts.join("-") ``` ```tql { path: "/home/user/documents/report.pdf", tags: "security,network,alert", parts: [ "", "home", "user", "documents", "report.pdf", ], tag_list: [ "security", "network", "alert", ], rejoined: "-home-user-documents-report.pdf", } ``` ### Split with regular expressions [Section titled ‚ÄúSplit with regular expressions‚Äù](#split-with-regular-expressions) Use [`split_regex()`](/reference/functions/split_regex) for complex splitting: ```tql from {text: "error:42|warning:7|info:125"} entries = text.split_regex("[:|]") ``` ```tql { text: "error:42|warning:7|info:125", entries: [ "error", "42", "warning", "7", "info", "125", ], } ``` ## Find and replace text [Section titled ‚ÄúFind and replace text‚Äù](#find-and-replace-text) Replace specific text or patterns within strings: ### Simple replacement [Section titled ‚ÄúSimple replacement‚Äù](#simple-replacement) ```tql from { log: "User 192.168.1.1 accessed /admin", template: "Hello {name}, welcome to {place}" } masked = log.replace("192.168.1.1", "xxx.xxx.xxx.xxx") filled = template.replace("{name}", "Alice").replace("{place}", "Tenzir") ``` ```tql { log: "User 192.168.1.1 accessed /admin", template: "Hello {name}, welcome to {place}", masked: "User xxx.xxx.xxx.xxx accessed /admin", filled: "Hello Alice, welcome to Tenzir", } ``` ### Pattern-based replacement [Section titled ‚ÄúPattern-based replacement‚Äù](#pattern-based-replacement) Use [`replace_regex()`](/reference/functions/replace_regex) for complex replacements: ```tql from { text: "Contact us at 555-1234 or 555-5678", log: "Error at 2024-01-15 10:30:45: Connection failed" } redacted = text.replace_regex("\\d{3}-\\d{4}", "XXX-XXXX") simple_log = log.replace_regex( "\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", "TIMESTAMP" ) ``` ```tql { text: "Contact us at 555-1234 or 555-5678", log: "Error at 2024-01-15 10:30:45: Connection failed", redacted: "Contact us at XXX-XXXX or XXX-XXXX", simple_log: "Error at TIMESTAMP: Connection failed", } ``` ## Match patterns [Section titled ‚ÄúMatch patterns‚Äù](#match-patterns) Check if strings match specific patterns: ```tql from { email: "alice@example.com", url: "https://tenzir.com", file: "report_2024.pdf" } is_email = email.match_regex("^[^@]+@[^@]+\\.[^@]+$") is_https = url.starts_with("https://") is_pdf = file.ends_with(".pdf") ``` ```tql { email: "alice@example.com", url: "https://tenzir.com", file: "report_2024.pdf", is_email: true, is_https: true, is_pdf: true, } ``` Pattern matching functions: * [`match_regex()`](/reference/functions/match_regex) - Test against regular expression * [`starts_with()`](/reference/functions/starts_with) - Check string prefix * [`ends_with()`](/reference/functions/ends_with) - Check string suffix ## Validate string content [Section titled ‚ÄúValidate string content‚Äù](#validate-string-content) Check what type of characters a string contains: ```tql from { id: "12345", name: "Alice", code: "abc123", mixed: "Hello World!", spaces: "hello world" } id_numeric = id.is_numeric() name_alpha = name.is_alpha() code_alnum = code.is_alnum() mixed_alpha = mixed.is_alpha() has_lower = spaces.is_lower() has_upper = name.is_title() ``` ```tql { id: "12345", name: "Alice", code: "abc123", mixed: "Hello World!", spaces: "hello world", id_numeric: true, name_alpha: true, code_alnum: true, mixed_alpha: false, has_lower: true, has_upper: true, } ``` Validation functions: * [`is_numeric()`](/reference/functions/is_numeric) - Contains only digits * [`is_alpha()`](/reference/functions/is_alpha) - Contains only letters * [`is_alnum()`](/reference/functions/is_alnum) - Contains only letters and digits * [`is_lower()`](/reference/functions/is_lower) - All cased characters are lowercase * [`is_upper()`](/reference/functions/is_upper) - All cased characters are uppercase * [`is_title()`](/reference/functions/is_title) - String is in title case * [`is_printable()`](/reference/functions/is_printable) - Contains only printable characters ## Measure string properties [Section titled ‚ÄúMeasure string properties‚Äù](#measure-string-properties) Get information about string characteristics: ```tql from { text: "Hello ‰∏ñÁïå", emoji: "üëã Hello!", path: "/var/log/system.log" } char_count = text.length_chars() byte_count = text.length_bytes() reversed = emoji.reverse() filename = path.file_name() directory = path.parent_dir() ``` ```tql { text: "Hello ‰∏ñÁïå", emoji: "üëã Hello!", path: "/var/log/system.log", char_count: 8, byte_count: 12, reversed: "!olleH üëã", filename: "system.log", directory: "/var/log", } ``` String property functions: * [`length_chars()`](/reference/functions/length_chars) - Count Unicode characters * [`length_bytes()`](/reference/functions/length_bytes) - Count bytes * [`reverse()`](/reference/functions/reverse) - Reverse character order * [`file_name()`](/reference/functions/file_name) - Extract filename from path * [`parent_dir()`](/reference/functions/parent_dir) - Extract directory from path ## Extract substrings [Section titled ‚ÄúExtract substrings‚Äù](#extract-substrings) Use [`slice()`](/reference/functions/slice) to extract portions of strings: ```tql from { text: "Hello, World!", id: "USER-12345-ACTIVE", timestamp: "2024-01-15T10:30:45" } greeting = text.slice(begin=0, end=5) user_num = id.slice(begin=5, end=10) date_part = timestamp.slice(begin=0, end=10) status = id.slice(begin=11) ``` ```tql { text: "Hello, World!", id: "USER-12345-ACTIVE", timestamp: "2024-01-15T10:30:45", greeting: "Hello", user_num: "12345", date_part: "2024-01-15", status: "ACTIVE", } ``` The `slice()` function parameters: * `begin` - Starting position (0-based, negative counts from end) * `end` - Ending position (exclusive, optional) * `stride` - Step between characters (optional, can be negative) ## Encode and decode strings [Section titled ‚ÄúEncode and decode strings‚Äù](#encode-and-decode-strings) Transform strings between different encodings: ### Base64 encoding [Section titled ‚ÄúBase64 encoding‚Äù](#base64-encoding) ```tql from {secret: "my-api-key-12345"} encoded = secret.encode_base64() decoded = encoded.decode_base64() ``` ```tql { secret: "my-api-key-12345", encoded: "bXktYXBpLWtleS0xMjM0NQ==", decoded: b"my-api-key-12345", } ``` ### Hex encoding [Section titled ‚ÄúHex encoding‚Äù](#hex-encoding) Use [`encode_hex()`](/reference/functions/encode_hex) and [`decode_hex()`](/reference/functions/decode_hex): ```tql from {data: "Hello", hex_string: "48656c6c6f"} hex = data.encode_hex() decoded = hex.decode_hex() decoded_blob = hex_string.decode_hex() ``` ```tql { data: "Hello", hex_string: "48656c6c6f", hex: "48656c6c6f", decoded: b"Hello", decoded_blob: b"Hello", } ``` ### URL encoding [Section titled ‚ÄúURL encoding‚Äù](#url-encoding) ```tql from {query: "search term with spaces & special=characters"} encoded = query.encode_url() decoded = encoded.decode_url() ``` ```tql { query: "search term with spaces & special=characters", encoded: "search%20term%20with%20spaces%20%26%20special%3Dcharacters", decoded: b"search term with spaces & special=characters", } ``` Encoding functions: * [`encode_base64()`](/reference/functions/encode_base64) / [`decode_base64()`](/reference/functions/decode_base64) * [`encode_hex()`](/reference/functions/encode_hex) / [`decode_hex()`](/reference/functions/decode_hex) * [`encode_url()`](/reference/functions/encode_url) / [`decode_url()`](/reference/functions/decode_url) ## Pad strings [Section titled ‚ÄúPad strings‚Äù](#pad-strings) Add characters to reach a specific length: ```tql from { id: "42", code: "ABC" } padded_id = id.pad_start(5, "0") padded_code = code.pad_end(10, "-") ``` ```tql { id: "42", code: "ABC", padded_id: "00042", padded_code: "ABC-------" } ``` Padding functions: * [`pad_start()`](/reference/functions/pad_start) - Add characters to the beginning * [`pad_end()`](/reference/functions/pad_end) - Add characters to the end ## Read file contents [Section titled ‚ÄúRead file contents‚Äù](#read-file-contents) Access text from files during processing: ```tql from {} hostname = file_contents("/etc/hostname") ``` ```tql { hostname: "my-server\n", } ``` The [`file_contents()`](/reference/functions/file_contents) function reads the entire file as a string. The file path must be a constant expression. Use with caution on large files. ## Practical examples [Section titled ‚ÄúPractical examples‚Äù](#practical-examples) ### Clean and normalize user input [Section titled ‚ÄúClean and normalize user input‚Äù](#clean-and-normalize-user-input) ```tql from { user_input: " JOHN.SMITH@EXAMPLE.COM ", phone: "(555) 123-4567" } email = user_input.trim().to_lower() clean_phone = phone.replace_regex("[^0-9]", "") ``` ```tql { user_input: " JOHN.SMITH@EXAMPLE.COM ", phone: "(555) 123-4567", email: "john.smith@example.com", clean_phone: "5551234567" } ``` ### Extract and validate identifiers [Section titled ‚ÄúExtract and validate identifiers‚Äù](#extract-and-validate-identifiers) ```tql from { log: "User ID: ABC-123-XYZ performed action", url: "https://api.example.com/v2/users/42" } user_id = log.split("User ID: ")[1].split(" ")[0] valid_id = user_id.match_regex("^[A-Z]{3}-\\d{3}-[A-Z]{3}$") api_version = url.split("/")[4] user_num = url.split("/").last() ``` ```tql { log: "User ID: ABC-123-XYZ performed action", url: "https://api.example.com/v2/users/42", user_id: "ABC-123-XYZ", valid_id: true, api_version: "v2", user_num: "42" } ``` ### Build formatted output [Section titled ‚ÄúBuild formatted output‚Äù](#build-formatted-output) ```tql from { first: "alice", last: "smith", dept: "engineering", id: 42 } full_name = first.capitalize() + " " + last.to_upper() email = first + "." + last + "@company.com" badge = dept.to_upper().slice(begin=0, end=3) + "-" + id.string() ``` ```tql { first: "alice", last: "smith", dept: "engineering", id: 42, full_name: "Alice SMITH", email: "alice.smith@company.com", badge: "ENG-42", } ``` ## Generate hash values [Section titled ‚ÄúGenerate hash values‚Äù](#generate-hash-values) Create checksums and identifiers using hash functions: ### Common hash algorithms [Section titled ‚ÄúCommon hash algorithms‚Äù](#common-hash-algorithms) ```tql from { data: "Hello, World!", secret: "my-api-key-123" } md5 = data.hash_md5() sha1 = data.hash_sha1() sha224 = data.hash_sha224() sha256 = data.hash_sha256() sha384 = data.hash_sha384() sha512 = data.hash_sha512() xxh3 = data.hash_xxh3() ``` ```tql { data: "Hello, World!", secret: "my-api-key-123", md5: "65a8e27d8879283831b664bd8b7f0ad4", sha1: "0a0a9f2a6772942557ab5355d76af442f8f65e01", sha224: "72a23dfa411ba6fde01dbfabf3b00a709c93ebf273dc29e2d8b261ff", sha256: "dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f", sha384: "5485cc9b3365b4305dfb4e8337e0a598a574f8242bf17289e0dd6c20a3cd44a089de16ab4ab308f63e44b1170eb5f515", sha512: "374d794a95cdcfd8b35993185fef9ba368f160d8daf432d08ba9f1ed1e5abe6cc69291e0fa2fe0006a52570ef18c19def4e617c33ce52ef0a6e5fbe318cb0387", xxh3: "c7269dc5f8602ca5", } ``` ### Create unique identifiers [Section titled ‚ÄúCreate unique identifiers‚Äù](#create-unique-identifiers) Use hashes to generate identifiers from multiple fields: ```tql from { user_id: "alice123", timestamp: "2024-01-15T10:30:00", action: "login" } event_id = f"{user_id}-{timestamp}-{action}".hash_sha256().slice(begin=0, end=16) short_hash = f"{user_id}{action}".hash_md5().slice(begin=0, end=8) numeric_id = user_id.hash_xxh3() ``` ```tql { user_id: "alice123", timestamp: "2024-01-15T10:30:00", action: "login", event_id: "d5f456083b8fee43", short_hash: "1616f7f2", numeric_id: "ac6dfe13bd512d81", } ``` ### Verify data integrity [Section titled ‚ÄúVerify data integrity‚Äù](#verify-data-integrity) ```tql from { file_content: "Important document content here...", expected_checksum: "dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f" } actual_checksum = file_content.hash_sha256() valid = actual_checksum == expected_checksum ``` ```tql { file_content: "Important document content here...", expected_checksum: "dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f", actual_checksum: "25f898ef7be64ead26e775e41778c6b5b5e5fe135d1b6658b6a27f9334c4f085", valid: false, } ``` ## Network security functions [Section titled ‚ÄúNetwork security functions‚Äù](#network-security-functions) Process network data with specialized security functions: ### Generate Community IDs [Section titled ‚ÄúGenerate Community IDs‚Äù](#generate-community-ids) Use [`community_id()`](/reference/functions/community_id) to create standardized flow hashes: ```tql from { src_ip: 192.168.1.100, dst_ip: 10.0.0.1, src_port: 54321, dst_port: 443, proto: "tcp" } flow_id = community_id( src_ip=src_ip, dst_ip=dst_ip, src_port=src_port, dst_port=dst_port, proto=proto ) ``` ```tql { src_ip: 192.168.1.100, dst_ip: 10.0.0.1, src_port: 54321, dst_port: 443, proto: "tcp", flow_id: "1:ZSU9hCO1tdr7pj3SCLkQ0XS3uvI=", } ``` ### Anonymize IP addresses [Section titled ‚ÄúAnonymize IP addresses‚Äù](#anonymize-ip-addresses) Use [`encrypt_cryptopan()`](/reference/functions/encrypt_cryptopan) for consistent IP anonymization: ```tql from { client_ip: 192.168.1.100, server_ip: 8.8.8.8, internal_ip: 10.0.0.5 } anon_client = client_ip.encrypt_cryptopan(seed="mysecretkey12345") anon_server = server_ip.encrypt_cryptopan(seed="mysecretkey12345") anon_internal = internal_ip.encrypt_cryptopan(seed="mysecretkey12345") ``` ```tql { client_ip: 192.168.1.100, server_ip: 8.8.8.8, internal_ip: 10.0.0.5, anon_client: 206.216.1.132, anon_server: 110.0.51.203, anon_internal: 109.255.195.194, } ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Chain operations efficiently**: Combine multiple string operations in one expression 2. **Validate before transforming**: Check string content before applying operations 3. **Handle edge cases**: Empty strings, null values, and special characters 4. **Use appropriate functions**: Choose `length_chars()` vs `length_bytes()` based on needs 5. **Be mindful of encoding**: Ensure correct encoding when dealing with international text ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse complex text formats * [Transform basic values](/guides/data-shaping/transform-basic-values) - Convert between data types * [Filter and select data](/guides/data-shaping/filter-and-select-data) - Use string functions in filters

# Reshape complex data

Real-world data is rarely flat. It contains nested structures, arrays of objects, and deeply hierarchical information. This guide shows advanced techniques for reshaping complex data structures to meet your analysis needs. ## Flatten nested structures [Section titled ‚ÄúFlatten nested structures‚Äù](#flatten-nested-structures) Transform deeply nested data into flat structures for easier analysis. ### Basic flattening [Section titled ‚ÄúBasic flattening‚Äù](#basic-flattening) Start with simple nested objects: ```tql from { user: { id: 123, profile: { name: "Alice", contact: { email: "alice@example.com", phone: "+1-555-0123" } } } } flat_user = { id: user.id, name: user.profile.name, email: user.profile.contact.email, phone: user.profile.contact.phone } ``` ```tql { user: {...}, flat_user: { id: 123, name: "Alice", email: "alice@example.com", phone: "+1-555-0123", }, } ``` ### Flatten with prefixes [Section titled ‚ÄúFlatten with prefixes‚Äù](#flatten-with-prefixes) Preserve context when flattening: ```tql from { event: { id: "evt-001", user: {name: "Alice", role: "admin"}, system: {name: "web-server", version: "2.1"} } } flattened = { event_id: event.id, user_name: event.user.name, user_role: event.user.role, system_name: event.system.name, system_version: event.system.version } ``` ```tql { event: {...}, flattened: { event_id: "evt-001", user_name: "Alice", user_role: "admin", system_name: "web-server", system_version: "2.1", } } ``` ## Unflatten data [Section titled ‚ÄúUnflatten data‚Äù](#unflatten-data) Reconstruct hierarchical structures from flattened data using [`unflatten()`](/reference/functions/unflatten): ### Basic unflattening [Section titled ‚ÄúBasic unflattening‚Äù](#basic-unflattening) Convert dotted field names back to nested structures: ```tql from { flattened: { "user.name": "Alice", "user.email": "alice@example.com", "user.address.city": "NYC", "user.address.zip": "10001", "status": "active" } } nested = flattened.unflatten() ``` ```tql { flattened: { "user.name": "Alice", "user.email": "alice@example.com", "user.address.city": "NYC", "user.address.zip": "10001", status: "active", }, nested: { user: { name: "Alice", email: "alice@example.com", address: { city: "NYC", zip: "10001", }, }, status: "active", }, } ``` ### Custom separator [Section titled ‚ÄúCustom separator‚Äù](#custom-separator) Use a different separator for field paths: ```tql from { metrics: { cpu_usage_percent: 45, memory_used_gb: 8, memory_total_gb: 16, disk_root_used: 100, disk_root_total: 500 } } structured = metrics.unflatten("_") ``` ```tql { metrics: { cpu_usage_percent: 45, memory_used_gb: 8, memory_total_gb: 16, disk_root_used: 100, disk_root_total: 500, }, structured: { cpu: { usage: { percent: 45, }, }, memory: { used: { gb: 8, }, total: { gb: 16, }, }, disk: { root: { used: 100, total: 500, }, }, }, } ``` ## Normalize denormalized data [Section titled ‚ÄúNormalize denormalized data‚Äù](#normalize-denormalized-data) Convert wide data to long format for better analysis. ### Wide to long transformation [Section titled ‚ÄúWide to long transformation‚Äù](#wide-to-long-transformation) ```tql from { metrics: { timestamp: "2024-01-15T10:00:00", cpu_usage: 45, memory_usage: 62, disk_usage: 78 } } long_format = [ {timestamp: metrics.timestamp, metric: "cpu", value: metrics.cpu_usage}, {timestamp: metrics.timestamp, metric: "memory", value: metrics.memory_usage}, {timestamp: metrics.timestamp, metric: "disk", value: metrics.disk_usage} ] ``` ```tql { metrics: { timestamp: "2024-01-15T10:00:00", cpu_usage: 45, memory_usage: 62, disk_usage: 78, }, long_format: [ { timestamp: "2024-01-15T10:00:00", metric: "cpu", value: 45, }, { timestamp: "2024-01-15T10:00:00", metric: "memory", value: 62, }, { timestamp: "2024-01-15T10:00:00", metric: "disk", value: 78, }, ], } ``` ### Transform to event stream [Section titled ‚ÄúTransform to event stream‚Äù](#transform-to-event-stream) Convert arrays to event streams for processing: ```tql from { readings: [ {sensor: "temp", location: "room1", value: 72}, {sensor: "humidity", location: "room1", value: 45}, {sensor: "temp", location: "room2", value: 68}, {sensor: "humidity", location: "room2", value: 50} ] } unroll readings select sensor=readings.sensor, location=readings.location, value=readings.value ``` ```tql {sensor: "temp", location: "room1", value: 72} {sensor: "humidity", location: "room1", value: 45} {sensor: "temp", location: "room2", value: 68} {sensor: "humidity", location: "room2", value: 50} ``` ## Extract arrays from objects [Section titled ‚ÄúExtract arrays from objects‚Äù](#extract-arrays-from-objects) Transform objects with numbered keys into proper arrays. ```tql from { response: { item_0: {name: "Widget", price: 9.99}, item_1: {name: "Gadget", price: 19.99}, item_2: {name: "Tool", price: 14.99}, total_items: 3 } } // Extract items manually when keys are known items = [ response.item_0, response.item_1, response.item_2 ] ``` ```tql { response: {...}, items: [ {name: "Widget", price: 9.99}, {name: "Gadget", price: 19.99}, {name: "Tool", price: 14.99}, ] } ``` ## Build hierarchical structures [Section titled ‚ÄúBuild hierarchical structures‚Äù](#build-hierarchical-structures) Create nested structures from flat data. ### Group data with summarize [Section titled ‚ÄúGroup data with summarize‚Äù](#group-data-with-summarize) Use the [`summarize`](/reference/operators/summarize) operator to group data: ```tql from { records: [ {dept: "Engineering", team: "Backend", member: "Alice"}, {dept: "Engineering", team: "Backend", member: "Bob"}, {dept: "Engineering", team: "Frontend", member: "Charlie"}, {dept: "Sales", team: "Direct", member: "David"} ] } unroll records summarize dept=records.dept, team=records.team, members=collect(records.member) ``` ```tql { dept: "Sales", team: "Direct", members: [ "David", ], } { dept: "Engineering", team: "Frontend", members: [ "Charlie", ], } { dept: "Engineering", team: "Backend", members: [ "Alice", "Bob", ], } ``` ### Extract path components [Section titled ‚ÄúExtract path components‚Äù](#extract-path-components) ```tql from { paths: [ "/home/user/docs/report.pdf", "/home/user/docs/summary.txt", "/home/user/images/photo.jpg", "/var/log/system.log" ] } path_info = paths.map(path => { full_path: path, directory: path.parent_dir(), filename: path.file_name(), parts: path.split("/").where(p => p.length_bytes() > 0) }) ``` ```tql { paths: [...], path_info: [ { full_path: "/home/user/docs/report.pdf", directory: "/home/user/docs", filename: "report.pdf", parts: ["home", "user", "docs", "report.pdf"], }, { full_path: "/home/user/docs/summary.txt", directory: "/home/user/docs", filename: "summary.txt", parts: ["home", "user", "docs", "summary.txt"], }, { full_path: "/home/user/images/photo.jpg", directory: "/home/user/images", filename: "photo.jpg", parts: ["home", "user", "images", "photo.jpg"], }, { full_path: "/var/log/system.log", directory: "/var/log", filename: "system.log", parts: ["var", "log", "system.log"], }, ], } ``` ## Merge fragmented data [Section titled ‚ÄúMerge fragmented data‚Äù](#merge-fragmented-data) Combine data split across multiple records. ### Merge records with spread operator [Section titled ‚ÄúMerge records with spread operator‚Äù](#merge-records-with-spread-operator) ```tql from { user: {id: 1, name: "Alice"}, profile: {email: "alice@example.com", dept: "Engineering"}, metrics: {logins: 45, actions: 234} } merged = { ...user, ...profile, ...metrics } ``` ```tql { user: { id: 1, name: "Alice", }, profile: { email: "alice@example.com", dept: "Engineering", }, metrics: { logins: 45, actions: 234, }, merged: { id: 1, name: "Alice", email: "alice@example.com", dept: "Engineering", logins: 45, actions: 234, }, } ``` ## Handle dynamic schemas [Section titled ‚ÄúHandle dynamic schemas‚Äù](#handle-dynamic-schemas) Work with data that has varying structures. ### Normalize inconsistent records [Section titled ‚ÄúNormalize inconsistent records‚Äù](#normalize-inconsistent-records) ```tql from { events: [ {type: "user", data: {name: "Alice", email: "alice@example.com"}}, {type: "system", data: {cpu: 45, memory: 1024}}, {type: "error", message: "Connection failed", code: 500} ] } select normalized = events.map(e => { event_type: e.type, timestamp: now(), // Use conditional assignment for type-specific fields user_name: e.data.name if e.type == "user", user_email: e.data.email if e.type == "user", system_cpu: e.data.cpu if e.type == "system", system_memory: e.data.memory if e.type == "system", error_message: e.message if e.type == "error", error_code: e.code if e.type == "error" }) ``` ```tql { normalized: [ { event_type: "user", timestamp: 2025-07-21T18:54:09.307412Z, user_name: "Alice", user_email: "alice@example.com", system_cpu: null, system_memory: null, error_message: null, error_code: null, }, { event_type: "system", timestamp: 2025-07-21T18:54:09.307412Z, user_name: null, user_email: null, system_cpu: 45, system_memory: 1024, error_message: null, error_code: null, }, { event_type: "error", timestamp: 2025-07-21T18:54:09.307412Z, user_name: null, user_email: null, system_cpu: null, system_memory: null, error_message: "Connection failed", error_code: 500, }, ], } ``` ## Working with aggregated events [Section titled ‚ÄúWorking with aggregated events‚Äù](#working-with-aggregated-events) ### Unroll arrays to individual events [Section titled ‚ÄúUnroll arrays to individual events‚Äù](#unroll-arrays-to-individual-events) Some data sources aggregate multiple events into a single record. Use [`unroll`](/reference/operators/unroll) to expand these into individual events: Expanding aggregated message types ```tql // DHCP logs may contain multiple message types in one record from { ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: ["DISCOVER", "OFFER", "REQUEST", "ACK"], client_addr: 192.168.1.100 } unroll msg_types // Now we have 4 separate events, one per message type activity_name = msg_types.to_title() ``` ```tql {ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: "DISCOVER", client_addr: 192.168.1.100, activity_name: "Discover"} {ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: "OFFER", client_addr: 192.168.1.100, activity_name: "Offer"} {ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: "REQUEST", client_addr: 192.168.1.100, activity_name: "Request"} {ts: 2024-01-15T10:00:00, uid: "C123abc", msg_types: "ACK", client_addr: 192.168.1.100, activity_name: "Ack"} ``` This pattern is essential when: * Converting aggregated logs to event-per-row formats * Normalizing data for OCSF or other schemas that expect individual events * Processing batched API responses ## Safe arithmetic with optional fields [Section titled ‚ÄúSafe arithmetic with optional fields‚Äù](#safe-arithmetic-with-optional-fields) ### Dynamic field computation with null safety [Section titled ‚ÄúDynamic field computation with null safety‚Äù](#dynamic-field-computation-with-null-safety) When computing metrics from fields that might be null, use the `else` keyword for safe fallbacks: Null-safe calculations ```tql from { packets_sent: 1000, packets_received: 950, duration: 10s, bytes_sent: null, // Missing data bytes_received: 5000 } // Safe division with fallback packets_per_second = packets_sent / duration.count_seconds() else 0 // Handle missing values in arithmetic loss_rate = (packets_sent - packets_received) / packets_sent else 0 // Compute only when both values exist throughput = (bytes_sent + bytes_received) / duration.count_seconds() if bytes_sent != null else null // Complex calculation with multiple fallbacks efficiency = (bytes_received / bytes_sent) * 100 if bytes_sent != null else 100 if bytes_received > 0 else 0 ``` ```tql { packets_sent: 1000, packets_received: 950, duration: 10s, bytes_sent: null, bytes_received: 5000, packets_per_second: 100.0, loss_rate: 0.05, throughput: null, efficiency: 100 } ``` ## Conditional aggregation patterns [Section titled ‚ÄúConditional aggregation patterns‚Äù](#conditional-aggregation-patterns) ### Selective data collection [Section titled ‚ÄúSelective data collection‚Äù](#selective-data-collection) Collect values conditionally during aggregation: Conditional collection in summarize ```tql from {src_ip: 10.0.0.5, dst_port: 22, bytes: 1024}, {src_ip: 192.168.1.10, dst_port: 80, bytes: 2048}, {src_ip: 10.0.0.5, dst_port: 443, bytes: 4096}, {src_ip: 192.168.1.10, dst_port: 22, bytes: 512} let $critical_ports = [22, 3389, 5985] summarize src_ip, total_bytes=sum(bytes), // Collect all unique ports all_ports=collect(dst_port), // Collect with conditional transformation port_types=collect("HIGH" if dst_port in $critical_ports else "LOW") ``` ```tql {src_ip: 192.168.1.10, total_bytes: 2560, all_ports: [80, 22], port_types: ["LOW", "HIGH"]} {src_ip: 10.0.0.5, total_bytes: 5120, all_ports: [22, 443], port_types: ["HIGH", "LOW"]} ``` This pattern enables: * Building risk profiles during aggregation * Transforming values during collection based on conditions * Creating categorical metrics from raw data ## Advanced transformations [Section titled ‚ÄúAdvanced transformations‚Äù](#advanced-transformations) ### Recursive flattening [Section titled ‚ÄúRecursive flattening‚Äù](#recursive-flattening) Flatten arbitrarily nested structures: ```tql from { data: { level1: { level2: { level3: { value: "deep", items: [1, 2, 3] } }, other: "value" } } } // Use flatten function for automatic recursive flattening select flattened = data.flatten() ``` ```tql { flattened: { "level1.level2.level3.value": "deep", "level1.level2.level3.items": [ 1, 2, 3, ], "level1.other": "value", }, } ``` ### Extract fields by prefix [Section titled ‚ÄúExtract fields by prefix‚Äù](#extract-fields-by-prefix) Use field access to extract specific configurations: ```tql from { config: { env_DATABASE_HOST: "db.example.com", env_DATABASE_PORT: 5432, env_API_KEY: "secret123", app_name: "MyApp", app_version: "1.0" } } // Extract specific fields directly select env_vars = { DATABASE_HOST: config.env_DATABASE_HOST, DATABASE_PORT: config.env_DATABASE_PORT, API_KEY: config.env_API_KEY }, app_config = { name: config.app_name, version: config.app_version } ``` ```tql { env_vars: { DATABASE_HOST: "db.example.com", DATABASE_PORT: 5432, API_KEY: "secret123", }, app_config: { name: "MyApp", version: "1.0", }, } ``` ## Practical examples [Section titled ‚ÄúPractical examples‚Äù](#practical-examples) ### Process nested API responses [Section titled ‚ÄúProcess nested API responses‚Äù](#process-nested-api-responses) ```tql from { api_response: { status: "success", data: { user: { id: 123, profile: { personal: {name: "Alice", age: 30}, professional: {title: "Engineer", company: "TechCorp"} } }, metadata: { request_id: "req-001", timestamp: "2024-01-15T10:00:00" } } } } // Extract and reshape for database storage select user_record = { user_id: api_response.data.user.id, name: api_response.data.user.profile.personal.name, age: api_response.data.user.profile.personal.age, job_title: api_response.data.user.profile.professional.title, company: api_response.data.user.profile.professional.company, last_updated: api_response.data.metadata.timestamp.parse_time("%Y-%m-%dT%H:%M:%S") } ``` ```tql { user_record: { user_id: 123, name: "Alice", age: 30, job_title: "Engineer", company: "TechCorp", last_updated: 2024-01-15T10:00:00Z, }, } ``` ### Transform log aggregations [Section titled ‚ÄúTransform log aggregations‚Äù](#transform-log-aggregations) ```tql from { log_stats: { "2024-01-15": { "/api/users": {GET: 150, POST: 20}, "/api/orders": {GET: 200, POST: 50, DELETE: 5} }, "2024-01-16": { "/api/users": {GET: 180, POST: 25}, "/api/orders": {GET: 220, POST: 60} } } } // Flatten nested structure into individual events select flattened = log_stats.flatten() ``` ```tql { flattened: { "2024-01-15./api/users.GET": 150, "2024-01-15./api/users.POST": 20, "2024-01-15./api/orders.GET": 200, "2024-01-15./api/orders.POST": 50, "2024-01-15./api/orders.DELETE": 5, "2024-01-16./api/users.GET": 180, "2024-01-16./api/users.POST": 25, "2024-01-16./api/orders.GET": 220, "2024-01-16./api/orders.POST": 60, }, } ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Plan your structure**: Design the target schema before transforming 2. **Handle missing fields**: Use conditional logic or defaults for optional data 3. **Preserve information**: Don‚Äôt lose data during transformation unless intentional 4. **Test edge cases**: Verify transformations work with incomplete or unusual data 5. **Document complex logic**: Add comments explaining non-obvious transformations ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Transform collections](/guides/data-shaping/transform-collections) - Basic collection operations * [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse before reshaping * [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations

# Shape data

TQL comes with numerous transformation [operators](/reference/operators) that change the shape of events, as well as [functions](/reference/functions) that work on values within a single event. This guide provides an overview of data shaping capabilities in TQL, showcasing the operators and functions that you need for day-to-day data transformation tasks. ## Understanding operators vs functions [Section titled ‚ÄúUnderstanding operators vs functions‚Äù](#understanding-operators-vs-functions) Before diving into specific operations, it‚Äôs important to understand a key distinction in TQL: * **Operators** work on streams of events and can keep state between multiple events (e.g., [`where`](/reference/operators/where), [`select`](/reference/operators/select), [`summarize`](/reference/operators/summarize)) * **Functions** work on individual values within a single event (e.g., [`starts_with()`](/reference/functions/starts_with), [`round()`](/reference/functions/round), [`merge()`](/reference/functions/merge)) Here is a visual overview of transformations that you can perform over a stream of events: ![Shaping Overview](/_astro/shape-data.C9Ucev8W_19DKCs.svg) ## Common data shaping tasks [Section titled ‚ÄúCommon data shaping tasks‚Äù](#common-data-shaping-tasks) Here are the most common data shaping operations organized by use case: ### Filter and select data [Section titled ‚ÄúFilter and select data‚Äù](#filter-and-select-data) The fundamental operations for controlling which events and fields flow through your pipeline. See the [Filter and select data](/guides/data-shaping/filter-and-select-data) guide for detailed examples of: * Filtering events with [`where`](/reference/operators/where) * Selecting fields with [`select`](/reference/operators/select) * Removing fields with [`drop`](/reference/operators/drop) * Adding fields with [`set`](/reference/operators/set) ### Control event flow [Section titled ‚ÄúControl event flow‚Äù](#control-event-flow) Manage how many events pass through your pipeline and in what order. See the [Slice and sample data](/guides/data-shaping/slice-and-sample-data) guide for: * Getting first/last events with [`head`](/reference/operators/head) and [`tail`](/reference/operators/tail) * Slicing ranges with [`slice`](/reference/operators/slice) * Sampling by schema with [`taste`](/reference/operators/taste) * Reversing order with [`reverse`](/reference/operators/reverse) ### Transform values [Section titled ‚ÄúTransform values‚Äù](#transform-values) Convert and manipulate individual values within events. See the [Transform basic values](/guides/data-shaping/transform-basic-values) guide for: * Type conversions ([`int()`](/reference/functions/int), [`float()`](/reference/functions/float), [`string()`](/reference/functions/string), [`time()`](/reference/functions/time)) * String operations ([`to_upper()`](/reference/functions/to_upper), [`trim()`](/reference/functions/trim), [`capitalize()`](/reference/functions/capitalize)) * Mathematical operations ([`round()`](/reference/functions/round), [`abs()`](/reference/functions/abs), [`sqrt()`](/reference/functions/sqrt)) * Handling null values with the `else` keyword ### Work with complex structures [Section titled ‚ÄúWork with complex structures‚Äù](#work-with-complex-structures) #### Relocate fields with `move` [Section titled ‚ÄúRelocate fields with move‚Äù](#relocate-fields-with-move) Use the [`move`](/reference/operators/move) operator to rename and relocate fields in one operation: ```tql from {old: 42} move new = old ``` ```tql {new: 42} ``` Moving multiple fields: ```tql from {foo: 1, bar: 2} move foo=bar, qux=foo ``` ```tql { foo: 2, qux: 1, } ``` #### Aggregate events with `summarize` [Section titled ‚ÄúAggregate events with summarize‚Äù](#aggregate-events-with-summarize) Use [`summarize`](/reference/operators/summarize) to group and aggregate data. This example groups events by the `y` field and sums up the `x` values for each group: ```tql from {x: 0, y: 0, z: 1}, {x: 1, y: 1, z: 2}, {x: 1, y: 1, z: 3} summarize y, x=sum(x) ``` ```tql {y: 0, x: 0} {y: 1, x: 2} ``` In this example: * Events are grouped by the value of `y` (0 or 1) * For `y=0`, there‚Äôs one event with `x=0`, so `sum(x)=0` * For `y=1`, there are two events with `x=1` each, so `sum(x)=2` See the [aggregation functions](/reference/functions#aggregation) reference for all available aggregation functions like `count()`, `mean()`, `max()`, etc. #### Reorder events with `sort` [Section titled ‚ÄúReorder events with sort‚Äù](#reorder-events-with-sort) Use [`sort`](/reference/operators/sort) to arrange events by field values: ```tql from {x: 2, y: "bar"}, {x: 3, y: "baz"}, {x: 1, y: "foo"} sort -x ``` ```tql {x: 3, y: "baz"} {x: 2, y: "bar"} {x: 1, y: "foo"} ``` Prepending the field with `-` reverses the sort order. #### Break up lists with `unroll` [Section titled ‚ÄúBreak up lists with unroll‚Äù](#break-up-lists-with-unroll) Use [`unroll`](/reference/operators/unroll) to expand lists into separate events: ```tql from { xs: [{a: 1}, {a: 2}], y: "foo", } unroll xs ``` ```tql { xs: { a: 1, }, y: "foo", } { xs: { a: 2, }, y: "foo", } ``` ### Manipulate records and lists [Section titled ‚ÄúManipulate records and lists‚Äù](#manipulate-records-and-lists) #### Combine records with `merge` [Section titled ‚ÄúCombine records with merge‚Äù](#combine-records-with-merge) Use the [`merge`](/reference/functions/merge) function to combine records. This is useful when you need to consolidate data from multiple sources: ```tql from { foo: { bar: 1, baz: 2, }, qux: { fred: 3, george: 4, bar: 5, } } set this = merge(foo, qux) ``` ```tql { bar: 5, baz: 2, fred: 3, george: 4 } ``` Note that the field `bar` appears in both records. The value from the second argument (`qux.bar = 5`) overwrites the value from the first (`foo.bar = 1`). You can also use the spread expression as shorthand: ```tql set this = {...foo, ...qux} ``` #### Combine lists with `concatenate` [Section titled ‚ÄúCombine lists with concatenate‚Äù](#combine-lists-with-concatenate) Use [`concatenate`](/reference/functions/concatenate) to join lists: ```tql from { xs: [1,2,3], ys: [4,5,6], } select result = concatenate(xs, ys) ``` ```tql { result: [ 1, 2, 3, 4, 5, 6, ], } ``` Or use the spread expression: ```tql select result = [...xs, ...ys] ``` #### Add values to lists [Section titled ‚ÄúAdd values to lists‚Äù](#add-values-to-lists) Use [`append`](/reference/functions/append) and [`prepend`](/reference/functions/prepend): ```tql from { xs: [2], } set xs = append(xs, 3) set xs = prepend(xs, 1) ``` ```tql { xs: [ 1, 2, 3, ], } ``` ### Specialized operations [Section titled ‚ÄúSpecialized operations‚Äù](#specialized-operations) #### Perform bitwise operations [Section titled ‚ÄúPerform bitwise operations‚Äù](#perform-bitwise-operations) TQL provides bitwise functions for low-level data manipulation using [`bit_and()`](/reference/functions/bit_and), [`bit_or()`](/reference/functions/bit_or), [`bit_xor()`](/reference/functions/bit_xor), [`bit_not()`](/reference/functions/bit_not), [`shift_left()`](/reference/functions/shift_left), and [`shift_right()`](/reference/functions/shift_right): ```tql from { band: bit_and(5, 3), bor: bit_or(5, 3), bxor: bit_xor(5, 3), bnot: bit_not(5), shl: shift_left(5, 2), shr: shift_right(5, 1), } ``` ```tql { band: 1, // (0101 & 0011 = 0001) bor: 7, // (0101 | 0011 = 0111) bxor: 6, // (0101 ^ 0011 = 0110) bnot: -6, // (~0101 = 1010) shl: 20, // (0101 << 2 = 10100) shr: 2, // (0101 >> 1 = 0010) } ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Understand the operator/function distinction**: Use operators for stream-level operations and functions for value-level transformations. 2. **Filter early**: Apply [`where`](/reference/operators/where) conditions as early as possible to reduce data volume. 3. **Select only what you need**: Use [`select`](/reference/operators/select) to keep only necessary fields, especially with large events. 4. **Choose the right tool**: * Use [`select`](/reference/operators/select) to keep specific fields * Use [`drop`](/reference/operators/drop) to remove a few fields from many * Use [`set`](/reference/operators/set) to add fields without changing existing ones * Use [`move`](/reference/operators/move) to rename fields efficiently 5. **Be mindful of performance**: Some operators like [`tail`](/reference/operators/tail) and [`reverse`](/reference/operators/reverse) must buffer all input before producing output. ## Where to go from here [Section titled ‚ÄúWhere to go from here‚Äù](#where-to-go-from-here) Explore these specialized guides for deeper coverage of specific topics: * [Filter and select data](/guides/data-shaping/filter-and-select-data) - Master filtering and field selection * [Transform basic values](/guides/data-shaping/transform-basic-values) - Type conversions and value manipulation * [Manipulate strings](/guides/data-shaping/manipulate-strings) - Text processing and formatting * [Work with time](/guides/data-shaping/work-with-time) - Parse, format, and calculate with timestamps * [Transform collections](/guides/data-shaping/transform-collections) - Work with lists and records * [Aggregate and summarize](/guides/data-shaping/aggregate-and-summarize) - Statistical operations and grouping * [Slice and sample data](/guides/data-shaping/slice-and-sample-data) - Control event flow * [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse complex text formats * [Convert data formats](/guides/data-shaping/convert-data-formats) - Transform between JSON, CSV, YAML, and more * [Reshape complex data](/guides/data-shaping/reshape-complex-data) - Advanced structural transformations * [Deduplicate events](/guides/data-shaping/deduplicate-events) - Remove duplicate events For a complete list of available operators and functions, see the [operators](/reference/operators) and [functions](/reference/functions) reference documentation.

# Slice and sample data

When working with data streams, you often need to control which events flow through your pipeline. This guide shows you how to slice event streams, sample data, and control event ordering using TQL operators. ## Understanding stream operators [Section titled ‚ÄúUnderstanding stream operators‚Äù](#understanding-stream-operators) The operators in this guide work on entire event streams: * **[head](/reference/operators/head)** and **[tail](/reference/operators/tail)** - Get events from the beginning or end * **[slice](/reference/operators/slice)** - Extract a specific range of events * **[taste](/reference/operators/taste)** - Sample events by schema * **[reverse](/reference/operators/reverse)** - Invert the order of events * **[sample](/reference/operators/sample)** - Randomly sample events These operators maintain state between events, unlike functions that work on individual values. ## Get events from the beginning [Section titled ‚ÄúGet events from the beginning‚Äù](#get-events-from-the-beginning) Use the [`head`](/reference/operators/head) operator to get the first N events from a stream. ### Basic usage [Section titled ‚ÄúBasic usage‚Äù](#basic-usage) Get the first 3 events: ```tql from {id: 1, value: "a"}, {id: 2, value: "b"}, {id: 3, value: "c"}, {id: 4, value: "d"}, {id: 5, value: "e"} head 3 ``` ```tql {id: 1, value: "a"} {id: 2, value: "b"} {id: 3, value: "c"} ``` ### Default behavior [Section titled ‚ÄúDefault behavior‚Äù](#default-behavior) Without an argument, [`head`](/reference/operators/head) returns all events: ```tql from {id: 1, value: "first"}, {id: 2, value: "second"}, {id: 3, value: "third"} head ``` ```tql {id: 1, value: "first"} {id: 2, value: "second"} {id: 3, value: "third"} ``` ## Get events from the end [Section titled ‚ÄúGet events from the end‚Äù](#get-events-from-the-end) Use the [`tail`](/reference/operators/tail) operator to get the last N events. ### Basic usage [Section titled ‚ÄúBasic usage‚Äù](#basic-usage-1) Get the last 2 events: ```tql from {id: 1, value: "a"}, {id: 2, value: "b"}, {id: 3, value: "c"}, {id: 4, value: "d"} tail 2 ``` ```tql {id: 3, value: "c"} {id: 4, value: "d"} ``` Performance consideration The [`tail`](/reference/operators/tail) operator must consume all input before producing output, making it memory-intensive for large streams. Use [`head`](/reference/operators/head) when possible for better performance. ## Slice event streams [Section titled ‚ÄúSlice event streams‚Äù](#slice-event-streams) The [`slice`](/reference/operators/slice) operator provides fine-grained control over which events to extract. ### Extract a range [Section titled ‚ÄúExtract a range‚Äù](#extract-a-range) Get events 2 through 4 (0-indexed): ```tql from {n: 1}, {n: 2}, {n: 3}, {n: 4}, {n: 5} slice begin=1, end=4 ``` ```tql {n: 2} {n: 3} {n: 4} ``` ### Skip events with stride [Section titled ‚ÄúSkip events with stride‚Äù](#skip-events-with-stride) Get every other event starting from the second: ```tql from {n: 1}, {n: 2}, {n: 3}, {n: 4}, {n: 5}, {n: 6} slice begin=1, stride=2 ``` ```tql {n: 2} {n: 4} {n: 6} ``` ### Combine parameters [Section titled ‚ÄúCombine parameters‚Äù](#combine-parameters) Skip 2, take every 3rd event, stop after 10 total: ```tql from {n: 1}, {n: 2}, {n: 3}, {n: 4}, {n: 5}, {n: 6}, {n: 7}, {n: 8}, {n: 9}, {n: 10} slice begin=2, end=10, stride=3 ``` ```tql {n: 3} {n: 6} {n: 9} ``` ## Sample events by schema [Section titled ‚ÄúSample events by schema‚Äù](#sample-events-by-schema) The [`taste`](/reference/operators/taste) operator samples events based on their structure, giving you examples of different data shapes in your stream. ### Get schema examples [Section titled ‚ÄúGet schema examples‚Äù](#get-schema-examples) See one example of each unique schema: ```tql from {type: "user", name: "alice"}, {type: "user", name: "bob"}, {type: "event", id: 1}, {type: "event", id: 2}, {value: 42} taste 1 ``` ```tql {type: "user", name: "alice"} {type: "event", id: 1} {value: 42} ``` ### Get multiple examples per schema [Section titled ‚ÄúGet multiple examples per schema‚Äù](#get-multiple-examples-per-schema) Get up to 2 examples of each schema: ```tql from {x: 1, y: 1}, {x: 2, y: 2}, {x: 3}, {x: 4}, {z: "a"}, {z: "b"} taste 2 ``` ```tql {x: 1, y: 1} {x: 2, y: 2} {x: 3} {x: 4} {z: "a"} {z: "b"} ``` ## Reverse event order [Section titled ‚ÄúReverse event order‚Äù](#reverse-event-order) Use the [`reverse`](/reference/operators/reverse) operator to invert the order of events in a stream: ```tql from {seq: 1, msg: "first"}, {seq: 2, msg: "second"}, {seq: 3, msg: "third"} reverse ``` ```tql {seq: 3, msg: "third"} {seq: 2, msg: "second"} {seq: 1, msg: "first"} ``` Memory usage Like [`tail`](/reference/operators/tail), the [`reverse`](/reference/operators/reverse) operator must buffer all input before producing output. Use with caution on large streams. ## Time-based sampling [Section titled ‚ÄúTime-based sampling‚Äù](#time-based-sampling) Use the [`sample`](/reference/operators/sample) operator to sample events based on time intervals: ### Sample by duration [Section titled ‚ÄúSample by duration‚Äù](#sample-by-duration) Sample events at regular time intervals: ```tql from {id: 1}, {id: 2}, {id: 3}, {id: 4}, {id: 5}, {id: 6}, {id: 7}, {id: 8}, {id: 9}, {id: 10} sample 1s ``` ```tql {id: 1} {id: 2} {id: 3} {id: 4} {id: 5} {id: 6} {id: 7} {id: 8} {id: 9} {id: 10} ``` Note: The sample operator uses duration-based sampling, not random probability sampling. ## Combining operators [Section titled ‚ÄúCombining operators‚Äù](#combining-operators) Chain operators to create more complex sampling strategies: ```tql from {user: "alice", action: "login", time: 1}, {user: "bob", action: "view", time: 2}, {user: "alice", action: "edit", time: 3}, {user: "charlie", action: "login", time: 4}, {user: "bob", action: "logout", time: 5} where action == "login" head 2 ``` ```tql {user: "alice", action: "login", time: 1} {user: "charlie", action: "login", time: 4} ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Prefer [`head`](/reference/operators/head) over [`tail`](/reference/operators/tail)**: [`head`](/reference/operators/head) stops processing once it has enough events, while [`tail`](/reference/operators/tail) must process everything. 2. **Use [`taste`](/reference/operators/taste) for exploration**: When working with unfamiliar data, [`taste`](/reference/operators/taste) quickly shows you the different schemas present. 3. **Be mindful of memory**: Operators like [`tail`](/reference/operators/tail) and [`reverse`](/reference/operators/reverse) buffer all input, which can consume significant memory for large streams. 4. **Combine with filters**: Use [`where`](/reference/operators/where) before slicing operators to reduce the amount of data processed. ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Filter and select data](/guides/data-shaping/filter-and-select-data) - Learn about filtering events * [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations * [Deduplicate events](/guides/data-shaping/deduplicate-events) - Remove duplicate events from streams

# Transform basic values

Transforming values is a fundamental part of data processing. This guide shows you how to convert between different data types, perform basic calculations, and manipulate simple values within your events. ## Type conversions [Section titled ‚ÄúType conversions‚Äù](#type-conversions) TQL provides functions to convert values between different types. This is essential when data arrives in the wrong format or when you need specific types for further processing. ### Convert to numbers [Section titled ‚ÄúConvert to numbers‚Äù](#convert-to-numbers) Use [`int()`](/reference/functions/int) and [`float()`](/reference/functions/float) to convert values to numeric types: ```tql from {price: "42", quantity: "3.5"}, {price: "99", quantity: "1.0"} price = price.int() quantity = quantity.float() ``` ```tql {price: 42, quantity: 3.5} {price: 99, quantity: 1.0} ``` ### Convert to strings [Section titled ‚ÄúConvert to strings‚Äù](#convert-to-strings) Use [`string()`](/reference/functions/string) to convert any value to its string representation: ```tql from {status: 200, ratio: 0.95}, {status: 404, ratio: 0.05} message = status.string() + " - " + (ratio * 100).string() + "%" ``` ```tql {status: 200, ratio: 0.95, message: "200 - 95.0%"} {status: 404, ratio: 0.05, message: "404 - 5.0%"} ``` ### Parse times and durations [Section titled ‚ÄúParse times and durations‚Äù](#parse-times-and-durations) Convert strings to time values with [`time()`](/reference/functions/time): ```tql from {timestamp: "2024-01-15"}, {timestamp: "2024-02-20"} parsed_time = timestamp.time() ``` ```tql {timestamp: "2024-01-15", parsed_time: 2024-01-15T00:00:00Z} {timestamp: "2024-02-20", parsed_time: 2024-02-20T00:00:00Z} ``` Convert strings to durations with [`duration()`](/reference/functions/duration): ```tql from {interval: "5s"}, {interval: "2.5min"} parsed_duration = interval.duration() ``` ```tql {interval: "5s", parsed_duration: 5s} {interval: "2.5min", parsed_duration: 2.5min} ``` ### Convert to unsigned integers [Section titled ‚ÄúConvert to unsigned integers‚Äù](#convert-to-unsigned-integers) Use [`uint()`](/reference/functions/uint) for non-negative integers: ```tql from {count: "42", ratio: 3.7}, {count: "-5", ratio: 2.3} count_uint = count.uint() ratio_uint = ratio.uint() ``` ```tql {count: "42", ratio: 3.7, count_uint: 42, ratio_uint: 3} {count: "-5", ratio: 2.3, count_uint: null, ratio_uint: 2} ``` This pipeline elicits the following warning: ```txt warning: `uint` failed to convert some string --> /tmp/pipeline.tql:3:14 | 3 | count_uint = count.uint() | ~~~~~ | ``` ### Work with IP addresses and subnets [Section titled ‚ÄúWork with IP addresses and subnets‚Äù](#work-with-ip-addresses-and-subnets) TQL supports IP address and subnet literals directly. You can also parse them from strings using [`ip()`](/reference/functions/ip) and [`subnet()`](/reference/functions/subnet): ```tql from {direct_ip: 192.168.1.1, direct_subnet: 10.0.0.0/24}, {direct_ip: ::1, direct_subnet: 2001:db8::/32} ipv6_check = direct_ip.is_v6() ``` ```tql { direct_ip: 192.168.1.1, direct_subnet: 10.0.0.0/24, ipv6_check: false, } { direct_ip: ::1, direct_subnet: 2001:db8::/32, ipv6_check: true, } ``` Parse from strings when needed: ```tql from {client: "192.168.1.1", network: "10.0.0.0/24"}, {client: "10.0.0.5", network: "192.168.0.0/16"} client_ip = client.ip() network_subnet = network.subnet() ``` ```tql { client: "192.168.1.1", network: "10.0.0.0/24", client_ip: 192.168.1.1, network_subnet: 10.0.0.0/24, } { client: "10.0.0.5", network: "192.168.0.0/16", client_ip: 10.0.0.5, network_subnet: 192.168.0.0/16, } ``` ## IP address inspection [Section titled ‚ÄúIP address inspection‚Äù](#ip-address-inspection) Analyze and categorize IP addresses with inspection functions: ### Check IP address types [Section titled ‚ÄúCheck IP address types‚Äù](#check-ip-address-types) Use IP inspection functions like [`is_v4()`](/reference/functions/is_v4), [`is_v6()`](/reference/functions/is_v6), [`is_private()`](/reference/functions/is_private), [`is_global()`](/reference/functions/is_global), [`is_loopback()`](/reference/functions/is_loopback), and [`is_multicast()`](/reference/functions/is_multicast) to analyze addresses: ```tql from {ip1: 192.168.1.1, ip2: 8.8.8.8, ip3: ::1}, {ip1: 10.0.0.1, ip2: 224.0.0.1, ip3: 2001:db8::1} is_v4 = ip1.is_v4() is_v6 = ip3.is_v6() is_private = ip1.is_private() is_global = ip2.is_global() is_loopback = ip3.is_loopback() is_multicast = ip2.is_multicast() ``` ```tql { ip1: 192.168.1.1, ip2: 8.8.8.8, ip3: ::1, is_v4: true, is_v6: true, is_private: true, is_global: true, is_loopback: true, is_multicast: false, } { ip1: 10.0.0.1, ip2: 224.0.0.1, ip3: 2001:db8::1, is_v4: true, is_v6: true, is_private: true, is_global: false, is_loopback: false, is_multicast: true, } ``` ### Categorize IP addresses [Section titled ‚ÄúCategorize IP addresses‚Äù](#categorize-ip-addresses) Get detailed IP address classification with [`ip_category()`](/reference/functions/ip_category): ```tql from {client: "192.168.1.100", server: "8.8.8.8", local: "127.0.0.1"}, {client: "10.0.0.5", server: "224.0.0.251", local: "::1"} client_category = client.ip().ip_category() server_category = server.ip().ip_category() local_category = local.ip().ip_category() ``` ```tql { client: "192.168.1.100", server: "8.8.8.8", local: "127.0.0.1", client_category: "private", server_category: "global", local_category: "loopback", } { client: "10.0.0.5", server: "224.0.0.251", local: "::1", client_category: "private", server_category: "multicast", local_category: "loopback", } ``` ### Check link-local addresses [Section titled ‚ÄúCheck link-local addresses‚Äù](#check-link-local-addresses) Identify link-local addresses with [`is_link_local()`](/reference/functions/is_link_local): ```tql from {addr1: 169.254.1.1, addr2: fe80::1, addr3: 192.168.1.1}, {addr1: 169.254.0.1, addr2: 2001:db8::1, addr3: 10.0.0.1} link_local1 = addr1.is_link_local() link_local2 = addr2.is_link_local() link_local3 = addr3.is_link_local() ``` ```tql { addr1: 169.254.1.1, addr2: fe80::1, addr3: 192.168.1.1, link_local1: true, link_local2: true, link_local3: false, } { addr1: 169.254.0.1, addr2: 2001:db8::1, addr3: 10.0.0.1, link_local1: true, link_local2: false, link_local3: false, } ``` ## Basic string operations [Section titled ‚ÄúBasic string operations‚Äù](#basic-string-operations) Transform strings with simple operations to clean and standardize your data. ### Change case [Section titled ‚ÄúChange case‚Äù](#change-case) Convert strings to different cases: ```tql from {name: "alice smith", code: "xyz"}, {name: "BOB JONES", code: "ABC"} name = name.to_title() code = code.to_upper() ``` ```tql {name: "Alice Smith", code: "XYZ"} {name: "Bob Jones", code: "ABC"} ``` ### Trim whitespace [Section titled ‚ÄúTrim whitespace‚Äù](#trim-whitespace) Remove unwanted whitespace from strings: ```tql from {input: " hello ", data: "world "}, {input: " test", data: " value "} input = input.trim() data = data.trim() ``` ```tql {input: "hello", data: "world"} {input: "test", data: "value"} ``` ### Capitalize strings [Section titled ‚ÄúCapitalize strings‚Äù](#capitalize-strings) Capitalize the first letter of a string: ```tql from {word: "hello", phrase: "good morning"}, {word: "world", phrase: "how are you"} word = word.capitalize() ``` ```tql {word: "Hello", phrase: "good morning"} {word: "World", phrase: "how are you"} ``` ## Mathematical operations [Section titled ‚ÄúMathematical operations‚Äù](#mathematical-operations) Perform calculations on numeric values within your events. ### Basic arithmetic [Section titled ‚ÄúBasic arithmetic‚Äù](#basic-arithmetic) Use standard arithmetic operators: ```tql from {a: 10, b: 3}, {a: 20, b: 4} sum = a + b diff = a - b product = a * b quotient = (a / b).int() ``` ```tql {a: 10, b: 3, sum: 13, diff: 7, product: 30, quotient: 3} {a: 20, b: 4, sum: 24, diff: 16, product: 80, quotient: 5} ``` ### Rounding numbers [Section titled ‚ÄúRounding numbers‚Äù](#rounding-numbers) Round numbers to specific precision: ```tql from {value: 3.14159}, {value: 2.71828} rounded = value.round() ceil_val = value.ceil() floor_val = value.floor() ``` ```tql {value: 3.14159, rounded: 3, ceil_val: 4, floor_val: 3} {value: 2.71828, rounded: 3, ceil_val: 3, floor_val: 2} ``` ### Mathematical functions [Section titled ‚ÄúMathematical functions‚Äù](#mathematical-functions) Use [`abs()`](/reference/functions/abs) for absolute values and [`sqrt()`](/reference/functions/sqrt) for square roots: ```tql from {x: -5, y: 16}, {x: -10, y: 25} abs_x = x.abs() sqrt_y = y.sqrt() ``` ```tql { x: -5, y: 16, abs_x: 5, sqrt_y: 4.0, } { x: -10, y: 25, abs_x: 10, sqrt_y: 5.0, } ``` ## Working with null values [Section titled ‚ÄúWorking with null values‚Äù](#working-with-null-values) Handle missing or null values gracefully in your data. ### Provide default values [Section titled ‚ÄúProvide default values‚Äù](#provide-default-values) Use the `else` keyword to replace null values: ```tql from {name: "alice", age: 30}, {name: "bob"}, {name: "charlie", age: 25} age = age? else 0 status = status? else "unknown" ``` ```tql {name: "alice", age: 30, status: "unknown"} {name: "bob", age: 0, status: "unknown"} {name: "charlie", age: 25, status: "unknown"} ``` ## Create new values [Section titled ‚ÄúCreate new values‚Äù](#create-new-values) Generate new values using built-in functions: ### Generate unique identifiers [Section titled ‚ÄúGenerate unique identifiers‚Äù](#generate-unique-identifiers) Use [`uuid()`](/reference/functions/uuid) to create unique identifiers: ```tql from {user: "alice", action: "login"}, {user: "bob", action: "create"} event_id = uuid(version="v7") session_id = uuid() ``` ```tql { user: "alice", action: "login", event_id: "0198147a-d167-7292-80fa-2665c1263279", session_id: "a09a7f44-b665-4f95-bc44-c52fbdb8f428", } { user: "bob", action: "create", event_id: "0198147a-d167-72ad-80b4-e052c2287add", session_id: "030349dc-2585-49ad-af58-d448ff718c05", } ``` ### Generate random numbers [Section titled ‚ÄúGenerate random numbers‚Äù](#generate-random-numbers) Use [`random()`](/reference/functions/random) to generate random values: ```tql from { random_float: random(), random_int: (random() * 100).int(), random_choice: "heads" if random() < 0.5 else "tails", } ``` ```tql { random_float: 0.3215780368890365, random_int: 88, random_choice: "tails", } ``` ## Access external values [Section titled ‚ÄúAccess external values‚Äù](#access-external-values) Retrieve values from external sources like the environment, configuration, or files: ### Read environment variables [Section titled ‚ÄúRead environment variables‚Äù](#read-environment-variables) Use [`env()`](/reference/functions/env) to access environment variables: ```tql from { home_dir: env("HOME"), shell: env("SHELL"), custom_var: env("MY_APP_CONFIG") else "/default/config", } ``` ```tql { home_dir: "/Users/alice", shell: "/opt/homebrew/bin/fish", custom_var: "/default/config", } ``` ### Access configuration [Section titled ‚ÄúAccess configuration‚Äù](#access-configuration) Use [`config()`](/reference/functions/config) to read Tenzir‚Äôs configuration: ```tql from { tenzir_config: config(), } ``` ### Read file contents [Section titled ‚ÄúRead file contents‚Äù](#read-file-contents) Use [`file_contents()`](/reference/functions/file_contents) to read files: ```tql from { api_key: file_contents("/etc/secrets/api_key"), } ``` ### Access secrets [Section titled ‚ÄúAccess secrets‚Äù](#access-secrets) Use [`secret()`](/reference/functions/secret) to retrieve [secrets](/explanations/secrets): ```tql from { auth_token: secret("AUTH_TOKEN"), } ``` ## Type inspection [Section titled ‚ÄúType inspection‚Äù](#type-inspection) Examine data types at runtime: ### Get type information [Section titled ‚ÄúGet type information‚Äù](#get-type-information) Use [`type_of()`](/reference/functions/type_of) to inspect value types. Note that this function returns detailed type information as objects: ```tql from { str: "hello", num: 42, float: 3.14, bool: true, arr: [1, 2, 3], obj: {key: "value"} } str_type = str.type_of() num_type = num.type_of() float_type = float.type_of() bool_type = bool.type_of() arr_type = arr.type_of() obj_type = obj.type_of() ``` ```tql { str: "hello", num: 42, float: 3.14, bool: true, arr: [1, 2, 3], obj: {key: "value"}, str_type: {name: null, kind: "string", attributes: [], state: null}, num_type: {name: null, kind: "int64", attributes: [], state: null}, float_type: {name: null, kind: "double", attributes: [], state: null}, bool_type: {name: null, kind: "bool", attributes: [], state: null}, arr_type: {name: null, kind: "list", attributes: [], state: {type: {name: null, kind: "int64", attributes: [], state: null}}}, obj_type: {name: null, kind: "record", attributes: [], state: {fields: [{name: "key", type: {name: null, kind: "string", attributes: [], state: null}}]}} } ``` ### Get type identifiers [Section titled ‚ÄúGet type identifiers‚Äù](#get-type-identifiers) Use [`type_id()`](/reference/functions/type_id) for type comparison: ```tql from {value1: "text", value2: 123, value3: "456"} type1 = value1.type_id() type2 = value2.type_id() type3 = value3.type_id() same_type = value1.type_id() == value3.type_id() ``` ```tql { value1: "text", value2: 123, value3: "456", type1: "2476398993549b5", type2: "5b0d4f0b0b167404", type3: "2476398993549b5", same_type: true, } ``` ## Combining transformations [Section titled ‚ÄúCombining transformations‚Äù](#combining-transformations) Real-world data often requires multiple transformations: ```tql from {temp_f: "72.5", location: " new york "}, {temp_f: "89.1", location: "los angeles"} temp_c = ((temp_f.float() - 32) * 5 / 9).round() location = location.trim().to_title() reading = f"{temp_c}¬∞C in {location}" ``` ```tql { temp_f: "72.5", location: "New York", temp_c: 23, reading: "23¬∞C in New York", } { temp_f: "89.1", location: "Los Angeles", temp_c: 32, reading: "32¬∞C in Los Angeles", } ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Validate before converting**: Check that values can be converted to avoid errors. 2. **Use appropriate types**: Convert to the most specific type needed (e.g., `int` instead of `float` for whole numbers). 3. **Handle edge cases**: Always consider what happens with null values or invalid input. 4. **Chain operations efficiently**: Combine multiple transformations in a single `set` statement when possible. ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Filter and select data](/guides/data-shaping/filter-and-select-data) - Learn about filtering and field selection * [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations * [Manipulate strings](/guides/data-shaping/manipulate-strings) - Advanced string manipulation techniques

# Transform collections

Lists and records are fundamental data structures in TQL. This guide shows you how to work with these collections - accessing elements, transforming values, and combining data structures. ## Work with lists [Section titled ‚ÄúWork with lists‚Äù](#work-with-lists) Lists (arrays) contain ordered sequences of values. Let‚Äôs explore how to manipulate them. ### Access list elements [Section titled ‚ÄúAccess list elements‚Äù](#access-list-elements) Get values from specific positions: ```tql from {items: ["first", "second", "third", "fourth"]} set first_item = items[0] set last_item = items[-1] set length = items.length() ``` ```tql { items: ["first", "second", "third", "fourth"], first_item: "first", last_item: "fourth", length: 4 } ``` Index notation: * `[0]` - First element (0-based indexing) * `[-1]` - Last element (negative indices count from end) * `.length()` - Get the number of elements ### Add elements to lists [Section titled ‚ÄúAdd elements to lists‚Äù](#add-elements-to-lists) Use [`append()`](/reference/functions/append) and [`prepend()`](/reference/functions/prepend): ```tql from {colors: ["red", "green"]} set with_blue = colors.append("blue") set with_yellow = with_blue.prepend("yellow") set multi_append = colors.append("blue").append("purple") ``` ```tql { colors: ["red", "green"], with_blue: ["red", "green", "blue"], with_yellow: ["yellow", "red", "green", "blue"], multi_append: ["red", "green", "blue", "purple"] } ``` ### Combine lists [Section titled ‚ÄúCombine lists‚Äù](#combine-lists) Join multiple lists with [`concatenate()`](/reference/functions/concatenate) or spread syntax: ```tql from { list1: [1, 2, 3], list2: [4, 5, 6], list3: [7, 8, 9] } set combined = concatenate(concatenate(list1, list2), list3) set spread = [...list1, ...list2, ...list3] set with_value = [...list1, 10, ...list2] ``` ```tql { list1: [1, 2, 3], list2: [4, 5, 6], list3: [7, 8, 9], combined: [1, 2, 3, 4, 5, 6, 7, 8, 9], spread: [1, 2, 3, 4, 5, 6, 7, 8, 9], with_value: [1, 2, 3, 10, 4, 5, 6] } ``` ### Transform list elements [Section titled ‚ÄúTransform list elements‚Äù](#transform-list-elements) Apply functions to each element with [`map()`](/reference/functions/map): ```tql from { prices: [10, 20, 30], names: ["alice", "bob", "charlie"] } set with_tax = prices.map(p => p * 1.1) set uppercase = names.map(n => n.to_upper()) set squared = prices.map(x => x * x) ``` ```tql { prices: [10, 20, 30], names: ["alice", "bob", "charlie"], with_tax: [11.0, 22.0, 33.0], uppercase: ["ALICE", "BOB", "CHARLIE"], squared: [100, 400, 900] } ``` ### Filter list elements [Section titled ‚ÄúFilter list elements‚Äù](#filter-list-elements) Keep only elements that match a condition with [`where()`](/reference/functions/where): ```tql from { numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], users: ["alice", "bob", "anna", "alex"] } // Note: The modulo operator (%) is not currently supported in TQL // Here's an alternative approach for filtering: set big_nums = numbers.where(n => n > 5) set small_nums = numbers.where(n => n <= 5) set a_names = users.where(u => u.starts_with("a")) ``` ```tql { numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], users: ["alice", "bob", "anna", "alex"], big_nums: [6, 7, 8, 9, 10], small_nums: [1, 2, 3, 4, 5], a_names: ["alice", "anna", "alex"] } ``` ### Sort lists [Section titled ‚ÄúSort lists‚Äù](#sort-lists) Order elements with [`sort()`](/reference/functions/sort): ```tql from { numbers: [3, 1, 4, 1, 5, 9], words: ["zebra", "apple", "banana"] } set sorted_nums = numbers.sort() set sorted_words = words.sort() // Note: reverse() is only for strings, not lists // To reverse a list, you would need to use the reverse operator in a pipeline ``` ```tql { numbers: [3, 1, 4, 1, 5, 9], words: ["zebra", "apple", "banana"], sorted_nums: [1, 1, 3, 4, 5, 9], sorted_words: ["apple", "banana", "zebra"] } ``` ### Get unique values [Section titled ‚ÄúGet unique values‚Äù](#get-unique-values) Remove duplicates with [`distinct()`](/reference/functions/distinct): ```tql from { items: ["a", "b", "a", "c", "b", "d"], numbers: [1, 2, 2, 3, 3, 3, 4] } set unique_items = distinct(items) set unique_nums = distinct(numbers) ``` ```tql { items: ["a", "b", "a", "c", "b", "d"], numbers: [1, 2, 2, 3, 3, 3, 4], unique_items: ["a", "b", "c", "d"], unique_nums: [1, 2, 3, 4] } ``` ### Flatten nested lists [Section titled ‚ÄúFlatten nested lists‚Äù](#flatten-nested-lists) Note: Direct list flattening is not currently supported in TQL. The [`flatten()`](/reference/functions/flatten) function is designed for flattening records, not lists. To work with nested lists, you would need to process them element by element. ## Work with records [Section titled ‚ÄúWork with records‚Äù](#work-with-records) Records (objects) contain key-value pairs. Here‚Äôs how to manipulate them. ### Access record fields [Section titled ‚ÄúAccess record fields‚Äù](#access-record-fields) Get values using dot notation or brackets: ```tql from { user: { name: "Alice", age: 30, address: { city: "NYC", zip: "10001" } } } set name = user.name set city = user.address.city set zip = user["address"]["zip"] set has_email = user.has("email") ``` ```tql { user: { name: "Alice", age: 30, address: {city: "NYC", zip: "10001"} }, name: "Alice", city: "NYC", zip: "10001", has_email: false } ``` ### Get keys and values [Section titled ‚ÄúGet keys and values‚Äù](#get-keys-and-values) Extract field names and values: ```tql from { config: { host: "localhost", port: 8080, ssl: true } } set field_names = config.keys() // Note: values() function is not available set num_fields = config.keys().length() ``` ```tql { config: {host: "localhost", port: 8080, ssl: true}, field_names: ["host", "port", "ssl"], num_fields: 3 } ``` ### Merge records [Section titled ‚ÄúMerge records‚Äù](#merge-records) Combine multiple records with [`merge()`](/reference/functions/merge) or spread syntax: ```tql from { defaults: {host: "localhost", port: 80, ssl: false}, custom: {port: 8080, ssl: true} } set merged = merge(defaults, custom) set spread = {...defaults, ...custom} set with_extra = {...defaults, ...custom, debug: true} ``` ```tql { defaults: {host: "localhost", port: 80, ssl: false}, custom: {port: 8080, ssl: true}, merged: {host: "localhost", port: 8080, ssl: true}, spread: {host: "localhost", port: 8080, ssl: true}, with_extra: {host: "localhost", port: 8080, ssl: true, debug: true} } ``` ### Transform record values [Section titled ‚ÄúTransform record values‚Äù](#transform-record-values) Note: The `map_values` function is not available in TQL. To transform record values, you would need to reconstruct the record with transformed values manually: ```tql from { prices: { apple: 1.50, banana: 0.75, orange: 2.00 } } // Manual transformation example: set with_tax = { apple: prices.apple * 1.1, banana: prices.banana * 1.1, orange: prices.orange * 1.1 } ``` ### Filter record fields [Section titled ‚ÄúFilter record fields‚Äù](#filter-record-fields) Keep only specific fields: ```tql from { user: { id: 123, name: "Alice", email: "alice@example.com", password: "secret", api_key: "xyz123" } } // Note: filter_keys and select functions are not available // Manual field selection: set public_info = { id: user.id, name: user.name, email: user.email } set contact = { name: user.name, email: user.email } ``` ```tql { user: { id: 123, name: "Alice", email: "alice@example.com", password: "secret", api_key: "xyz123" }, public_info: { id: 123, name: "Alice", email: "alice@example.com" }, contact: { name: "Alice", email: "alice@example.com" } } ``` ## Combine lists and records [Section titled ‚ÄúCombine lists and records‚Äù](#combine-lists-and-records) Work with collections of records: ```tql from { users: [ {name: "Alice", age: 30, city: "NYC"}, {name: "Bob", age: 25, city: "SF"}, {name: "Charlie", age: 35, city: "NYC"} ] } set names = users.map(u => u.name) set nyc_users = users.where(u => u.city == "NYC") set avg_age = users.map(u => u.age).sum() / users.length() ``` ```tql { users: [ {name: "Alice", age: 30, city: "NYC"}, {name: "Bob", age: 25, city: "SF"}, {name: "Charlie", age: 35, city: "NYC"} ], names: ["Alice", "Bob", "Charlie"], nyc_users: [ {name: "Alice", age: 30, city: "NYC"}, {name: "Charlie", age: 35, city: "NYC"} ], avg_age: 30.0 } ``` ## Advanced transformations [Section titled ‚ÄúAdvanced transformations‚Äù](#advanced-transformations) ### Zip lists together [Section titled ‚ÄúZip lists together‚Äù](#zip-lists-together) Combine parallel lists with [`zip()`](/reference/functions/zip): ```tql from { names: ["Alice", "Bob", "Charlie"], ages: [30, 25, 35], cities: ["NYC", "SF", "LA"] } // Note: zip only takes 2 arguments, returns records with left/right fields set name_age = zip(names, ages) set zipped = zip(name_age, cities) set users = zipped.map(z => { name: z.left.left, age: z.left.right, city: z.right }) ``` ```tql { names: ["Alice", "Bob", "Charlie"], ages: [30, 25, 35], cities: ["NYC", "SF", "LA"], name_age: [ {left: "Alice", right: 30}, {left: "Bob", right: 25}, {left: "Charlie", right: 35} ], zipped: [ {left: {left: "Alice", right: 30}, right: "NYC"}, {left: {left: "Bob", right: 25}, right: "SF"}, {left: {left: "Charlie", right: 35}, right: "LA"} ], users: [ {name: "Alice", age: 30, city: "NYC"}, {name: "Bob", age: 25, city: "SF"}, {name: "Charlie", age: 35, city: "LA"} ] } ``` ### Practical example: Pairing related data [Section titled ‚ÄúPractical example: Pairing related data‚Äù](#practical-example-pairing-related-data) Combine parallel arrays with transformations for data normalization: ```tql from { // DNS query data with parallel arrays answers: ["192.168.1.1", "10.0.0.1", "172.16.0.1"], ttls: [300s, 600s, 900s], // Certificate SAN names domains: ["example.com", "www.example.com", "api.example.com"] } // Pair DNS answers with their TTLs and transform dns_records = zip(answers, ttls).map(x => { rdata: x.left, ttl_seconds: x.right.count_seconds(), cached_until: now() + x.right }) // Transform simple arrays to structured data san_entries = domains.map(name => { name: name, type: "DNSName", verified: name.ends_with(".example.com") }) ``` ```tql { answers: ["192.168.1.1", "10.0.0.1", "172.16.0.1"], ttls: [300s, 600s, 900s], domains: ["example.com", "www.example.com", "api.example.com"], dns_records: [ { rdata: "192.168.1.1", ttl_seconds: 300, cached_until: 2025-08-14T12:36:45.123456Z }, { rdata: "10.0.0.1", ttl_seconds: 600, cached_until: 2025-08-14T12:41:45.123456Z }, { rdata: "172.16.0.1", ttl_seconds: 900, cached_until: 2025-08-14T12:46:45.123456Z } ], san_entries: [ { name: "example.com", type: "DNSName", verified: true }, { name: "www.example.com", type: "DNSName", verified: true }, { name: "api.example.com", type: "DNSName", verified: true } ] } ``` This pattern is particularly useful when: * Converting parallel arrays from APIs or logs into structured records * Normalizing data for standard formats (like OCSF) * Adding computed fields during the transformation ### Enumerate with indices [Section titled ‚ÄúEnumerate with indices‚Äù](#enumerate-with-indices) Add row numbers to your data using the [`enumerate`](/reference/operators/enumerate) operator: ```tql from {item: "apple"}, {item: "banana"}, {item: "cherry"} enumerate row ``` ```tql {row: 0, item: "apple"} {row: 1, item: "banana"} {row: 2, item: "cherry"} ``` This is useful for tracking position in sequences or creating unique identifiers for each event. ## Practical examples [Section titled ‚ÄúPractical examples‚Äù](#practical-examples) ### Extract and transform nested data [Section titled ‚ÄúExtract and transform nested data‚Äù](#extract-and-transform-nested-data) ```tql from { response: { status: 200, data: { users: [ {id: 1, name: "Alice", scores: [85, 92, 88]}, {id: 2, name: "Bob", scores: [78, 81, 85]} ] } } } set users = response.data.users set summaries = users.map(u, { name: u.name, avg_score: u.scores.sum() / u.scores.length(), max_score: u.scores.max() }) ``` ```tql { response: {...}, users: [ {id: 1, name: "Alice", scores: [85, 92, 88]}, {id: 2, name: "Bob", scores: [78, 81, 85]} ], summaries: [ {name: "Alice", avg_score: 88.33333333333333, max_score: 92}, {name: "Bob", avg_score: 81.33333333333333, max_score: 85} ] } ``` ### Work with indexed data [Section titled ‚ÄúWork with indexed data‚Äù](#work-with-indexed-data) Create lookups by extracting specific fields: ```tql from { items: [ {id: "A001", name: "Widget", price: 10}, {id: "B002", name: "Gadget", price: 20}, {id: "C003", name: "Tool", price: 15} ] } set first_item = items.first() set ids = items.map(item => item.id) set names = items.map(item => item.name) set expensive = items.where(item => item.price > 15) ``` ```tql { items: [...], first_item: {id: "A001", name: "Widget", price: 10}, ids: ["A001", "B002", "C003"], names: ["Widget", "Gadget", "Tool"], expensive: [ {id: "B002", name: "Gadget", price: 20} ] } ``` ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Choose the right structure**: Use lists for ordered data, records for named fields 2. **Avoid deep nesting**: Flatten structures when possible for easier access 3. **Use functional methods**: Prefer `map()`, `filter()`, etc. over manual loops 4. **Handle empty collections**: Check length before accessing elements 5. **Preserve immutability**: Collection functions return new values, not modify existing ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Filter and select data](/guides/data-shaping/filter-and-select-data) - Filter entire event streams * [Transform basic values](/guides/data-shaping/transform-basic-values) - Work with simple types * [Shape data](/guides/data-shaping/shape-data) - Overview of all shaping operations

# Work with time

Time is fundamental in data analysis. Whether you‚Äôre analyzing logs, tracking events, or monitoring systems, you need to parse timestamps, calculate durations, and format dates. This guide shows you how to work with time values in TQL. ## Understand time types [Section titled ‚ÄúUnderstand time types‚Äù](#understand-time-types) TQL has two main time-related types: * **`time`**: A specific point in time (timestamp) * **`duration`**: A span of time (interval) ```tql from { timestamp: 2024-01-15T10:30:45.123456, interval: 5min } later = timestamp + interval earlier = timestamp - 2h ``` ```tql { timestamp: 2024-01-15T10:30:45.123456Z, interval: 5min, later: 2024-01-15T10:35:45.123456Z, earlier: 2024-01-15T08:30:45.123456Z } ``` ## Get the current time [Section titled ‚ÄúGet the current time‚Äù](#get-the-current-time) Use [`now()`](/reference/functions/now) to get the current timestamp: ```tql from { current_time: now() } today = current_time.round(1d) ``` ```tql { current_time: 2025-07-21T19:06:55.047259Z, today: 2025-07-22T00:00:00Z, } ``` ## Parse time from strings [Section titled ‚ÄúParse time from strings‚Äù](#parse-time-from-strings) Convert string representations to proper timestamps with [`parse_time()`](/reference/functions/parse_time): ```tql from { iso: "2024-01-15T10:30:45", custom: "15/Jan/2024:10:30:45", unix: "1705316445" } iso_time = iso.parse_time("%Y-%m-%dT%H:%M:%S") custom_time = custom.parse_time("%d/%b/%Y:%H:%M:%S") unix_time = unix.int().seconds().from_epoch() ``` ```tql { iso: "2024-01-15T10:30:45", custom: "15/Jan/2024:10:30:45", unix: "1705316445", iso_time: 2024-01-15T10:30:45Z, custom_time: 2024-01-15T10:30:45Z, unix_time: 2024-01-15T11:00:45Z } ``` Common format specifiers: * `%Y` - 4-digit year * `%m` - Month (01-12) * `%d` - Day (01-31) * `%H` - Hour (00-23) * `%M` - Minute (00-59) * `%S` - Second (00-59) * `%b` - Month name (Jan, Feb, etc.) * `%a` - Weekday name (Mon, Tue, etc.) ## Format time to strings [Section titled ‚ÄúFormat time to strings‚Äù](#format-time-to-strings) Convert timestamps to custom string formats with [`format_time()`](/reference/functions/format_time): ```tql from {event_time: 2024-01-15T10:30:45.123456} iso = event_time.format_time("%Y-%m-%dT%H:%M:%S.%f") date_only = event_time.format_time("%Y-%m-%d") us_format = event_time.format_time("%m/%d/%Y %I:%M %p") log_format = event_time.format_time("%d/%b/%Y:%H:%M:%S") ``` ```tql { event_time: 2024-01-15T10:30:45.123456Z, iso: "2024-01-15T10:30:45.123456000.%f", date_only: "2024-01-15", us_format: "01/15/2024 10:30 AM", log_format: "15/Jan/2024:10:30:45.123456000" } ``` ## Extract time components [Section titled ‚ÄúExtract time components‚Äù](#extract-time-components) Get individual parts of a timestamp using [`year()`](/reference/functions/year), [`month()`](/reference/functions/month), [`day()`](/reference/functions/day), [`hour()`](/reference/functions/hour), [`minute()`](/reference/functions/minute), and [`second()`](/reference/functions/second): ```tql from {timestamp: 2024-01-15T10:30:45.123456} year = timestamp.year() month = timestamp.month() day = timestamp.day() hour = timestamp.hour() minute = timestamp.minute() second = timestamp.second() ``` ```tql { timestamp: 2024-01-15T10:30:45.123456Z, year: 2024, month: 1, day: 15, hour: 10, minute: 30, second: 45.123456 } ``` ## Work with durations [Section titled ‚ÄúWork with durations‚Äù](#work-with-durations) Create and manipulate time intervals: ```tql from { start: 2024-01-15T10:00:00, end: 2024-01-15T14:30:00 } elapsed = end - start hours = elapsed.count_hours() minutes = elapsed.count_minutes() seconds = elapsed.count_seconds() ``` ```tql { start: 2024-01-15T10:00:00Z, end: 2024-01-15T14:30:00Z, elapsed: 4.5h, hours: 4.5, minutes: 270.0, seconds: 16200.0 } ``` ### Count duration components [Section titled ‚ÄúCount duration components‚Äù](#count-duration-components) Extract different time units from durations: ```tql from { duration: 90d + 4h + 30min + 45s + 123ms + 456us + 789ns } years = duration.count_years() months = duration.count_months() weeks = duration.count_weeks() days = duration.count_days() hours = duration.count_hours() minutes = duration.count_minutes() seconds = duration.count_seconds() milliseconds = duration.count_milliseconds() microseconds = duration.count_microseconds() nanoseconds = duration.count_nanoseconds() ``` ```tql { duration: 90.18802226223136d, years: 0.24692641809819874, months: 2.963117017178385, weeks: 12.884003180318764, days: 90.18802226223136, hours: 2164.5125342935526, minutes: 129870.75205761315, seconds: 7792245.123456789, milliseconds: 7792245123.456789, microseconds: 7792245123456.789, nanoseconds: 7792245123456789, } ``` ### Convert between time units [Section titled ‚ÄúConvert between time units‚Äù](#convert-between-time-units) Use [`months()`](/reference/functions/months) to create month-based durations: ```tql from { quarterly_period: 3 } quarter = quarterly_period.months() days_in_quarter = quarter.count_days() weeks_in_quarter = quarter.count_weeks() ``` ```tql { quarterly_period: 3, quarter: 91.310625d, days_in_quarter: 91.310625, weeks_in_quarter: 13.044375 } ``` ### Create durations [Section titled ‚ÄúCreate durations‚Äù](#create-durations) Use duration literals or functions: ```tql from { five_minutes: 5min, one_hour: 1h, custom: 90.seconds(), from_parts: 2.hours() + 30.minutes(), } ``` ```tql { five_minutes: 5min, one_hour: 1h, custom: 1.5min, from_parts: 2.5h, } ``` Duration units: * `ns` or `nanoseconds()` - Nanoseconds * `us` or `microseconds()` - Microseconds * `ms` or `milliseconds()` - Milliseconds * `s` or `seconds()` - Seconds * `min` or `minutes()` - Minutes * `h` or `hours()` - Hours * `d` or `days()` - Days (24 hours) * `w` or `weeks()` - Weeks (7 days) * `y` or `years()` - Years (365 days) ## Calculate time differences [Section titled ‚ÄúCalculate time differences‚Äù](#calculate-time-differences) Find elapsed time between events: ```tql from { login: 2024-01-15T09:00:00, first_action: 2024-01-15T09:05:30, logout: 2024-01-15T17:30:00, } time_to_action = first_action - login session_duration = logout - login active_hours = session_duration.count_hours() ``` ```tql { login: 2024-01-15T09:00:00Z, first_action: 2024-01-15T09:05:30Z, logout: 2024-01-15T17:30:00Z, time_to_action: 5.5min, session_duration: 8.5h, active_hours: 8.5, } ``` ## Add and subtract time [Section titled ‚ÄúAdd and subtract time‚Äù](#add-and-subtract-time) Perform time arithmetic: ```tql from { event_time: 2024-01-15T10:30:00 } one_hour_later = event_time + 1h yesterday = event_time - 1d next_week = event_time + 7d thirty_mins_ago = event_time - 30min ``` ```tql { event_time: 2024-01-15T10:30:00Z, one_hour_later: 2024-01-15T11:30:00Z, yesterday: 2024-01-14T10:30:00Z, next_week: 2024-01-22T10:30:00Z, thirty_mins_ago: 2024-01-15T10:00:00Z } ``` ## Round timestamps [Section titled ‚ÄúRound timestamps‚Äù](#round-timestamps) Round timestamps to specific intervals: ```tql from { precise_time: 2024-01-15T10:37:42.847621 } to_minute = precise_time.round(1min) to_hour = precise_time.round(1h) to_day = precise_time.round(1d) to_5min = precise_time.round(5min) ``` ```tql { precise_time: 2024-01-15T10:37:42.847621Z, to_minute: 2024-01-15T10:38:00Z, to_hour: 2024-01-15T11:00:00Z, to_day: 2024-01-15T00:00:00Z, to_5min: 2024-01-15T10:40:00Z } ``` ## Convert Unix timestamps [Section titled ‚ÄúConvert Unix timestamps‚Äù](#convert-unix-timestamps) Work with Unix epoch timestamps using [`from_epoch()`](/reference/functions/from_epoch) and [`since_epoch()`](/reference/functions/since_epoch): ```tql from { unix_seconds: 1705316445, unix_millis: 1705316445123, unix_micros: 1705316445123456 } from_seconds = unix_seconds.seconds().from_epoch() from_millis = unix_millis.milliseconds().from_epoch() from_micros = unix_micros.microseconds().from_epoch() back_to_unix = from_seconds.since_epoch().count_seconds() ``` ```tql { unix_seconds: 1705316445, unix_millis: 1705316445123, unix_micros: 1705316445123456, from_seconds: 2024-01-15T11:00:45Z, from_millis: 2024-01-15T11:00:45.123Z, from_micros: 2024-01-15T11:00:45.123456Z, back_to_unix: 1705316445.0 } ``` ## Practical examples [Section titled ‚ÄúPractical examples‚Äù](#practical-examples) ### Calculate request duration [Section titled ‚ÄúCalculate request duration‚Äù](#calculate-request-duration) ```tql from { request_start: 2024-01-15T10:30:45.123, request_end: 2024-01-15T10:30:47.456, } duration = request_end - request_start duration_ms = duration.count_milliseconds() ``` ```tql { request_start: 2024-01-15T10:30:45.123Z, request_end: 2024-01-15T10:30:47.456Z, duration: 2.333s, duration_ms: 2333.0, } ``` ### Group events by time window [Section titled ‚ÄúGroup events by time window‚Äù](#group-events-by-time-window) ```tql from { event_time: 2024-01-15T10:37:42.847621, event_type: "login", } hour_bucket = event_time.round(1h) day_bucket = event_time.round(1d) five_min_bucket = event_time.round(5min) ``` ```tql { event_time: 2024-01-15T10:37:42.847621Z, event_type: "login", hour_bucket: 2024-01-15T11:00:00Z, day_bucket: 2024-01-15T00:00:00Z, five_min_bucket: 2024-01-15T10:40:00Z, } ``` ### Calculate age from timestamp [Section titled ‚ÄúCalculate age from timestamp‚Äù](#calculate-age-from-timestamp) ```tql from { created_at: 2024-01-01T00:00:00 } age = now() - created_at days_old = age.count_days() hours_old = age.count_hours() human_readable = f"{days_old.round()} days ago" ``` ```tql { created_at: 2024-01-01T00:00:00Z, age: 567.7989434994097d, days_old: 567.7989434994097, hours_old: 13627.174643985833, human_readable: "568 days ago", } ``` ### Parse various log timestamps [Section titled ‚ÄúParse various log timestamps‚Äù](#parse-various-log-timestamps) ```tql from { apache: "15/Jan/2024:10:30:45 +0000", nginx: "2024/01/15 10:30:45", syslog: "Jan 15 10:30:45" } apache_time = apache.parse_time("%d/%b/%Y:%H:%M:%S %z") nginx_time = nginx.parse_time("%Y/%m/%d %H:%M:%S") // For syslog, we need to add the year syslog_time = ("2024 " + syslog).parse_time("%Y %b %d %H:%M:%S") ``` ```tql { apache: "15/Jan/2024:10:30:45 +0000", nginx: "2024/01/15 10:30:45", syslog: "Jan 15 10:30:45", apache_time: 2024-01-15T10:30:45Z, nginx_time: 2024-01-15T10:30:45Z, syslog_time: 2024-01-15T10:30:45Z, } ``` ## Replay and adjust time series [Section titled ‚ÄúReplay and adjust time series‚Äù](#replay-and-adjust-time-series) When working with historical data, you often need to replay events with their original timing or adjust timestamps for analysis. TQL provides two operators for this: [`delay`](/reference/operators/delay) and [`timeshift`](/reference/operators/timeshift). ### Adjust timestamps with timeshift [Section titled ‚ÄúAdjust timestamps with timeshift‚Äù](#adjust-timestamps-with-timeshift) The `timeshift` operator adjusts timestamps to a new baseline while preserving relative time differences. This is essential when you need to merge datasets from different time periods into a coherent timeline for comparative analysis. For example, you might want to overlay security incidents from multiple years to identify recurring patterns, or align test data from different runs to compare performance metrics side-by-side. ```tql from {event: "login", ts: 2020-06-15T09:00:00}, {event: "action", ts: 2020-06-15T09:05:30}, {event: "logout", ts: 2020-06-15T17:30:00} timeshift ts, start=2024-01-01 ``` ```tql {event: "login", ts: 2024-01-01T00:00:00Z} {event: "action", ts: 2024-01-01T00:05:30Z} {event: "logout", ts: 2024-01-01T08:30:00Z} ``` Notice how the 5.5-minute gap between login and action, and the 8.5-hour session duration are preserved, but all timestamps now start from January 1, 2024. You can also scale the time intervals with the `speed` parameter: ```tql from {event: "start", ts: 2020-01-01T00:00:00}, {event: "middle", ts: 2020-01-01T00:30:00}, {event: "end", ts: 2020-01-01T01:00:00} // Make intervals 10x longer timeshift ts, start=2024-01-01, speed=0.1 ``` ```tql {event: "start", ts: 2024-01-01T00:00:00Z} {event: "middle", ts: 2024-01-01T05:00:00Z} {event: "end", ts: 2024-01-01T10:00:00Z} ``` The 30-minute intervals became 5-hour intervals (10x longer with speed=0.1). ### Replay events in real time with delay [Section titled ‚ÄúReplay events in real time with delay‚Äù](#replay-events-in-real-time-with-delay) The `delay` operator replays events according to their timestamps by introducing sleep periods between events: ```tql from {ts: 2024-01-01T00:00:00, msg: "first"}, {ts: 2024-01-01T00:00:02, msg: "second"}, {ts: 2024-01-01T00:00:03, msg: "third"} delay ts, start=now(), speed=0.1 ``` With `speed=0.1`, the 2-second gap between first and second events becomes 20 seconds, and the 1-second gap between second and third becomes 10 seconds. This slower replay makes it easy to observe the delay in action. For replaying historical data with original timing: ```tql let $zeek_logs = "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" from_http $zeek_logs { decompress_zstd read_zeek_tsv } delay ts ``` This replays the logs with real-world inter-arrival times. If an event occurred at 10:00:00 and the next at 10:00:05, the operator waits 5 seconds between emitting them. ### Combine with periodic generation [Section titled ‚ÄúCombine with periodic generation‚Äù](#combine-with-periodic-generation) Use [`every`](/reference/operators/every) to generate events periodically, then replay them with modified timing: ```tql every 1s { from { ts: now(), message: "Periodic event" } } head 5 delay ts, speed=0.5 ``` This generates events every second but replays them at half speed (2 seconds between events). ### Practical example: Simulate real-time monitoring [Section titled ‚ÄúPractical example: Simulate real-time monitoring‚Äù](#practical-example-simulate-real-time-monitoring) Combine both operators to replay historical logs as if they‚Äôre happening now: ```tql // Load historical logs from_file "/path/to/historical-logs.json" // Shift timestamps to current time timeshift timestamp, start=now() // Replay at 5x speed to quickly review a day's worth of logs delay timestamp, speed=5.0 // Continue with normal processing // ... ``` ### Key differences [Section titled ‚ÄúKey differences‚Äù](#key-differences) * **`timeshift`**: Instantly adjusts all timestamps without delays * **`delay`**: Introduces real-time delays between events based on their timestamps Use `timeshift` when you need to analyze historical data with updated timestamps. Use `delay` when you want to replay events with realistic timing, such as for testing real-time processing systems or simulating live data streams. ## Best practices [Section titled ‚ÄúBest practices‚Äù](#best-practices) 1. **Use proper types**: Convert strings to time values early in your pipeline 2. **Be consistent**: Standardize timestamp formats across your data 3. **Consider timezones**: Be aware that TQL timestamps are timezone-aware 4. **Round appropriately**: Use rounding to group events into time buckets 5. **Handle null values**: Check for missing timestamps before calculations ## Related guides [Section titled ‚ÄúRelated guides‚Äù](#related-guides) * [Transform basic values](/guides/data-shaping/transform-basic-values) - Convert between data types * [Extract structured data from text](/guides/data-shaping/extract-structured-data-from-text) - Parse timestamps from logs * [Filter and select data](/guides/data-shaping/filter-and-select-data) - Filter by time ranges

# Build from source

Tenzir uses [CMake](https://cmake.org) as build system. Aside from a modern C++23 compiler, you need to ensure availability of the dependencies in the table below. ## Dependencies [Section titled ‚ÄúDependencies‚Äù](#dependencies) Every [release](https://github.com/tenzir/tenzir/releases) of Tenzir includes an [SBOM](https://en.wikipedia.org/wiki/Software_bill_of_materials) in [SPDX](https://spdx.dev) format that lists all dependencies and their versions. [Latest SBOM](https://github.com/tenzir/tenzir/releases/latest/download/tenzir.spdx.json) | Required | Dependency | Version | Description | | :------: | :------------------------------------------------------------: | :------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | | ‚úì | C++ Compiler | C++23 required | Tenzir is tested to compile with GCC >= 14.0 and Clang >= 19.0. | | ‚úì | [CMake](https://cmake.org) | >= 3.30 | Cross-platform tool for building, testing and packaging software. | | ‚úì | [CAF](https://github.com/actor-framework/actor-framework) | >= 1.1.0 | Implementation of the actor model in C++. (Bundled as submodule.) | | ‚úì | [OpenSSL](https://www.openssl.org) | | Utilities for secure networking and cryptography. | | ‚úì | [FlatBuffers](https://google.github.io/flatbuffers/) | >= 2.0.8 | Memory-efficient cross-platform serialization library. | | ‚úì | [Boost](https://www.boost.org) | >= 1.83.0 | Required as a general utility library. | | ‚úì | [Apache Arrow](https://arrow.apache.org) | >= 18.0.0 | Required for in-memory data representation. Must be built with Compute, Filesystem, S3, Zstd and Parquet enabled. For the `gcs` plugin, GCS needs to be enabled. | | ‚úì | [re2](https://github.com/google/re2) | | Required for regular expressione evaluation. | | ‚úì | [yaml-cpp](https://github.com/jbeder/yaml-cpp) | >= 0.6.2 | Required for reading YAML configuration files. | | ‚úì | [simdjson](https://github.com/simdjson/simdjson) | >= 4.0.0 | Required for high-performance JSON parsing. (Bundled as submodule.) | | ‚úì | [spdlog](https://github.com/gabime/spdlog) | >= 1.5 | Required for logging. | | ‚úì | [fmt](https://fmt.dev) | >= 10.0.0 | Required for formatted text output. | | ‚úì | [xxHash](https://github.com/Cyan4973/xxHash) | >= 0.8.0 | Required for computing fast hash digests. | | ‚úì | [robin-map](https://github.com/Tessil/robin-map) | >= 0.6.3 | Fast hash map and hash set using robin hood hashing. (Bundled as subtree.) | | ‚úì | [fast\_float](https://github.com/FastFloat/fast_float) | >= 3.2.0 | Required for parsing floating point numbers. (Bundled as submodule.) | | ‚úì | [libbacktrace](https://github.com/ianlancetaylor/libbacktrace) | >= 1.0 | Required for generating stack traces. (Only on Linux.) | | ‚úì | [libmaxminddb](https://github.com/maxmind/libmaxminddb) | >= 1.8.0 | Required for the `geoip` context. | | ‚úì | [reproc++](https://github.com/DaanDeMeyer/reproc) | >= v14.2.5 | Required for subprocess control. | | | [libpcap](https://www.tcpdump.org) | | Required for building the `pcap` plugin. | | | [librdkafka](https://github.com/confluentinc/librdkafka) | | Required for building the `kafka` plugin. | | | [http-parser](https://github.com/nodejs/http-parser) | | Required for building the `web` plugin. | | | [cppzmq](https://github.com/zeromq/cppzmq) | | Required for building the `zmq` plugin. | | | [clickhouse-cpp](https://github.com/clickhouse/clickhouse-cpp) | >= fbd7945 | Required for building the `clickhouse` plugin. (Bundled as submodule.) | | | [pfs](https://github.com/dtrugman/pfs) | | Required for the `processes` and `sockets` operators on Linux. | | | [Protocol Buffers](https://protobuf.dev) | >= 1.4.1 | Required for building the `velociraptor` plugin. | | | [gRPC](https://grpci.io) | >= 1.51 | Required for building the `velociraptor` plugin. | | | [rabbitmq-c](https://github.com/alanxz/rabbitmq-c) | | Required for building the `rabbitmq` plugin. | | | [yara](https://yara.readthedocs.io/) | >= 4.4.0 | Required for building the `yara` plugin. | | | [poetry](https://python-poetry.org) | | Required for building the Python bindings. | | | [Doxygen](http://www.doxygen.org) | | Required to build documentation for libtenzir. | | | [Pandoc](https://github.com/jgm/pandoc) | | Required to build the manpage for Tenzir. | | | [bash](https://www.gnu.org/software/bash/) | >= 4.0.0 | Required to run the integration tests. | | | [bats](https://bats-core.readthedocs.io) | >= 1.8.0 | Required to run the integration tests. | | | [uv](https://github.com/astral-sh/uv) | >= 0.7.2 | Required to run the python operator. | The minimum specified versions reflect those versions that we use in CI and manual testing. Older versions may still work in select cases. ## Compile [Section titled ‚ÄúCompile‚Äù](#compile) Building Tenzir involves the following steps: Clone the repository recursively: ```sh git clone https://github.com/tenzir/tenzir cd tenzir git submodule update --init --recursive -- libtenzir plugins ``` Configure the build with CMake. For faster builds, we recommend passing `-G Ninja` to `cmake`. ```sh cmake -B build # CMake defaults to a "Debug" build. When performance matters, use "Release" cmake -B build -DCMAKE_BUILD_TYPE=Release ``` Optionally, you can use the CMake TUI to visually configure the build: ```sh ccmake build ``` The source tree also contains a set of CMake presets that combine various configuration options into curated build flavors. You can list them with: ```sh cmake --list-presets ``` Build the executable: ```sh cmake --build build --target all ``` ## Test [Section titled ‚ÄúTest‚Äù](#test) After you have built the executable, run the unit and integration tests to verify that your build works as expected. ### Run unit tests [Section titled ‚ÄúRun unit tests‚Äù](#run-unit-tests) Run component-level unit tests via CTest: ```sh ctest --test-dir build ``` ### Run integration tests [Section titled ‚ÄúRun integration tests‚Äù](#run-integration-tests) Run end-to-end integration tests via [tenzir-test](https://github.com/tenzir/tenzir-test): ```sh cd test uvx tenzir-test ``` ## Install [Section titled ‚ÄúInstall‚Äù](#install) Install Tenzir system-wide: ```sh cmake --install build ``` If you prefer to install into a custom install prefix, install with `--prefix /path/to/install/prefix`. To remove debug symbols from the installed binaries and libraries, pass `--strip`. To install only files relevant for running Tenzir and not for plugin development pass `--component Runtime`. ## Clean [Section titled ‚ÄúClean‚Äù](#clean) In case you want to make changes to your build environment, we recommend deleting the build tree entirely: ```sh rm -rf build ``` This avoids subtle configuration glitches of transitive dependencies. For example, CMake doesn‚Äôt disable assertions when switching from a `Debug` to a `Release` build, but would do so when starting with a fresh build of type `Release`.

# Build the Docker image

Our [Tenzir Node Dockerfile](https://github.com/tenzir/tenzir/blob/main/Dockerfile) has two starting points: a *development* and *production* layer. Before building the image, make sure to fetch all submodules: ```sh git clone --recursive https://github.com/tenzir/tenzir cd tenzir git submodule update --init --recursive -- libtenzir plugins tenzir ``` ## Build the production image [Section titled ‚ÄúBuild the production image‚Äù](#build-the-production-image) The production image is optimized for size and security. This is the official `tenzir/tenzir` image. From the repository root, build it as follows: ```sh docker build -t tenzir/tenzir . ``` ## Build the development image [Section titled ‚ÄúBuild the development image‚Äù](#build-the-development-image) The development image `tenzir/tenzir-dev` contains all build-time dependencies of Tenzir. It runs with a `root` user to allow for building custom images that build additional Tenzir plugins. By default, Tenzir loads all installed plugins in our images. Build the development image by specifying it as `--target`: ```sh docker build -t tenzir/tenzir-dev --target development . ```

# Write a node plugin

Implementing a new plugin requires the following steps: 1. [Setup the scaffolding](#setup-the-scaffolding) 2. [Choose a plugin type](#choose-a-plugin-type) 3. [Implement the plugin interface](#implement-the-plugin-interface) 4. [Process configuration options](#process-configuration-options) 5. [Compile the source code](#compile-the-source-code) 6. [Add unit and integration tests](#add-unit-and-integration-tests) 7. [Package it](#package-it) Next, we‚Äôll discuss each step in more detail. ## Setup the scaffolding [Section titled ‚ÄúSetup the scaffolding‚Äù](#setup-the-scaffolding) The scaffolding of a plugin includes the CMake glue that makes it possible to use as static or dynamic plugin. Pass `-DTENZIR_ENABLE_STATIC_PLUGINS:BOOL=ON` to `cmake` to build plugins alongside Tenzir as static plugins. This option is always on for static binary builds. Tenzir ships with many plugins that showcase what a typical scaffold looks like. Have a look at the the [plugins](https://github.com/tenzir/tenzir/tree/main/plugins) directory, and an [example `CMakeLists.txt` file from the AMQP plugin](https://github.com/tenzir/tenzir/blob/main/plugins/amqp/CMakeLists.txt). We highly urge calling the provided `TenzirRegisterPlugin` CMake in your plugin‚Äôs `CMakeLists.txt` file instead of handrolling your CMake build scaffolding code. This ensures that your plugin always uses the recommended defaults. Non-static installations of Tenzir contain the `TenzirRegisterPlugin.cmake` modules. The typical structure of a plugin directory includes the following files/directories: * `README.md`: An overview of the plugin and how to use it. * `CHANGELOG.md`: A trail of user-facing changes. * `schema/`: new schemas that ship with this plugin. * `<plugin>.yaml.example`: the configuration knobs of the plugin. We comment out all options by default so that the file serves as reference. Users can uncomment specific settings they would like to adapt. The CMake build scaffolding installs all of the above files/directories, if present. ## Choose a plugin type [Section titled ‚ÄúChoose a plugin type‚Äù](#choose-a-plugin-type) Tenzir offers [a variety of customization points](/explanations/architecture), each of which defines its own API by inheriting from the plugin base class `tenzir::plugin`. When writing a new plugin, you can choose a subset of available types by inheriting from the respective plugin classes. Dreaded Diamond To avoid common issues with multiple inheritance, all intermediate plugin classes that inherit from `tenzir::plugin` use *virtual inheritance* to avoid issues with the [dreaded diamond](https://isocpp.org/wiki/faq/multiple-inheritance#mi-diamond). ## Implement the plugin interface [Section titled ‚ÄúImplement the plugin interface‚Äù](#implement-the-plugin-interface) After having the necessary CMake in place, you can now derive from one or more plugin base classes to define your own plugin. Based on the chosen plugin types, you must override one or more virtual functions with an implementation of your own. The basic anatomy of a plugin class looks as follows: ```cpp class example_plugin final : public virtual component_plugin, public virtual command_plugin { public: /// Loading logic. example_plugin(); /// Teardown logic. ~example_plugin() override; /// Initializes a plugin with its respective entries from the YAML config /// file, i.e., `plugin.<NAME>`. /// @param plugin_config The relevant subsection of the configuration. /// @param global_config The entire Tenzir configuration for potential access /// to global options. caf::error initialize(const record& plugin_config, const record& global_config) override; /// Returns the unique name of the plugin. std::string name() const override; // TODO: override pure virtual functions from the base classes. // ... }; ``` The plugin constructor should only perform minimal actions to instantiate a well-defined plugin instance. In particular, it should not throw or perform any operations that may potentially fail. For the actual plugin ramp up, please use the `initialize` function that processes the user configuration. The purpose of the destructor is to free any used resources owned by the plugin. Each plugin must have a unique name. This returned string should consicely identify the plugin internally. Please consult the documentation specific to each plugin type above to figure out what virtual function need overriding. In the above example, we have a `command_plugin` and a `component_plugin`. This requires implementing the following two interfaces: ```cpp component_plugin_actor make_component( node_actor::stateful_pointer<node_state> node) const override; std::pair<std::unique_ptr<command>, command::factory> make_command() const override; ``` After completing the implementation, you must now register the plugin. For example, to register the `example` plugin, include the following line after the plugin class definition: ```cpp // This line must not be in a namespace. TENZIR_REGISTER_PLUGIN(tenzir::plugins::example_plugin) ``` ## Process configuration options [Section titled ‚ÄúProcess configuration options‚Äù](#process-configuration-options) To configure a plugin at runtime, Tenzir first looks whether the YAML configuration contains a key with the plugin name under the top-level key `plugins`. Consider our example plugin with the name `example`: ```yaml plugins: example: option: 42 ``` Here, the plugin receives the record `{option: 42}` at load time. A plugin can process the configuration snippet by overriding the following function of `tenzir::plugin`: ```plaintext caf::error initialize(const record& plugin_config, const record& global_config) override; ``` Tenzir expects the plugin to be fully operational after calling `initialize`. Subsequent calls to the implemented customization points must have a well-defined behavior. ## Compile the source code [Section titled ‚ÄúCompile the source code‚Äù](#compile-the-source-code) ### Building alongside Tenzir [Section titled ‚ÄúBuilding alongside Tenzir‚Äù](#building-alongside-tenzir) When configuring the Tenzir build, you need to tell CMake the path to the plugin source directory. The CMake variable `TENZIR_PLUGINS` holds a comma-separated list of paths to plugin directories. To test that Tenzir loads the plugin properly, you can use `tenzir --plugins=example version` and look into the `plugins`. A key-value pair with your plugin name and version should exist in the output. Refer to the [plugin loading](/explanations/configuration) section of the documentation to find out how to explicitly de-/activate plugins. ### Building against an installed Tenzir [Section titled ‚ÄúBuilding against an installed Tenzir‚Äù](#building-against-an-installed-tenzir) It is also possible to build plugins against an installed Tenzir. The `TenzirRegisterPlugin` CMake function contains the required scaffolding to set up `test` and `bats` targets that mimic Tenzir‚Äôs targets. Here‚Äôs how you can use it: ```sh # Configure the build. Requires Tenzir to be installed in the CMake Module Path. cmake -S path/to/plugin -B build # Optionally you can manually specify a non-standard Tenzir install root: # TENZIR_DIR=/opt/tenzir cmake -S path/to/plugin -B build cmake --build build # Run plugin-specific unit tests. ctest --test-dir build # Install to where Tenzir is also installed. cmake --install build # Optionally you can manually specify a non-standard Tenzir install root: # cmake --install build --prefix /opt/tenzir # Run plugin-specific integration tests against the installed Tenzir. cmake --build build --target bats ``` ## Add unit and integration tests [Section titled ‚ÄúAdd unit and integration tests‚Äù](#add-unit-and-integration-tests) Tenzir comes with unit and integration tests. So does a robust plugin implementation. We now look at how you can hook into the testing frameworks. ### Unit tests [Section titled ‚ÄúUnit tests‚Äù](#unit-tests) Every plugin ideally comes with unit tests. The `TenzirRegisterPlugin` CMake function takes an optional `TEST_SOURCES` argument that creates a test binary `<plugin>-test` with `<plugin>` being the plugin name. The test binary links against the `tenzir::test` target. ou can find the test binary in `bin` within your build directory. To execute registered unit tests, you can also simply run the test binary `<plugin>-test`, where `<plugin>` is the name of your plugin. The build target `test` sequentially runs tests for all plugins and Tenzir itself. ### Integration tests [Section titled ‚ÄúIntegration tests‚Äù](#integration-tests) Every plugin ideally comes with integration tests as well. Our convention is that integration tests reside in an `integration` subdirectory. If you add a file called `bats/*.bats`, Tenzir runs them alongside the regular integration tests. Note that plugins may affect the overall behavior of Tenzir. Therefore we recommend to to run all integrations regularly by running the build target `bats`. To execute plugin-specific integration tests only, run the build target `bats-<plugin>`, where `<plugin>` is the name of your plugin. ## Package it [Section titled ‚ÄúPackage it‚Äù](#package-it) If you plan to publish your plugin, you may want to create a GitHub repository. Please let us know if you do so, we can then link to community plugins from the documentation.

# Export from a node

Exporting (or *querying*) data can be done by [running a pipeline](/guides/basic-usage/run-pipelines) that begins with the [`export`](/reference/operators/export) input operator. When managing a pipeline through the app or the API, all pipeline operators run within the node. When using the CLI, at least the `export` operator runs within the node. ![Export](/_astro/export-from-a-node.BCWt8NLC_19DKCs.svg) Let‚Äôs bring back a sample of historical data we [imported in the previous section](/guides/edge-storage/import-into-a-node): ```tql export head ``` Think of `export` being the entire data at a node. As this can grow quickly, you may query only subsets of it, e.g., by filtering using [`where`](/reference/operators/where): ```tql export where orig_bytes < 1 KiB ``` Logically, this query would *first* export the entire historical data, and *then* begin filtering the data. But since Tenzir does *predicate pushdown*, the pipeline executor will analyze the query and push the expression in `where` with the predicate `orig_bytes < 1 KiB` ‚Äúdown‚Äù to the `export` operator. Tenzir‚Äôs storage engine then asks its catalog to identify the relevant subset of partitions that the query should execute on. This dramatically improves the query performance for selective workloads, such as point queries for single values or specific time ranges. To figure out the shape of the data to query, you can [show available schemas](/reference/operators/schemas).

# Import into a node

Importing (or *ingesting*) data can be done by [running a pipeline](/guides/basic-usage/run-pipelines) that ends with the [`import`](/reference/operators/import) output operator. When managing a pipeline through the app or the API, all pipeline operators run within the node. When using the CLI, at least the `import` operator runs within the node. ![Import](/_astro/import-into-a-node.B6oNZwnu_19DKCs.svg) Consider this example that takes a Zeek conn.log from our M57 dataset: ```tql load_file "Zeek/conn.log" read_zeek_tsv select id.orig_h, id.resp_h, orig_bytes, resp_bytes where orig_bytes > 1 Mi import ``` The [`import`](/reference/operators/import) operator requires a running node. To run the above pipeline successfully, you need to first [setup a node](/guides/node-setup/provision-a-node).

# Show available schemas

When you write a pipeline, you often reference field names. If you do not know the shape of your data, you can look up available schemas, i.e., the record types describing top-level events. Many SQL databases have a `SHOW TABLES` command to show all available table names, and `SHOW COLUMNS` to display the individual fiels of a given table. In Tenzir, the [`fields`](/reference/operators/fields) operator offers the ability for detailed schema introspection. Use it to display all schema fields; each event represents a single field. ```tql fields head ``` ```json {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "ts", "path": ["ts"], "index": [0], "type": {"kind": "time", "category": "atomic", "lists": 0, "name": "", "attributes": []}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "uid", "path": ["uid"], "index": [1], "type": {"kind": "string", "category": "atomic", "lists": 0, "name": "", "attributes": [{"key": "index", "value": "hash"}]}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "orig_h", "path": ["id", "orig_h"], "index": [2, 0], "type": {"kind": "ip", "category": "atomic", "lists": 0, "name": "", "attributes": []}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "orig_p", "path": ["id", "orig_p"], "index": [2, 1], "type": {"kind": "uint64", "category": "atomic", "lists": 0, "name": "", "attributes": []}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "resp_h", "path": ["id", "resp_h"], "index": [2, 2], "type": {"kind": "ip", "category": "atomic", "lists": 0, "name": "", "attributes": []}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "resp_p", "path": ["id", "resp_p"], "index": [2, 3], "type": {"kind": "uint64", "category": "atomic", "lists": 0, "name": "", "attributes": []}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "proto", "path": ["proto"], "index": [3], "type": {"kind": "string", "category": "atomic", "lists": 0, "name": "", "attributes": []}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "trans_id", "path": ["trans_id"], "index": [4], "type": {"kind": "uint64", "category": "atomic", "lists": 0, "name": "", "attributes": []}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "rtt", "path": ["rtt"], "index": [5], "type": {"kind": "duration", "category": "atomic", "lists": 0, "name": "", "attributes": []}} {"schema": "zeek.dns", "schema_id": "1b1de9d8225b12af", "field": "query", "path": ["query"], "index": [6], "type": {"kind": "string", "category": "atomic", "lists": 0, "name": "", "attributes": []}} ```

# Transform data at rest

Currently CLI only This feature is currently only available on the command line using the `tenzir-ctl` binary. We‚Äôre working on bringing it back as an operator so that you can also use it from the app. Tenzir provdides several features to transform historical data at a node. ## Delete old data when reaching storage quota [Section titled ‚ÄúDelete old data when reaching storage quota‚Äù](#delete-old-data-when-reaching-storage-quota) The disk-monitoring feature enables periodic deletion of events based on utilized disk storage. To limit the disk space used by a node, configure a disk quota: ```sh tenzir-node --disk-quota-high=1TiB ``` Whenever a node detects that its database has exceeded the configured quota, it will erase the oldest data. You can specify a corridor for the disk space usage by additionally providing the option `--disk-quota-low`. This can be used to avoid running permanently at the upper limit and to instad batch the deletion operations together. The full set of available options looks like this: ```yaml tenzir: start: # Triggers removal of old data when the DB dir exceeds the disk budget. disk-budget-high: 0K # When the DB dir exceeds the budget, Tenzir erases data until the directory # size falls below this value. disk-budget-low: 0K # Seconds between successive disk space checks. disk-budget-check-interval: 90 ``` ## Transform old data when reaching storage quota [Section titled ‚ÄúTransform old data when reaching storage quota‚Äù](#transform-old-data-when-reaching-storage-quota) Instead of deleting data periodically, a node can also trigger **spatial compaction** when exceeding a given disk budget. A spatial compaction cycle transforms data until disk usage falls below the budget, e.g., by removing columns or rows from certain events, or by deleting them entirely. When the disk budget exceeds the configured threshold, the node decides what data to compact. The compaction *mode* defines how this happens. Currently, there exists only one mode: weighted age. To compute the weighted age, the node divides the actual age of an event with the weight assigned to this event type. For example, applying a weight of 100 to an event that is 100 days old would yield a weighted age of 1 day. This causes it to be transformed after events that are 50 days old. Conversely, a weights less than one results in an older weighted age, resulting in earlier consideration in a compaction cycle. The default weight is 1 for all event types. Here is an example configuration that adjusts the weights: ```yaml tenzir: plugins: [compaction] plugins: compaction: space: mode: weighted-age interval: 6 hours disk-budget-high: 10TiB disk-budget-low: 8TiB weights: - weight: 0.1 types: [suricata.flow] #pipeline: ‚Ä¶ - weight: 100 types: [suricata.alert] #pipeline: ‚Ä¶ ``` The `pipeline` key for each type is optional. If present, the corresponding pipeline processes all matching events. If absent, the nodes deletes matching events. Two additional keys are useful to fine-tune the behavior of the compaction plugin: 1. `compaction.space.scan-binary`: an absolute path to a binary that should be executed to determine the current disk usage 2. `compaction.space.step-size`: adjust how many compaction candidates should be processed before re-checking the size of the database directory ## Transform data after exceeding a retention span [Section titled ‚ÄúTransform data after exceeding a retention span‚Äù](#transform-data-after-exceeding-a-retention-span) A node triggers **temporal compaction** according to a set of rules that define how to transform events after they reach a specfic age. This declarative specification makes it easy to express fine-grained data retention policies, which is often needed for regulatory requirements and compliance. For each compaction cycle, the node processes all rules and identifies what subset of the data has become subject to transformation. To this end, each rule defines a *minimum* age, i.e., a lower bound that must be exceeded before the corresponding events undergo their configured pipeline. To configure temporal compaction, provide a list of compaction rules under the key `plugins.compaction.time` in the configuration. A compaction rule defines the minimum age using key `after`, the pipeline to apply with the key `pipeline`, the scope in terms of schema using the key `types`, and a name to uniquely refer to the rule. Omitting the `types` key causes temporal compaction rules to be applied to all schemas. By default, a compaction rule consumes its input, i.e., it erases the original events from the database and replaces them with the transformed events. The `preserve-input` option can be specified on a temporal compaction rule to override this behavior and to keep the input partitions available. The pipelines referenced in the compaction configuration must be defined in your configuration. ```yaml plugins: compaction: time: # How often to check the `after` condition below. interval: 1 day rules: - after: 2 days name: uri_scrubbing pipeline: | replace net.url="xxx" types: - zeek.http - suricata.http - after: 7 days name: flow_reduction pipeline: | summarize pkts_toserver=sum(flow.pkts_toserver), pkts_toclient=sum(flow.pkts_toclient), bytes_toserver=sum(flow.bytes_toserver), bytes_toclient=sum(flow.bytes_toclient), start=min(flow.start), end=max(flow.end) by timestamp, src_ip, dest_ip resolution 10 mins preserve-input: true types: - suricata.flow ``` ## Trigger a compaction cycle manually [Section titled ‚ÄúTrigger a compaction cycle manually‚Äù](#trigger-a-compaction-cycle-manually) You can also interact with the compaction plugin on the command line, through the `compaction` subcommand. Use the `list` subcommand to show all configured compaction rules: ```sh tenzir-ctl compaction list ``` You can then trigger a compaction manually via `run`: ```sh tenzir-ctl compaction run <rule> ```

# Enrich with network inventory

Tenzir‚Äôs [enrichment framework](/explanations/enrichment) features *lookup tables* that you can use to enrich data in your pipelines. Lookup tables have a unique property that makes them attractive for tracking information associated with CIDR subnets: when you use `subnet` values as keys, you can probe the lookup table with `ip` values and will get a longest-prefix match. To illustrate, consider this lookup table: | Subnet | Mapping | | ----------- | -------- | | 10.0.0.0/22 | Machines | | 10.0.0.0/24 | Servers | | 10.0.1.0/24 | Clients | When you have subnets as keys as above, you can query them with an IP address during enrichment. Say you want to enrich IP address `10.0.0.1`. Since the longest (bitwise) prefix match is `10.0.0.0/24`, you will get `Servers` as a result. The same goes for IP address `10.0.0.255`, but `10.0.1.1` will yield `Clients`. The IP address `10.0.2.1` yields Machines, since it is neither in `10.0.0.0/24` nor `10.0.1.0/24`, but `10.0.0.0/21`. The IP adress `10.0.4.1` won‚Äôt match at all, because it‚Äôs not any of the three subnets. ## Populate subnet mappings from a CSV file [Section titled ‚ÄúPopulate subnet mappings from a CSV file‚Äù](#populate-subnet-mappings-from-a-csv-file) It‚Äôs common to have Excel sheets or exported CSV files of inventory data. Let‚Äôs consider this example: inventory.csv ```csv subnet,owner,function 10.0.0.0/22,John,machines 10.0.0.0/24,Derek,servers 10.0.1.0/24,Peter,clients ``` First, create the context: ```tql context::create_lookup_table "subnets" ``` Then populate it: ```tql from_file "inventory.csv" { read_csv } context::update "subnets", key=subnet ``` ## Enrich IP addresses with the subnet table [Section titled ‚ÄúEnrich IP addresses with the subnet table‚Äù](#enrich-ip-addresses-with-the-subnet-table) Now that we have a lookup table with subnet keys, we can enrich any data containing IP addresses with it. For example, let‚Äôs consider this simplified Suricata flow record: sample.json ```json { "timestamp": "2021-11-17T13:32:43.237882", "src_ip": "10.0.0.1", "src_port": 54402, "dest_ip": "10.1.1.254", "dest_port": 53, "proto": "UDP", "event_type": "flow", "app_proto": "dns" } ``` Let‚Äôs use the `enrich` operator to add the subnet context to the two IP address fields: ```tql load_file "/tmp/sample.json" read_json context::enrich "subnets", key=src_ip, into=src_ip_context context::enrich "subnets", key=dest_ip, into=dest_ip_context ``` ```tql { timestamp: 2021-11-17T13:32:43.237882, src_ip: 10.0.0.1, src_port: 54402, dest_ip: 10.1.1.254, dest_port: 53, proto: "UDP", event_type: "flow", app_proto: "dns", src_ip_context: { subnet: 10.0.0.0/24, owner: "Derek", function: "servers", }, dest_ip_context: null } ``` We have enriched all IP addresses in the flow event with the `subnets` context. Now go hunt down Derek!

# Enrich with threat intel

Tenzir has a powerful [enrichment framework](/explanations/enrichment) for real-time contextualization. The heart of the framework is a **context**‚Äîa stateful object that can be managed and used with pipelines. ## Setup a context [Section titled ‚ÄúSetup a context‚Äù](#setup-a-context) Prior to enriching, you need to populate a context with data. First, create a context called `threatfox` that uses a lookup table, i.e., a key-value mapping where a key is used to perform the lookup and the value can be any structured additional data. ```tql context::create_lookup_table "threatfox" ``` After creating a context, we load data into the context. In our example, we load data from the [ThreatFox](https://threatfox.abuse.ch/) API: ```tql from_http "https://threatfox-api.abuse.ch/api/v1/", body={query: "get_iocs", days: 1} { read_json } unroll data where data.ioc_type == "domain" context::update "threatfox", key=ioc, value=data ``` ## Enrich with a context [Section titled ‚ÄúEnrich with a context‚Äù](#enrich-with-a-context) Now that we loaded IoCs into the context, we can enrich with it in other pipelines. Since we previously imported only domains, we look for fields in the data of that type. The following pipeline subscribes to the import feed of all data arriving at the node via `export live=true` and applies the `threatfox` context to Suricata DNS requests in field `dns.rrname` via [`context::enrich`](/reference/operators/context/enrich): ```tql export live=true where @name == "suricata.dns" context::enrich "threatfox", key=dns.rrname ``` Here is a sample of an event that the above pipeline yields: ```tql { timestamp: 2021-11-17T16:57:42.389824, flow_id: 1542499730911936, pcap_cnt: 3167, vlan: null, in_iface: null, src_ip: 45.85.90.164, src_port: 56462, dest_ip: 198.71.247.91, dest_port: 53, proto: "UDP", event_type: "dns", community_id: null, dns: { version: null, type: "query", id: 1, flags: null, qr: null, rd: null, ra: null, aa: null, tc: null, rrname: "bza.fartit.com", rrtype: "RRSIG", rcode: null, ttl: null, tx_id: 0, grouped: null, answers: null }, threatfox: { key: "bza.fartit.com", context: { id: "1209087", ioc: "bza.fartit.com", threat_type: "payload_delivery", threat_type_desc: "Indicator that identifies a malware distribution server (payload delivery)", ioc_type: "domain", ioc_type_desc: "Domain name that delivers a malware payload", malware: "apk.irata", malware_printable: "IRATA", malware_alias: null, malware_malpedia: "https://malpedia.caad.fkie.fraunhofer.de/details/apk.irata", confidence_level: 100, first_seen: "2023-12-03 14:05:20 UTC", last_seen: null, reference: "", reporter: "onecert_ir", tags: ["irata"] }, timestamp: 2023-12-04T13:52:49.043157 } } ``` The sub-record `threatfox` holds the enrichment details. The field `key` contains the matching key. The field `context` is the row from the lookup table at key `bza.fartit.com`. The field `timestamp` is the time when the enrichment occurred.

# Execute Sigma rules

Tenzir supports executing [Sigma rules](https://github.com/SigmaHQ/sigma) using the [`sigma`](/reference/operators/sigma) operator. This allows you to run your Sigma rules in the pipeline. The operator transpiles the provided rules into an expression, and wraps matching events into a sighting record along with the matched rule. Semantically, you can think of executing Sigma rules as applying the [`where`](/reference/operators/where) operator to the input. At a high level, the translation process looks as follows: ![Sigma Execution](/_astro/execute-sigma-rules.DU7cNKaT_19DKCs.svg) ## Run a Sigma rule on an EVTX file [Section titled ‚ÄúRun a Sigma rule on an EVTX file‚Äù](#run-a-sigma-rule-on-an-evtx-file) You can run a Sigma rule on any pipeline input. For example, to apply a Sigma rule to an EVTX file, we can use the utility [`evtx_dump`](https://github.com/omerbenamram/evtx) to convert the binary EVTX format into JSON and then pipe it to `sigma` on the command line: ```bash evtx_dump -o jsonl file.evtx | tenzir 'read_json | sigma "rule.yaml"' ```

# Work with lookup tables

A [lookup table](/explanations/enrichment#lookup-table) is a specific type of *context* in Tenzir‚Äôs [enrichment framework](/explanations/enrichment). It has ‚Äútwo ends‚Äù in that you can use pipelines to update it, as well as pipelines to perform lookups and attach the results to events. Lookup tables live in a node and multiple pipelines can safely use the same lookup table. All update operations propagate to disk, persisting the changes and making them resilient against node restarts. Lookup tables are particularly powerful for: * **Threat Intelligence**: Track indicators of compromise (IoCs) like malicious domains, IPs, or file hashes. * **Asset Inventory**: Map IP addresses or subnets to organizational data. * **Entity Tracking**: Build passive DNS tables or track user-to-host mappings. * **Dynamic Enrichment**: Update context in real-time as your environment changes. ## Create a lookup table [Section titled ‚ÄúCreate a lookup table‚Äù](#create-a-lookup-table) You can create a lookup table with the [`context::create_lookup_table`](/reference/operators/context/create_lookup_table) operator as a pipeline, or interactively in the platform. ### Create a lookup table from a pipeline [Section titled ‚ÄúCreate a lookup table from a pipeline‚Äù](#create-a-lookup-table-from-a-pipeline) The [`context::create_lookup_table`](/reference/operators/context/create_lookup_table) operator creates a new, empty lookup table: ```tql context::create_lookup_table "my_lookup_table" ``` ### Create a lookup table as code [Section titled ‚ÄúCreate a lookup table as code‚Äù](#create-a-lookup-table-as-code) You can also create a lookup table as code by adding it to `tenzir.contexts` in your `tenzir.yaml` configuration file: \<prefix>/etc/tenzir/tenzir.yaml ```yaml tenzir: contexts: my-lookup-table: type: lookup-table ``` This approach is useful for: * **Infrastructure as Code**: Define lookup tables in version control * **Automated Deployments**: Ensure lookup tables exist on node startup * **Consistent Environments**: Replicate the same contexts across multiple nodes ### Create a lookup table in the platform [Section titled ‚ÄúCreate a lookup table in the platform‚Äù](#create-a-lookup-table-in-the-platform) The following steps 1. In the **Contexts** tab in your node, click the `+` button: ![Create context](/_astro/context-add.CjA5yN5-_Z26lvns.webp) 2. Select type `lookup-table` and enter a name: ![Choose name and type](/_astro/context-name.BK7qez7U_ZY5Xxk.webp) 3. Click **Create** and observe the new lookup table context: ![View created context](/_astro/context-created.CLPG07O__1Umj0P.webp) ## Show all available lookup tables [Section titled ‚ÄúShow all available lookup tables‚Äù](#show-all-available-lookup-tables) Use the [`context::list`](/reference/operators/context/list) operator to retrieve all contexts in your node: ```tql context::list ``` This shows all contexts including lookup tables, bloom filters, and GeoIP databases. Here‚Äôs an example output after creating different context types: ```tql // First, create some contexts. context::create_lookup_table "threat_intel" context::create_bloom_filter "malware_hashes", capacity=1M, fp_probability=0.01 ``` Now list them. ```tql context::list ``` ```tql { num_entries: 0, name: "threat_intel", configured: false } { num_elements: 0, parameters: {m: 9585058, n: 1000000, p: 0.01, k: 7}, name: "malware_hashes", configured: false } { name: "geo", type: "geoip" } ``` Filter the results to find specific contexts: ```tql context::list where name.match_regex("[T|t]hreat") ``` ## Delete a lookup table [Section titled ‚ÄúDelete a lookup table‚Äù](#delete-a-lookup-table) Use the [`context::remove`](/reference/operators/context/remove) operator to delete a lookup table: ```tql context::remove "my_lookup_table" ``` This permanently deletes the specified lookup table and its persisted data. ## Perform lookups [Section titled ‚ÄúPerform lookups‚Äù](#perform-lookups) Use the [`context::enrich`](/reference/operators/context/enrich) operator to enrich events with data from a lookup table. ### Perform a point lookup [Section titled ‚ÄúPerform a point lookup‚Äù](#perform-a-point-lookup) First, create a simple lookup table: ```tql context::create_lookup_table "user_roles" ``` Populate it with data: ```tql from {user_id: 1001, role: "admin", department: "IT"}, {user_id: 1002, role: "analyst", department: "Security"}, {user_id: 1003, role: "engineer", department: "DevOps"} context::update "user_roles", key=user_id ``` Now enrich events with this lookup table: ```tql from {user_id: 1002, action: "login", timestamp: 2024-01-15T10:30:00} context::enrich "user_roles", key=user_id ``` ```tql { user_id: 1002, action: "login", timestamp: 2024-01-15T10:30:00, user_roles: { user_id: 1002, role: "analyst", department: "Security" } } ``` Specify where to place the enrichment: ```tql from {user_id: 1002, action: "login"} context::enrich "user_roles", key=user_id, into=user_info ``` ```tql { user_id: 1002, action: "login", user_info: { user_id: 1002, role: "analyst", department: "Security" } } ``` Use OCSF format for standardized enrichment: ```tql from {user_id: 1002, action: "login"} context::enrich "user_roles", key=user_id, format="ocsf" ``` ```tql { user_id: 1002, action: "login", user_roles: { created_time: 2024-11-18T16:35:48.069981, name: "user_id", value: 1002, data: { user_id: 1002, role: "analyst", department: "Security" } } } ``` ### Perform a subnet lookup with an IP address key [Section titled ‚ÄúPerform a subnet lookup with an IP address key‚Äù](#perform-a-subnet-lookup-with-an-ip-address-key) When lookup table keys are of type `subnet`, you can probe the table with `ip` values. The lookup performs a longest-prefix match, perfect for network inventory and CMDB use cases: ```tql context::create_lookup_table "network_inventory" ``` Populate with network infrastructure data: ```tql from {subnet: 10.0.0.0/22, owner: "IT", location: "Datacenter-A"}, {subnet: 10.0.0.0/24, owner: "IT-Web", location: "Datacenter-A"}, {subnet: 10.0.1.0/24, owner: "IT-DB", location: "Datacenter-A"}, {subnet: 10.0.2.0/24, owner: "Dev", location: "Office-B"}, {subnet: 192.168.0.0/16, owner: "Guest", location: "All"} context::update "network_inventory", key=subnet ``` Now enrich network traffic with infrastructure context: ```tql from { timestamp: 2024-01-15T14:23:45, src_ip: 10.0.0.15, dst_ip: 10.0.1.20, bytes: 1048576, proto: "tcp" } context::enrich "network_inventory", key=src_ip, into=src_network context::enrich "network_inventory", key=dst_ip, into=dst_network ``` ```tql { timestamp: 2024-01-15T14:23:45, src_ip: 10.0.0.15, dst_ip: 10.0.1.20, bytes: 1048576, proto: "tcp", src_network: { subnet: 10.0.0.0/24, owner: "IT-Web", location: "Datacenter-A" }, dst_network: { subnet: 10.0.1.0/24, owner: "IT-DB", location: "Datacenter-A" } } ``` The IP `10.0.0.15` matches `10.0.0.0/24` (Web frontends) rather than `10.0.0.0/22` because `/24` is a longer (more specific) prefix match. ### Perform a lookup with compound key [Section titled ‚ÄúPerform a lookup with compound key‚Äù](#perform-a-lookup-with-compound-key) Use record types as compound keys for complex matching scenarios. ```tql context::create_lookup_table "threat_intel" ``` Populate a table with a compound key: ```tql from { threat_id: "APT-2024-001", indicators: { domain: "malicious.example.com", port: 443 }, severity: "critical", campaign: "DarkStorm", first_seen: 2024-01-10T00:00:00 } context::update "threat_intel", key=indicators ``` Pick a compound key for the table lookup: ```tql from { timestamp: 2024-01-15T10:30:00, dest_domain: "malicious.example.com", dest_port: 443, src_ip: "10.0.1.50" } context::enrich "threat_intel", key={domain: dest_domain, port: dest_port}, into=threat_info ``` Implement zone-based access control using compound keys: ```tql context::create_lookup_table "access_rules" ``` Define rules for zone pairs: ```tql from ( { key: {source_zone: "internet", dest_zone: "dmz"}, action: "allow", log: true }, { key: {source_zone: "internet", dest_zone: "internal"}, action: "deny", log: true, alert: true } ) context::update "access_rules", key=key, value=this ``` Check access for firewall events: ```tql from { timestamp: 2024-01-15T10:30:00, source_zone: "internet", dest_zone: "internal", src_ip: "203.0.113.10", dst_ip: "10.0.1.50" } context::enrich "access_rules", key={source_zone: source_zone, dest_zone: dest_zone}, into=policy ``` Compound keys enable sophisticated matching based on multiple fields. ## Add/overwrite entries in lookup table [Section titled ‚ÄúAdd/overwrite entries in lookup table‚Äù](#addoverwrite-entries-in-lookup-table) Use the [`context::update`](/reference/operators/context/update) operator to add or update entries. This is ideal for maintaining dynamic threat intelligence or asset inventory: Update threat intelligence from an API: ```tql from_http "https://threatfox-api.abuse.ch/api/v1/", body={query: "get_iocs", days: 1} { read_json } unroll data where data.ioc_type == "domain" context::update "threatfox", key=ioc, value=data ``` Track user login statistics: ```tql from { user: "alice@company.com", login_time: 2024-01-15T09:00:00, source_ip: "10.0.50.100", success: true } context::update "user_logins", key=user, value={ last_login: login_time, last_ip: source_ip, status: if success then "active" else "failed" } ``` ### Associate timeouts with entries [Section titled ‚ÄúAssociate timeouts with entries‚Äù](#associate-timeouts-with-entries) Timeouts are essential for managing the lifecycle of threat intelligence and maintaining fresh context. You can set expiration timeouts on lookup table entries using the [`context::update`](/reference/operators/context/update) operator: Most IoCs have a short half-life. Automatically expire stale entries: ```tql from_file "threat_feed.json" { read_json } where confidence_score >= 70 context::update "active_threats", key=indicator, value={ threat_type: threat_type, severity: severity, source: "ThreatFeed-Premium" }, create_timeout=7d, // Remove after 7 days regardless write_timeout=72h // Remove if not updated for 3 days ``` Track sessions with activity-based expiration: ```tql from { session_id: "sess_abc123", user: "alice@company.com", login_time: 2024-01-15T09:00:00, ip: "10.0.50.100" } context::update "active_sessions", key=session_id, write_timeout=30min, // Session expires after 30 min of inactivity read_timeout=30min // Also expire if not accessed for 30 min ``` Track DHCP leases with automatic expiration: ```tql from { mac: "00:11:22:33:44:55", ip: "10.0.100.50", hostname: "laptop-alice", lease_time: 2024-01-15T10:00:00 } context::update "dhcp_leases", key=mac, value={ip: ip, hostname: hostname, assigned: lease_time}, create_timeout=4h // DHCP lease duration ``` Implement API rate limiting with sliding windows: ```tql from {api_key: "key_123", request_time: now()} context::update "api_rate_limits", key=api_key, value={request_count: count()}, create_timeout=1h, // Reset counter every hour write_timeout=1h // Also reset if no requests for 1 hour ``` The three timeout types work together: * `create_timeout`: Hard expiration - useful for data with known shelf life * `write_timeout`: Expire stale data - useful for removing inactive entries * `read_timeout`: Expire unused data - useful for caching scenarios ## Remove entries from a lookup table [Section titled ‚ÄúRemove entries from a lookup table‚Äù](#remove-entries-from-a-lookup-table) Use the [`context::erase`](/reference/operators/context/erase) operator to remove specific entries, useful for allowlisting, removing false positives, or cleaning up outdated data: Remove false positives from threat intelligence: ```tql from {indicator: "legitimate-site.com", reason: "false_positive"} context::erase "threat_indicators", key=indicator ``` Remove IPs from blocklist after remediation: ```tql from_file "remediated_hosts.csv" { read_csv } where remediation_confirmed == true context::erase "compromised_hosts", key=ip_address ``` Clean up old sessions on logout: ```tql from { event_type: "logout", session_id: "sess_xyz789", user: "alice@company.com", timestamp: 2024-01-15T17:00:00 } where event_type == "logout" context::erase "active_sessions", key=session_id ``` Remove all entries older than 30 days from a context: ```tql context::inspect "temp_indicators" where first_seen > now() - 30d context::erase "temp_indicators", key=indicator ``` ## Show entries in a lookup table [Section titled ‚ÄúShow entries in a lookup table‚Äù](#show-entries-in-a-lookup-table) Use the [`context::inspect`](/reference/operators/context/inspect) operator to view and analyze the contents of a lookup table: ```tql // View all entries context::inspect "threat_indicators" ``` ```tql { key: "malicious.site.com", value: { threat_type: "phishing", first_seen: 2024-01-10T08:00:00, last_seen: 2024-01-15T14:30:00, severity: "high", source: "PhishTank" } } ``` Analyze lookup table contents: ```tql context::inspect "network_inventory" top subnet ``` Find specific entries: ```tql context::inspect "user_sessions" where key.user == "alice@company.com" ``` Export data for reporting: ```tql context::inspect "asset_inventory" select asset_id=key, value.owner, value.department, value.last_seen to "asset_report.csv" ``` Check table size and find old entries: ```tql context::inspect "passive_dns" set age = now() - value.last_seen where age > 7d summarize old_entries=count() ``` ```tql { old_entries: 42 } ``` ## Update lookup tables from APIs [Section titled ‚ÄúUpdate lookup tables from APIs‚Äù](#update-lookup-tables-from-apis) Periodically poll APIs to maintain fresh reference data, threat intelligence, or asset information in lookup tables. ### Basic periodic updates [Section titled ‚ÄúBasic periodic updates‚Äù](#basic-periodic-updates) Use the [`every`](/reference/operators/every) operator to schedule regular API polls that update a lookup table: ```tql every 1h { from_http "https://threatfeed.example.com/api/v1/indicators" } where confidence >= 80 context::update "threat_indicators", key=indicator, value=metadata ``` ### Update with expiration [Section titled ‚ÄúUpdate with expiration‚Äù](#update-with-expiration) Combine periodic updates with timeouts to automatically remove stale entries: ```tql every 30min { from_http "https://dns-blocklist.example.com/domains.json" } unroll domains context::update "blocklist", key=domains.domain, value={ category: domains.category, severity: domains.severity, last_updated: now() }, create_timeout=24h, // Remove after 24 hours write_timeout=2h // Remove if not updated for 2 hours ``` ## Export and import lookup table state [Section titled ‚ÄúExport and import lookup table state‚Äù](#export-and-import-lookup-table-state) ### Export lookup table state [Section titled ‚ÄúExport lookup table state‚Äù](#export-lookup-table-state) Use the [`context::save`](/reference/operators/context/save) operator to create backups or migrate lookup tables between nodes: Backup critical threat intelligence: ```tql context::save "threat_indicators" save_file "threat_intel_backup_2024_01_15.bin" ``` Export for migration to another node: ```tql context::save "network_inventory" save_file "network_inventory_prod.bin" ``` Automated daily backup: ```tql every 1d { context::save "asset_tracking" save_file f"backups/assets_{now().format('%Y%m%d')}.bin" } ``` ### Import lookup table state [Section titled ‚ÄúImport lookup table state‚Äù](#import-lookup-table-state) Use the [`context::load`](/reference/operators/context/load) operator to restore lookup tables from backups or migrate data: ```tql load_file "threat_intel_backup_2024_01_15.bin" context::load "threat_indicators" ``` Caution Loading replaces the entire lookup table state. Existing entries will be lost. Consider backing up with `context::save` before loading new data. ## Best Practices [Section titled ‚ÄúBest Practices‚Äù](#best-practices) 1. **Design efficient keys**: Use the most selective field as the key to minimize table size 2. **Set appropriate timeouts**: Base timeouts on data freshness requirements to prevent unbounded growth 3. **Monitor table size**: Regularly inspect tables to prevent excessive growth 4. **Backup critical contexts**: Schedule regular exports of important tables 5. **Test enrichments**: Verify logic with `context::inspect` before production deployment

# Install MCP Server

To install the [Tenzir MCP Server](/reference/mcp-server) server, you can choose between two options: 1. **Docker**: `docker run -i tenzir/mcp` 2. **Native**: `uvx tenzir-mcp` The Docker version runs a container that bundles the MCP server along with a Tenzir Node installation as a convenient one-stop solution. The native version only runs the MCP server and you need to make sure that it can access a Tenzir Node installation yourself. ## AI agent configuration [Section titled ‚ÄúAI agent configuration‚Äù](#ai-agent-configuration) All AI agents use the same JSON configuration structure for the Tenzir MCP server. The configuration always follows this pattern: * Docker ```json { "mcpServers": { "tenzir": { "command": "docker", "args": ["run", "--pull=always", "-i", "tenzir/mcp"], "env": {} } } } ``` * Native ```json { "mcpServers": { "tenzir": { "command": "uvx", "args": ["tenzir-mcp@latest"], "env": {} } } } ``` The configuration file location varies by agent. See the specific sections below. ### Claude [Section titled ‚ÄúClaude‚Äù](#claude) Configure the Tenzir MCP server for [Claude Code](https://claude.ai/code) and [Claude Desktop](https://claude.ai/download). #### Claude Code [Section titled ‚ÄúClaude Code‚Äù](#claude-code) For automatic configuration: * Docker ```bash claude mcp add tenzir --scope user -- docker run --pull=always -i tenzir/mcp ``` * Native ```bash claude mcp add tenzir --scope user -- uvx tenzir-mcp@latest ``` For manual configuration, edit `~/.mcp.json` for user-wide settings or `.mcp.json` in your project directory for project-specific settings. #### Claude Desktop [Section titled ‚ÄúClaude Desktop‚Äù](#claude-desktop) Edit the configuration file directly. The location depends on your operating system: * **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json` * **Windows**: `%APPDATA%\Claude\claude_desktop_config.json` * **Linux**: `~/.config/Claude/claude_desktop_config.json` After updating the configuration, restart Claude Desktop to load the MCP server. ### Gemini CLI [Section titled ‚ÄúGemini CLI‚Äù](#gemini-cli) Google‚Äôs [Gemini CLI](https://github.com/google-gemini/gemini-cli) supports MCP servers natively. Configure the Tenzir MCP server by editing `~/.gemini/settings.json`. Gemini Code Assist in VS Code shares the same MCP technology. The configuration automatically applies to both the CLI and VS Code integration. ### VS Code [Section titled ‚ÄúVS Code‚Äù](#vs-code) [VS Code](https://code.visualstudio.com/) supports MCP servers through [GitHub Copilot](https://github.com/features/copilot) starting with version 1.102. For project-specific configuration, create `.vscode/mcp.json` in your project root. For user-wide configuration: 1. Open Command Palette (`Ctrl+Shift+P` or `Cmd+Shift+P`) 2. Run `MCP: Open User Configuration` 3. Add the Tenzir server configuration After configuration: 1. Open the GitHub Copilot chat panel 2. Look for the MCP icon in the chat input 3. Select the Tenzir server from available MCP servers 4. Start interacting with your security pipelines

# Overview

The [Tenzir Node](/explanations/architecture/node) is the vehicle to run [pipelines](/explanations/architecture/pipeline). It is light-weight server application that can be deployed on-premises or in the cloud. Setting up a node involves the following basic steps: 1. [Provisioning](/guides/node-setup/provision-a-node): Create a new node in the [platform](/explanations/architecture/platform) and obtain a token. 2. [Sizing](/guides/node-setup/size-a-node): Determine the hardware requirements for the node based on the expected workload. 3. [Deploying](/guides/node-setup/deploy-a-node): Deploy the node to a server or cloud provider. 4. [Configuring](/guides/node-setup/configure-a-node): Tweak the default settings to optimize performance and security.

# Configure a node

The default node configuration is optimized for most common scenarios. But you can fine-tune the settings to match your specific requirements. We recommend beginning with learning [how the node configuration process](/explanations/configuration) works, and then browse the [example configuration](/reference/node/configuration) for tuning knobs. Here are few common configuration scenarios. ## Accept incoming connections [Section titled ‚ÄúAccept incoming connections‚Äù](#accept-incoming-connections) When your node starts it will listen for node-to-node connections on the TCP endpoint `127.0.0.1:5158`. Select a different endpoint via the `tenzir.endpoint` option. For example, to bind to an IPv6 address use `[::1]:42000`. ## Refuse incoming connections [Section titled ‚ÄúRefuse incoming connections‚Äù](#refuse-incoming-connections) Set `tenzir.endpoint` to `false` to disable the endpoint, making the node exclusively accessible through the Tenzir Platform. This effectively prevents connections from other `tenzir` or `tenzir-node` processes. ## Configure pipeline subprocesses [Section titled ‚ÄúConfigure pipeline subprocesses‚Äù](#configure-pipeline-subprocesses) Pipelines that run in a node are now partially moved to a subprocess for improved error resilience and resource utilization. Operators that need to communicate with a component still run inside the main node process for architectural reasons. You can set `tenzir.disable-forked-pipelines: true` in `tenzir.yaml` or `TENZIR_DISABLE_FORKED_PIPELINES=true` on the command line to opt out. This feature is enabled by default on Linux. Learn more about [pipeline subprocesses](/explanations/architecture/node#pipeline-subprocesses) and their trade-offs.

# Deploy a node

Deploying a node means spinning it up in one of the supported runtimes. The primary choice is between a containerized with Docker or a native deployment with our static binary that runs on amd64 and arm64 architectures. ## Docker [Section titled ‚ÄúDocker‚Äù](#docker) We recommend using Docker to deploy a Tenzir node, as it‚Äôs the easiest way to get started. After [provisioning a node](/guides/node-setup/provision-a-node), proceed as follows: 1. Select the Docker tab and click the download button to obtain the `docker-compose.NODE.yaml` configuration file, where `NODE` is the name you entered for your node. 2. Run ```bash docker compose -f docker-compose.NODE.yaml up --detach ``` Edit the Docker Compose file and change [environment variables](/explanations/configuration) to adjust your configuration. ### Stop a node [Section titled ‚ÄúStop a node‚Äù](#stop-a-node) Stop a node via the `down` command: ```bash docker compose -f docker-compose.NODE.yaml down ``` Stop a node and delete its persistent state by adding `--volumes`: ```bash docker compose -f docker-compose.NODE.yaml down --volumes ``` ### Update a node [Section titled ‚ÄúUpdate a node‚Äù](#update-a-node) Run the following commands to update a Docker Compose deployment with a configuration file `docker-compose.NODE.yaml`: ```bash docker compose -f docker-compose.NODE.yaml pull docker compose -f docker-compose.NODE.yaml down docker compose -f docker-compose.NODE.yaml up --detach ``` Note that we `pull` first so that the subsequent downtime between `down` and `up` is minimal. ## Linux [Section titled ‚ÄúLinux‚Äù](#linux) We offer a static binary package on various Linux distributions. ### Install a node [Section titled ‚ÄúInstall a node‚Äù](#install-a-node) Second, choose the Linux tab and proceed as follows: 1. [Provision the node](/guides/node-setup/provision-a-node) and download its config. 2. Create a directory for the platform configuration. ```bash mkdir -p /opt/tenzir/etc/tenzir/plugin ``` 3. Move the downloaded `platform.yaml` configuration file to the directory so that the node can find it during startup: ```bash mv platform.yaml /opt/tenzir/etc/tenzir/plugin ``` 4. Run the installer and follow the instructions to download and start the node: ```bash curl https://get.tenzir.app | sh ``` The installer script asks for confirmation before performing the installation. If you prefer a manual installation you can also perform the installer steps yourself. See the [configuration files documentation](/explanations/configuration) for details on how the node loads config files at startup. * Debian Download the latest [Debian package](https://github.com/tenzir/tenzir/releases/latest/download/tenzir-static-amd64-linux.deb) and install it via `dpkg`: ```bash dpkg -i tenzir-static-amd64-linux.deb ``` You can uninstall the Tenzir package via `apt-get remove tenzir`. Use `purge` instead of `remove` if you also want to delete the state directory and leave no trace. * RPM-based (RedHat, OpenSUSE, Fedora) Download the latest [RPM package](https://github.com/tenzir/tenzir/releases/latest/download/tenzir-static-amd64-linux.rpm) and install it via `rpm`: ```bash rpm -i tenzir-static-amd64-linux.rpm ``` * Nix Use our `flake.nix` to run an ad-hoc `tenzir` pipeline: ```bash nix run github:tenzir/tenzir/latest ``` Or run a node without installing: ```bash nix shell github:tenzir/tenzir/latest -c tenzir-node ``` Install a Tenzir Node by adding `github:tenzir/tenzir/latest` to your flake inputs, or use your preferred method to include third-party modules on classic NixOS. * Any Download a tarball with our [static binary](https://github.com/tenzir/tenzir/releases/latest/download/tenzir-static-x86_64-linux.tar.gz) for all Linux distributions and unpack it into `/opt/tenzir`: ```bash tar xzf tenzir-static-x86_64-linux.tar.gz -C / ``` We also offer prebuilt statically linked binaries for every Git commit to the `main` branch. ```bash curl -O https://storage.googleapis.com/tenzir-dist-public/packages/main/tarball/tenzir-static-main.gz ``` ### Start a node manually [Section titled ‚ÄúStart a node manually‚Äù](#start-a-node-manually) The installer script uses the package manager of your Linux distribution to install the Tenzir package. This typically also creates a [systemd](https://systemd.io) unit and starts the node automatically. For testing, development, our troubleshooting, run the `tenzir-node` executable to start a node manually: ```bash tenzir-node ``` ```plaintext _____ _____ _ _ ________ ____ |_ _| ____| \ | |__ /_ _| _ \ | | | _| | \| | / / | || |_) | | | | |___| |\ |/ /_ | || _ < |_| |_____|_| \_/____|___|_| \_\ v4.0.0-rc6-0-gf193b51f1f Visit https://app.tenzir.com to get started. [16:50:26.741] node listens for node-to-node connections on tcp://127.0.0.1:5158 [16:50:26.982] node connected to platform via wss://ws.tenzir.app:443/production ``` ### Stop a node [Section titled ‚ÄúStop a node‚Äù](#stop-a-node-1) There exist two ways stop a server: 1. Hit CTRL+C in the same TTY where you ran `tenzir-node`. 2. Send the process a SIGINT or SIGTERM signal, e.g., via `pkill -2 tenzir-node`. Hitting CTRL+C is equivalent to manually sending a SIGTERM signal. ## AWS [Section titled ‚ÄúAWS‚Äù](#aws) The recommended way to run a Tenzir node in AWS is with [Elastic Container Service (ECS)](https://aws.amazon.com/ecs/). The diagram below shows the high-level architecture of an AWS deployment. ![AWS Architecture](/_astro/deploy-node-on-aws.CJGSeWtf_19DKCs.svg) You can deploy a Tenzir node using either [CloudFormation](https://aws.amazon.com/cloudformation/) for automated setup or manually through the AWS console. Both methods support deploying as many nodes as you need. ### Subscribe to Tenzir Node on AWS Marketplace [Section titled ‚ÄúSubscribe to Tenzir Node on AWS Marketplace‚Äù](#subscribe-to-tenzir-node-on-aws-marketplace) Before deploying with either method, you need to subscribe to the Tenzir Node product: 1. Go to [AWS Marketplace](https://console.aws.amazon.com/marketplace/) and subscribe to the free [Tenzir Node](https://console.aws.amazon.com/marketplace/search/listing/prodview-gsofc3z6f3vsu) product. ![AWS Marketplace Tenzir Node](/_astro/01-aws-marketplace.BV34EsMs_Z1SWAhS.webp) 2. Accept the terms to subscribe to the offering. ![AWS Marketplace Tenzir Node](/_astro/02-subscribe.BuWO6X8P_Z1LfgkR.webp) ### Choose your deployment method [Section titled ‚ÄúChoose your deployment method‚Äù](#choose-your-deployment-method) * CloudFormation Deploy a Tenzir node using our CloudFormation template for automated setup. Click the button below to launch the CloudFormation console with our template pre-loaded: [Launch CloudFormation Stack ](https://console.aws.amazon.com/cloudformation/home?#/stacks/create?templateURL=https://s3.amazonaws.com/tenzir-marketplace-resources/single-node-container.yaml) After clicking the button above, follow these steps in the AWS console: 1. **Review the pre-filled template**: The CloudFormation console will open with our single-node container template already loaded: ![CloudFormation Create Stack](/_astro/01-create-stack.GmdJ5tXH_2okx2I.webp) 2. **Configure your stack**: * **Stack name**: Choose a unique name for your stack (e.g., `tenzir-node-prod`) * **Parameters**: Enter your `TENZIR_TOKEN` in the provided field Double-check that the container image URL points to the Tenzir Node version you want to deploy. ![Stack Parameters](/_astro/02-stack-parameters.B4Oua0h9_Z2mdd0o.webp) 3. **Accept the default stack options**: For most deployments, the defaults work perfectly: ![Stack Options](/_astro/03-stack-options.5n7bQip1_1voPe2.webp) 4. **Review and acknowledge**: Confirm your configuration and check the acknowledgment box: ![Review Stack](/_astro/04-review-stack.DSPyQTC8_Z1t888J.webp) 5. **Deploy your node**: Click *Submit* to start the deployment. Monitor progress in the *Events* tab. 6. **Wait for connection**: Your node will automatically connect to your workspace once the stack creation completes (typically 2-3 minutes). * Manual Follow these steps for manual deployment through the AWS console: 1. Navigate to [Amazon Elastic Container Service (ECS)](https://console.aws.amazon.com/ecs). ![Amazon Elastic Container Service (ECS)](/_astro/01-ecs.D-9AB8KO_Z1P4kme.webp) 2. Create a new cluster. Choose between **EC2** or **Fargate** based on your needs: * **EC2 clusters** give you full control over the underlying instances. They‚Äôre ideal for long-running workloads with consistent resource requirements. * **Fargate clusters** provide serverless container execution where you pay only for the resources you use. They‚Äôre cost-effective for workloads with variable demand. ![Create a Cluster](/_astro/02-create-cluster.DWOcGwyQ_Z1wYnvS.webp) 3. Create a task definition to specify how the Tenzir node container should run. ![Create a Cluster](/_astro/03-create-task-definition.Ds8CzEXJ_1y9Jew.webp) In the *Containers* section, enter the repository URL from your AWS Marketplace subscription: ```plaintext 709825985650.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-node:v<MAJOR>.<MINOR>.<PATCH> ``` Each node version has its own image tag. Replace `v<MAJOR>.<MINOR>.<PATCH>` with the desired version. Check the [node changelog](/changelog/node/) for the latest version. 4. Return to your cluster, navigate to the *Tasks* tab, and click *Run new task*. Select the task definition you just created. ![Create a Cluster](/_astro/04-run-task.BaW9wNbx_Z1TYNR4.webp) In the *Container overrides* section, add the `TENZIR_TOKEN` environment variable with the token value corresponding to your node. ![Container Overrides](/_astro/05-container-overrides.DgPIqnAZ_2rS1Oj.webp) Click *Create* to launch your node. 5. Once the container starts successfully, your node will automatically connect to your workspace. ## Azure [Section titled ‚ÄúAzure‚Äù](#azure) To run a node in Azure, we recommend using [Azure Container Instances (ACI)](https://azure.microsoft.com/en-us/products/container-instances), which allows you to run Docker containers without having to setup VMs. ### Azure Container Instances (ACI) [Section titled ‚ÄúAzure Container Instances (ACI)‚Äù](#azure-container-instances-aci) The following steps guide you through deploying a Tenzir Node using ACI. #### Create a new container instance [Section titled ‚ÄúCreate a new container instance‚Äù](#create-a-new-container-instance) 1. Open [portal.azure.com](https://portal.azure.com/). 2. Navigate to the *Container instances*. 3. Click the *Create* button. #### Basics [Section titled ‚ÄúBasics‚Äù](#basics) In the *Basics* tab, perform the following action: 1. Choose a container name. 2. For *Image source*, select *Other registry*. 3. For *Image*, enter `tenzir/tenzir-node`. ![Basics](/_astro/basics.bOCDyyP__Z18rgYp.webp) #### Networking [Section titled ‚ÄúNetworking‚Äù](#networking) In the *Networking* tab, configure the ports you plan to use for pipelines that receive incoming connections. ![Networking](/_astro/networking.DpOSLd0n_2quA37.webp) #### Advanced [Section titled ‚ÄúAdvanced‚Äù](#advanced) In the *Advanced* tab, enter the `TENZIR_TOKEN` environment variable from your Docker Compose file. ![Advanced](/_astro/advanced.afPiy2h8_Z76Ic4.webp) #### Create [Section titled ‚ÄúCreate‚Äù](#create) Once you‚Äôve completed the configuration, click the *Create* button. Your node is now up and running. ## macOS [Section titled ‚ÄúmacOS‚Äù](#macos) Looking for a native macOS package? We‚Äôre not quite there yet‚Äîbut you can still run Tenzir smoothly on macOS using [Docker](/guides/node-setup/deploy-a-node#docker). Want to see a native macOS build? Let us know! Drop your vote in our [Discord community](/discord)‚Äîwe prioritize what our users need most. Tenzir *does* run well on macOS under the hood. Docker just bridges the gap for now. ## Ansible [Section titled ‚ÄúAnsible‚Äù](#ansible) The Ansible role for Tenzir allows for easy integration of Tenzir into existing Ansible setups. The role uses either the Tenzir Debian package or the tarball installation method depending on which is appropriate for the target environment. The role definition is in the [`ansible/roles/tenzir`](https://github.com/tenzir/tenzir/tree/main/ansible/roles/tenzir) directory of the Tenzir repository. You need a local copy of this directory so you can use it in your playbook. ### Example [Section titled ‚ÄúExample‚Äù](#example) This example playbook shows how to run a Tenzir service on the machine `example_tenzir_server`: ```yaml - name: Deploy Tenzir become: true hosts: example_tenzir_server remote_user: example_ansible_user roles: - role: tenzir vars: tenzir_config_dir: ./tenzir tenzir_read_write_paths: [/tmp] tenzir_archive: ./tenzir.tar.gz tenzir_debian_package: ./tenzir.deb ``` ### Variables [Section titled ‚ÄúVariables‚Äù](#variables) #### `tenzir_config_dir` (required) [Section titled ‚Äútenzir\_config\_dir (required)‚Äù](#tenzir_config_dir-required) A path to directory containing a [`tenzir.yaml`](/reference/node/configuration) relative to the playbook. #### `tenzir_read_write_paths` [Section titled ‚Äútenzir\_read\_write\_paths‚Äù](#tenzir_read_write_paths) A list of paths that Tenzir shall be granted access to in addition to its own state and log directories. #### `tenzir_archive` [Section titled ‚Äútenzir\_archive‚Äù](#tenzir_archive) A tarball of Tenzir structured like those that can be downloaded from the [GitHub Releases Page](https://github.com/tenzir/tenzir/releases). This is used for target distributions that are not based on the `apt` package manager. #### `tenzir_debian_package` [Section titled ‚Äútenzir\_debian\_package‚Äù](#tenzir_debian_package) A Debian package (`.deb`). This package is used for Debian and Debian-based Linux distributions.

# Provision a node

Provisioning a node means creating one in the [platform](/explanations/architecture/platform) in your workspace. After provisioning, you can download configuration file with an authentication token‚Äîready to then deploy the node. When you start out with a new Tenzir account, you have an empty workspace without any nodes: ![Landing page](/_astro/new-account.Qul5lKOD_1KFSe1.webp) You have two options: either provision a self-hosted node or play with a cloud-hosted demo with a lifetime of 2 hours. ## Cloud-hosted Demo Node [Section titled ‚ÄúCloud-hosted Demo Node‚Äù](#cloud-hosted-demo-node) Provision a cloud-hosted demo node by following these steps: 1. Click **Cloud-hosted demo-node**. 2. Click **Add node**. 3. Click **Get Started**. üôå You‚Äôre good to go. It takes up to 2 minutes for your node to be usable. Upon provisioning, the documentation pops in automatically so that you can familiarize yourself with key concepts in the meantime. ## Self-hosted Node [Section titled ‚ÄúSelf-hosted Node‚Äù](#self-hosted-node) Provision a self-hosted node by following these steps: 1. Click **Self-hosted node**. 2. Click **Add node**. 3. Enter a name for your node. 4. Click **Add node**. üö¢ Your node is ready to be [deployed](/guides/node-setup/deploy-a-node). The easiest way to continue is by spinning up a node with [Docker](/guides/node-setup/deploy-a-node#docker).

# Size a node

To better understand what resources you need to run a node, we provide guidance on sizing and a [calculator](#calculator) to derive concrete **CPU**, **RAM**, and **storage** requirements. ## Considerations [Section titled ‚ÄúConsiderations‚Äù](#considerations) Several factors have an impact on sizing. Since you can run many types of workloads in pipelines, it is difficult to make a one-size-fits-all recommendation. The following considerations affect your resource requirements: ### Workloads [Section titled ‚ÄúWorkloads‚Äù](#workloads) Depending on what you do with pipelines, you may generate a different resource profile. #### Data Shaping [Section titled ‚ÄúData Shaping‚Äù](#data-shaping) [Shaping](/guides/data-shaping/shape-data) operation changes the form of the data, e.g., filtering events, removing columns, or changing values. This workload predominantly incurs **CPU** load. #### Aggregation [Section titled ‚ÄúAggregation‚Äù](#aggregation) Performing in-stream or historical aggregations often requires extensive buffering of the aggregation groups, which adds to your **RAM** requirements. If you run intricate custom aggregation functions, you also may see an additional increase CPU usage. #### Enrichment [Section titled ‚ÄúEnrichment‚Äù](#enrichment) [Enriching](/explanations/enrichment) dataflows with contexts requires holding in-memory state proportional to the context size. Therefore, enrichment affects your **RAM** requirements. Bloom filters are a fixed-size space-efficient structure for representing large sets, and lookup tables grow linearly with the number of entries. ### Data Diversity [Section titled ‚ÄúData Diversity‚Äù](#data-diversity) The more data sources you have, the more pipelines you run. In the simplest scenario where you just import all data into a node, you deploy one pipeline per data source. The number of data sources is a thus a lower bound for the number of pipelines. ### Data Volume [Section titled ‚ÄúData Volume‚Äù](#data-volume) The throughput of pipeline has an impact on performance. Pipelines with low data volume do not strain the system much, but high-volume pipelines substantially affect **CPU** and **RAM** requirements. Therefore, understanding your ingress volume, either as events per second or bytes per day, is helpful for sizing your node proportionally. ### Retention [Section titled ‚ÄúRetention‚Äù](#retention) When you leverage the node‚Äôs built-in storage engine by [importing](/guides/edge-storage/import-into-a-node) and [exporting](/guides/edge-storage/export-from-a-node) data, you need persistent **storage**. To assess your retention span, you need to understand your data volume and your capacity. Tenzir storage engine builds sparse indexes to accelerate historical queries. Based on how aggressively configure indexing, your **RAM** requirements may vary. ## Calculator [Section titled ‚ÄúCalculator‚Äù](#calculator)

# Start the API

The node offers a [REST API](/reference/node/api) for CRUD-style pipeline management. By default, the API is not accessible from the outside. Only the platform can access it internaly through the existing node-to-platform connection. To enable the API for direct access, you need to configure the built in web server that exposes the API. ## Expose the REST API [Section titled ‚ÄúExpose the REST API‚Äù](#expose-the-rest-api) To expose the REST API, start the web server component by adding the following to your configuration: tenzir.yaml ```yaml tenzir: start: commands: - web server \ --certfile=/path/to/server.certificate \ --keyfile=/path/to/private.key \ --mode=MODE ``` Replace `MODE` with the TLS mode that best suits your deployment, as [explained below](#choose-a-tls-deployment-mode). The YAML configuration is equivalent to the following command-line invocation: ```bash tenzir-node --commands="web server [...]" ``` The server will only accept TLS requests by default. To allow clients to connect successfully, you need to pass a valid certificate and corresponding private key with the `--certfile` and `--keyfile` arguments. ## Generate an authentication token [Section titled ‚ÄúGenerate an authentication token‚Äù](#generate-an-authentication-token) Clients must authenticate all requests with a valid token. The token is a short string that clients put in the `X-Tenzir-Token` request header. You can generate a valid token on the command line: ```bash tenzir-ctl web generate-token ``` For local testing and development, generating suitable certificates and tokens can be a hassle. For this scenario, you can start the server in [developer mode](#developer-mode) where it accepts plain HTTP connections are does not perform token authentication. ## Choose a TLS deployment mode [Section titled ‚ÄúChoose a TLS deployment mode‚Äù](#choose-a-tls-deployment-mode) There exist four modes to start the REST API, each of which suits a slightly different use case. ### Developer mode [Section titled ‚ÄúDeveloper mode‚Äù](#developer-mode) The developer mode bypasses encryption and authentication token verification. ![Developer Mode](/_astro/rest-api-mode-developer.Bs3kilub_19DKCs.svg) Pass `--mode=dev` to start the REST API in developer mode. ### Server mode [Section titled ‚ÄúServer mode‚Äù](#server-mode) The server mode reflects the ‚Äútraditional‚Äù mode of operation where the server binds to a network interface. This mode only accepts HTTPS connections and requires a valid authentication token for every request. This is the default mode of operation. ![Server Mode](/_astro/rest-api-mode-server.BTJHvLsL_19DKCs.svg) Pass `--mode=server` to start the REST API in server mode. ### Upstream TLS mode [Section titled ‚ÄúUpstream TLS mode‚Äù](#upstream-tls-mode) The upstream TLS mode is suitable when Tenzir sits upstream of a separate TLS terminator that is running on the same machine. This kind of setup is commonly encountered when running nginx as a reverse proxy. ![Upstream TLS Mode](/_astro/rest-api-mode-upstream.CLNud8Cc_19DKCs.svg) Tenzir only listens on localhost addresses, accepts plain HTTP but still checks authentication tokens. Pass `--mode=upstream` to start the REST API in server mode. ### Mutual TLS (mTLS) mode [Section titled ‚ÄúMutual TLS (mTLS) mode‚Äù](#mutual-tls-mtls-mode) The mutual TLS mode is suitable when Tenzir sits upstream of a separate TLS terminator that may be running on a different machine. This setup is commonly encountered when running [nginx](https://nginx.org) as a load balancer. Tenzir would typically be configured to use a self-signed certificate in this setup. Tenzir only accepts HTTPS requests, requires TLS client certificates for incoming connections, and requires valid authentication tokens for any authenticated endpoints. ![mTLS Mode](/_astro/rest-api-mode-mtls.DhDa_SVb_19DKCs.svg) Pass `--mode=mtls` to start the REST API in server mode.

# Tune performance

This section describes tuning configuration knobs that have a notable effect on the performance of the node. ## Demand [Section titled ‚ÄúDemand‚Äù](#demand) Tenzir schedules operators asynchronously. Within a pipeline, every operator sends elements (events or bytes) downstream to the next operator, and demand upstream to the previous operator. If an operator has no downstream demand, Tenzir‚Äôs pipeline execution engine stops scheduling the operator. The configuration section `tenzir.demand` controls how operators issue demand to their upstream operators. See the [example configuration](/reference/node/configuration) for all available options. For example, to minimize memory usage of pipelines at the cost of performance, set the following option: ```yaml tenzir: demand: max-batches: 1 ``` ## Batching [Section titled ‚ÄúBatching‚Äù](#batching) Tenzir processes events in batches. Because the structured data has the shape of a table, we call these batches *table slices*. The following options control their shape and behavior. ### Size [Section titled ‚ÄúSize‚Äù](#size) Most components in Tenzir operate on table slices, which makes the table slice size a fundamental tuning knob on the spectrum of throughput and latency. Small table slices allow for shorter processing times, resulting in more scheduler context switches and a more balanced workload. But the increased pressure on the scheduler comes at the cost of throughput. Conversely, a large table slice size creates more work for each actor invocation and makes them yield less frequently to the scheduler. As a result, other actors scheduled on the same thread may have to wait a little longer. The option `tenzir.import.batch-size` sets an upper bound for the number of events per table slice. It defaults to 65,536. The option controls the maximum number of events per table slice, but not necessarily the number of events until a component forwards a batch to the next stage in a stream. The CAF streaming framework uses a credit-based flow-control mechanism to determine buffering of tables slices. Caution Setting `tenzir.import.batch-size` to `0` causes the table slice size to be unbounded and leaves it to `tenzir.import.batch-timeout` to produce table slices. This can lead to very large table slices for sources with high data rates, and is not recommended. ### Import Timeout [Section titled ‚ÄúImport Timeout‚Äù](#import-timeout) The `tenzir.import.batch-timeout` option sets a timeout for forwarding buffered table slices to the remote Tenzir node. If the timeout fires before a table slice reaches `tenzir.import.batch-size`, then the table slice will contain fewer events and ship immediately. The `tenzir.import.read-timeout` option determines how long a call to read data from the input will block. After the read timeout elapses, Tenzir tries again at a later. The default value is 10 seconds. ## Storage Engine [Section titled ‚ÄúStorage Engine‚Äù](#storage-engine) The central component of Tenzir‚Äôs storage engine is the *catalog*. It owns the partitions, keeps metadata about them, and maintains a set of sparse secondary indexes to identify relevant partitions for a given query. ![Catalog Indexes](/_astro/catalog-indexes.CTn-znil_19DKCs.svg) The catalog‚Äôs secondary indexes are space-efficient sketch data structures (e.g., Bloom filters, min-max summaries) that have a low memory footprint but may yield false positives. Tenzir keeps all sketches in memory. The amount of memory that the storage engine can consume is not explicitly configurable, but there exist various options that have a direct impact. ### Control the partition size [Section titled ‚ÄúControl the partition size‚Äù](#control-the-partition-size) Tenzir groups table slices with the same schema in a partition. There exist mutable *active partitions* that Tenzir writes to during ingestion, and immutable *passive partitions* that Tenzir reads from during query execution. When constructing a partition, the parameter `tenzir.max-partition-size` (default: 4Mi / 2^22) sets an upper bound on the number of records in a partition, across all table slices. The parameter `tenzir.active-partition-timeout` (default: 10 seconds) provides a time-based upper bound: once reached, Tenzir considers the partition as complete, regardless of the number of records. The two parameters are decoupled to allow for independent control of throughput and freshness. Tenzir also merges undersized partitions asynchronously in the background, which counter-acts the fragmentation effect from choosing a low partition timeout. ### Tune catalog fragmentation [Section titled ‚ÄúTune catalog fragmentation‚Äù](#tune-catalog-fragmentation) The catalog keeps state that grows linear in the number of partitions. The configuration option `tenzir.max-partition-size` determines an upper bound of the number of records per partition, which is inversely linked to the number of partitions. For example, a large value yields fewer partitions whereas a small value creates more partitions. In other words, increasing `tenzir.max-partition-size` is an effective method to reduce the memory footprint of the catalog, at the cost of creating larger partitions. ### Configure the catalog [Section titled ‚ÄúConfigure the catalog‚Äù](#configure-the-catalog) You can configure catalog and partition indexes under the key `tenzir.index`. The configuration `tenzir.index.rules` is an array of indexing rules, each of which configures the indexing behavior of a set of extractors. A rule has the following keys: * `targets`: a list of extractors to describe the set of fields whose values to add to the sketch. * `fp-rate`: an optional value to control the false-positive rate of the sketch. #### Tune catalog index parameters [Section titled ‚ÄúTune catalog index parameters‚Äù](#tune-catalog-index-parameters) Catalog indexes may produce false positives that can have a noticeable impact on the query latency by materializing irrelevant partitions. Based on the cost of I/O, this penalty may be substantial. Conversely, reducing the false positive rate increases the memory consumption, leading to a higher resident set size and larger RAM requirements. You can control the false positive probability with the `fp-rate` key in an index rule. By default, Tenzir creates one sketch per type, but not additional field-level sketches unless a dedicated rule with a matching target configuration exists. Here is an example configuration that adds extra field-level sketches: ```yaml tenzir: index: # Set the default false-positive rate for type-level sketches default-fp-rate: 0.001 rules: - targets: # field sketches require a fully qualified field name - suricata.http.http.url fp-rate: 0.005 - targets: - :ip fp-rate: 0.1 ``` This configuration includes two rules (= two catalog indexes) where the first rule includes a field extractor and the second a type extractor. The first rule applies to a single field, `suricata.http.http.url`, and has false-positive rate of 0.5%. The second rule creates one sketch for all fields of type `ip` that has a false-positive rate of 10%. ### Adjust the store compression [Section titled ‚ÄúAdjust the store compression‚Äù](#adjust-the-store-compression) Tenzir compresses partitions using Zstd for partitions at rest. To fine-tune the space-time trade-off, Tenzir offers a setting, `tenzir.zstd-compression-level` to allow fine-tuning the compression level: ```yaml tenzir: zstd-compression-level: 1 ``` Currently, the default value is taken from Apache Arrow itself. ### Rebuild partitions [Section titled ‚ÄúRebuild partitions‚Äù](#rebuild-partitions) The `rebuild` command re-ingests events from existing partitions and replaces them with new partitions. This makes it possible to upgrade persistent state to a newer version, or recreate persistent state after changing configuration parameters, e.g., switching from the Feather to the Parquet store backend. The following diagram illustrates this ‚Äúdefragmentation‚Äù process: ![Rebuild](/_astro/rebuild.P1w_CnP2_19DKCs.svg) Rebuilding partitions also recreates their sketches. The process takes place asynchronously in the background. Control this behavior in your `tenzir.yaml` configuration file, to disable or adjust the resources to spend on automatic rebuilding: ```yaml tenzir: # Automatically rebuild undersized and outdated partitions in the background. # The given number controls how much resources to spend on it. Set to 0 to # disable. Defaults to 1. automatic-rebuild: 1 ``` This is how you run it manually: ```bash tenzir-ctl rebuild start [--all] [--undersized] [--parallel=<number>] [--max-partitions=<number>] [--detached] [<expression>] ``` A rebuild is not only useful when upgrading outdated partitions, but also when changing parameters of up-to-date partitions. (Internally, Tenzir versions the partition state via FlatBuffers. An outdated partition is one whose version number is not the newest.) The `--undersized` flag causes Tenzir to rebuild partitions that are under the configured partition size limit `tenzir.max-partition-size`. The `--all` flag causes Tenzir to rebuild all partitions. The `--parallel` options is a performance tuning knob. The parallelism level controls how many sets of partitions to rebuild in parallel. This value defaults to 1 to limit the CPU and memory requirements of the rebuilding process, which grow linearly with the selected parallelism level. The `--max-partitions` option allows for setting an upper bound to the number of partitions to rebuild. An optional expression allows for restricting the set of partitions to rebuild. Tenzir performs a catalog lookup with the expression to identify the set of candidate partitions. This process may yield false positives, as with regular queries, which may cause unaffected partitions to undergo a rebuild. For example, to rebuild outdated partitions containing `suricata.flow` events older than 2 weeks, run the following command: ```bash tenzir-ctl rebuild start '#schema == "suricata.flow" && #import_time < 2 weeks ago' ``` To stop an ongoing rebuild, use `tenzir-ctl rebuild stop`. ## Logging [Section titled ‚ÄúLogging‚Äù](#logging) The Tenzir Node writes log files into a file named `server.log` in the database directory by default. Set the option `tenzir.log-file` to change the location of the log file. A `tenzir` client process does not write logs by default. Set the option `tenzir.client-log-file` to enable logging. Note that relative paths are interpreted relative to the current working directory of the client process. Node log files rotate automatically after 10 MiB. The option `tenzir.disable-log-rotation` allows for disabling log rotation entirely, and the option `tenzir.log-rotation-threshold` sets the size limit when a log file should be rotated. Tenzir processes log messages in a dedicated thread, which by default buffers up to 1M messages for servers, and 100 for clients. The option `tenzir.log-queue-size` controls this setting. ## Caching [Section titled ‚ÄúCaching‚Äù](#caching) Tenzir Nodes cache results for results for pipelines used in the Tenzir Platform‚Äôs Explorer. Internally, this utilizes the [`cache`](/reference/operators/cache) operator. Caches have two primary tuning knobs: 1. The `tenzir.cache.capacity` option controls the total memory usage in bytes caches may use in the node. If this capacity is exceeded, the oldest caches will be removed. The option defaults to 1Gi, and must be set to at least 64Mi. 2. The `tenzir.cache.lifetime` option controls the maximum age of each cache. The option defaults to 10min. If you expect that many users will be using the Explorer with a node at the same time, or if you want to explore large data sets with the Explorer, we recommend increasing the cache capacity option of the node. Otherwise, the Explorer may need to re-run pipelines more frequently because the caches get evicted earlier. We recommend reducing the cache capacity to its minimum of 64Mi only when running in a memory-constrained environment. Here‚Äôs how you can set the options: /opt/tenzir/etc/tenzir/tenzir.yaml ```yaml tenzir: cache: # Total cache capacity in bytes for the node. Must be at least 64Mi. # Defaults to 1Gi. capacity: 1Gi # Maximum lifetime of a cache. Defaults to 10min. lifetime: 10min ``` The options can also be specified as the `TENZIR_CACHE__CAPACITY` and `TENZIR_CACHE__LIFETIME` environment variables, respectively. Restart the node after changing these options for them to take effect. Additionally, in the Explorer, users can control how many events they want to cache per run of a pipeline. The available options are 1k, 10k, 100k, or 1M events, defaulting to 10k. Avoid using higher limits to be fair to other users of the Explorer on the same node, and only increase it when you must.

# Configure dashboards

You can pre-define dashboards for your [static workspaces](/guides/platform-management/configure-workspaces#define-static-workspaces). This practice provides users with ready-to-use visualizations when they access the workspace. Use the `dashboards` key in your workspace configuration: workspace.yaml ```yaml workspaces: static0: name: Example Workspace # Other workspace configuration... dashboards: dashboard1: # Unique dashboard identifier name: Example Dashboard # Display name in the UI cells: # Dashboard cells/widgets - name: Partition Summary # Cell name definition: | # Pipeline to execute partitions where not internal summarize events=sum(events), schema sort -events type: table # Visualization type (table, bar, line, etc.) x: 0 # X-coordinate in the dashboard grid y: 0 # Y-coordinate in the dashboard grid w: 12 # Width in grid units h: 12 # Height in grid units ``` Users can modify these dashboards via the UI after their initial setup. However, the platform resets any changes to the configuration-defined state when it restarts.

# Configure workspaces

Workspaces in the [platform](/explanations/architecture/platform) logically group nodes, secrets, and dashboards. You can configure workspaces in two ways: 1. **Dynamically**: Create, update, or delete workspaces using the command line interface. üëâ Suited for ad-hoc workspace management. 2. **Statically**: Pre-define workspaces declaratively in configuration files. üëâ Suited for GitOps-based infrastructure management. ## Manage workspaces dynamically [Section titled ‚ÄúManage workspaces dynamically‚Äù](#manage-workspaces-dynamically) The Tenzir Platform CLI allows administrators to create, modify, and delete workspaces on demand. On-premise setup required This CLI functionality requires an on-premise platform deployment, available with the [Sovereign Edition](https://tenzir.com/pricing). Only local platform administrators can manage workspaces dynamically. The [`TENZIR_PLATFORM_OIDC_ADMIN_RULES` variable](/guides/platform-setup/configure-identity-provider) defines who‚Äôs an administrator in your platform deployment. ### Creating and managing workspaces [Section titled ‚ÄúCreating and managing workspaces‚Äù](#creating-and-managing-workspaces) You can create workspaces for either individual users or organizations. When you create a user workspace, it‚Äôs automatically configured with access for that specific user. Organization workspaces start with no access rules, giving you full control over who can access them. For a detailed overview, of the available commands, take a look at our [CLI reference documentation](/reference/platform/command-line-interface). ### Example: Setting up an organization workspace [Section titled ‚ÄúExample: Setting up an organization workspace‚Äù](#example-setting-up-an-organization-workspace) Let‚Äôs walk through creating a workspace for the fictional ‚ÄúScrooge & Marley Counting House‚Äù organization: 1. **Create the organization workspace:** ```bash tenzir-platform admin create-workspace organization scrooge-marley --name "Scrooge & Marley Counting House" ``` This creates a workspace with the organization ID `scrooge-marley` and the display name ‚ÄúScrooge & Marley Counting House‚Äù. 2. **Configure access for employees with company email addresses:** ```bash tenzir-platform admin add-auth-rule email-domain <workspace_id> <connection> '@scroogemarley.com' ``` 3. **Add a specific user:** The specific format of the user id depends on the OIDC provider you configured. The provided id must match the `sub` of the generated OIDC token in order to allow access to the workspace. ```bash tenzir-platform admin add-auth-rule user <workspace_id> 'sub|12345678901' ``` 4. **Grant access to users with the ‚Äúaccountant‚Äù role:** ```bash tenzir-platform admin add-auth-rule organization-role <workspace_id> <connection> roles accountant organization scrooge-marley ``` If you later need to remove the workspace: ```bash tenzir-platform admin delete-workspace <workspace_id> ``` ### Understanding access control [Section titled ‚ÄúUnderstanding access control‚Äù](#understanding-access-control) Access rules determine who can enter a workspace. Users gain access to the workspace if they match any configured rule. Think of rules as multiple keys to the same door. The platform supports various rule types, from allowing everyone to restricting access to specific users or organization members. For a complete reference of all available authentication rules and their parameters, see the [CLI reference documentation](/reference/platform/command-line-interface#configure-access-rules). ## Define static workspaces [Section titled ‚ÄúDefine static workspaces‚Äù](#define-static-workspaces) You pre-define **static workspaces** declaratively in configuration files. This ‚Äúas-code‚Äù approach differs from the dynamic management approach, which you manage with the [command line interface](/reference/platform/command-line-interface). Here‚Äôs a minimal example of a static workspace configuration: workspaces.yaml ```yaml workspaces: static0: # Unique workspace identifier name: Tenzir # Display name in the UI category: Statically Configured Workspaces # Grouping category in the UI icon-url: https://storage.googleapis.com/tenzir-public-data/icons/tenzir-logo-square.svg auth-rules: - { "auth_fn": "auth_allow_all" } # Authentication rule (this allows everyone) ``` The `auth-rules` section defines who can access the workspace. The example above uses `auth_allow_all`. This rule grants access to everyone. The `platform` service in a Tenzir Platform deployment uses the `WORKSPACE_CONFIG_FILE` environment variable to locate its static workspace configuration file: docker-compose.yaml ```yaml services: platform: environment: # Other environment variables... - WORKSPACE_CONFIG_FILE=/etc/tenzir/workspaces.yaml volumes: # Mount your config file. - ./workspaces.yaml:/etc/tenzir/workspaces.yaml ``` This example mounts a local `workspaces.yaml` file into the container. The platform service then accesses it at the location that `WORKSPACE_CONFIG_FILE` specifies.

# Use ephemeral nodes

An **ephemeral node** is ideal for temporary or auto-scaling deployments. It is a temporary node that you do not have to provision manually first, and it disappears from the workspace when the connection to the platform ends. Using ephemeral nodes requires that you define a *workspace token*, a shared secret that you pass to the node so that it can self-register. You can define a workspace token in your workspace configuration: workspaces.yaml ```yaml workspaces: static0: name: Tenzir # Other configuration... token: wsk_e9ee76d4faf4b213745dd5c99a9be11f501d7009ded63f2d5NmDS38vXR ``` Caution Workspace tokens have a specific format. Do not create them manually! Use the `tenzir-platform tools generate-workspace-token` command to create valid tokens, or read the *Workspace Token Format* section below for more details. For improved security, store the token in a separate file: workspaces.yaml ```yaml workspaces: static0: name: Tenzir # Other configuration... token-file: /run/secrets/workspace_token ``` This approach works well when you use Docker or Kubernetes secrets. ### Deploy an ephemeral node [Section titled ‚ÄúDeploy an ephemeral node‚Äù](#deploy-an-ephemeral-node) To spawn an ephemeral node, create a configuration file with the workspace token: config.yaml ```yaml tenzir: token: wsk_e9ee76d4faf4b213745dd5c99a9be11f501d7009ded63f2d5NmDS38vXR platform-control-endpoint: http://tenzir-platform.example.org:3001 ``` Then run the node with this configuration: ```bash tenzir-node --config=config.yaml ``` ### Workspace Token Format [Section titled ‚ÄúWorkspace Token Format‚Äù](#workspace-token-format) A valid workspace token starts with the string `wsk_`, continues with 24 bytes of hex-encoded randomness, and ends with the base58-encoded workspace id. More precisely, the Tenzir Platform CLI generates a workspace token according to the following logic: ```python import os import base58 def print_workspace_token(workspace_id: str) -> None: base58_workspace_id = base58.b58encode(workspace_id.encode()).decode() random_bytes = os.urandom(24).hex() print(f"wsk_{random_bytes}{base58_workspace_id}") ``` The `tenzir-platform tools generate-workspace-token` command generates a valid workspace key using exactly this logic. However, if you want to avoid external dependencies, you can use any other tool that prints a string in the format described above.

# Overview

The **Tenzir Platform** acts as a fleet management control plane for Tenzir Nodes. Use its web interface to explore data, create pipelines, and build dashboards. ## Services [Section titled ‚ÄúServices‚Äù](#services) The platform integrates three types of services, as the diagram below illustrates: ![Platform Services](/_astro/platform-services.-sBCic5A_19DKCs.svg) 1. **Internal**: Tenzir provides the (gray) internal services. They are core to the platform‚Äôs operation. 2. **External**: You provide the (blue) external services. We do not ship these, so you must bring your own. 3. **Configurable**: You can use our bundled (yellow) configurable services or provide your own. ## Deployment options [Section titled ‚ÄúDeployment options‚Äù](#deployment-options) There exist two ways to deploy the platform: 1. **Cloud**: Deploy the platform on a cloud provider. 2. **On-premises**: Deploy the platform on your own infrastructure. ### Get started in the cloud [Section titled ‚ÄúGet started in the cloud‚Äù](#get-started-in-the-cloud) For Amazon, read our [AWS deployment guide](/guides/platform-setup/deploy-on-aws) for an integrated approach to deploying the platform services in your own account using a **CloudFormation** template. For Azure and GCP, we are still working on providing turnkey deployment setups. ### Get started on your own premises [Section titled ‚ÄúGet started on your own premises‚Äù](#get-started-on-your-own-premises) Follow these steps to set up the platform on your own premises: 1. [Choose a scenario](/guides/platform-setup/choose-a-scenario): we package several deployment options that match common deployments. 2. Configure the services: you must set up the external API endpoints so nodes and browsers can interact with the platform. * [Configure reverse proxy](/guides/platform-setup/configure-reverse-proxy) * [Configure internal services](/guides/platform-setup/configure-internal-services) * [Configure identity provider](/guides/platform-setup/configure-identity-provider) * [Configure database](/guides/platform-setup/configure-database) * [Configure blob storage](/guides/platform-setup/configure-blob-storage) You may skip some steps depending on your chosen scenario. 3. [Run the platform](/guides/platform-setup/run-the-platform): After you create a configuration, start the platform.

# Choose a scenario

We provide several examples of possible platform deployment scenarios. Pick one that best suits your needs. ## Download the platform files [Section titled ‚ÄúDownload the platform files‚Äù](#download-the-platform-files) Start by downloading the [latest Tenzir Platform release](https://github.com/tenzir/platform/releases/latest) and unpack the archive. ## Choose a scenario [Section titled ‚ÄúChoose a scenario‚Äù](#choose-a-scenario) The `examples` directory contains example scenarios in the form of Docker Compose configurations. Choose from the following options: 1. **localdev**: This scenario is designed for local development and testing purposes. It operates as a ‚ÄúTenzir-in-a-box,‚Äù meaning all necessary components are bundled together for a quick and easy setup, making it ideal for single-user exploration and experimentation. Due to its simplified nature and focus on ease of use, it has a low barrier to entry. However, it is not optimized for performance, security, or scalability, making it ill-suited for production environments. 2. **keycloak**: The `keycloak` scenario provides a minimal yet complete setup for multi-user environments. It integrates Keycloak for robust authentication and authorization, enabling secure access for multiple users. This configuration includes all essential Tenzir services and is designed to be readily usable, especially when deployed behind a reverse proxy, which can handle TLS termination and provide an additional layer of security and load balancing. 3. **onprem**: Our `onprem` scenario represents an enterprise-grade deployment. It is structured with the assumption that critical services such as Kafka, S3, and the identity provider (like Keycloak) are already established and managed externally within your on-premises infrastructure. This allows Tenzir to integrate seamlessly into existing robust, production-ready environments, leveraging your organization‚Äôs established operational practices and infrastructure investments. To customize a scenario to your needs, configure the services by populating a `.env` file with your settings as environment variables.

# Configure blob storage

The **blob storage** service exists for exchanging files between the platform and nodes. It facilitates not only downloading data from nodes, but also uploading files from your browser to the platform. For example, when you click the *Download* button in the Explorer, you see various formats in which you can download the data, such as JSON, CSV, Parquet, and more. Once you initiate the download, a pipeline writes the requested data into the platform‚Äôs blob storage. Similarly, when you drag files into the Explorer, you upload them to the blob storage for use as pipeline inputs by nodes. The following variables control the blob storage service: ```sh TENZIR_PLATFORM_DOWNLOADS_ENDPOINT= TENZIR_PLATFORM_INTERNAL_BUCKET_NAME= TENZIR_PLATFORM_INTERNAL_ACCESS_KEY_ID= TENZIR_PLATFORM_INTERNAL_SECRET_ACCESS_KEY= ``` When you use S3 or another external blob storage, you must create the bucket and provide a valid access key with read and write permissions on the bucket. When you use the bundled seaweed instance, you can set these values to arbitrary strings. The bundled Seaweed container automatically provides a bucket with the specified name and access key.

# Configure database

A PostgreSQL database stores the internal state of the platform. Provide the following environment variables so the platform can connect to the Postgres instance: ```sh TENZIR_PLATFORM_POSTGRES_USER= TENZIR_PLATFORM_POSTGRES_PASSWORD= TENZIR_PLATFORM_POSTGRES_DB= TENZIR_PLATFORM_POSTGRES_HOSTNAME= ```

# Configure identity provider

The identity provider (IdP) handles authentication for the Tenzir Platform. When you click the *Login* button in the Tenzir UI, the system redirects you to your chosen identity provider, which creates a signed token that certifies your identity. You can either use an external identity provider that‚Äôs already set up inside your organization, or run an identity provider that‚Äôs bundled with our example `docker-compose.yaml`. The Tenzir Platform supports the OpenID Connect (OIDC) protocol for external identity providers. Below, we describe the requirements for using a generic identity provider, as well as two hands-on guides for setting up Keycloak and Entra ID as identity providers for the Tenzir Platform. ## Generic Identity Provider [Section titled ‚ÄúGeneric Identity Provider‚Äù](#generic-identity-provider) To use an external IdP, ensure that it supports the OIDC protocol, including the *OIDC Discovery* extension, and configure it to provide valid RS256 ID tokens. Set up the external identity provider by creating two clients (also called applications in Auth0, or app registrations in Microsoft Entra) named `tenzir-app` and `tenzir-cli`. The `tenzir-app` client handles logging into the Tenzir Platform in the web browser. * You must enable the **Authorization Code** flow. * The allowed redirect URLs must include `https://app.platform.example/login/oauth/callback`. * You should note down the client secret so you can add it to the configuration of the Tenzir Platform in the next step. The `tenzir-cli` client handles authentication with the `tenzir-platform` CLI. * You must enable the **Device Code** flow. * The identity provider must either return an `id_token` for the device code flow, or an `access_token` in JWT format. You may want to run CLI commands in environments where no user is available to perform the device code authorization flow, for example when you run CLI commands as part of a CI job. In this case, you can set up another client with the **Client Credentials** flow enabled. The `access_token` you obtain from this client must be in JWT format. The CLI automatically attempts to use the client credentials flow if the `TENZIR_PLATFORM_CLI_CLIENT_SECRET` environment variable is set. You can also force the use of the client credentials flow by using the `tenzir-platform auth login --non-interactive` option. You must provide the following environment variables for the OIDC provider configuration used for logging into the platform: ```sh TENZIR_PLATFORM_OIDC_PROVIDER_NAME=example-idp TENZIR_PLATFORM_OIDC_PROVIDER_ISSUER_URL=https://my.idp.example TENZIR_PLATFORM_OIDC_CLI_CLIENT_ID=tenzir-cli TENZIR_PLATFORM_OIDC_APP_CLIENT_ID=tenzir-app TENZIR_PLATFORM_OIDC_APP_CLIENT_SECRET=xxxxxxxxxxxxxxxxxxxxxxxx ``` You must provide the following environment variable containing a JSON object with the OIDC issuer and audiences that the platform should accept. You can also provide an array of objects to configure multiple trusted issuers. ```sh # Single issuer configuration TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='{"issuer": "http://platform.local:3004/realms/master","audiences": ["tenzir-app"]}' # Multiple issuers configuration TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='[ {"issuer": "https://accounts.google.com", "audiences": ["audience1"]}, {"issuer": "http://platform.local:3004/realms/master", "audiences": ["tenzir-app"]} ]' ``` ## Keycloak [Section titled ‚ÄúKeycloak‚Äù](#keycloak) This section explains how to configure Keycloak as an external Identity Provider for use with the Tenzir Platform. We assume that you already have a Keycloak instance up and running that you can reach with a browser. We provide an example stack of the Tenzir Platform with a bundled Keycloak instance [here](https://github.com/tenzir/platform/tree/main/examples/keycloak). The remainder of this section assumes you‚Äôre running this example stack, but the setup is the same for any other Keycloak instance. ### Setting up the Keycloak Instance [Section titled ‚ÄúSetting up the Keycloak Instance‚Äù](#setting-up-the-keycloak-instance) #### Admin Login [Section titled ‚ÄúAdmin Login‚Äù](#admin-login) Navigate to the Keycloak instance and log in as a user with admin permissions. If you use the bundled Keycloak instance, the initial username is `admin` and password is `changeme`. Remember to change the default password after logging in for the first time, and to set up 2-factor authentication for your admin user. #### Create a new `tenzir` realm (optional) [Section titled ‚ÄúCreate a new tenzir realm (optional)‚Äù](#create-a-new-tenzir-realm-optional) Keycloak defaults to the `master` realm and the bundled Docker Compose stack uses this realm by default. If you want to use a different realm, or if you already have an existing one, update the `TENZIR_PLATFORM_OIDC_ISSUER_URL` variable to point to the new realm instead. ![Configuration 0](/_astro/keycloak-create-realm.Bya4pY8v_R2aSw.webp) #### Create a client for the app [Section titled ‚ÄúCreate a client for the app‚Äù](#create-a-client-for-the-app) Use the `Add Client` button in the `Clients` menu on the left. Configure the new client as follows: Under *General settings*, set the client type to *OpenID Connect* and the client ID to `tenzir-app`. If you use a different client ID, remember to update the `TENZIR_PLATFORM_OIDC_PROVIDER_CLIENT_ID` variable in your Tenzir Platform configuration accordingly. ![Configuration 0](/_astro/keycloak-create-app-client-0.B0dL2lBu_Z1GlGsq.webp) Under *Capability config*, enable client authentication and the *Standard flow* access method. ![Configuration 1](/_astro/keycloak-create-app-client-1.BwnUkGAP_ZBKCgB.webp) Under *Login settings*, enter a redirect URL that points to `${TENZIR_PLATFORM_DOMAIN}/login/oauth/callback`, where `TENZIR_PLATFORM_DOMAIN` is the domain you configure in your Tenzir Platform configuration. For example, if the app runs under `https://tenzir-app.example.org` then this should be `https://tenzir-app.example.org/login/oauth/callback`. ![Configuration 2](/_astro/keycloak-create-app-client-2.CcGxuXQl_1UJfDb.webp) Finally, in the client view, go to the Credentials tab and copy the value of the generated client secret. You must add this to the Tenzir Platform configuration under `TENZIR_PLATFORM_OIDC_PROVIDER_CLIENT_SECRET`. ![Configuration 3](/_astro/keycloak-create-app-client-3.nrdSE4hA_ZFjsuw.webp) ### Create a client for the CLI [Section titled ‚ÄúCreate a client for the CLI‚Äù](#create-a-client-for-the-cli) To use the `tenzir-platform` CLI, you need to set up an additional client that supports device code authentication. (It‚Äôs possible but not recommended to use the same client for both the Tenzir UI and Tenzir Platform CLI) To do this, proceed exactly as above, but use `tenzir-cli` as the client ID and under *Capability config* disable the *Client authentication* setting and enable the *OAuth 2.0 Device Authorization Grant* authentication flow. ![Create CLI client](/_astro/keycloak-create-cli-client-0.Bw33IN8V_Z1pKCTe.webp) ## Microsoft Entra Identity [Section titled ‚ÄúMicrosoft Entra Identity‚Äù](#microsoft-entra-identity) To use Microsoft Entra Identity as an OIDC provider, you need to create two app registrations in Entra ID and configure them for use with the Tenzir UI and the Tenzir Platform CLI. Follow these steps to create the required resources: 1. Navigate to `portal.azure.com` and open the page for ‚ÄúMicrosoft Entra ID‚Äù. ![Navigate to Entra](/_astro/entra-oidc-navigate.UgOk1iDU_kfeLm.webp) 2. Open the ‚ÄúApp registrations‚Äù sub page. ![Select App registrations](/_astro/entra-oidc-app-registrations.oSzkSuHl_28LLnA.webp) 3. Create a new registration named `Tenzir Platform CLI`. ![Register CLI](/_astro/entra-oidc-register-cli.Bg_oTrMa_Z1IgiIE.webp) 4. Enable the public client flows for this app. ![Enable public client flows](/_astro/entra-oidc-cli-public-client-flows.s5Ak9_BR_UEIqg.webp) 5. Create a second registration named `Tenzir UI`. ![Register UI](/_astro/entra-oidc-register-ui.jbBJuTxF_1ha3e4.webp) 6. For this registration, open ‚ÄúCertificates & Secrets‚Äù. ![Navigate Secrets](/_astro/entra-oidc-create-secret-1.DXZEOI8E_Z1EWunp.webp) 7. Create a new secret and give it a descriptive name. ![Create Secret](/_astro/entra-oidc-create-secret-2.DnIODWhC_ZN2d9t.webp) 8. Make a local copy of the secret value. ![Copy Secret](/_astro/entra-oidc-copy-secret.Ci3SrFI__28yGWS.webp) 9. Copy the client id to your local machine. ![Copy Client ID](/_astro/entra-oidc-client-id.DCKjpMwg_2928iB.webp) 10. Copy the issuer url to your local machine. ![Copy Issuer URL](/_astro/entra-oidc-issuer-url.C08Q45Ve_Z1kjxK6.webp) Now you can supply the created resources and values to the stack by editing the `.env` file in your Compose folder: ```sh TENZIR_PLATFORM_OIDC_PROVIDER_NAME="Entra ID" # The client id of the registration created in step 3. TENZIR_PLATFORM_OIDC_CLI_CLIENT_ID=082a9391-b645-4278-a16e-3cf54fb1bcf0 # The client id and secret created in steps 8 and 9. TENZIR_PLATFORM_OIDC_APP_CLIENT_SECRET=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx TENZIR_PLATFORM_OIDC_APP_CLIENT_ID=d8ea5612-6745-47bc-b9fe-5024b1ca18fe # The issuer url copied in step 10. TENZIR_PLATFORM_OIDC_PROVIDER_ISSUER_URL=https://login.microsoftonline.com/40431729-d276-4582-abb4-01e21c8b58fe/v2.0 TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='{"issuer": "https://login.microsoftonline.com/40431729-d276-4582-abb4-01e21c8b58fe/v2.0","audiences": ["d8ea5612-6745-47bc-b9fe-5024b1ca18fe", "082a9391-b645-4278-a16e-3cf54fb1bcf0"]}' ``` Now you can restart your Tenzir Platform stack and the system will redirect you to a Microsoft login page when you next log in to the Tenzir UI. ### Additional Configuration for Client Credentials Flow (Optional) [Section titled ‚ÄúAdditional Configuration for Client Credentials Flow (Optional)‚Äù](#additional-configuration-for-client-credentials-flow-optional) If you plan to use the client credentials flow for non-interactive CLI usage, you need to configure additional settings for the CLI registration created in step 3: 11. Navigate to ‚ÄúExpose an API‚Äù and add an Application ID URI (you can use the suggested default value). ![Expose API Configuration](/_astro/entra-oidc-expose-api.abcXYbMN_Z1aHdB.webp) 12. Navigate to ‚ÄúManifest‚Äù and update the `requestedAccessTokenVersion` to `2` in the JSON manifest, then save the changes. ![Update Access Token Version](/_astro/entra-oidc-client-api-version.B78ur6Zl_Z2tAea4.webp) 13. When using the client credentials flow with this registration, you must override the scope by setting `TENZIR_PLATFORM_CLI_SCOPE` to `${APPLICATION_ID_URL}/.default`, where `${APPLICATION_ID_URL}` is the Application ID URI you configured in step 11. ## Single-user Mode [Section titled ‚ÄúSingle-user Mode‚Äù](#single-user-mode) In some scenarios handling multiple users is not necessary for the Tenzir Platform. For example for purely local instances used for development or experimentation, or where access is already gated through some external mechanism. For these situations, the Tenzir Platform offers the single-user mode. In this mode, the Tenzir Platform itself acts as the identity provider, and creates a static, global admin user with no password. To stress, this means that everyone who can reach the Tenzir Platform will have full access to every Tenzir Node and every workspace, so this is not suitable for generic production deployments. To use the single-user mode, we recommend starting from [this](https://github.com/tenzir/platform/tree/main/examples/localdev) example setup. This example is already fully configured and can run as-is, skipping the manual configuration described in the sections below. ### Tenzir Platform API [Section titled ‚ÄúTenzir Platform API‚Äù](#tenzir-platform-api) In terms of configuration, the `platform` service needs to be put into single-user mode by setting the `TENANT_MANAGER_AUTH__SINGLE_USER_MODE` environment variable to true. Additionally, the platform needs to have additional configuration that would usually be configured in the external identity provider, in particular the issuer url it should use for signing its own JWTs and the allowed redirect URLs and audiences of the UI and CLI: ```sh TENANT_MANAGER_AUTH__SINGLE_USER_MODE=true TENANT_MANAGER_AUTH__ISSUER_URL=http://platform:5000/oidc TENANT_MANAGER_AUTH__PUBLIC_BASE_URL=${TENZIR_PLATFORM_API_ENDPOINT}/oidc TENANT_MANAGER_AUTH__APP_AUDIENCE=tenzir-app TENANT_MANAGER_AUTH__APP_REDIRECT_URLS=${TENZIR_PLATFORM_UI_ENDPOINT}/login/oauth/callback TENANT_MANAGER_AUTH__CLI_AUDIENCE=tenzir-cli ``` ### Tenzir UI [Section titled ‚ÄúTenzir UI‚Äù](#tenzir-ui) The Tenzir UI needs to be pointed to the `platform` service as the new identity provider. Since the single-user mode does not provide true user authentication, the corresponding client secret is also not truly secret but set to the static string ‚Äúxxxx‚Äù. ```sh PRIVATE_OIDC_PROVIDER_NAME=tenzir PRIVATE_OIDC_PROVIDER_CLIENT_ID=tenzir-app PRIVATE_OIDC_PROVIDER_CLIENT_SECRET=xxxx PRIVATE_OIDC_PROVIDER_ISSUER_URL=http://platform:5000/oidc ``` ### Tenzir Platform CLI [Section titled ‚ÄúTenzir Platform CLI‚Äù](#tenzir-platform-cli) In single-user mode, the Tenzir Platform CLI can be set to non-interactive mode by adding the `TENZIR_PLATFORM_CLI_CLIENT_SECRET` environment variable. Note that when running the Tenzir Platform inside a docker compose stack, in single-user mode the CLI must run the same docker compose stack as the platform. When running outside of docker compose, the requirement is that the CLI must be able to establish a network connection to the configured ISSUER\_URL. ```sh TENZIR_PLATFORM_CLI_CLIENT_ID=tenzir-cli TENZIR_PLATFORM_CLI_CLIENT_SECRET=xxxx TENZIR_PLATFORM_CLI_ISSUER_URL=http://platform:5000/oidc ``` ## Advanced Topics [Section titled ‚ÄúAdvanced Topics‚Äù](#advanced-topics) ### Custom Scopes [Section titled ‚ÄúCustom Scopes‚Äù](#custom-scopes) By default, the Tenzir Platform requests the `profile email openid offline_access` scopes when you log in. To adjust this, set the `PUBLIC_OIDC_SCOPES` environment variable to a space-separated list of scope names. ### Profile Pictures [Section titled ‚ÄúProfile Pictures‚Äù](#profile-pictures) To include custom profile pictures, include a `picture` claim in the returned ID token that contains a URL to the image file. The Tenzir Platform reads that claim and uses it as the profile picture in the top right corner of the user interface, or falls back to a default image if the claim isn‚Äôt present. ### Refresh Tokens [Section titled ‚ÄúRefresh Tokens‚Äù](#refresh-tokens) The Tenzir Platform supports the use of refresh tokens and by default requests the `offline_access` scope to automatically refresh sessions after the initial ID token has expired. To this end, the `offline_access` scope is requested by default. Unfortunately, the OIDC spec is ambiguous on the precise semantics of the `offline_access` scope, and Keycloak‚Äôs interpretation differs from most other OIDC providers: it always includes refresh tokens by default, and adds additional permissions to the token when the `offline_access` scope is requested. Therefore, some organizations forbid the use of tokens with `offline_access` permissions for security reasons. In that case, add an environment variable `PUBLIC_OIDC_SCOPES=profile email oidc` to the `app` environment to explicitly remove the scope request. The bundled Docker Compose file in this directory does this by default. ### External JWT Authentication [Section titled ‚ÄúExternal JWT Authentication‚Äù](#external-jwt-authentication) The Tenzir Platform supports authentication using externally-supplied JWTs, which is useful for scenarios where the platform runs behind a proxy that handles authentication, such as Google Cloud IAP or other enterprise proxies. To configure external JWT authentication, add the `PRIVATE_JWT_FROM_HEADER` environment variable with the name of the HTTP header that contains the JWT token to the environment of the Tenzir UI: ```sh PRIVATE_JWT_FROM_HEADER=X-Goog-IAP-JWT-Assertion ``` You must also configure the trusted issuers and audiences using the `TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES` environment variable. This variable accepts a JSON object or array of objects containing the issuer and audiences that the platform should accept: ```sh # Single issuer configuration TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='{"issuer": "https://cloud.google.com/iap", "audiences": ["your-audience"]}' # Multiple issuers configuration TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES='[ {"issuer": "https://accounts.google.com", "audiences": ["audience1"]}, {"issuer": "https://cloud.google.com/iap", "audiences": ["audience2"]} ]' ```

# Configure internal services

This guide explains how to configure the three internal Tenzir services: the Tenzir UI, the Tenzir Gateway, and the Tenzir Platform API. These core platform services require internal configuration. The platform runs as a set of containers in a Docker Compose stack. The [example files](https://github.com/tenzir/platform/tree/main/examples) read configuration parameters from environment variables. Create a file named `.env` in the same directory as your `docker-compose.yaml` file, and set the environment variables described below. ## Tenzir Platform API [Section titled ‚ÄúTenzir Platform API‚Äù](#tenzir-platform-api) This section describes how to configure the Tenzir Platform API. ### Admin rules [Section titled ‚ÄúAdmin rules‚Äù](#admin-rules) The platform distinguishes between *regular* and *admin* users. Admin users can create new workspaces and view, edit, and delete all existing workspaces. Currently, only admin users can create shared workspaces by creating a new workspace and editing the access rules. Configure admin users by providing a list of authentication rules. The platform considers any user who matches the provided rules an admin and allows them to use the `tenzir-platform admin` CLI commands. Use the `tenzir-platform tools print-auth-rule` CLI command to generate valid rules for insertion here. ```sh # Make everybody an admin. # Use for deployments with few users who fully trust each other. TENZIR_PLATFORM_ADMIN_RULES=[{"auth_fn":"auth_allow_all"}] # Make the two users with IDs `google-oauth2:12345678901` and # `google-oauth2:12345678902` admins of this platform instance. TENZIR_PLATFORM_ADMIN_RULES=[{"auth_fn":"auth_user","user_id":"google-oauth2:12345678901"}, {"auth_fn":"auth_user","user_id":"google-oauth2:12345678902"}] ``` ### Random seeds [Section titled ‚ÄúRandom seeds‚Äù](#random-seeds) The platform requires random strings for encryption functions. Generate these strings with a secure random number generator and use unique values for each platform instance. ```sh # Random string to encrypt frontend cookies. # Generate with `openssl rand -hex 32`. TENZIR_PLATFORM_INTERNAL_AUTH_SECRET= # Random string to generate user keys. # Generate with `openssl rand 32 | base64`. TENZIR_PLATFORM_INTERNAL_TENANT_TOKEN_ENCRYPTION_KEY= # Random string for the app to access restricted API endpoints. # Generate with `openssl rand -hex 32`. TENZIR_PLATFORM_INTERNAL_APP_API_KEY= ``` ### Node blob storage configuration [Section titled ‚ÄúNode blob storage configuration‚Äù](#node-blob-storage-configuration) When the Tenzir UI and Tenzir Nodes run in separate networks, the nodes may need to use a different URL to reach the blob storage service from the one the UI uses. For example, the Tenzir UI may run on a browser of a remote employee, who accesses the configured blob storage via `https://downloads.example.com`. However, the nodes may run in a strictly regulated network that only allows access to the outside world through a reverse proxy. In this example, these nodes would have to talk to `https://downloads.corporate-proxy.example.com`. To allow both sides to use different URLs to talk to the same blob storage service, you can override the public endpoint. Use the `BLOB_STORAGE__NODES_PUBLIC_ENDPOINT_URL` environment variable to override the URL that nodes use to access S3-compatible blob storage. Note that you need to add this variable to the `platform` container directly, since it currently doesn‚Äôt appear in any of the example setups: ```sh # Override the blob storage URL for nodes BLOB_STORAGE__NODES_PUBLIC_ENDPOINT_URL=https://s3.internal.example.com ``` This setting is particularly useful when: * Nodes run in a separate network from the Tenzir UI * Internal DNS resolution differs between the UI and node networks * You need to route node traffic through specific network paths ## Tenzir UI [Section titled ‚ÄúTenzir UI‚Äù](#tenzir-ui) This section describes how to configure the Tenzir Platform UI. ### Internal proxy [Section titled ‚ÄúInternal proxy‚Äù](#internal-proxy) By default, the Tenzir UI sends requests directly from the browser to the Tenzir Gateway to query the status of connected nodes. This setup can cause problems in some network topologies, especially in zero-trust networks where accessing the Tenzir Gateway requires additional authentication headers. Enable the internal proxy to route these requests through the Tenzir UI backend instead, which makes all requests from the Tenzir UI frontend go to the same domain. This approach adds some latency through an additional proxy hop. ```sh TENZIR_PLATFORM_USE_INTERNAL_WS_PROXY=true ``` ### External JWT authentication [Section titled ‚ÄúExternal JWT authentication‚Äù](#external-jwt-authentication) The Tenzir Platform supports accepting externally-supplied JWTs from HTTP headers, which is useful when running behind authentication proxies like Google Cloud IAP or other enterprise authentication systems. Configure the `app` service to accept JWTs from a specific HTTP header. Note that you need to add this variable to the `app` container directly, since it currently doesn‚Äôt appear in any of the example setups: ```sh # Accept JWTs from the specified HTTP header PRIVATE_JWT_FROM_HEADER=X-Goog-IAP-JWT-Assertion ``` When you set this variable, the platform bypasses the normal OIDC login flow and instead validates the JWT token provided in the specified header. You must also configure the trusted issuers and audiences to match the values that the external JWT contains using the `TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES` environment variable for the Tenzir Platform API. For more information about external JWT configuration, see the [external JWT authentication section](/guides/platform-setup/configure-identity-provider#external-jwt-authentication) in the identity provider configuration guide. ## Tenzir Gateway [Section titled ‚ÄúTenzir Gateway‚Äù](#tenzir-gateway) The Tenzir Gateway doesn‚Äôt offer any user-controlled settings at the moment.

# Configure reverse proxy

The platform uses four distinct entry points with four different URLs: 1. **Tenzir UI**: The URL your browser connects to, e.g., `app.platform.example`. This serves the web frontend where you interact with the platform. 2. **Tenzir Gateway**: The URL nodes connect to, e.g., `nodes.platform.example`. Tenzir Nodes connect to this URL to establish long-running WebSocket connections. 3. **Blob Storage**: The URL where the platform‚Äôs S3-compatible blob storage is accessible, e.g., `downloads.platform.example`. When you click the *Download* button, the platform generates download links under this URL. When you drag files into the explorer window, the platform stores them here so that Tenzir Nodes can access them. 4. **Platform API**: The URL the Tenzir Platform CLI connects to, e.g., `api.platform.example`. The typical setup for the Tenzir Platform uses a reverse proxy to terminate TLS connections and forward incoming traffic to the correct services. The choice of technology varies with your deployment scenario and ranges from an additional [Traefik](https://doc.traefik.io/traefik/getting-started/install-traefik/) container within the Docker Compose stack, over an nginx instance running on the same host, to global load balancing services offered by a commercial cloud provider. In our example scenarios, set the following environment variables to configure these four endpoints: ```sh # The domain under which the platform frontend is reachable. # Must include the `http://` or `https://` scheme. TENZIR_PLATFORM_UI_ENDPOINT=https://app.platform.example # The endpoint to which Tenzir nodes should connect. # Must include the `ws://` or `wss://` scheme. TENZIR_PLATFORM_NODES_ENDPOINT=wss://nodes.platform.example # The URL at which the platform's S3-compatible blob storage is accessible. TENZIR_PLATFORM_DOWNLOADS_ENDPOINT=https://downloads.platform.example # The URL at which the platform API is accessible. TENZIR_PLATFORM_API_ENDPOINT=https://api.platform.example ``` ## Native TLS [Section titled ‚ÄúNative TLS‚Äù](#native-tls) A reverse proxy typically runs on the same host as other platform containers and terminates TLS. When you can‚Äôt guarantee that the reverse proxy runs on the same host as other platform containers, or when you deploy the containers to different machines, enable native TLS support for individual platform containers. ### Obtaining Certificates [Section titled ‚ÄúObtaining Certificates‚Äù](#obtaining-certificates) Purchase the domain name you want for the platform and use one of the globally trusted certificate authorities (CAs) to obtain a valid certificate. This approach provides the most straightforward and recommended certificate acquisition method. If you run the platform in a private or air-gapped network, use methods like the DNS challenge offered by [Let‚Äôs Encrypt](https://letsencrypt.org/) and other providers to generate a certificate and transfer it to the target machine. When you can‚Äôt use a globally trusted CA, use a corporate root CA instead. Naturally, certificates from this CA will only be trusted inside your organization or your Tenzir Platform setup. If you don‚Äôt possess a corporate root CA, create a private CA for yourself. Signing and provisioning root certificates is a complex task, so we recommend using a tool like `trustme` for this purpose. We provide a [sample script](https://github.com/tenzir/platform/tree/main/examples/native-tls) that shows how to create the necessary certificates for all components. Certificate Validation Idiosyncrasies Some libraries ignore the system-wide CA certificate store and use alternative, more strictly curated bundles. For example, Mozilla‚Äôs NSS root store is a popular choice. Additionally, the operating system‚Äôs default certificate bundles shipped within our Docker containers won‚Äôt trust private CAs by default. Therefore, when you use a private CA, perform the same configuration for corporate root CAs from a publicly trusted CA or your self-created private CA. ### Self-signed Certificates [Section titled ‚ÄúSelf-signed Certificates‚Äù](#self-signed-certificates) Instead of creating a private CA, create a self-signed certificate that combines certificate and CA in a single file. This approach simplifies setup and management compared to a private CA, but reduces security guarantees. For example, it nullifies several TLS security guarantees and provides only protection against passive eavesdropping. Below we assume you store valid TLS certificates in files named `ssl/app-cert.pem`, `ssl/platform-cert.pem`, etc., where each file contains both the TLS certificate and private key. If you store the certificate and private key in separate files, mount both into the containers and adjust the environment variables to point towards the correct file. When you use a private CA, store the public key of that CA in the file `ssl/ca.pem`. #### Tenzir UI [Section titled ‚ÄúTenzir UI‚Äù](#tenzir-ui) To have the Tenzir UI serve its traffic using TLS, add the following environment variables and volumes to your `docker-compose.yaml`: ```yaml services: app: environment: - TLS_CERTFILE=/ssl/app-cert.pem - TLS_KEYFILE=/ssl/app-cert.pem volumes: - ./ssl/app-cert.pem:/ssl/app-cert.pem ``` When you use a private CA, add the following configuration: ```yaml services: app: environment: - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt volumes: - ./ssl/ca.pem:/etc/ssl/certs/ca-certificates.crt ``` #### Tenzir Gateway [Section titled ‚ÄúTenzir Gateway‚Äù](#tenzir-gateway) To enable TLS serving for the gateway, mount the certificate into the container and set the `TLS_CERTFILE` and `TLS_KEYFILE` environment variables: ```yaml services: websocket-gateway: environment: - TLS_CERTFILE=/ssl/gateway-cert.pem - TLS_KEYFILE=/ssl/gateway-cert.pem volumes: - ./ssl/gateway-cert.pem:/ssl/gateway-cert.pem ``` When you use a private CA, add the following configuration: ```yaml services: websocket-gateway: environment: - TLS_CAFILE=/ssl/ca.pem volumes: - ./ssl/ca.pem:/ssl/ca.pem ``` #### Platform API [Section titled ‚ÄúPlatform API‚Äù](#platform-api) To enable TLS serving for the Platform API, mount the certificate into the container and set the `TLS_CERTFILE` and `TLS_KEYFILE` environment variables. This follows the same process as for the `websocket-gateway` container: ```yaml services: platform: environment: - TLS_CERTFILE=/ssl/platform-cert.pem - TLS_KEYFILE=/ssl/platform-cert.pem volumes: - ./ssl/platform-cert.pem:/ssl/platform-cert.pem ``` When you use a private CA, add the following configuration: ```yaml services: platform: environment: # 'requests' uses a baked-in CA bundle, so point it to our CA explicitly. - REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt - SSL_CERT_FILE=/ssl/ca.pem volumes: - ./ssl/ca.pem:/etc/ssl/certs/ca-certificates.crt ``` ### Example [Section titled ‚ÄúExample‚Äù](#example) Refer to our [native TLS example](https://github.com/tenzir/platform/tree/main/examples/native-tls) for a complete configuration example, including the native TLS setup for the default bundled database, S3 storage and IdP services. ## Node TLS Settings [Section titled ‚ÄúNode TLS Settings‚Äù](#node-tls-settings) Nodes connect to the platform using the Tenzir Gateway via a TLS connection. When using a custom certificate for the gateway, provide it to the node to successfully establish a connection. Set the following option to point to a CA certificate file in PEM format without password protection: ```sh TENZIR_PLATFORM_CACERT=/path/to/ca-certificate.pem ``` When using a self-signed TLS certificate, additionally disable TLS certificate validation: ```sh TENZIR_PLATFORM_SKIP_PEER_VERIFICATION=true ```

# Configure secret store

The Tenzir Platform provides a secret store for each workspace. All Tenzir Nodes connected to the workspace can access its secrets. You can manage secrets using the CLI or the web interface. Alternatively, you can use an external secret store. Read more about how secrets work in our [explanations page](/explanations/secrets). ## Configuring the platform secret store [Section titled ‚ÄúConfiguring the platform secret store‚Äù](#configuring-the-platform-secret-store) ### Managing Secrets via the CLI [Section titled ‚ÄúManaging Secrets via the CLI‚Äù](#managing-secrets-via-the-cli) To add a new secret to the Platform‚Äôs secret store: Add value to the Platform's secret store ```sh tenzir-platform secret add geheim --value=1528F9F3-FAFA-45B4-BC3C-B755D0E0D9C2 ``` Refer to the [CLI reference](/reference/platform/command-line-interface#manage-secrets) for more details on updating or deleting secrets. ### Managing Secrets via Web Interface [Section titled ‚ÄúManaging Secrets via Web Interface‚Äù](#managing-secrets-via-web-interface) To manage secrets from the web interface, go to the `Workspace Settings` screen by clicking the gear icon in the workspace selector. ![Screenshot](/_astro/secrets.Dd085Pjn_ZqXKpL.webp) ## Configuring External Secret Stores [Section titled ‚ÄúConfiguring External Secret Stores‚Äù](#configuring-external-secret-stores) You can configure the Tenzir Platform to provide access to secrets stored in an external secret store instead of using it own store. This access is read-only. ### AWS Secrets Manager [Section titled ‚ÄúAWS Secrets Manager‚Äù](#aws-secrets-manager) To add AWS Secrets Manager as an external secret store, use the CLI: Add AWS Secrets Manager as external secret store ```sh tenzir-platform secret store add aws \ --region='eu-west-1' \ --assumed-role-arn='arn:aws:iam::1234567890:role/tenzir-platform-secrets-access' \ --prefix=tenzir/ ``` * The Tenzir Platform must have permissions to read secrets under the specified prefix from the external store. * The platform must be able to assume the specified role in AWS. See the [CLI reference](/reference/platform/command-line-interface#manage-external-secret-stores) for more details.

# Deploy on AWS

This guide shows you how to deploy the Tenzir Platform Sovereign Edition on AWS using a CloudFormation template. The deployment automates the setup of all required infrastructure components. ## Architecture overview [Section titled ‚ÄúArchitecture overview‚Äù](#architecture-overview) The deployment creates a complete platform infrastructure with the following components: ![Platform AWS Architecture](/_astro/aws-architecture.CTZmMWF-_19DKCs.svg) The architecture consists of: * **Frontend services**: Web UI (App Runner), Platform API (Lambda), and API Gateway for custom domains * **Gateway service**: ECS Fargate tasks behind an Application Load Balancer for node connections * **Data layer**: RDS PostgreSQL for platform state, S3 buckets for blob and sidepath data * **Security**: Cognito for authentication, Secrets Manager for credentials, ACM for TLS certificates * **Networking**: VPC with public/private subnets, NAT Gateway for outbound connectivity * **DNS**: Route53 for domain management with automatic subdomain creation ## Prerequisites [Section titled ‚ÄúPrerequisites‚Äù](#prerequisites) Before you begin, you need: 1. **AWS Account**: An AWS account with permissions to create CloudFormation stacks, VPCs, ECS services, Lambda functions, RDS databases, and other resources. 2. **Domain in Route53**: A registered domain name with a Route53 hosted zone in your AWS account. The deployment automatically discovers your hosted zone and creates DNS records for the platform services. 3. **Marketplace Subscription**: Subscribe to the Tenzir Platform - Sovereign Edition in the [AWS Marketplace](https://aws.amazon.com/marketplace). 4. **AWS CLI** (optional): Install and configure the [AWS CLI](https://aws.amazon.com/cli/) if you prefer CLI-based deployment. ## Step 1: Configure your domain [Section titled ‚ÄúStep 1: Configure your domain‚Äù](#step-1-configure-your-domain) The platform requires a custom domain. Create a Route53 hosted zone for your domain if you don‚Äôt have one. * AWS Console 1. Open the [Route53 console](https://console.aws.amazon.com/route53/) 2. Click **Create hosted zone** 3. Enter your domain name (e.g., `example.org`) 4. Select **Public hosted zone** 5. Click **Create hosted zone** 6. Update your domain‚Äôs nameservers at your domain registrar with the Route53 nameservers that appear in the hosted zone * AWS CLI Create a hosted zone for your domain: ```bash aws route53 create-hosted-zone \ --name example.org \ --caller-reference $(date +%s) ``` Retrieve the nameservers for your hosted zone: ```bash aws route53 list-resource-record-sets \ --hosted-zone-id $(aws route53 list-hosted-zones-by-name \ --dns-name example.org \ --query 'HostedZones[0].Id' \ --output text) \ --query "ResourceRecordSets[?Type=='NS'].ResourceRecords[*].Value" \ --output text ``` Update your domain‚Äôs nameservers at your domain registrar with the Route53 nameservers that this command returns. The CloudFormation template automatically discovers the hosted zone based on the domain name you provide. ## Step 2: Prepare container images [Section titled ‚ÄúStep 2: Prepare container images‚Äù](#step-2-prepare-container-images) The platform requires three core service images (UI, API, and Gateway) in your AWS account. You must copy the container images from the AWS Marketplace to your ECR repositories before deploying the CloudFormation stack. Check your version Replace `VERSION` with the version you subscribed to in AWS Marketplace. Using an incorrect version will cause the image pull to fail. Check your marketplace subscription or contact Tenzir support to confirm the latest version. * Automated script Use this script to create the ECR repositories and copy the container images from the AWS Marketplace to your ECR repositories: ```bash #!/bin/bash set -e # IMPORTANT: Set this to your Marketplace subscription version VERSION="${VERSION:-REPLACE_WITH_YOUR_VERSION}" # e.g., v1.0.0, v1.1.0, etc. AWS_REGION="${AWS_REGION:-$(aws configure get region)}" AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text) # Verify VERSION is set if [ "$VERSION" = "REPLACE_WITH_YOUR_VERSION" ]; then echo "ERROR: You must set VERSION to match your Marketplace subscription" echo "Example: VERSION=v1.0.0 ./copy-images.sh" exit 1 fi MARKETPLACE_REGISTRY="709825985650.dkr.ecr.us-east-1.amazonaws.com" LOCAL_REGISTRY="$AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com" IMAGES=( "tenzir/tenzir-platform-ui" "tenzir/tenzir-platform-api" "tenzir/tenzir-platform-gateway" ) echo "Creating ECR repositories..." for IMAGE in "${IMAGES[@]}"; do aws ecr create-repository --repository-name "$IMAGE" 2>/dev/null || echo "Repository $IMAGE already exists" done echo "Copying Tenzir Platform images version $VERSION to $LOCAL_REGISTRY" # Login to marketplace ECR aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $MARKETPLACE_REGISTRY # Login to local ECR aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $LOCAL_REGISTRY # Copy each image for IMAGE in "${IMAGES[@]}"; do echo "Copying $IMAGE:$VERSION..." SOURCE_IMAGE="$MARKETPLACE_REGISTRY/$IMAGE:$VERSION" TARGET_IMAGE="$LOCAL_REGISTRY/$IMAGE:$VERSION" LATEST_IMAGE="$LOCAL_REGISTRY/$IMAGE:latest" docker pull "$SOURCE_IMAGE" docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE" docker push "$TARGET_IMAGE" docker tag "$TARGET_IMAGE" "$LATEST_IMAGE" docker push "$LATEST_IMAGE" done echo "Successfully copied all images!" ``` Save this script to a file (e.g., `copy-images.sh`), make it executable, and run it with your version: ```bash chmod +x copy-images.sh VERSION=v1.0.0 ./copy-images.sh # Replace v1.0.0 with your subscription version ``` The script copies the three core service container images to your ECR repositories. This process typically takes 10-15 minutes depending on your network speed. * Manual steps Follow these steps to manually create the ECR repositories and copy the images. ### Create ECR repositories [Section titled ‚ÄúCreate ECR repositories‚Äù](#create-ecr-repositories) Create three ECR repositories using the AWS Console: 1. Open the [Amazon ECR console](https://console.aws.amazon.com/ecr/) 2. Click **Create repository** 3. Enter the repository name: `tenzir/tenzir-platform-ui` 4. Leave other settings as default 5. Click **Create repository** 6. Repeat for the remaining repositories: * `tenzir/tenzir-platform-api` * `tenzir/tenzir-platform-gateway` ### Copy images from Marketplace [Section titled ‚ÄúCopy images from Marketplace‚Äù](#copy-images-from-marketplace) After creating the repositories, copy the images using Docker: ```bash # IMPORTANT: Replace with your Marketplace subscription version export VERSION=REPLACE_WITH_YOUR_VERSION # e.g., v1.0.0 export AWS_REGION=$(aws configure get region) export AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text) # Login to Marketplace ECR aws ecr get-login-password --region us-east-1 | \ docker login --username AWS --password-stdin \ 709825985650.dkr.ecr.us-east-1.amazonaws.com # Login to your ECR aws ecr get-login-password --region $AWS_REGION | \ docker login --username AWS --password-stdin \ $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com # Copy each image for IMAGE in ui api gateway; do docker pull 709825985650.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION docker tag 709825985650.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION \ $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION docker push $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION docker tag $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:$VERSION \ $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:latest docker push $AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-$IMAGE:latest done ``` This process typically takes 10-15 minutes depending on your network speed. ## Step 3: Deploy the CloudFormation stack [Section titled ‚ÄúStep 3: Deploy the CloudFormation stack‚Äù](#step-3-deploy-the-cloudformation-stack) After copying the images, deploy the CloudFormation stack using our publicly hosted template. * AWS Console 1. Open the [CloudFormation console](https://console.aws.amazon.com/cloudformation/) 2. Click **Create stack** ‚Üí **With new resources** 3. Select **Amazon S3 URL** and enter: ```plaintext https://tenzir-marketplace-resources.s3.eu-west-1.amazonaws.com/tenzir-platform.yml ``` 4. Click **Next** 5. Enter a stack name (e.g., `tenzir-platform`) 6. Configure the parameters: * **Domain Name**: Your domain (e.g., `example.org`) * **Use Random Subdomain**: Whether to add a random subdomain prefix (`false` for production) * **UI Container Image**: Full image URI with the `latest` tag you pushed in Step 2 (e.g., `123456789012.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-ui:latest`) * **API Container Image**: Full image URI with the `latest` tag you pushed in Step 2 (e.g., `123456789012.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-api:latest`) * **Gateway Container Image**: Full image URI with the `latest` tag you pushed in Step 2 (e.g., `123456789012.dkr.ecr.us-east-1.amazonaws.com/tenzir/tenzir-platform-gateway:latest`) * **Demo Node Container Image**: (Optional) Full image URI. Leave as default to use the public demo node image. * **Use External OIDC Provider**: Whether to use an external identity provider instead of [AWS Cognito](https://aws.amazon.com/cognito/) (`false` to use Cognito) 7. Click **Next** through the remaining screens 8. On the final screen, check the box to acknowledge that CloudFormation will create IAM resources 9. Click **Create stack** The `latest` tags resolve to the Marketplace version you copied in Step 2. Re-run the copy step whenever you update to a newer release. The stack creation takes approximately 15-20 minutes. * AWS CLI Create the stack with your configuration: ```bash # Use the tag you pushed in Step 2 (default: latest) IMAGE_TAG=latest AWS_REGION=$(aws configure get region) AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text) aws cloudformation create-stack \ --stack-name tenzir-platform \ --template-url https://tenzir-marketplace-resources.s3.eu-west-1.amazonaws.com/tenzir-platform.yml \ --parameters \ ParameterKey=DomainName,ParameterValue=example.org \ ParameterKey=RandomSubdomain,ParameterValue=false \ ParameterKey=UseExternalOIDC,ParameterValue=false \ ParameterKey=ContainerImageUI,ParameterValue=$AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-ui:$IMAGE_TAG \ ParameterKey=ContainerImageAPI,ParameterValue=$AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-api:$IMAGE_TAG \ ParameterKey=ContainerImageGateway,ParameterValue=$AWS_ACCOUNT.dkr.ecr.$AWS_REGION.amazonaws.com/tenzir/tenzir-platform-gateway:$IMAGE_TAG \ --capabilities CAPABILITY_NAMED_IAM ``` Update these values: * Set `example.org` to your domain name. * Set `IMAGE_TAG` to a specific version tag if you prefer to pin the deployment. The stack creation takes approximately 15-20 minutes. ### Parameter details [Section titled ‚ÄúParameter details‚Äù](#parameter-details) The template accepts these key parameters: * **Domain Configuration**: * `DomainName`: Your base domain name (required) * `RandomSubdomain`: Add a random subdomain prefix. For example, with `example.org`, this creates `ui.abc123.example.org` instead of `ui.example.org` (default: `false`) * **Container Images**: * `ContainerImageUI`: Full URI for the UI container image (required) * `ContainerImageAPI`: Full URI for the API container image (required) * `ContainerImageGateway`: Full URI for the Gateway container image (required) * `ContainerImageNode`: Full URI for the Demo Node container image (optional, defaults to public ghcr.io image) * **Authentication Configuration**: * `UseExternalOIDC`: Use external OIDC instead of Cognito (default: `false`) * External OIDC requires these additional parameters: * `ExternalOIDCIssuerURL`: OIDC issuer URL * `ExternalOIDCClientID`: OIDC client ID * `ExternalOIDCClientSecret`: OIDC client secret ### What gets deployed [Section titled ‚ÄúWhat gets deployed‚Äù](#what-gets-deployed) The CloudFormation template creates a complete platform infrastructure: #### Networking [Section titled ‚ÄúNetworking‚Äù](#networking) * VPC with public and private subnets across multiple availability zones * Internet Gateway and NAT Gateway for outbound connectivity * Security groups for each service * VPC endpoints for AWS services (Secrets Manager, ECR, S3, STS) #### Compute [Section titled ‚ÄúCompute‚Äù](#compute) * **ECS Cluster**: Runs the gateway service as a Fargate task * **Lambda Function**: Hosts the platform API * **App Runner Service**: Hosts the web UI * **Application Load Balancer**: Routes traffic to the gateway service #### Storage [Section titled ‚ÄúStorage‚Äù](#storage) * **RDS PostgreSQL Database**: Stores platform state and metadata * **S3 Buckets**: Store blob data and sidepath data #### Security & Authentication [Section titled ‚ÄúSecurity & Authentication‚Äù](#security--authentication) * **AWS Cognito User Pool** (optional): Provides authentication with a default admin user * **Secrets Manager**: Stores database credentials and encryption keys * **IAM Roles**: Provide least-privilege access for each service * **ACM Certificates**: Automatically provision SSL/TLS certificates for your domain #### DNS & Routing [Section titled ‚ÄúDNS & Routing‚Äù](#dns--routing) * **Route53 DNS Records**: Create `api.`, `ui.`, and `nodes.` subdomains * **API Gateway**: Provides a custom domain for the Lambda-based API ## Step 4: Access the platform [Section titled ‚ÄúStep 4: Access the platform‚Äù](#step-4-access-the-platform) Once the stack creation completes, retrieve the platform URLs and credentials from the CloudFormation outputs: ```bash aws cloudformation describe-stacks \ --stack-name tenzir-platform \ --query 'Stacks[0].Outputs' ``` The outputs include: * **UIDomain**: The web UI URL (e.g., `https://ui.example.org`) * **APIDomain**: The API URL (e.g., `https://api.example.org`) * **AdminUsername**: Default admin username (when using Cognito) * **AdminInitialPassword**: Initial admin password (when using Cognito, stored in Secrets Manager) * **OIDCProviderType**: The authentication provider type (`cognito` or `external`) ### Access the web UI [Section titled ‚ÄúAccess the web UI‚Äù](#access-the-web-ui) 1. Navigate to the UI URL in your browser 2. Log in with the admin credentials (if using Cognito) 3. If using Cognito, the system prompts you to change your password on first login ### Retrieve the admin password [Section titled ‚ÄúRetrieve the admin password‚Äù](#retrieve-the-admin-password) If using Cognito, retrieve the admin password from the CloudFormation outputs: ```bash aws cloudformation describe-stacks \ --stack-name tenzir-platform \ --query 'Stacks[0].Outputs[?OutputKey==`AdminInitialPassword`].OutputValue' \ --output text ``` ## Next steps [Section titled ‚ÄúNext steps‚Äù](#next-steps) After deploying the platform: 1. **Change the admin password**: If using Cognito, change the default admin password on first login 2. **Configure users**: Add additional users through the Cognito User Pool or your external OIDC provider 3. **Connect nodes**: Deploy Tenzir Nodes and connect them to the platform using the gateway endpoint 4. **Configure workspaces**: Organize your nodes and pipelines into workspaces ## Troubleshooting [Section titled ‚ÄúTroubleshooting‚Äù](#troubleshooting) ### Certificate validation [Section titled ‚ÄúCertificate validation‚Äù](#certificate-validation) ACM certificates require DNS validation. The template automatically creates the necessary DNS records in Route53, but validation can take 5-30 minutes. Monitor the certificate status in the [ACM console](https://console.aws.amazon.com/acm/). ### Service health [Section titled ‚ÄúService health‚Äù](#service-health) Monitor service health in the AWS Console: * **ECS**: Check the gateway service status * **Lambda**: Check the API function logs in CloudWatch * **App Runner**: Check the UI service status and logs * **RDS**: Verify database connectivity ### Stack deletion [Section titled ‚ÄúStack deletion‚Äù](#stack-deletion) To delete the stack and all resources: ```bash aws cloudformation delete-stack --stack-name tenzir-platform ``` Caution Empty the S3 buckets before deleting the stack. CloudFormation cannot delete buckets that contain data. Manually empty the blobs and sidepath buckets first. ## Cost considerations [Section titled ‚ÄúCost considerations‚Äù](#cost-considerations) The deployed infrastructure incurs AWS costs. Key cost factors include: * **RDS Database**: db.t3.micro instance (adjustable in the template) * **NAT Gateway**: Data processing charges apply * **App Runner**: Pay-per-use based on compute and memory * **Lambda**: Pay-per-invocation * **ECS Fargate**: Pay-per-task * **Data Transfer**: Outbound data transfer charges For production deployments, consider reserved instances or savings plans to reduce costs.

# Run the platform

## Set up Docker registry access [Section titled ‚ÄúSet up Docker registry access‚Äù](#set-up-docker-registry-access) As part of your Sovereign Edition distribution, we provided you with an authentication token (`YOUR_DOCKER_TOKEN` below) to fetch the Docker images. Log in with the token as follows: ```bash echo YOUR_DOCKER_TOKEN | docker login ghcr.io -u tenzir-distribution --password-stdin ``` You are now ready to pull the images. ## Start the platform [Section titled ‚ÄúStart the platform‚Äù](#start-the-platform) Once you have configured all platform services and created a `docker-compose.yaml`, start the platform in the foreground with ```sh docker compose up ``` or in the background with `docker compose up --detach`. ‚ùØ docker compose up ```text [+] Running 5/5 ‚úî Container compose-app-1 Running ‚úî Container compose-websocket-gateway-1 Running ‚úî Container compose-seaweed-1 Running ‚úî Container compose-platform-1 Running Attaching to app-1, platform-1, postgres-1, seaweed-1, websocket-gateway-1 platform-1 | {"event": "connecting to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.205616Z"} platform-1 | {"event": "connected to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.210667Z"} platform-1 | {"event": "created table", "level": "info", "ts": "2024-04-10T10:13:20.210883Z"} platform-1 | {"event": "connecting to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.217700Z"} platform-1 | {"event": "connected to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.221194Z"} platform-1 | {"event": "creating a table", "level": "info", "ts": "2024-04-10T10:13:20.221248Z"} platform-1 | {"event": "connecting to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.221464Z"} platform-1 | {"event": "connected to postgres", "level": "debug", "ts": "2024-04-10T10:13:20.224226Z"} app-1 | Listening on 0.0.0.0:3000 websocket-gateway-1 | {"event": "connecting to postgres", "level": "debug", "ts": "2024-04-10T10:15:37.033632Z"} websocket-gateway-1 | {"event": "connected to postgres", "gtlevel": "debug", "ts": "2024-04-10T10:15:37.038510Z"} websocket-gateway-1 | {"event": "created table", "level": "info", "ts": "2024-04-10T10:15:37.042555Z"} websocket-gateway-1 | {"host": "0.0.0.0", "port": 5000, "common_env": {"base_path": "", "tenzir_proxy_timeout": 60.0}, "local_env": {"store": {"postgres_uri": "postgresql://postgres:postgres@postgres:5432/platform"}, "tenant_manager_app_api_key": "d3d185cc4d9a1bde0e07e24c2eb0bfe9d2726acb3a386f8882113727ac6e90cf", "tenant_manager_tenant_token_encryption_key": "CBOXE4x37RKRLHyUNKeAsfg8Tbejm2N251aKnBXakpU="}, "event": "HTTP server running", "level": "info", "ts": "2024-04-10T10:15:37.045244Z"} ... ``` It may take up to a minute for all services to be fully available. ## Update the platform [Section titled ‚ÄúUpdate the platform‚Äù](#update-the-platform) To update to the latest platform version, pull the latest images: ```sh docker compose pull ```

# Quickstart

Drowning in logs, alerts, and rigid tools? Meet **Tenzir**‚Äîyour engine for taming security data. In just a few minutes, you'll be ingesting, transforming, and enriching data on your terms, with full control. Here's what you'll accomplish: 1. Use Tenzir instantly 2. Deploy your first pipeline 3. See results in action ## Prerequisites [Section titled ‚ÄúPrerequisites‚Äù](#prerequisites) You need zero infrastructure to get started‚Äîjust a browser and access to [app.tenzir.com](https://app.tenzir.com). It helps if you have basic familiarity with logs or security telemetry, but it's not required. ## Setup & deploy a demo node [Section titled ‚ÄúSetup \&amp; deploy a demo node‚Äù](#setup--deploy-a-demo-node) Visit [app.tenzir.com](https://app.tenzir.com), sign in to [create a free account](/guides/account-creation), and you'll see this page: ![Landing page](/_astro/new-account.Qul5lKOD_VK4oR.webp) Begin with deploying a node: 1. Select the **Cloud-hosted demo-node** tab. 2. Click **Add node**. 3. Click **Get Started**. Spinning up a demo node can take up to 2 minutes. Sorry for the wait, we'll cut down this time soon! Grab a coffee or learn more about the [key Tenzir concepts](/explanations/architecture) while we get everything set up. ‚òï ## Get started with demo data [Section titled ‚ÄúGet started with demo data‚Äù](#get-started-with-demo-data) Our demo nodes have the Demo Node [package](/explanations/packages) pre-installed, giving you sample pipelines that fetch data from a public cloud bucket and [store it into the node's edge storage](/guides/edge-storage/import-into-a-node). Once you have the sample data in the node, it will be simpler to work with it. The **Packages** tab shows that the demo node package is installed: ![Demo node package installed](/_astro/demo-node-packages.B7a3szfC_Z2eXOXM.webp) When you go back to the **Pipelines** tab, you see the pipelines churning away: ![Nodes page after demo node package](/_astro/demo-node-pipelines.B3TjVIac_1MXejo.webp) Note the two new pipelines that import data into our demo node. If you click on them, a context pane opens on the right and you'll see details about their activity as well as the their definition. ## Explore the demo data [Section titled ‚ÄúExplore the demo data‚Äù](#explore-the-demo-data) The first step in understanding new data sources is getting a sense of their structural complexlity, or simply put, how messy or clean the data is. Let's take a taste of the demo data. Click the **Explorer** tab and [run](/guides/basic-usage/run-pipelines) this pipeline: ```tql export taste ``` This pipelines does the following: [`export`](/reference/operators/export) references all data in the node's edge storage, and [`taste`](/reference/operators/taste) samples 10 events of every unique schema. You'll now see Explorer filling up with events. ![Getting a taste](/_astro/demo-node-export-taste.DKQLkuB8_2mLjT6.webp) Also note the **Schemas** pane. It gives you an overview of how heterogeneous the data is. Click on a schema to zoom into all events having the same shape. Later, you'll learn to normalize the data to make it more homogeneous by reducing the number of unique schemas. Now click on a row in the results table. The **Inspector** pops up for a vertical view of the event, which can be helpful for seeing the full structure of an event, especially for wide schemas with many fields. One more detail: you can **uncollapse nested records** in the results table by clicking on the column name. This switches from the record-style display to more of a data-frame-style view, allowing you to see more data at once. ## Reshape data at ease [Section titled ‚ÄúReshape data at ease‚Äù](#reshape-data-at-ease) Now that you have a rough understanding of our cockpit, let's wrangle that data. This is what we've designed TQL for, so it should be fun‚Äîor at least more fun compared to other tools. Begin with selecting a subset of available schemas: ```tql export where @name.starts_with("zeek") ``` Here we filter on event metadata, starting with `@`. The special `@name` field is a string that contains the name of the event. Actually, let's hone in on the connection logs only, once for [Zeek](https://zeek.org) and once for [Suricata](https://suricata.io): * Zeek ```tql export where @name == "zeek.conn" set src_endpoint = { ip: id.orig_h, port: id.orig_p, } set dst_endpoint = { ip: id.resp_h, port: id.resp_p, } select src_endpoint, dst_endpoint, protocol=proto set @name = "flow" ``` ```tql { src_endpoint: { ip: 89.248.165.145, port: 43831 }, dst_endpoint: { ip: 198.71.247.91, port: 52806 }, protocol: "tcp" } ``` * Suricata ```tql export where @name == "suricata.flow" set src_endpoint = { ip: src_ip, port: src_port, } set dst_endpoint = { ip: dest_ip, port: dest_port, } select src_endpoint, dst_endpoint, protocol=proto.to_lower() set @name = "flow" ``` ```tql { src_endpoint: { ip: 10.0.0.167, port: 51666 }, dst_endpoint: { ip: 97.74.135.10, port: 993 }, protocol: "tcp" } ``` A few notes: * The [`set`](/reference/operators/set) operator performs an assignment and creates new fields. * Because `set` is *the* most frequently used operator, it is "implied" and you just write `x = y` instead of `set x = y`. We generally recommend doing so and write it out only out for didactic reasons. * You can use `set` to assign schema names, e.g., `@name = "new-schema-name"`. * [`select`](/reference/operators/select) selects the fields to keep, but also supports an assignment to rename the new field in one shot. * As you can see in the `select` operator (Suricata tab) above, TQL expressions have [functions](/reference/functions) like [`to_lower`](/reference/functions/to_lower), which makes working with values a breeze. ![Reshaping Suricata flow logs](/_astro/demo-node-reshape-flow.DGbYIpBh_ZSMfAP.webp) Now what do you do with this normalized data from these two data sources? It just has a new shape, so what? Read on, we'll show you next. ## Composing pipelines via publish/subscribe [Section titled ‚ÄúComposing pipelines via publish/subscribe‚Äù](#composing-pipelines-via-publishsubscribe) The above example starts with a specific input operator (`export`) and no output operator (we used the Explorer). This is useful for explorative data analysis, but in practice you'd want these sorts of transformations to run continuously. In fact, what you really want is a streaming pipeline that accepts data, potentially from multiple sources, and exposes its results in a way so that you can route it to multiple destinations. To this end, nodes have a publish/subscribe feature, allowing you to efficiently connect pipelines using static topics (and very soon dynamic routes). The [`publish`](/reference/operators/publish) and [`subscribe`](/reference/operators/subscribe) operators are all you need for this. The typical pipeline pattern for composable pipelines looks like this: ```tql subscribe "in" // transformations go here publish "out" ``` Let's adapt our transformation pipelines from above: * Zeek ```tql subscribe "zeek" where @name == "zeek.conn" set src_endpoint = { ip: id.orig_h, port: id.orig_p, } set dst_endpoint = { ip: id.resp_h, port: id.resp_p, } select src_endpoint, dst_endpoint, protocol=proto publish "flow" ``` * Suricata ```tql subscribe "suricata" where @name == "suricata.flow" set src_endpoint = { ip: src_ip, port: src_port, } set dst_endpoint = { ip: dest_ip, port: dest_port, } select src_endpoint, dst_endpoint, protocol=proto.to_lower() publish "flow" ``` When clicking the **Run** button for these pipeline, the events will *not* show up in the Explorer because we now use `publish` as output operator. Instead, you'll see this deployment modal: ![Deployment modal](/_astro/demo-node-deploy-modal.C8YJw_nJ_Z1xvfub.webp) After you give the pipeline a name (or leave it blank for a dummy name), click **Confirm** to deploy the pipeline. You'll see it popping up on the **Pipelines** tab: ![New pipeline](/_astro/demo-node-pipelines-new.BSYbUO4v_22Vvze.webp) Now that you've deployed one pipeline with two topics as its "interface," you can direct data to it from other pipelines. For example, you can create a pipeline that accepts logs via [Syslog](/integrations/syslog) and forwards them to the transformation pipeline. Then you can write two more pipelines that each take a subset to implement split-routing scenario. ## What's Next? [Section titled ‚ÄúWhat\&#39;s Next?‚Äù](#whats-next) You've just scratched the surface. Here's where to go next: 1. Explore the [Library](https://app.tenzir.com/library) and browse through packages of pre-built pipelines. 2. [Visualize pipeline insights and build dashboards](/tutorials/plot-data-with-charts) 3. [Map your logs to OCSF](/tutorials/map-data-to-ocsf) 4. Send events to your data lake, such as [Amazon Security Lake](/integrations/amazon/security-lake) Curious how it all fits together? Brush up on the [Tenzir architecture](/explanations/architecture) to learn more about all moving parts. We're here to help. Join us at our friendly [Tenzir Discord](/discord) if you have any questions.

# Add custom runners

Runners tell `tenzir-test` how to execute a discovered file. This guide shows you how to register the XXD runner from the example project so you can compare binary artifacts by dumping their hexadecimal representation with `xxd`. ## Prerequisites [Section titled ‚ÄúPrerequisites‚Äù](#prerequisites) * Complete the setup in [write tests](/guides/testing/write-tests) so you have a project root with `runners/` and at least one passing test. * Install the [`xxd`](https://man.archlinux.org/man/xxd.1.en) utility. It ships with most platforms. ## Step 1: Prepare the runners package [Section titled ‚ÄúStep 1: Prepare the runners package‚Äù](#step-1-prepare-the-runners-package) Create `runners/__init__.py` if it does not exist yet. The harness imports this module automatically on start-up. ```sh touch runners/__init__.py ``` ## Step 2: Implement the XXD runner [Section titled ‚ÄúStep 2: Implement the XXD runner‚Äù](#step-2-implement-the-xxd-runner) Copy the runner from the example project and keep it under version control so teammates can use the same convention. The latest reference implementation lives in [example-project/runners/xxd.py](https://github.com/tenzir/test/blob/main/example-project/runners/xxd.py). If you prefer to start from a scaffold, drop the following template into `runners/xxd.py` and fill in the `TODO` notes where your implementation needs to differ (for example when you call a different tool or emit additional artifacts). runners/xxd.py ```python from __future__ import annotations from pathlib import Path from tenzir_test import runners from tenzir_test.runners._utils import get_run_module class XxdRunner(runners.ExtRunner): """Hexdump runner that turns *.xxd files into reference artifacts.""" def __init__(self) -> None: super().__init__(name="xxd", ext="xxd") def run(self, test: Path, update: bool, coverage: bool = False) -> bool: del coverage # this runner does not integrate with LLVM coverage run_mod = get_run_module() passthrough = run_mod.is_passthrough_enabled() # 1. Prepare the command. Adjust flags or the executable for your tool. cmd = ["xxd", "-g1", str(test)] # TODO: Replace "xxd" or tweak arguments when you wrap a different command. try: completed = run_mod.run_subprocess( cmd, capture_output=not passthrough, ) except FileNotFoundError: run_mod.report_failure(test, "‚îî‚îÄ‚ñ∂ xxd is not available on PATH") return False if completed.returncode != 0: # 2. Surface a readable error message and bail out early. run_mod.report_failure(test, f"‚îî‚îÄ‚ñ∂ xxd exited with {completed.returncode}") return False if passthrough: # 3. Passthrough runs stop after executing the command. run_mod.success(test) return True output = completed.stdout or b"" # 4. Update reference artifacts when requested. ref_path = test.with_suffix(".txt") if update: ref_path.write_bytes(output) run_mod.success(test) return True if not ref_path.exists(): run_mod.report_failure(test, f"‚îî‚îÄ‚ñ∂ Missing reference file {ref_path}") return False # 5. Compare against the baseline and print a diff on mismatch. expected = ref_path.read_bytes() if expected != output: run_mod.report_failure(test, "") run_mod.print_diff(expected, output, ref_path) return False run_mod.success(test) return True runners.register(XxdRunner()) ``` Finally, expose the runner from `runners/__init__.py` so the harness picks it up on start-up: runners/\_\_init\_\_.py ```python """Project runners.""" # Import bundled runners so they register on package import. from . import xxd # noqa: F401 __all__ = ["xxd"] ``` ## Step 3: Add a hexdump test [Section titled ‚ÄúStep 3: Add a hexdump test‚Äù](#step-3-add-a-hexdump-test) Create a directory for the new tests and add a sample input string. ```sh mkdir -p tests/hex cat <<EOD > tests/hex/hello.xxd Hello Tenzir! EOD ``` ## Step 4: Capture the reference output [Section titled ‚ÄúStep 4: Capture the reference output‚Äù](#step-4-capture-the-reference-output) Run the harness in update mode so it generates the expected hexdump next to the `.xxd` file. ```sh uvx tenzir-test --update ``` The command produces `tests/hex/hello.txt` similar to the following snippet: ```text 00000000: 48 65 6c 6c 6f 20 54 65 6e 7a 69 72 21 0a Hello Tenzir!. ``` Subsequent runs without `--update` rerun `xxd` and compare the fresh dump with the stored baseline. Pass `--debug` when you want inline runner and fixture details together with the comparison activity. Use `--summary` if you prefer the tabular breakdown and failure tree at the end, or set `TENZIR_TEST_DEBUG=1` in CI to enable the same diagnostics without passing the flag explicitly. ## Step 5: Reuse the runner across projects [Section titled ‚ÄúStep 5: Reuse the runner across projects‚Äù](#step-5-reuse-the-runner-across-projects) Keep the runner in your template repository or internal tooling so other projects can copy it verbatim. Use `runners.register_alias("xxd-hexdump", "xxd")` when you prefer a more descriptive name in frontmatter. When you invoke the harness with multiple projects in the same command, pass `--all-projects` so the root project executes alongside the satellites. The positional paths you list after the flags form the **selection**; in this case it usually only names satellite roots. `--all-projects` opts the root back in, its runners load first, and satellites reuse them automatically. That makes it simple to keep shared runners (like `xxd`) in a central project while satellite projects focus on their own tests. ## Next steps [Section titled ‚ÄúNext steps‚Äù](#next-steps) * Pair the runner with fixtures that download or generate binary artifacts before each test. * Use directory-level `test.yaml` files or per-test frontmatter to set `inputs:` when the runner should read data from a different directory than the project default. * Extend the runner to emit `*.diff` artifacts when the hexdumps diverge. * Branch on `run.get_harness_mode()` or `run.is_passthrough_enabled()` when you need bespoke behaviour for passthrough runs, but prefer to rely on `run.run_subprocess()` for most cases so output handling stays consistent. * Review the [test framework reference](/reference/test-framework/) to explore additional runner hooks and helpers.

# Create fixtures

Fixtures let `tenzir-test` prepare external services before a scenario runs and clean everything up afterwards. In this guide you build an HTTP echo fixture from scratch, wire it into the harness, and exercise it with a TQL test that posts a JSON payload via [`http`](/reference/operators/http). ## Prerequisites [Section titled ‚ÄúPrerequisites‚Äù](#prerequisites) * Follow [write tests](/guides/testing/write-tests) to scaffold a project and install `tenzir-test`. * Make sure your project root already contains `fixtures/`, `inputs/`, and `tests/` directories (they can be empty). ## Step 1: Expose a fixtures package [Section titled ‚ÄúStep 1: Expose a fixtures package‚Äù](#step-1-expose-a-fixtures-package) `tenzir-test` imports `fixtures/__init__.py` automatically. Import the modules that host your fixtures so their decorators run as soon as the package loads. fixtures/\_\_init\_\_.py ```python """Project fixtures.""" from . import http # noqa: F401 (side effect: register fixture) __all__ = ["http"] ``` ## Step 2: Implement the HTTP echo fixture [Section titled ‚ÄúStep 2: Implement the HTTP echo fixture‚Äù](#step-2-implement-the-http-echo-fixture) Create `fixtures/http.py` with a tiny HTTP server that echoes POST bodies. The fixture yields the server URL with `@fixture()` and tears the server down inside its `finally` block. fixtures/http.py ```python from __future__ import annotations import threading from http import HTTPStatus from http.server import BaseHTTPRequestHandler, ThreadingHTTPServer from typing import Iterator from tenzir_test import fixture class EchoHandler(BaseHTTPRequestHandler): def do_POST(self) -> None: # noqa: N802 stated_length = self.headers.get("Content-Length", "0") try: length = int(stated_length) except ValueError: length = 0 body = self.rfile.read(length) if length else b"{}" self._reply(body or b"{}") def log_message(self, *_: object) -> None: # noqa: D401 return # Keep the console quiet. def _reply(self, payload: bytes) -> None: self.send_response(HTTPStatus.OK) self.send_header("Content-Type", "application/json") self.send_header("Content-Length", str(len(payload))) self.end_headers() self.wfile.write(payload) @fixture() def http() -> Iterator[dict[str, str]]: server = ThreadingHTTPServer(("127.0.0.1", 0), EchoHandler) worker = threading.Thread(target=server.serve_forever, daemon=True) worker.start() try: port = server.server_address[1] url = f"http://127.0.0.1:{port}/" yield {"HTTP_FIXTURE_URL": url} finally: server.shutdown() worker.join() ``` ## Step 3: Author a test that uses the fixture [Section titled ‚ÄúStep 3: Author a test that uses the fixture‚Äù](#step-3-author-a-test-that-uses-the-fixture) Create `tests/http/echo-read.tql` and request the `http` fixture in the frontmatter (or rely on `tests/http/test.yaml`). The pipeline creates sample data and then crafts a HTTP request body using the [`http`](/reference/operators/http) operator. The fixture echoes the payload back into the result stream. ```tql --- fixtures: [http] --- from {x: 42, y: "foo"} http env("HTTP_FIXTURE_URL"), body=this ``` Fixtures receive the same per-test scratch directory as the scenario itself via `TENZIR_TMP_DIR`. Use it to stage temporary files or logs that should disappear after the run. Launch the harness with `--keep` when you want to retain those artifacts for debugging. ## Step 4: Capture the reference output [Section titled ‚ÄúStep 4: Capture the reference output‚Äù](#step-4-capture-the-reference-output) Run the harness in update mode so it records the HTTP response next to the test. ```sh uvx tenzir-test --update ``` You should now have `tests/http/echo-read.txt` with the echoed payload: ```tql { x: 42, y: "foo", } ``` Subsequent runs without `--update` bring the fixture online, hit the server, and compare the live response against the baseline. Need to inspect the fixture while it runs? Append `--debug` to emit lifecycle logs together with comparison targets. In CI-only scenarios set `TENZIR_TEST_DEBUG=1` to enable the same diagnostics without passing the flag at runtime, and add `--summary` if you also need the tabular breakdown and failure tree afterwards. ### Stabilise flaky fixture scenarios [Section titled ‚ÄúStabilise flaky fixture scenarios‚Äù](#stabilise-flaky-fixture-scenarios) Occasionally a fixture-backed test may need a couple of tries‚Äîfor example when a service takes slightly longer to initialise. Add `retry` to the frontmatter to let the harness rerun the test before giving up: ```yaml --- fixtures: [http] retry: 4 --- ``` The number is the total attempt budget (four in the example above). Intermediate attempts stay quiet and the final log line reports how many tries were required. Treat this as a temporary safety net and investigate persistent flakes; long retry chains mask underlying race conditions. ## Step 5: Iterate on the fixture [Section titled ‚ÄúStep 5: Iterate on the fixture‚Äù](#step-5-iterate-on-the-fixture) With the echo workflow in place you can: * Serve canned responses from files under `inputs/` for more realistic scenarios, or redirect `TENZIR_INPUTS` with an `inputs:` entry in `test.yaml` when the data lives elsewhere. * Add environment keys (for example `HTTP_FIXTURE_TOKEN`) so tests can assert on authentication behavior. * Harden the cleanup in your `finally` block (or factor it into a helper) when fixtures manage external processes, containers, or cloud resources. * Reuse the fixture from satellite projects. When you call `tenzir-test --root main-project --all-projects satellite-a`, the selection only names the satellite root, so `--all-projects` keeps the main project in the run. The satellite automatically imports the root project‚Äôs fixtures before loading its own. If a satellite needs specialized behavior, it can register an additional fixture with a new name while still leaning on the shared implementations. Check the [test framework reference](/reference/test-framework/) for more fixture APIs, including helpers that tell you which fixtures the current test requested. In Python-mode tests you can call `fixtures()` to inspect the selection (for example `fixtures().http` returns `True` when the fixture is active). ## Step 6: Share the fixture across multiple tests [Section titled ‚ÄúStep 6: Share the fixture across multiple tests‚Äù](#step-6-share-the-fixture-across-multiple-tests) Suites keep a fixture alive for several tests. Declare one in a directory-level `test.yaml`; every descendant test joins automatically. tests/http/test.yaml ```yaml suite: smoke-http fixtures: [http] timeout: 45 ``` With that configuration the harness: * Starts the `http` fixture once, runs all members in lexicographic order, and tears the fixture down afterwards. * Pins the suite to a single worker while still running different suites (and standalone tests) in parallel when `--jobs` permits it. * Keeps policies such as `fixtures`, `timeout`, and `retry` in the directory defaults. Tests inside the suite cannot set `suite`, `fixtures`, or `retry` in their own frontmatter; everything else (for example `inputs:` or additional metadata) remains overridable per file. Outside of suites you can still use frontmatter for any of those keys. Run the directory that owns the suite when you want to focus on it: ```sh uvx tenzir-test tests/http ``` Selecting a single file inside that suite now fails fast with a descriptive error. This keeps the lifecycle predictable and avoids partially exercised fixtures. ## Manual controllers [Section titled ‚ÄúManual controllers‚Äù](#manual-controllers) Python-mode tests often need fine-grained control over fixture lifecycle to simulate crashes, restarts, or configuration changes. The manual controller API builds on the same registration flow shown above and lets you call directly into the fixture from a test. The harness preloads common helpers (including `fixtures()` for quick introspection and `acquire_fixture()` for manual control) so you can use them without imports. ```python # runner: python # timeout: 30 import fixtures with acquire_fixture("http") as http: env = http.env # provided by the fixture's start() client = Executor.from_env(env) # Exercise the system under test while the fixture runs client.run("status") # Restart explicitly when you need another lifecycle http = acquire_fixture("http") http.start() http.stop() ``` Key ideas: * `acquire_fixture(name)` returns a controller that wraps the registered fixture factory. Calling `start()` (or using `with acquire_fixture(...) as controller:`) enters the context manager lazily and stores the returned environment mapping on `controller.env` so you can reuse it between calls. * `with acquire_fixture(...)` is shorthand for `start()`/`stop()`; call those methods manually when you need to restart or interleave multiple lifecycle steps. * The Python runner serialises the active test context into `TENZIR_PYTHON_FIXTURE_CONTEXT`; the helpers in `tenzir_test.fixtures` consume it automatically so manual controllers see the same configuration as the declarative flow. * Fixtures advertise optional operations by returning a `FixtureHandle` with extra callables (for example `kill`, `restart`, or `configure`). Treat these hooks as part of the fixture contract‚Äîassert their presence (or rely on type hints) when your test depends on them so failures are immediate and obvious. * `Executor.from_env(env)` and similar helpers make it straightforward to reuse the active fixture environment in client calls while the fixture is running; in Python-mode tests these helpers are imported for you automatically. * For stricter typing you can import lightweight `Protocol` definitions (e.g. `SupportsKill`) and cast controllers once a fixture documents the hooks it exposes. The declarative workflow (`fixtures: [http]` combined with automatic activation) remains the default. Manual controllers complement it when tests need imperative control over setup and teardown‚Äîskip the frontmatter entry when you prefer to start and stop the fixture yourself from Python code. ### Fixture contracts [Section titled ‚ÄúFixture contracts‚Äù](#fixture-contracts) The controller API works best when fixture authors and consumers agree on the available hooks. **Fixture author** ```python import signal @fixture() def node(): process = _start_node() def _kill(signal_no: int = signal.SIGTERM) -> None: process.send_signal(signal_no) return FixtureHandle( env=_make_env(process), teardown=lambda: _stop_node(process), hooks={"kill": _kill}, ) ``` **Test author** ```python import signal node = acquire_fixture("node") node.start() assert hasattr(node, "kill"), "node fixture must expose kill() for this test" node.kill(signal.SIGKILL) node.stop() ``` If the fixture stops exporting `kill`, the assertion fails immediately so the test never produces misleading results.

# Write tests

This guide walks you through creating a standalone repository for integration tests, wiring it up to [`tenzir-test`](/reference/test-framework), and running your first scenarios end to end. You will create a minimal project structure, add a pipeline test, record reference output, and rerun the harness to make sure everything passes. ## Prerequisites [Section titled ‚ÄúPrerequisites‚Äù](#prerequisites) * A working installation of Tenzir. Place the `tenzir` and `tenzir-node` binaries on your `PATH`, or be ready to pass explicit paths to the harness. * Python 3.12 or later. The `tenzir-test` package distributes as a standard Python project. * [`uv`](https://docs.astral.sh/uv/) or `pip` to install Python dependencies. ## Step 1: Scaffold a project [Section titled ‚ÄúStep 1: Scaffold a project‚Äù](#step-1-scaffold-a-project) Create a clean directory that holds nothing but integration tests and their shared assets. The harness treats this directory as the **project root**. ```sh mkdir demo cd demo ``` ## Step 2: Check the harness [Section titled ‚ÄúStep 2: Check the harness‚Äù](#step-2-check-the-harness) Run the harness through `uvx` to make sure the tooling works without setting up a virtual environment. `uvx` downloads and caches the latest release when needed. ```sh uvx tenzir-test --help ``` If the command succeeds, you‚Äôre ready to add tests. ## Step 3: Add shared data [Section titled ‚ÄúStep 3: Add shared data‚Äù](#step-3-add-shared-data) Populate `inputs/` with artifacts that tests will read. The example below stores a short NDJSON dataset that models a few alerts. ```json {"id": 1, "severity": 5, "message": "Disk usage above 90%"} {"id": 2, "severity": 2, "message": "Routine backup completed"} {"id": 3, "severity": 7, "message": "Authentication failure on admin"} ``` Save the snippet as `inputs/alerts.ndjson`. ## Step 4: Author a pipeline test [Section titled ‚ÄúStep 4: Author a pipeline test‚Äù](#step-4-author-a-pipeline-test) Create your first scenario under `tests/`. The harness discovers tests recursively, so you can organize them by feature or risk level. Here, you create `tests/high-severity.tql`. tests/high-severity.tql ```tql from_file f"{env("TENZIR_INPUTS")}/alerts.ndjson" where severity >= 5 project id, message sort id ``` The harness also injects a unique scratch directory into `TENZIR_TMP_DIR` while each test executes. Use it for transient files you do not want under version control; pass `--keep` when you run `tenzir-test` if you need to inspect the generated artifacts afterwards. ### Stream raw output while iterating [Section titled ‚ÄúStream raw output while iterating‚Äù](#stream-raw-output-while-iterating) During early iterations you may want to inspect command output before you record reference artifacts. Enable *passthrough mode* via `--passthrough` (`-p`) to pipe the `tenzir` process output directly to your terminal while the harness still provisions fixtures and environment variables: ```sh uvx tenzir-test --passthrough tests/high-severity.tql ``` The harness enforces the exit code but skips comparisons, letting you decide when to capture the baseline with `--update`. ## Step 5: Capture the reference output [Section titled ‚ÄúStep 5: Capture the reference output‚Äù](#step-5-capture-the-reference-output) Run the harness once in update mode to execute the pipeline and write the expected output next to the test. ```sh uvx tenzir-test --update ``` The command produces `tests/high-severity.txt` with the captured stdout. ```json {"id":1,"message":"Disk usage above 90%"} {"id":3,"message":"Authentication failure on admin"} ``` Review the reference file, adjust the pipeline if needed, and rerun `--update` until you are satisfied with the results. Commit the `.tql` test and `.txt` baseline together so future runs can compare against known-good output. ## Step 6: Rerun the tests [Section titled ‚ÄúStep 6: Rerun the tests‚Äù](#step-6-rerun-the-tests) After you check in the reference output, execute the tests *without* `--update`. The harness verifies that the actual output matches the baseline. ```sh uvx tenzir-test ``` When the output diverges, the harness prints a diff and returns a non-zero exit code. Use `--debug` to see comparison targets alongside the usual harness diagnostics. For CI-only visibility you can set `TENZIR_TEST_DEBUG=1`. Add `--summary` when you also want the tabular breakdown and failure tree at the end. ### Retry flaky tests (sparingly) [Section titled ‚ÄúRetry flaky tests (sparingly)‚Äù](#retry-flaky-tests-sparingly) If a scenario fails intermittently, add a `retry` entry to its frontmatter so the harness reruns it before flagging a failure. The value is the **total** attempt budget: ```yaml --- retry: 3 --- ``` With `retry: 3`, the test runs up to three times. Intermediate attempts stay quiet; the final result line includes `attempts=3/3` (or the actual number on a success). Use this as a guardrail while you investigate the underlying flake and keep the budget small to avoid masking issues. ### Run multiple projects together [Section titled ‚ÄúRun multiple projects together‚Äù](#run-multiple-projects-together) Large organisations often split tests across several repositories but still want an aggregated run. List additional project directories after `--root` and add `--all-projects` to execute the root alongside its satellites under a single invocation. Those positional paths form the selection; here it only names the satellite project: ```sh uvx tenzir-test --root example-project --all-projects ../example-satellite ``` The root project (`example-project` above) supplies the shared fixtures and runners. Satellites inherit those definitions, can register their own helpers, and run their tests in isolation. Because the selection only listed the satellite, `--all-projects` keeps the root in scope. The CLI prints a compact summary showing how many tests each project contributes and which runners are involved. Add `--summary` when you prefer the tabular breakdown and detailed failure listing after each project. ## Step 7: Introduce a fixture [Section titled ‚ÄúStep 7: Introduce a fixture‚Äù](#step-7-introduce-a-fixture) Fixtures let you bootstrap external resources and expose their configuration through environment variables. Add a simple `node`-driven test to exercise a running Tenzir node. Create `tests/node/ping.tql` with the following contents: ```tql --- fixtures: [node] timeout: 10 --- // Get the version from the running node. remote { version } ``` Because the test needs a node to run, include the built-in `node` fixture and give it a reasonable timeout. The fixture starts `tenzir-node`, injects connection details into the environment, and tears the process down after the run. Capture the baseline via `--update` just like before. The fixture launches `tenzir-node` from the directory that owns the test file, so `tenzir-node.yaml` placed next to the scenario can refer to files with relative paths (for example `../inputs/alerts.ndjson`). ### Reuse fixtures with suites [Section titled ‚ÄúReuse fixtures with suites‚Äù](#reuse-fixtures-with-suites) When several tests should share the same fixture lifecycle, promote their directory to a **suite**. Add `suite:` to the directory‚Äôs `test.yaml` and keep the fixture selection alongside the other defaults: tests/http/test.yaml ```yaml suite: smoke-http fixtures: [http] timeout: 45 retry: 2 ``` Key behaviour: * Suites are directory-scoped. Once a `test.yaml` declares `suite`, every test in that directory *and its subdirectories* joins automatically. Move the scenarios that should remain independent into a sibling directory. * Suites run sequentially on a single worker. The harness activates the shared fixtures once, executes members in lexicographic order of their relative paths, and tears the fixtures down afterwards. Other suites (and standalone tests) still run in parallel when `--jobs` allows it. * Per-test frontmatter cannot introduce `suite`, and suite members may not define their own `fixtures` or `retry`. Keep those policies in the directory defaults so every member agrees on the shared lifecycle. Outside a suite, frontmatter can still set `fixtures`, `retry`, or `timeout` as before. * Tests can override other keys (for example `inputs:` or additional metadata) on a per-file basis when necessary. Run the `http` directory that defines the suite when you iterate on it: ```sh uvx tenzir-test tests/http ``` Selecting a single file inside that suite fails fast with a descriptive error, which keeps the fixture lifecycle predictable and prevents partial runs from leaving shared state behind. ### Drive fixtures manually [Section titled ‚ÄúDrive fixtures manually‚Äù](#drive-fixtures-manually) When you switch to the Python runner you can drive fixtures manually. The controller API makes it easy to start, stop, or even crash the same `node` fixture inside a single test: ```python # runner: python # fixtures: [node] import signal # Context-manager style: `with` automatically calls `start()` and `stop()` on # the fixture. with acquire_fixture("node") as node: tenzir = Executor.from_env(node.env) tenzir.run("remove { version }") # talk to the running node # Without the context manager, you need to call `start()` and `stop()` manually. node.start() Executor.from_env(node.env).run("version") node.stop() ``` This imperative style complements the declarative `fixtures: [node]` flow and is especially useful for fault-injection scenarios. The harness preloads helpers like `acquire_fixture`, `Executor`, and `fixtures()`, so Python-mode tests can call them directly. When you restart the same controller, the node keeps using the state and cache directories it created during the first `start()`. Those paths (exported via `TENZIR_NODE_STATE_DIRECTORY` and `TENZIR_NODE_CACHE_DIRECTORY`) live inside the test‚Äôs scratch directory by default and are cleaned up automatically when the controller goes out of scope. Acquire a fresh controller when you need a brand new workspace. ## Step 8: Organize defaults with `test.yaml` [Section titled ‚ÄúStep 8: Organize defaults with test.yaml‚Äù](#step-8-organize-defaults-with-testyaml) As suites grow, you can extract shared configuration into directory-level defaults. Place a `tests/node/test.yaml` file with convenient settings: ```yaml fixtures: [node] timeout: 120 # Optional: reuse datasets that live in tests/data/ instead of the project root. inputs: ../data ``` The harness merges this mapping into every test under `tests/node/`. Relative paths resolve against the directory that owns the YAML file, so `inputs: ../data` points at `tests/data/`. Individual files still override keys in their frontmatter when necessary. ## Step 9: Automate runs [Section titled ‚ÄúStep 9: Automate runs‚Äù](#step-9-automate-runs) Once the suite passes locally, integrate it into your CI pipeline. Configure the job to install Python 3.12, install `tenzir-test`, provision or download the required Tenzir binaries, and execute `uvx tenzir-test --root .`. For reproducible results, keep your datasets small and deterministic, and prefer fixtures that wipe state between runs. ## Next steps [Section titled ‚ÄúNext steps‚Äù](#next-steps) You now have a project that owns its inputs, tests, fixtures, and baselines. From here you can: * Add custom runners under `runners/` when you need specialized logic around `tenzir` invocations. * Build Python fixtures that publish or verify data through the helper APIs in `tenzir_test.fixtures`. * Explore coverage collection by passing `--coverage` to the harness. Refer back to the [test framework reference](/reference/test-framework/) whenever you need deeper details about runners, fixtures, or configuration knobs.