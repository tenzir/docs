<SYSTEM>Explanations</SYSTEM>

# Overview

Tenzir’s architecture consists three primary abstractions: 1. [**Pipeline**](/explanations/architecture/pipeline): A sequence of operators responsible for loading, parsing, transforming, and routing data. Pipelines are the core mechanism for data processing. 2. [**Node**](/explanations/architecture/node): A running process that manages and executes pipelines. 3. [**Platform**](/explanations/architecture/platform): A higher-level management layer that provides oversight and control over multiple nodes. Here’s how they relate schematically: ![Pipelines, Nodes, Platform](/_astro/pipelines-nodes-platform.tEUv8QT0_19DKCs.svg) When a node starts, it will automatically attempt to connect to the platform, giving you a seamless way to manage and deploy pipelines through a web interface. However, using the platform is optional—pipelines can also be controlled directly via the node’s API with a CRUD-style approach. The platform, beyond pipeline management, offers user and workspace administration, authentication support via external identity providers (IdP), and dashboards that consists of charts. Tenzir hosts one instance of the platform at [app.tenzir.com](https://app.tenzir.com), but you can also [deploy the platform on premises](/guides/platform-setup) in fully air-gapped environments.

# Node

A **node** is a running process that manages and executes pipelines. When a node starts, it will automatically attempt to connect to the [platform](/explanations/architecture/platform), giving you a seamless way to manage and deploy pipelines through a web interface. However, using the platform is optional—you can also be manually manage pipelines via the node’s [REST API](/reference/node/api). ## Standalone vs. Managed Pipeline Execution [Section titled “Standalone vs. Managed Pipeline Execution”](#standalone-vs-managed-pipeline-execution) To understand the benefits of a node, let’s first consider how you can run pipelines without one. You run a single pipeline directly from the command line using the `tenzir` binary: ![Standalone Execution](/_astro/standalone-execution.D-GySmuS_19DKCs.svg) This *standalone execution* mode of a pipeline is ideal for ad-hoc data transformations akin to how one would use `jq`, but with much broader data handling capabilities. For continuous and more dependable data processing, you will quickly realize that you also need scheduled execution, automatic restarting, monitoring of warnings/errors, and more advanced execution capabilities, like real-time enrichment with contextual data or correlation with historical data. This is where a node comes into play, offering a vehicle to execute one or more pipelines in a managed fashion. You can spawn a node with the `tenzir-node` binary or by running the Docker container that contains this binary: ![Managed Execution](/_astro/managed-execution.C42wjIQe_19DKCs.svg) ## Pipeline Subprocesses [Section titled “Pipeline Subprocesses”](#pipeline-subprocesses) A node can execute pipelines in two different modes: as separate subprocesses or within the same `tenzir-node` process. By default, pipelines run as separate subprocesses. Each approach offers distinct advantages and trade-offs. ![Pipeline Subprocesses](/_astro/pipeline-subprocesses.BBG1UNAC_19DKCs.svg) When pipelines run as separate subprocesses, they don’t share fate with each other. If one pipeline crashes, it won’t affect other running pipelines, which improves overall reliability. This isolation also enables better scaling across available CPU cores since each pipeline can utilize cores independently. Vertically scaling a node becomes more efficient as this approach reduces pressure on the node’s internal scheduler and improves overall system responsiveness. However, subprocess execution comes with costs: each process requires its own memory space and system resources, and the system must serialize data when crossing process boundaries. In contrast, when pipelines run within the same `tenzir-node` process, they operate with less OS pressure due to running as a single process with a fixed number of threads and efficient user-level task scheduling. Pipelines can pass data directly without serialization at their boundaries, which can noticeably lower overall resource consumption. The trade-off here involves shared fate—a critical error in one pipeline could potentially affect the entire node process. Additionally, all pipelines must share the same process resources, which can create bottlenecks under heavy load and limit parallelism. Choose between subprocess and in-process execution based on your specific requirements for reliability, performance, and resource efficiency. You can [configure this behavior](/guides/node-setup/configure-a-node#configure-pipeline-subprocesses) in your node settings.

# Pipeline

A Tenzir **pipeline** is a chain of **operators** that represents a dataflow. Operators are the atomic building blocks that produce, transform, or consume data. Think of them as Unix or Powershell commands where the result from one command is feeding into the next: ![Pipeline Chaining](/_astro/operator-chaining.CjiL40XZ_19DKCs.svg) Our pipelines have 3 types of operators: **inputs** that produce data, **outputs** that consume data, and **transformations** that do both: ![Pipeline Structure](/_astro/operator-types.B4v1eusl_19DKCs.svg) You write pipelines in the [Tenzir Query Language (TQL)](/explanations/language), a language that we developed from the ground up to concisely describe such dataflows. ## Typed Operators [Section titled “Typed Operators”](#typed-operators) Tenzir pipelines operate both ond unstructured stream of bytes and typed event streams. The execution model ensures type safety while maintaining high performance through batching and parallel processing. An operator has an **upstream** and **downstream** type: ![Upstream and Downstream Types](/_astro/operator-table.ChkIKyOz_19DKCs.svg) This typing ensures pipelines are well-formed. Adjacent operators must have matching types: the downstream type of one operator must match the upstream type of the next, i.e., upstream/downstream types of adjacent operators have to match. Otherwise the pipeline is malformed. With these operators as building blocks, you can create all kinds of pipelines, as long as they follow the two principal rules of (1) sequencing inputs, transformations, and outputs, and (2) ensuring that operator upstream/downstream types match. Here are examples of other valid pipeline variations: ![Operator Composition Examples](/_astro/operator-composition-variations.8EYlYgMP_19DKCs.svg) ## Multi-Schema Dataflows [Section titled “Multi-Schema Dataflows”](#multi-schema-dataflows) As mentioned above, pipelines can transport both *bytes* and *events*. Let’s go deeper into the details of Tenzir represents events. Every event that flows through a pipeline is part of a *data frame* with a schema. Internally, these data frames are represented as Apache Arrow record batches, encoding potentially of tens of thousands of events in a single block of data. This innate batching is the reason why the pipelines can achieve high throughput. Unique about Tenzir’s pipeline executor is that a single pipeline can process events with *multiple schemas*. When you typically work with data frames, your workload runs on input with a fixed schema, e.g., when you query a database table. In Tenzir, schemas can change dynamically during the execution of a pipeline, much like document-oriented engines that work on JSON or have one-event-at-a-time processing semantics. Tenzir is unique in that it gives the user the feeling of operating on a single event at a time while hiding the structured data frame batching behind the scenes. Thus, Tenzir combines the performance of structured query engines with the flexibility of document-oriented engines, making it perfect fit for processing *semi-structured data* at scale: ![Structured vs document-oriented engines](/_astro/document-vs-structured.y0QQZq3T_19DKCs.svg) The schema variance begins early in the data flow, where parsers emit events with changing schemas as they encounter changing fields. If an operator detects a schema changes, it creates a new batch of events. In terms of performance, the worst case for Tenzir is a ordered stream of schema-switching events, with every event having a new schema than the previous one. But even for those scenarios operators can efficiently build homogeneous batches when the inter-event order does not matter. Similar to predicate pushdown, Tenzir operators support *ordering pushdown* to signal to upstream operators that the event order only matters intra-schema but not inter-schema. In this case the operator transparently “demultiplex” a heterogeneous event stream into N homogeneous streams. The [`sort`](/reference/operators/sort) operator is an example of such an operator; it pushes its ordering requirements upstream, allowing parsers to efficiently create multiple streams events in parallel. ![Multi-schema Example](/_astro/multi-schema-example.Ddg0yxO5_19DKCs.svg) Some operators only work with exactly one instance per schema internally, such as [`write_csv`](/reference/operators/write_csv), which first writes a header and then all subsequent rows have to adhere to the emitted schema. Such operators cannot handle events with changing schemas. It’s important to mention that most of the time you don’t have to worry about schemas. They are there for you when you want to work with them, but it’s often enough to just specified the fields that you want to work with, e.g., `where id.orig_h in 10.0.0.0/8`, or `select src_ip, dest_ip, proto`. Schemas are inferred automatically in parsers, but you can also seed a parser with a schema that you define explicitly. ## Unified Live Stream Processing and Historical Queries [Section titled “Unified Live Stream Processing and Historical Queries”](#unified-live-stream-processing-and-historical-queries) Tenzir’s execution engine transparently processes both historical data and real-time event streams within a single, unified pipeline model. [TQL](/explanations/language) empowers you to switch between these workloads by simply changing the data source at the start of your pipeline. ![Unified Processing](/_astro/unified-processing.l9So0oFr_19DKCs.svg) This design lets you reuse the same logic for exploring existing data and for deploying it on live streams, which streamlines the entire analytics workflow. Each Tenzir Node includes a lightweight **edge storage** engine for efficient local data persistence. You interact with this storage engine using just two dedicated operators to store and retrieve data. The retrievial goes much beyond replay. A naive interpretation would be that [`export`](/reference/operators/export) first retrieves all its data, which subsequent operators then filter. However, Tenzir actively optimizes this process using **predicate pushdown**. Before a pipeline runs, Tenzir pushes filter conditions from later stages down to the initial storage source. This allows the source to intelligently fetch only the necessary data, often using fast index lookups and avoiding costly full scans. Tenzir’s unique edge storage engine enables this powerful optimization. The diagram below illustrates how the engine works: ![Database Architecture](/_astro/storage-engine-architecture.DQ2lar3W_19DKCs.svg) The edge storage engine is not a traditional database but a lightweight **catalog** that maintains a thin indexing layer over immutable Apache Parquet and Feather files. It maintains **sparse indexes**, such as min-max synopses and Bloom filters, that act as a table of contents. These indexes allow the engine to quickly rule out data partitions that do not match a query’s filter, avoiding unnecessary scans. The catalog also tracks evolving schemas and provides a transactional interface for partition operations. Because the engine handles these optimizations automatically, the same pipeline logic can be seamlessly repurposed. A pipeline developed for historical analysis can be deployed on a live data stream by simply exchanging the historical data source for a streaming one. This unified model streamlines the path from interactive exploration to production deployment.

# Platform

The **platform** provides *fleet management* for [nodes](/explanations/architecture/node). With an API and web interface, the platform offers user and workspace administration, authentication via external identity providers (IdP), and dashboards consisting of pipeline-powered charts. There exist three primary entities in the platform: 1. **Users**: Authenticated by an Identity Provider (IdP) 2. **Organizations**: Manage billing/licencse, members, and workspaces 3. **Workspace**: A logical grouping of nodes, secrets, and dashboards. The diagram below illustrates their relationship. ![Platform Components](/_astro/platform-components.Bpqj86Ga_19DKCs.svg) A user inside an organization is called a **member**. The organization configures what members have access to what workspaces. ## Data Model [Section titled “Data Model”](#data-model) The following diagram visualizes the platform’s data model (highlighted) and how the entities relate to each other with respect to their multiplicities. ![Platform Data Model](/_astro/platform-data-model.BNks0SgU_19DKCs.svg) It’s important to note that a node can only be part of one workspace. There is no support for “multi-homing” as it would create non-trivial questions about how to reconcile secrets and permissions from multiple workspaces. ## Deployment Modes [Section titled “Deployment Modes”](#deployment-modes) Based on the [Edition](https://tenzir.com/pricing) of Tenzir, you have different deployment modes of the platform. The below diagram illustrates the variants. ![Deployment Modes](/_astro/deployment-modes.CUEE2kAI_19DKCs.svg) * **Community Edition**: geared towards single-user deployments, the Community Edition only associates a personal workspace with every user. * **Professional Edition**: geared towards small-business deployments, the Professional Edition features organizations for allowing multiple users to collaborate. * **Enterprise Edition**: geared towards large enterprise deployments, the Enterprise Edition supports multiple additional enterprise features like external secrets management, RBAC, and audit logs. * **Sovereign Edition**: geared towards on-prem deployments, the Sovereign Edition allows full control over all aspects of the Tenzir Platform, including multiple platform instances, multiple organizations within each platform, and integration with existing on-prem infrastructure. The Sovereign Edition is best suited for service providers that need strict data segregation, either by deploying one platform instance per customer or by instantiating one organization per customer. Dedicated platforms per customer provide physical data separation at the cost of higher management overhead, whereas an organization-based multi tenancy approach is a logical separation method with shared underlying resources, yet easier to manage.

# Configuration

There exist multiple options to configure the behavior of the Tenzir pipeline executor (`tenzir`) and Tenzir Node (`tenzir-node`) that ship with a Tenzir package: 1. Command-line arguments 2. Environment variables 3. Configuration files 4. Compile-time defaults The options are sorted by precedence, i.e., command-line arguments override environment variables, which override configuration file settings. Compile-time defaults can only be changed by rebuilding Tenzir from source. Let's discuss the first three options in more detail. ## Command Line Arguments [Section titled “Command Line Arguments”](#command-line-arguments) The command line arguments of the executables have the following synopsis: ```plaintext tenzir [opts] <pipeline> tenzir-node [opts] ``` We have both long `--long=X` and short `-s X` options. Boolean options do not require explicit specification of a value, and it suffices to write `--long` and `-s` to set an option to true. ## Environment Variables [Section titled “Environment Variables”](#environment-variables) You can use environment variables as an alternative method to passing command line options. This comes in handy when working with non-interactive deployments where the command line is hard-coded, such as in Docker containers. An environment variable has the form `KEY=VALUE`, and we describe the format of `KEY` and `VALUE` below. Tenzir processes only environment variables that have the form `TENZIR_{KEY}=VALUE`. For example, `TENZIR_ENDPOINT=1.2.3.4` translates to the command line option `--endpoint=1.2.3.4` and YAML configuration `tenzir.endpoint: 1.2.3.4`. ### Keys [Section titled “Keys”](#keys) There exists a one-to-one mapping from configuration file keys to environment variable names. Here are two examples: * `tenzir.import.batch-size` 👈 configuration file key * `TENZIR_IMPORT__BATCH_SIZE` 👈 environment variable A hierarchical key of the form `tenzir.x.y.z` maps to the environment variable `TENZIR_X__Y__Z`. More generally, the `KEY` in `TENZIR_{KEY}=VALUE` adheres to the following rules: 1. Double underscores map to the `.` separator of YAML dictionaries. 2. Single underscores `_` map to a `-` in the corresponding configuration file key. This is unambiguous because Tenzir does not have any options that include a literal underscore. From the perspective of the command line, setting the `--foo` option via `tenzir --foo` or `tenzir-node --foo` maps onto the environment variable `TENZIR_FOO` and the configuration file key `tenzir.foo`. Here are two examples with identical behavior: ```sh TENZIR_ENDPOINT=0.0.0.0:42000 tenzir-node tenzir-node --endpoint=0.0.0.0:42000 ``` CAF and Plugin Settings To provide [CAF](https://github.com/actor-framework/actor-framework) and plugin settings, which have the form `caf.x.y.z` and `plugins.name.x.y.z` in the configuration file, the environment variable must have the form `TENZIR_CAF__X__Y__Z` and `TENZIR_PLUGINS__NAME__X__Y__Z` respectively. The configuration file is an exception in this regard: `tenzir.caf.` and `tenzir.plugins.` are invalid key prefixes. Instead, CAF and plugin configuration file keys have the prefixes `caf.` and `plugins.`, i.e., they are hoisted into the global scope. ### Values [Section titled “Values”](#values) While all environment variables are strings on the shell, Tenzir parses them into a typed value internally. In general, parsing values from the environment follows the same syntactical rules as command line parsing. In particular, this applies to lists. For example, `TENZIR_PLUGINS="foo,bar"` is equivalent to `--plugins=foo,bar`. Tenzir ignores environment variables with an empty value because the type cannot be inferred. For example, `TENZIR_PLUGINS=` will not be considered. ## Configuration files [Section titled “Configuration files”](#configuration-files) Tenzir's configuration file is in YAML format. On startup, Tenzir attempts to read configuration files from the following places, in order: 1. `/opt/tenzir/etc/tenzir/tenzir.yaml` for system-wide configuration, where `/opt/tenzir` is the default install prefix and `/etc/tenzir` is the default sysconfdir. 2. `~/.config/tenzir/tenzir.yaml` for user-specific configuration. Tenzir respects the XDG base directory specification and its environment variables. 3. A path to a configuration file passed via `--config=/path/to/tenzir.yaml`. If there exist configuration files in multiple locations, options from all configuration files are merged in order, with the latter files receiving a higher precedence than former ones. For lists, merging means concatenating the list elements. ### Plugin Configuration Files [Section titled “Plugin Configuration Files”](#plugin-configuration-files) In addition to `tenzir/tenzir.yaml`, Tenzir loads `tenzir/plugin/<plugin>.yaml` for plugin-specific configuration for a given plugin named `<plugin>`. The same rules apply as for the regular configuration file directory lookup. ### Bare Mode [Section titled “Bare Mode”](#bare-mode) Sometimes, users may wish to run Tenzir without side effects, e.g., when wrapping Tenzir in their own scripts. Run with `--bare-mode` to disable looking at all system- and user-specified configuration paths. ## Plugins [Section titled “Plugins”](#plugins) Tenzir's plugin architecture allows for flexible replacement and enhancement of functionality at various pre-defined customization points. There exist **dynamic plugins** that ship as shared libraries and **static plugins** that are compiled into libtenzir. ### Install plugins [Section titled “Install plugins”](#install-plugins) Dynamic plugins are just shared libraries and can be placed at a location of your choice. We recommend putting them into a single directory and add the path to the `tenzir.plugin-dirs` configuration option.. Static plugins do not require installation since they are compiled into Tenzir. ### Load plugins [Section titled “Load plugins”](#load-plugins) The configuration key `tenzir.plugins` specifies the list of plugins that should load at startup. The `all` plugin name is reserved. When `all` is specified Tenzir loads all available plugins in the configured plugin directories. If no `tenzir.plugins` key is specified, Tenzir will load `all` plugins by default. To load no plugins at all, specify a `tenzir.plugins` configuration key with no plugin values, e.g. the configuration file entry `plugins: []` or launch parameter `--plugins=`. Since dynamic plugins are shared libraries, they must be loaded first into the running Tenzir process. At startup, Tenzir looks for the `tenzir.plugins` inside the `tenzir.plugin-dirs` directories configured in `tenzir.yaml`. For example: ```yaml tenzir: plugin-dirs: - . - /opt/foo/lib plugins: - example - /opt/bar/lib/libtenzir-plugin-example.so ``` Before executing plugin code, Tenzir loads the specified plugins via `dlopen(3)` and attempts to initialize them as plugins. Part of the initialization is passing configuration options to the plugin. To this end, Tenzir looks for a YAML dictionary under `plugins.<name>` in the `tenzir.yaml` file. For example: ```yaml plugins: example: option: 42 ``` Alternatively, you can specify a `plugin/<plugin>.yaml` file. The example configurations above and below are equivalent. This makes plugin deployments easier, as plugins can be installed and uninstalled alongside their respective configuration. ```yaml option: 42 ``` After initialization with the configuration options, the plugin is fully operational and Tenzir will call its functions at the plugin-specific customization points. ### List plugins [Section titled “List plugins”](#list-plugins) You can get the list of available plugins using the [`plugins`](/reference/operators/plugins) operator: ```bash tenzir 'plugins' ``` ### Block plugins [Section titled “Block plugins”](#block-plugins) As part of your Tenzir deployment, you can selectively disable plugins by name. For example, if you do not want the `shell` operator and the `kafka` connector to be available, set this in your configuration: ```yaml tenzir: disable-plugins: - shell - kafka ```

# Enrichment

Enrichment means adding contextual data to events. The purpose of this added context is to allow for making better decisions, e.g., to triage alerts and weed out false positive, to leverage country information to classify logins as malicious, or to flag a sighting of an indicator of compromise. Tenzir comes a flexible enrichment framework where the central abstraction is a **context**: a stateful object that can be updated with pipelines and used for enrichment in other pipelines: ![Context update & enrich](/_astro/context-update-enrich.s9uj4OUs_19DKCs.svg) The update and enrich operations can occur concurrently. This allows for creating highly dynamic use cases where context state rapidly changes, such as when modelling the threat landscape or internal network infrastructure. Reusing pipelines as mechanism for context updates (as opposed to other systems that, say, offer a separate interface to only load static CSV files) has the benefit that we can leverage the full power of TQL. In other words, we can reuse *all* existing connectors, formats, periodic scheduling, and more. ## Enrichment Modes [Section titled “Enrichment Modes”](#enrichment-modes) In general, we distinguish three different contextualization modes: ![Contextualization Modes](/_astro/contextualization-modes.D2N6G2W8_19DKCs.svg) 1. **In-band**. The context data is co-located with the pipeline that enriches the dataflow. For high-velocity pipelines with thousands of events per second, this is of often the only way to enrich. 2. **Out-of-band**. The context data is outside of the to-be-contextualized dataflow. The most common example of this kind are REST APIs. Enrichment then means performing one API call per event, waiting for the result, and then merging it into the event to continue processing. For public APIs, latencies are in the tens to hundreds of milliseconds, making this mode suitable for low-velocity. 3. **Hybrid**. An excerpt of the context sits inline and the bulk of it remotely. When both performance matters and state is not possible to ship to the contextualization point itself, then a hybrid approach can be a viable middle ground. [Google Safe Browsing](https://security.googleblog.com/2022/08/how-hash-based-safe-browsing-works-in.html) is an example of this kind, where the Chrome browser keeps a subset of context state that represents threat actor URLs in the form of partial hashes, and when a users visits a URL where a partial match occurs, Chrome performs a candidate check using an API call. More than 99% of checked URLs never make it to the remote API, making this approach scalable. Note that the extra layer of hashing also protects the privacy of the entity performing the context lookup. To date, Tenzir only supports in-band enrichment, but out-of-band and hybrid modes are already planned. ## Context Types [Section titled “Context Types”](#context-types) Tenzir features several context types: * **Lookup table**: a hash table that associates arbitrary information with a given enrichment key. * **GeoIP database**: a special-purpose table for attaching geographic information to IP addresses. * **Bloom filter**: a compact representation of a sets that allows for membership tests only, with the space efficiency coming at the cost a false positives during lookup. ![Context types](/_astro/context-types.B_enb-EP_19DKCs.svg) If these built-in types do not suffice for your needs, you can also write your own C++ context plugin. ### Lookup Table [Section titled “Lookup Table”](#lookup-table) Tenzir’s lookup table context is a hash table that can have both keys and values of different types. For example, you can have IP addresses, subnets, and numbers as table keys all in the same table, and every value can have a different type. There are three particularly powerful features of lookup tables that we describe next. #### 1. Longest-Prefix Subnet Match for Subnet Keys [Section titled “1. Longest-Prefix Subnet Match for Subnet Keys”](#1-longest-prefix-subnet-match-for-subnet-keys) When table keys are of type `subnet`, you can probe the lookup table with values of type `ip` in addition. For example, if you have a key `10.0.0.0/8` you can perform a lookup with `10.0.0.1`, `10.1.1.1`, etc. The lookup table will always return the value associated with `10.0.0.0/8`. ![Subnet Keys](/_astro/lookup-table-subnet-keys.GRqTyUEi_19DKCs.svg) When a table contains subnet keys with overlapping ranges, such as `10.0.0.0/22` and `10.0.0.0/24`, a lookup returns the entry with the *longest-prefix match* of the key. For example, probing the table with `10.0.0.1` returns the value associated with the `/24` subnet, because it is a longer match (24 > 22). #### 2. Per-Key Create/Write/Read Expiration [Section titled “2. Per-Key Create/Write/Read Expiration”](#2-per-key-createwriteread-expiration) Every table entry has three optional expiration timeouts that evict the entry. These timeout are compounding, so it suffices if *one* of them fires to trigger eviction. ![Expiration](/_astro/lookup-table-expiration.4H8rBJ1d_19DKCs.svg) The three timeout types are: 1. **Create timeout**: Whenever a new entry is created in the table, e.g., through `context::update`, this timeout starts counting down. This happens exactly once and the timer cannot be reset. Therefore, a create timeout is an *upper bound* on the lifetime of an entry. 2. **Write timeout**: Whenever an update modifies a table entry by writing a new value for a given key with `context::update`, this timeout resets. 3. **Read timeout**: Whenever an entry is accessed, which happens during `context::enrich`, this timeout resets. All three timers start at table entry creation, i.e., creating an entry is the “0th” write and read. #### 3. Aggregation Functions as Values [Section titled “3. Aggregation Functions as Values”](#3-aggregation-functions-as-values) Lookup tables offer more than just entries with static values. You can also aggregate into values with [aggregation functions](/reference/functions#aggregation). In this case an update of a table entry does not write the new value directly, but rather hands that value to the configured aggregation function, which in turn updates the table value. For example, the `min` aggregation function computes the minimum over its values. Consider a sequence of context updates with values `3`, `4`, `2`, `3`, `1` for the updated key. Then the value after each update would be `3`, `3`, `2`, `2`, `1`. The example below uses `min` and `max` to implement a first-seen and last-seen timestamps—a common pattern during entity tracking. ![Aggregation](/_astro/lookup-table-aggregation.CIhz1i3Q_19DKCs.svg) ### Bloom Filter [Section titled “Bloom Filter”](#bloom-filter) [Bloom filters](https://en.wikipedia.org/wiki/Bloom_filter) are a space-efficient representation of a set. In case you have a massive number of elements but only need to check for set membership. However, the compact spatial representation comes at a cost of a false probability during lookup. You can think of it as a [lookup table](#lookup-table) without values—just keys, but where looking up a key may say “yes” even though the key doesn’t actually exist. The probability of this happening is fortunately configurable. A Bloom filter has two tuning knobs: 1. **Capacity**: the maximum number of items in the filter. 2. **False-positive probability**: the chance of reporting an item not in the filter. These two parameters dictate the space usage of the Bloom filter. Consult Thomas Hurst’s [Bloom Filter Calculator](https://hur.st/bloomfilter/) for finding the optimal configuration for your use case. Tenzir’s Bloom filter implementation is a C++ rebuild of DCSO’s [bloom](https://github.com/DCSO/bloom) library. It is binary-compatible and uses the exact same method for FNV1 hashing and parameter calculation, making it a drop-in replacement for `bloom` users. ### GeoIP Database [Section titled “GeoIP Database”](#geoip-database) The GeoIP context provides geo-spatial information about a given IP address. Conceptually, it is a lookup table with IP addresses as keys that maps to country codes/names, region codes/names, cities, zip codes, and geographic coordinates. Since a naive table representation would explode in memory, [MaxMind](https://www.maxmind.com/) came up with a custom binary [MMDB format](https://maxmind.github.io/MaxMind-DB/) along with a [corresponding library](https://github.com/maxmind/libmaxminddb) to perform IP lookups. This format is the de-facto standard for geo-spatial IP address enrichment today. Tenzir supports it natively.

# FAQs

This page provides answers to frequently asked questions (FAQs) about Tenzir. ## What is Tenzir? [Section titled “What is Tenzir?”](#what-is-tenzir) Tenzir is the data pipeline engine for security teams, enabling you to collect, parse, shape, normalize, aggregate, route, store, and query your security telemetry at ease. Tenzir is also the name of the startup behind the product. ## What part of Tenzir is open and what part is closed source? [Section titled “What part of Tenzir is open and what part is closed source?”](#what-part-of-tenzir-is-open-and-what-part-is-closed-source) To create a healthy open core business with a thriving open source foundation, we aim to find the right balance between enabling open source enthusiast whilst offering a commercial package for those seeking a turn-key solution: ![Open source vs. closed source](/_astro/open-vs-closed-source.B517vmB2_19DKCs.svg) There exist three moving parts: 1. **Executor**: The pipeline executor is open source and available under a permissive BSD 3-clause licence at [GitHub](https://github.com/tenzir/tenzir). 2. **Node**: The node makes pipeline management easy. A node orchestrates multiple pipelines and offers additional features, such as contexts for enrichment and an indexed storage engine. 3. **Platform**: The platform is the control plane for managing nodes and offers a web-based interface. You can either [build the open source version](/guides/development/build-from-source) yourself and [add your own plugins](/explanations/architecture), or use our compiled binary packages that include the command line tool and the node. We offer the platform as Docker Compose files. We host one platform instance at [app.tenzir.com](https://app.tenzir.com). Tenzir comes in severals editions, including a free [Community Edition](https://tenzir.com/pricing). If you have any questions, don’t hesitate to [reach out](https://tenzir.com/contact) what best suits your needs. ## Can Tenzir see my data? [Section titled “Can Tenzir see my data?”](#can-tenzir-see-my-data) *No*, but let us explain. A Tenzir deployment consists of *nodes* that you manage, and a *platform* available as SaaS from us or operated by you. The *app* runs in your browser to access the platform. All computation and storage takes place at your nodes. The platform acts as rendezvous point that connects two TLS-encrypted channels, one from the node to the platform, and one from the browser to the platform: ![Platform Connections](/_astro/platform-connections.BdzkU-en_19DKCs.svg) We connect these two channels at the platform. Therefore, whoever operates the platform *could* interpose on data that travels from the nodes to the app. In the [Professional Edition](https://tenzir.com/pricing) and [Enterprise Edition](https://tenzir.com/pricing), we run the platform. However, we emphasize that data privacy is of utmost importance to us and our customers. As a mission-driven company with strong ethics, our engineering follows state-of-the-art infrastructure-as-code practices and we are performing security audits to ensure that our code quality meets the highest standards. We have plans to make this a single, end-to-end encrypted channel, so that we no longer have the theoretical ability to interpose on the data transfer between app and node. If you have more stringent requirements, you can also run the platform yourself with the [Sovereign Edition](https://tenzir.com/pricing). ## Does Tenzir run on-premise? [Section titled “Does Tenzir run on-premise?”](#does-tenzir-run-on-premise) Yes, Tenzir can run on premise and supports fully air-gapped environments. The [Sovereign Edition](https://tenzir.com/pricing) allows you to [deploy the entire platform](/guides/platform-setup) in a Dockerized environment. The [Community Edition](https://tenzir.com/pricing), [Professional Edition](https://tenzir.com/pricing) and [Enterprise Edition](https://tenzir.com/pricing) are backed by a Tenzir-hosted instance of the platform in the public cloud (AWS in Europe). Read more on [how Tenzir works](/explanations/architecture) to understand what component of Tenzir runs where. ## Does Tenzir offer cloud-native nodes? [Section titled “Does Tenzir offer cloud-native nodes?”](#does-tenzir-offer-cloud-native-nodes) Tenzir currently does not offer cloud-hosted nodes. You can only run nodes in your own environment, including your cloud environment. However, we offer a cloud-native *demo node* that you can deploy as part of every account. ## Why Did You Create a New Query Language? Why Not SQL? [Section titled “Why Did You Create a New Query Language? Why Not SQL?”](#why-did-you-create-a-new-query-language-why-not-sql) We opted for our own language—the [**Tenzir Query Language**](/explanations/language) or **TQL**— for several reasons that we outline below. At Tenzir, we have a clear target audience: security practitioners. They are rarely data engineers fluent in SQL or experienced with lower-level data tools. Rather, they identify as blue/purple teamers, incident responders, threat hunters, detection engineers, threat intelligence analysts, and other domain experts. **Why Not Stick with SQL?** SQL, while powerful and pervasive, comes with significant usability challenges. Despite decades of success, SQL’s syntax can be hard to learn, read, and modify, even for experts. Some of the key limitations include: * **Rigid Clause Order**: SQL’s fixed structure (e.g., `SELECT ... FROM ... WHERE ...`) forces users to think in an “inside-out” manner that doesn’t match the natural flow of data transformations. This often leads to complex subqueries and redundant clauses. * **Complex Subqueries**: Expressing multi-level aggregations or intermediate transformations typically requires deeply nested subqueries, which hurt readability and make edits labor-intensive. * **Difficult Debugging**: SQL’s non-linear data flow makes tracing logic through large queries cumbersome, impeding efficient debugging. These challenges make SQL difficult for security practitioners who need to focus on quick, intuitive data analysis without getting bogged down by the intricacies of query structuring. **Why a Pipeline Language?** We chose a pipeline-based approach for our query language because it enhances user experience and addresses the pain points of SQL. Here’s how: * **Sequential and Intuitive Data Flow**: Pipeline syntax expresses data operations in a top-to-bottom sequence, reflecting the logical order of data transformations. This makes it easier to follow and understand, especially for complex queries. * **Simplified Query Construction**: With a pipeline language, operations can be chained step-by-step without requiring nested subqueries or repetitive constructs. This improves readability and allows users to build and modify queries incrementally. * **Easier Debugging**: Each stage in a pipeline can be isolated and inspected, simplifying the process of identifying issues or making adjustments. This is in stark contrast to SQL, where changes often ripple through multiple interconnected parts of a query. **Lessons from Other Languages.** We spoke to numerous security analysts with extensive experience using SIEMs. Splunk’s Search Processing Language (SPL), for instance, has set a high standard in user experience, catering well to its non-engineer user base. This inspired us to create a language with: * The *familiarity* of [Splunk](https://splunk.com) * The *power* of [Kusto](https://github.com/microsoft/Kusto-Query-Language) * The *flexibility* of [jq](https://stedolan.github.io/jq/) * The *clarity* of [PRQL](https://prql-lang.org/) * The *expressiveness* of [dplyr](https://dplyr.tidyverse.org/) * The *ambition* of [SuperSQL](https://superdb.org/) * The *composability* of [Nu](https://www.nushell.sh/) Even Elastic recognized the need for more intuitive languages by introducing **ES|QL**, which leans into a pipeline style. Nearly every major SIEM and observability tool has adopted some version of a pipeline language, underscoring the trend toward simplified, step-by-step data handling. **Balancing Streaming and Batch Workloads.** One of our key design goals was to create a language that effortlessly handles both streaming and batch processing. By allowing users to switch between historical and live data inputs with minimal change to the pipeline, our language maintains flexibility without introducing complexity. Our decision to develop a new language was not taken lightly. We aimed to build on the strengths of SQL while eliminating its weaknesses, creating an intuitive, powerful tool tailored for security practitioners. By combining insights from existing successful pipeline languages and leveraging modern data standards, we offer a user-friendly and future-proof solution for security data analysis. ## What database does Tenzir use? [Section titled “What database does Tenzir use?”](#what-database-does-tenzir-use) Tenzir does not rely on a third-party database. Tenzir nodes include a light-weight storage engine on top of Feather or Parquet files, accessible via the [`import`](/reference/operators/import) and [`export`](/reference/operators/export) operators. The storage engine comes with a catalog that tracks schema meta data and a thin layer of indexing to accelerate queries. Our [tuning guide](/guides/node-setup/tune-performance) has further details on the inner workings. ## Does a Tenzir node run on platform *X*? [Section titled “Does a Tenzir node run on platform X?”](#does-a-tenzir-node-run-on-platform-x) We support the platforms that we mention in our [deployment instructions](/guides/node-setup/provision-a-node). For any other platform, the answer is most likely *no*. Please [talk to us](/discord) to let us know what is missing, or dive right in by contributing to our [open source repository](https://github.com/tenzir/tenzir). ## Do you have an integration for *X*? [Section titled “Do you have an integration for X?”](#do-you-have-an-integration-for-x) Tenzir has multiple layers where integrations can occur. If you cannot find *X* in the list of [existing integration](/integrations), . 1. **Application**. If *X* does not require a built-in integration, our [Community Library](https://github.com/tenzir/library) may contain a package for it. Many integration to specific tools are just thin API wrappers that only require composing a few operators. 2. **Format**. If your *X* is a wire format, either text-based like JSON or binary like PCAP, then look for [parsing operators](/reference/operators#parsing) and [printing operators](/reference/operators#printing). Those start with `read_*` and `write_*`, respectively. Similarly, there exist [parsing functions](/reference/functions#parsing) and [printing functions](/reference/functions#printing) that start with `parse_*` and `print_*`. 3. **Fluent Bit**. Sometimes we can compensate for the lack of an existing integration by going through one level of indirection. Tenzir ships with all of Fluent Bit’s [inputs](https://docs.fluentbit.io/manual/pipeline/inputs/) and [outputs](https://docs.fluentbit.io/manual/pipeline/outputs/), because the Fluent Bit library is baked into every Tenzir binary. Use the [`from_fluent_bit`](/reference/operators/from_fluent_bit) operator to get events in via Fluent Bit and the [`to_fluent_bit`](/reference/operators/to_fluent_bit) operator to send events via Fluent Bit. 4. **Escape Hatches**. As last resort, you can bring in Shell and Python scripts to make up for native support for *X*. The [`shell`](/reference/operators/shell) operator brings byte streams via standard input and output into a pipeline, and the [`python`](/reference/operators/python) operator allows you to perform arbitrary event-to-event transformation using the full power of Python. 5. **Community**. Still unlucky? Then please let us know in our friendly [Discord server](/discord). Perhaps we are already working on an integration for *X* or it is somewhere on the roadmap. If you’re feeling adventurous and want to contribute to our open source core, let us know beforehand! We’re happy to guide you such that your contribution gets successfully into our code base.

# Glossary

This page defines central terms in the Tenzir ecosystem. ## App [Section titled “App”](#app) Web user interface to access [platform](#platform) at [app.tenzir.com](https://app.tenzir.com). The app is a web application that partially runs in the user’s browser. It is written in [Svelte](https://svelte.dev/). ## Catalog [Section titled “Catalog”](#catalog) Maintains [partition](#partition) ownership and metadata. The catalog is a component in the [node](#node) that owns the [partitions](#partition), keeps metadata about them, and maintains a set of sparse secondary indexes to identify relevant partitions for a given query. It offers a transactional interface for adding and removing partitions. ## Connector [Section titled “Connector”](#connector) Manages chunks of raw bytes by interacting with a resource. A connector is either a *loader* that acquires bytes from a resource, or a *saver* that sends bytes to a resource. Loaders are implemented as ordinary [operators](/reference/operators) prefixed with `load_*` while savers are prefixed with `save_*`. ## Context [Section titled “Context”](#context) A stateful object used for in-band enrichment. Contexts come in various types, such as a lookup table, Bloom filter, and GeoIP database. They live inside a node and you can enrich with them in other pipelines. * Read more about [enrichment](/explanations/enrichment) ## Destination [Section titled “Destination”](#destination) An pipeline ending with an [output](#output) operator preceded by a [`subscribe`](/reference/operators/subscribe) input operator. * Learn more about [pipelines](/explanations/architecture/pipeline) ## Edge Storage [Section titled “Edge Storage”](#edge-storage) The indexed storage that pipelines can use at the [node](#node). Every node has a light-weight storage engine for importing and exporting events. You must mount the storage into the node such that it can be used from [pipelines](#pipeline) using the [`import`](/reference/operators/import) and [`export`](/reference/operators/export) [operators](#operator). The storage cengine comes with a [catalog](#catalog) that tracks [partitions](#partition) and keeps sparse [indexes](#index) to accelerate historical queries. * [Ingest data into the node’s edge storage](/guides/edge-storage/import-into-a-node) * [Query the node’s edge storage](/guides/edge-storage/export-from-a-node) ## Event [Section titled “Event”](#event) A record of typed data. Think of events as JSON objects, but with a richer [type system](/explanations/language/types) that also has timestamps, durations, IP addresses, and more. Events have fields and can contain numerous shapes that describe its types (= the [schema](#schema)). * Learn more about [pipelines](/explanations/architecture/pipeline) ## Format [Section titled “Format”](#format) Translates between bytes and events. A format is either supported by a *parser* that converts bytes to events, or a *printer* that converts events to bytes. Example formats are JSON, CEF, or PCAP. * See available [operators for parsing](/reference/operators#parsing) * See available [operators for printing](/reference/operators#printing) * See available [functions for parsing](/reference/functions#parsing) * See available [functions for printing](/reference/functions#printing) ## Function [Section titled “Function”](#function) Computes something over a value in an [event](#event). Unlike operators that work on streams of events, functions can only act on single values. * See available [functions](/reference/functions) ## Index [Section titled “Index”](#index) Optional data structures for accelerating queries involving the node’s [edge storage](#edge-storage). Tenzir featres in-memory *sparse* indexes that point to [partitions](#partition). * [Configure the catalog](/guides/node-setup/tune-performance#configure-the-catalog) ## Input [Section titled “Input”](#input) An [operator](#operator) that only producing data, without consuming anything. * Learn more about [pipelines](/explanations/architecture/pipeline) ## Integration [Section titled “Integration”](#integration) A set of pipelines to integrate with a third-party product. An integration describes use cases in combination with a specific product or tool. Based on the depth of the configuration, this may require configuration on either end. * List of [all integrations](/integrations) * [Does Tenzir have an integration for *X*?](/explanations/faqs#do-you-have-an-integration-for-x) ## Library [Section titled “Library”](#library) A collection of [packages](#package). Our community library is [freely available at GitHub](https://github.com/tenzir/library). ## Loader [Section titled “Loader”](#loader) A connector that acquires bytes. A loader is the dual to a [saver](#saver). It has a no input and only performs a side effect that acquires bytes. Use a loader implicitly with the [`from`](/reference/operators/from) operator or explicitly with the `load_*` operators. * Learn more about [pipelines](/explanations/architecture/pipeline) ## Node [Section titled “Node”](#node) A host for [pipelines](#pipeline) and storage reachable over the network. The `tenzir-node` binary starts a node in a dedicated server process that listens on TCP port 5158. * [Deploy a node](/guides/node-setup/provision-a-node) * Use the [REST API](/reference/node/api) to manage a node * [Import into a node](/guides/edge-storage/import-into-a-node) * [Export from a node](/guides/edge-storage/export-from-a-node) ## Metrics [Section titled “Metrics”](#metrics) Runtime statistics about the node and pipeline execution. * [Collect metrics](/guides/basic-usage/collect-metrics) ## OCSF [Section titled “OCSF”](#ocsf) The [Open Cybersecurity Schema Framework (OCSF)](https://schema.ocsf.io) is a cross-vendor schema for security event data. Our [community library](#library) contains packages that map data sources to OCSF. * [Map data to OCSF](/tutorials/map-data-to-ocsf) ## Operator [Section titled “Operator”](#operator) The building block of a [pipeline](#pipeline). An operator is an [input](#input), a [transformation](#transformation), or an [output](#output). * See all available [operators](/reference/operators) ## Output [Section titled “Output”](#output) An [operator](#operator) consuming data, without producing anything. * Learn more about [pipelines](/explanations/architecture/pipeline) ## PaC [Section titled “PaC”](#pac) The acronym PaC stands for *Pipelines as Code*. It is meant as an adaptation of [Infrastructure as Code (IaC)](https://en.wikipedia.org/wiki/Infrastructure_as_code) with pipelines represent the (data) infrastructure that is provisioning as code. * Learn how to provision [piplines as code](/guides/basic-usage/run-pipelines#as-code). ## Package [Section titled “Package”](#package) A collection of [pipelines](#pipeline) and [contexts](#context). * Read more about [packages](/explanations/packages) * [Learn how to write a package](/tutorials/write-a-package) ## Parser [Section titled “Parser”](#parser) A bytes-to-events operator. A parser is the dual to a [printer](#printer). You use a parser implicitly in the [`from`](/reference/operators/from) operator, or via the `read_*` operators. There exist also [functions](#function) for applying parsers to string values. * Learn more about [pipelines](/explanations/architecture/pipeline) * See available [operators for parsing](/reference/operators#parsing) * See available [functions for parsing](/reference/functions#parsing) ## Partition [Section titled “Partition”](#partition) The horizontal scaling unit of the storage attached to a [node](#node). A partition contains the raw data and optionally a set of indexes. Supported formats are [Parquet](https://parquet.apache.org) or [Feather](https://arrow.apache.org/docs/python/feather.html). ## Pipeline [Section titled “Pipeline”](#pipeline) Combines a set of [operators](#operator) into a dataflow graph. * Learn more about [pipelines](/explanations/architecture/pipeline) * [Run a pipeline](/guides/basic-usage/run-pipelines) ## Platform [Section titled “Platform”](#platform) The control plane for nodes and pipelines, accessible at [app.tenzir.com](https://app.tenzir.com). * Understand the [Tenzir architecture](/explanations/architecture) ## Printer [Section titled “Printer”](#printer) An events-to-bytes operator. A [format](#format) that translates events into bytes. A printer is the dual to a [parser](#parser). Use a parser implicitly in the [`to`](/reference/operators/to) operator. * Learn more about [pipelines](/explanations/architecture/pipeline) * See available [operators for printing](/reference/operators#printing) * See available [functions for printing](/reference/functions#printing) ## Saver [Section titled “Saver”](#saver) A [connector](#connector) that emits bytes. A saver is the dual to a [loader](#loader). It has a no output and only performs a side effect that emits bytes. Use a saver implicitly with the [`to`](/reference/operators/to) operator or explicitly with the `save_*` operators. * Learn more about [pipelines](/explanations/architecture/pipeline) ## Schema [Section titled “Schema”](#schema) A top-level record type of an event. * [Show available schemas in the edge storage](/guides/edge-storage/show-available-schemas) ## Source [Section titled “Source”](#source) An pipeline starting with an [input](#input) operator followed by a [`publish`](/reference/operators/publish) output operator. * Learn more about [pipelines](/explanations/architecture/pipeline) ## TQL [Section titled “TQL”](#tql) An acronym for *Tenzir Query Language*. TQL is the language in which users write [pipelines](#pipeline). * Learn more about the [language](/explanations/language) ## Transformation [Section titled “Transformation”](#transformation) An [operator](#operator) consuming both input and producing output. * Learn more about [pipelines](/explanations/architecture/pipeline)

# Overview

The **Tenzir Query Language (TQL)** is a dataflow language designed for processing of unstructured byte-streams and semi-structured events. TQL is built on a powerful streaming execution engine, but it shields you from the complexity of low-level data processing. It provides a rich set of building blocks to create intricate [pipelines](/explanations/architecture/pipeline) that collect, transform, and route data. You can also embed your TQL programs into reusable [packages](/explanations/packages) to create one-click deployable use cases. ![TQL Layers](/_astro/tql-layers.CMohzmcx_19DKCs.svg) ## Why TQL? [Section titled “Why TQL?”](#why-tql) Security practitioners need to collect, transform, and analyze telemetry without the complexity of general-purpose data engineering tools. We created TQL to meet this need by synthesizing the best ideas from languages that security teams already use: * **Splunk SPL’s familiarity**: Operators that security analysts recognize * **Kusto’s power**: Rich aggregation and time-series capabilities * **Unix pipes’ composability**: Small, focused operators that chain together * **jq’s flexibility**: Powerful transformations on semi-structured data TQL combines the power of a streaming execution engine with an intuitive pipeline syntax that mirrors how practitioners think about data processing. This approach is validated by nearly every modern SIEM (Splunk SPL, Elastic ES|QL, Microsoft KQL) and offers several advantages over traditional query languages like SQL. ## Core Concepts [Section titled “Core Concepts”](#core-concepts) ### Unified streaming and batch processing [Section titled “Unified streaming and batch processing”](#unified-streaming-and-batch-processing) TQL seamlessly handles both real-time and historical analysis. Unlike traditional tools that require separate codebases for streaming and batch workflows, TQL uses the same pipeline logic for both. Process archived data from a data lake: ```tql from_file "s3://bucket/logs/2024-01/*.parquet" where timestamp > 2024-01-15T00:00:00 ``` Or monitor a live stream from a message bus: ```tql from "kafka://topic:9092" where timestamp > now() - 1h ``` TQL draws inspiration from Unix pipes, where data flows through a sequence of transformations. But unlike shell pipelines that primarily work on text, TQL operates on both unstructured data (bytes) and structured data (events). ### Multi-schema philosophy [Section titled “Multi-schema philosophy”](#multi-schema-philosophy) Unlike traditional databases that require strict schemas, TQL embraces **heterogeneous data** as a first-class concept. Real-world data pipelines process multiple event types simultaneously—firewall logs, DNS queries, authentication events—each with different schemas. TQL’s operators are **polymorphic**, adapting to different schemas at runtime: ```tql // Single pipeline processing multiple event types from "mixed_security_logs.json" where timestamp > now() - 1h where severity? == "high" or risk_score? > 0.8 select \ timestamp, event_type=@name, // Capture the schema name message, // Common field src_ip?, // Present in network events username?, // Present in auth events dns_query? // Present in DNS events ``` This philosophy enables powerful patterns: * **Type-aware aggregation**: Group and aggregate across different schemas * **Unified processing**: Apply common transformations to diverse data * **Schema evolution**: Handle changing schemas without pipeline updates * **Mixed-source correlation**: Join events from different systems ## Language Structure [Section titled “Language Structure”](#language-structure) The language documentation is organized into four main sections: * [**Types**](/explanations/language/types): TQL’s type system with domain-specific types for security and network data * [**Expressions**](/explanations/language/expressions): The computational core of TQL, from literals to complex evaluations * [**Statements**](/explanations/language/statements): Control and structure with bindings, operators, and control flow * [**Programs**](/explanations/language/programs): Complete data processing workflows and execution model

# Expressions

Expressions form the computational core of TQL. They range from simple literals to complex evaluations. ## Types and Operations [Section titled “Types and Operations”](#types-and-operations) Each type in TQL provides specific operations, starting from the simplest and building up to more complex types. For functions that work with these types, see the [functions reference](/reference/functions). ### Null [Section titled “Null”](#null) The `null` type represents absent or invalid values using the literal `null`: ```tql from { value: null, is_null: null == null, has_value: 42 != null, } ``` ```tql { value: null, is_null: true, has_value: true, } ``` The `else` operator provides null coalescing: ```tql from { result: null else "default", } ``` ```tql { result: "default", } ``` ### Boolean [Section titled “Boolean”](#boolean) Boolean values (`bool`) support logical operations `and`, `or`, and `not`: ```tql from { x: true and false, y: true or false, z: not true, } ``` ```tql { x: false, y: true, z: false, } ``` TQL implements *short-circuit evaluation*: it stops evaluating once it determines the result. ### String [Section titled “String”](#string) Strings support several formats: * **Regular strings**: `"hello\nworld"` (with escape sequences) * **Raw strings**: `r"C:\path\to\file"` (no escape processing) * **Raw strings with quotes**: `r#"They said "hello""#` (allows quotes inside) Strings support concatenation via `+` and substring checking via `in`: ```tql from { name: "World", greeting: "Hello, " + name + "!", has_hello: "Hello" in greeting, } ``` ```tql { name: "World", greeting: "Hello, World!", has_hello: true, } ``` #### Format Strings (f-strings) [Section titled “Format Strings (f-strings)”](#format-strings-f-strings) Format strings provide a concise way to build dynamic strings using embedded expressions. They’re much more readable than string concatenation. For example, instead of: ```tql percent = round(found / total * 100).string() message = "Found " + found.string() + "/" + total.string() + ": " + percent + "%" ``` You can simply write: ```tql message = f"Found {found}/{total}: {round(found / total * 100)}%" ``` To include literal braces, double them: ```tql from { name: "TQL", template: f"Use {{braces}} in {name} like this: {{example}}", } ``` ```tql { name: "TQL", template: "Use {braces} in TQL like this: {example}", } ``` ### Blob [Section titled “Blob”](#blob) Blobs represent raw binary data. Use them for handling non-textual data like network packets, encrypted payloads, or file contents. Read [why TQL has binary blob types](/explanations/language/types/#why-binary-blob-types) for details. ### Numbers [Section titled “Numbers”](#numbers) Numeric literals can include magnitude suffixes for readability: * **Power-of-ten suffixes**: `k` (1,000), `M` (1,000,000), `G`, `T`, `P`, `E` * **Power-of-two suffixes**: `Ki` (1,024), `Mi` (1,048,576), `Gi`, `Ti`, `Pi`, `Ei` For example, `2k` equals `2000` and `2Ki` equals `2048`. All numeric types support standard arithmetic operations: ```tql from { sum: 10 + 5, diff: 10 - 5, product: 10 * 5, quotient: 10 / 5, } ``` ```tql { sum: 15, diff: 5, product: 50, quotient: 2.0, } ``` #### Type Coercion [Section titled “Type Coercion”](#type-coercion) When mixing numeric types, TQL automatically coerces to the type that can hold the most values: | Left Type | Operator | Right Type | Result Type | | :-------- | :---------: | :--------- | :---------- | | `int64` | +, -, \*, / | `int64` | `int64` | | `int64` | +, -, \*, / | `uint64` | `int64` | | `int64` | +, -, \*, / | `double` | `double` | | `uint64` | +, -, \*, / | `uint64` | `uint64` | | `uint64` | +, -, \*, / | `double` | `double` | | `double` | +, -, \*, / | `double` | `double` | #### Overflow and Error Handling [Section titled “Overflow and Error Handling”](#overflow-and-error-handling) TQL handles numeric errors gracefully by emitting warnings in the following cases: * **Overflow/Underflow**: Returns `null` (no wrapping) * **Division by zero**: Returns `null` * **Invalid operations**: Returns `null` This design prevents silent data corruption and makes errors explicit in your data. Example: ```tql let $x = 42 / 0 from { x: $x, } ``` This emits the following warning: ```plaintext warning: division by zero --> <input>:1:10 | 1 | let $x = 42 / 0 | ~~~~~~ | ``` ### Duration [Section titled “Duration”](#duration) Create durations using time unit suffixes: * Nanoseconds: `ns` * Microseconds: `us` * Milliseconds: `ms` * Seconds: `s` * Minutes: `min` * Hours: `h` * Days: `d` * Weeks: `w` * Months: `mo` * Years: `y` Example: `30s`, `5min`, `2h30min` Durations support arithmetic operations for time calculations: ```tql from { total: 1h + 30min, doubled: 30min * 2, half: 2h / 4, ratio: 30min / 1h, // 0.5 } ``` ```tql { total: 1h30min, doubled: 1h, half: 30min, ratio: 0.5, } ``` ### Time [Section titled “Time”](#time) Write dates and timestamps using the [ISO 8601 standard](https://en.wikipedia.org/wiki/ISO_8601): * Date only: `2024-10-03` * Full timestamp: `2024-10-03T14:30:00Z` * With timezone offset: `2024-10-03T14:30:00+02:00` Time points represent specific moments and support arithmetic with durations: ```tql from { start: 2024-01-01T00:00:00Z, one_day_later: start + 24h, one_hour_earlier: start - 1h, } ``` ```tql { start: 2024-01-01T00:00:00Z, one_day_later: 2024-01-02T00:00:00Z, one_hour_earlier: 2023-12-31T23:00:00Z, } ``` Calculating elapsed time is a common operation that converts two time points into a duration via subtraction: ```tql from { start: 2024-01-01T00:00:00Z, end: 2024-01-01T12:30:00Z, elapsed: end - start, } ``` ```tql { start: 2024-01-01T00:00:00Z, end: 2024-01-01T12:30:00Z, elapsed: 12h30min, } ``` ### IP and Subnet [Section titled “IP and Subnet”](#ip-and-subnet) The `ip` type handles both IPv4 and IPv6 addresses. * IPv4: `192.168.1.1`, `10.0.0.1` * IPv6: `::1`, `2001:db8::1` This also applies to subnets: both `10.0.0.0/8` and `2001:db8::/32` are valid subnets. IP addresses and subnets support membership and containment testing: ```tql let $ip = 192.168.1.100; let $network = 10.1.0.0/24; from { ip: $ip, network: $network, is_private: $ip in 192.168.0.0/16, is_loopback: $ip in 127.0.0.0/8, contains_ip: 10.1.0.5 in $network, contains_subnet: 10.1.0.0/28 in $network, } ``` ```tql { ip: 192.168.1.100, network: 10.1.0.0/24, is_private: true, is_loopback: false, contains_ip: true, contains_subnet: true, } ``` ### Secret [Section titled “Secret”](#secret) [Secrets](/explanations/secrets) protect sensitive values like authentication tokens and passwords. The `secret` type contains only a secret’s name, not its actual value, which is resolved asynchronously when needed. Create secrets using the [`secret`](/reference/functions/secret) function or pass string literals directly to operators that accept secrets: ```tql // Using managed secret auth_header = "Bearer " + secret("api-token") // Using format string (produces a secret) connection = f"https://{secret("user")}:{secret("pass")}@api.example.com" ``` Secrets support concatenation with `+` and can be used in format strings. When a format string contains a secret, the result is also a secret. Converting a secret to a string yields a masked value (`"***"`) to prevent accidental exposure. ### List [Section titled “List”](#list) TQL has *typed* lists, which means that the type of the elements in a list is fixed and must not change per element. Lists use brackets to sequence data. `[]` denotes the empty list. Specify items with comma-delimited expressions: ```tql let $ports = [80, 443, 8080] let $mixed = [1, 2+3, foo()] // Can contain expressions ``` Lists support indexing with `[]` and membership testing with `in`, with negative indices counting from the end of the list (`-1` refers to the last element): ```tql let $items = [10, 20, 30] from { first: $items[0], last: $items[-1], has_twenty: 20 in $items, } ``` ```tql { first: 10, last: 30, has_twenty: true, } ``` Use `?` for safe indexing that returns `null` instead of generating a warning: ```tql from { items: [1, 2], third: items[2]? else 0, } ``` ```tql { items: [ 1, 2, ], third: 0, } ``` The spread operator `...` expands lists into other lists: ```tql let $base = [1, 2] let $extended = [...$base, 3] // Results in [1, 2, 3] ``` ### Records [Section titled “Records”](#records) Records use braces to structure data. `{}` denotes the empty record. Specify fields using identifiers followed by a colon and an expression. Use quoted strings for invalid field names. For example: ```tql let $my_record = { name: "Tom", age: 42, friends: ["Jerry", "Brutus"], "detailed summary": "Jerry is a cat." // strings for invalid identifiers } ``` The spread operator `...` expands records into other records: Lifting nested fields ```tql from { type: "alert", context: { severity: "high", source: 1.2.3.4, } } this = {type: type, ...context} ``` ```tql { type: "alert", severity: "high", source: 1.2.3.4, } ``` Fields must be unique, and later values overwrite earlier ones. The spread operator `...` expands records: ```tql let $base = {a: 1, b: 2} from { extended: {...$base, c: 3}, } ``` ```tql { extended: { a: 1, b: 2, c: 3, }, } ``` ## Field Access [Section titled “Field Access”](#field-access) TQL provides multiple ways to access and manipulate fields within records and events. ### Basic Access [Section titled “Basic Access”](#basic-access) Use a single identifier to refer to a top-level field: ```tql from { name: "Alice", age: 30, } adult = age >= 18 ``` Chain identifiers with dots to access nested fields: ```tql from { user: { profile: { name: "Alice" } }, } username = user.profile.name ``` ```tql { user: { profile: { name: "Alice" } }, username: "Alice", } ``` ### The `this` Keyword [Section titled “The this Keyword”](#the-this-keyword) `this` references the entire top-level event: ```tql from { x: 1, y: 2, } z = this ``` ```tql { x: 1, y: 2, z: { x: 1, y: 2, }, } ``` You can also overwrite the entire event: ```tql this = {transformed: true, data: this} ``` ### Non-existent Fields [Section titled “Non-existent Fields”](#non-existent-fields) Trying to access a field that does not exist in an event will raise a warning and evaluate to `null`. ### Optional Access with `?` [Section titled “Optional Access with ?”](#optional-access-with) The optional field access operator (`?`) suppresses warnings when accessing non-existent fields: Processing events with optional fields ```tql from {event: "logon", user: {id: 123, name: "John Doe"}}, {event: "logon", user: {id: 456}}, {event: "logoff", user: {id: 123}} select event, user_id=user.id, name=user.name? ``` ```tql {event: "logon", user_id: 123, name: "John Doe"} {event: "logon", user_id: 456, name: null} // No warning for missing `user.name` {event: "logoff", user_id: 123, name: null} // No warning for missing `user.name` ``` Optional access also works on nested paths: ```tql from { user: {address: {city: "NYC"}}, } city = user.address?.city? // No warning if `address` or `address.city` do not exist. ``` ### Fallback with `else` [Section titled “Fallback with else”](#fallback-with-else) The `else` keyword provides default values when used with `?`: ```tql from \ { severity: 10, priority: null }, \ { severity: null, priority: null } severity_level = severity? else "unknown" // If `severity` is `null`, use `"unknown"` instead priority = priority? else 3 // if `priority` is `null`, default it to `3` ``` Without `else`, the `?` operator returns `null` when the field doesn’t exist. With `else`, you get a sensible default value instead: ```tql from { foo: 1, bar: 2, } select value = missing?, // null with_default = missing? else "default" // "default" ``` ### Indexing [Section titled “Indexing”](#indexing) Both lists and records support indexing operations to access their elements. #### List Indexing [Section titled “List Indexing”](#list-indexing) Access list elements using integral indices, starting with `0`: ```tql let $my_list = ["Hello", "World"] first = $my_list[0] // "Hello" second = $my_list[1] // "World" ``` Use `?` to handle out-of-bounds access: ```tql let $ports = [80, 443] third = $ports[2]? else 8080 // Fallback when index doesn't exist ``` #### Record Indexing [Section titled “Record Indexing”](#record-indexing) Bracket notation accesses fields with special characters or runtime values: ```tql let $answers = {"the ultimate question": 42} result = $answers["the ultimate question"] ``` Access fields based on runtime values: ```tql let $severity_to_level = {"ERROR": 1, "WARNING": 2, "INFO": 3} from { severity: "ERROR", } level = $severity_to_level[severity] // Dynamic field access ``` Indexing expressions (see next section below) support numeric indices for records: Accessing a field by position ```tql from { foo: "Hello", bar: "World", } select first_field = this[0] // "Hello" ``` ### Moving Fields [Section titled “Moving Fields”](#moving-fields) The `move` expression transfers a field’s value and removes the original field in one atomic operation. Use the `move` keyword in front of a field to relocate it as part of an assignment: ```tql from {foo: 1, bar: 2} qux = move bar + 2 ``` ```tql {foo: 1, qux: 4} // Note: bar is gone ``` Use `move` in assignments to avoid separate delete operations: ```tql // Clean approach new_field = move old_field // Instead of verbose new_field = old_field drop old_field ``` In addition to the `move` keyword, there exists a [`move`](/reference/operators/move) operator that is a convenient alternative when relocating multiple fields. For example, this sequence of assignments with the `move` keyword: ```tql x = move foo y = move bar z = move baz ``` can be rewritten succinctly with the [`move`](/reference/operators/move) operator: ```tql move x=foo, y=bar, z=baz ``` ### Metadata [Section titled “Metadata”](#metadata) Events carry both data and metadata. Access metadata fields using the `@` prefix. For instance, `@name` holds the name of the event. Currently, available metadata fields include `@name`, `@import_time`, and `@internal`. Future updates may allow defining custom metadata fields. ```tql from { event_name: @name, // The schema name import_time: @import_time, // When the event was imported } ``` ## Additional Operations [Section titled “Additional Operations”](#additional-operations) Beyond type-specific operations, TQL provides general-purpose operators for working with data. ### Unary Operations [Section titled “Unary Operations”](#unary-operations) TQL supports unary operators: * `-` for numbers and durations (negation) * `not` for boolean values (logical NOT) ```tql from { value: 42, flag: true, } negative = -value inverted = not flag ``` ```tql { value: 42, flag: true, negative: -42, inverted: false, } ``` ### Binary Operations [Section titled “Binary Operations”](#binary-operations) Binary operators work on two operands. The supported operations depend on the data types involved. #### Arithmetic Operations Summary [Section titled “Arithmetic Operations Summary”](#arithmetic-operations-summary) | Operation | Example | Behavior | | -------------- | ------- | -------------------------------- | | Addition | `a + b` | Type coercion to wider type | | Subtraction | `a - b` | Returns null on underflow | | Multiplication | `a * b` | Returns null on overflow | | Division | `a / b` | Returns null on division by zero | #### Time and Duration Arithmetic Summary [Section titled “Time and Duration Arithmetic Summary”](#time-and-duration-arithmetic-summary) | Operation | Result Type | Example | | --------------------- | ----------- | ----------------------- | | `time + duration` | `time` | `now() + 5min` | | `time - duration` | `time` | `timestamp - 1h` | | `time - time` | `duration` | `end_time - start_time` | | `duration + duration` | `duration` | `5min + 30s` | | `duration * number` | `duration` | `5min * 3` | | `duration / number` | `duration` | `1h / 2` | | `duration / duration` | `double` | `30min / 1h` → `0.5` | For detailed type coercion rules and more examples, see the specific type sections above. #### Comparison [Section titled “Comparison”](#comparison) All types support equality comparison (`==`, `!=`). Additionally, ordered types support relational comparisons (`<`, `<=`, `>`, `>=`): ```tql from { a: 5, b: 10, } set equal = a == b set not_equal = a != b set less = a < b set less_equal = a <= b set greater = a > b set greater_equal = a >= b ``` ```tql { a: 5, b: 10, equal: false, not_equal: true, less: true, less_equal: true, greater: false, greater_equal: false, } ``` **Comparison rules by type:** * **All types**: Can compare equality with themselves and with `null` * **Numeric types**: Can compare across different numeric types; ordered by magnitude * **Strings**: Compare lexicographically (dictionary order) * **IP addresses**: Ordered by their IPv6 bit pattern * **Subnets**: Ordered by their IPv6 bit pattern * **Times**: Chronologically ordered * **Durations**: Ordered by length #### Logical [Section titled “Logical”](#logical) Combine boolean expressions with `and` and `or`: ```tql where timestamp > now() - 1d and severity == "critical" where port == 22 or port == 3389 ``` #### Membership Testing (`in`) [Section titled “Membership Testing (in)”](#membership-testing-in) The `in` operator tests containment across different types: | Expression | Checks if… | | :-------------------- | :-------------------------------------- | | `value in list` | List contains the value | | `substring in string` | String contains the substring | | `ip in subnet` | IP address is within the subnet range | | `subnet in subnet` | First subnet is contained in the second | ```tql from { ip: 10.0.0.5, port: 443, message: "connection error", } in_private = ip in 10.0.0.0/8 is_https = port in [443, 8443] has_error = "error" in message ``` ```tql { ip: 10.0.0.5, port: 443, message: "connection error", in_private: true, is_https: true, has_error: true, } ``` To negate membership tests, use `not in` or `not (value in container)`. ### Operator Precedence [Section titled “Operator Precedence”](#operator-precedence) Operations follow standard precedence rules: | Precedence | Operators | Associativity | | ----------- | ---------------------------------------- | ------------- | | 1 (highest) | Method call, field access, `[]` indexing | - | | 2 | Unary `+`, `-` | - | | 3 | `*`, `/` | Left | | 4 | Binary `+`, `-` | Left | | 5 | `==`, `!=`, `<`, `<=`, `>`, `>=`, `in` | Left | | 6 | `not` | - | | 7 | `and` | Left | | 8 (lowest) | `or` | Left | Expressions like `1 - 2 * 3 + 4` follow these precedence and associativity rules. The expression evaluates as `(1 - (2 * 3)) + 4`. Example: `1 + 2 * 3` evaluates as `1 + (2 * 3)` = 7 ## Conditional Expressions [Section titled “Conditional Expressions”](#conditional-expressions) ### Python-style Conditionals [Section titled “Python-style Conditionals”](#python-style-conditionals) TQL uses Python-style conditional expressions, i.e., `x if condition else y` where `x`, `y`, and `condition` are expressions. Use conditionals in assignments and format strings: ```tql from { response_code: 200, success: true, } status = "OK" if response_code == 200 else "ERROR" message = f"Status: {'✓' if success else '✗'}" ``` Chaining is allowed but discouraged for readability: ```tql from { severity: "high", } priority = 1 if severity == "critical" else 2 if severity == "high" else 3 ``` ### Standalone `if` [Section titled “Standalone if”](#standalone-if) `if` acts as a guard, returning `null` when false: ```tql from { performance: "good", should_compute: false, } bonus = 1000 if performance == "excellent" // null otherwise result = now() if should_compute // null since should_compute is false ``` ```tql { performance: "good", should_compute: false, bonus: null, result: null, } ``` ### Standalone `else` [Section titled “Standalone else”](#standalone-else) `else` performs null coalescing: ```tql from { field: null, } value = field else "default" // Use "default" if field is null ``` ## Functions and Methods [Section titled “Functions and Methods”](#functions-and-methods) Functions and take positional and/or named arguments, producing a value as a result of their computation. Call **free functions** with parentheses and comma-delimited arguments: ```tql from { result: sqrt(16), rounded: round(3.7, 1), current: now(), } ``` Call **methods** using dot notation: ```tql from { text: " hello ", message: "world", } trimmed = text.trim() length = message.length() ``` ### Uniform Function Call Syntax (UFCS) [Section titled “Uniform Function Call Syntax (UFCS)”](#uniform-function-call-syntax-ufcs) TQL supports the [uniform function call syntax (UFCS)](https://en.wikipedia.org/wiki/Uniform_Function_Call_Syntax), which allows you to interchangeably call a function with at least one argument either as free function or method. For example, `length(str)` and `str.length()` resolve to the identical function call. The latter syntax is particularly suitable for function chaining, e.g., `x.f().g().h()` reads left-to-right as “start with `x`, apply `f()`, then `g()` and then `h()`,” compared to `h(g(f(x)))`, which reads “inside out.” * Free function ```tql from {input: " hello "} output = capitalize(trim(input)) ``` ```tql { input: " hello ", output: "Hello", } ``` * Method ```tql from {input: " hello "} output = input.trim().capitalize() ``` ```tql { input: " hello ", output: "Hello", } ``` Note the improved readability of function chaining: * Free function ```tql from {message: " HELLO world "} message = replace(to_lower(trim(message)), " ", "_") ``` ```tql { message: "hello_world", } ``` * Method ```tql from {message: " HELLO world "} message = message .trim() // Remove whitespace .to_lower() // Normalize case .replace(" ", "_") // Replace spaces ``` ```tql { message: "hello_world", } ``` For a comprehensive list of functions, see the [functions reference](/reference/functions). ## Advanced Expressions [Section titled “Advanced Expressions”](#advanced-expressions) ### Lambda Expressions [Section titled “Lambda Expressions”](#lambda-expressions) Some operators and functions accept **lambda expressions** of the form `arg => expr`: ```tql let $list = [1, 2, 3, 4, 5] let $data = [{value: 1}, {value: 2}] from { threshold: 3, } doubled = [1, 2, 3].map(x => x * 2) filtered = $list.where(x => x > threshold) transformed = $data.map(item => item.value * 100) ``` ```tql { threshold: 3, doubled: [ 2, 4, 6, ], filtered: [], transformed: [ 100, 200, ], } ``` The input gets an explicit name and the expression evaluates for each element. ### Pipeline Expressions [Section titled “Pipeline Expressions”](#pipeline-expressions) Some operators accept pipeline expressions as arguments, written with braces: ```tql every 10s { from_http "https://api.example.com/" select id, data } fork { to_hive "s3://bucket/path/", partition_by=[id], format="json" } ``` If the pipeline expression is the last argument, omit the preceding comma. Braces can contain multiple statements separated by newlines. ### Let Substitution [Section titled “Let Substitution”](#let-substitution) Reference previously defined [`let`](/explanations/language/statements#let) bindings using `$`-prefixed names: ```tql let $pi = 3.14159 let $radius = 5 from { area: $radius * $radius * $pi, } ``` ```tql { area: 78.53975, } ``` Evaluation Time Constants evaluate *once* at pipeline start and remain available throughout. ## Expression Evaluation [Section titled “Expression Evaluation”](#expression-evaluation) TQL expressions can be evaluated at different times: at pipeline start (constant) or per event (runtime). ### Constant Expressions [Section titled “Constant Expressions”](#constant-expressions) A **constant expression** evaluates to a constant when the pipeline containing it starts. Many pipeline operators require constant arguments: ```tql head 5 // Valid: 5 is constant head count // Invalid: count depends on events ``` Functions like `now()` and `random()` can be constant-evaluated: ```tql let $start_time = now() // Evaluated once at pipeline start where timestamp > $start_time - 1h ``` They are evaluated once at pipeline start, and the result is treated as a constant. ### Runtime Evaluation [Section titled “Runtime Evaluation”](#runtime-evaluation) Most expressions evaluate per event at runtime: ```tql // These evaluate for each event score = impact * likelihood is_recent = timestamp > now() - 5min formatted = f"Alert: {severity} at {timestamp}" ```

# Programs

TQL **programs** compose [statements](/explanations/language/statements) into complete data processing workflows that can execute. Valid TQL programs adhere to the following rules: 1. Adjacent operators must have identical types. 2. A pipeline must be **closed**, i.e., begin with void input and end with void output. ## Statement chaining [Section titled “Statement chaining”](#statement-chaining) You chain statements with either a newline (`\n`) or pipe symbol (`|`). We purposefully offer choice to cater to two primary styles: 1. Vertical structuring with newlines for full-text editing 2. Horizontal inline pipe composition for command-line usage Prefer the vertical approach for readability in files and documentation. Throughout this documentation, we only use the vertical style for clarity and consistency. Let’s juxtapose the two styles. Here’s a vertical TQL program: ```tql let $ports = [22, 443] from_file "/tmp/logs.json" where port in $ports select src_ip, dst_ip, bytes summarize src_ip, total=sum(bytes) ``` And here a horziontal one: ```tql let $ports = [22, 443] | from "/tmp/logs.json" | where port in $ports | select src_ip, dst_ip, bytes | summarize src_ip, total=sum(bytes) ``` In theory, you can combine pipes and newlines to write programs that resemble Kusto and similar languages. However, we discourage this practice because it can make the code harder to read and maintainespecially when adding nested pipelines that increase the level of indentation. ## Diagnostics [Section titled “Diagnostics”](#diagnostics) TQL’s diagnostic system is designed to give you insights into what happens during data processing. There exist two types of diagnostics: 1. **Errors**: Stop pipeline execution immediately (critical failures) 2. **Warnings**: Signal data quality issues but continue processing When a pipeline emits an error, it stops execution. Unless you configured the pipeline to restart on error, it now requires human intervention to resolve the issue and resume execution. Warnings do not cause a screeching halt of the pipeline. They are useful for identifying potential issues that may impact the quality of the processed data, such as missing or unexpected values. ## Pipeline nesting [Section titled “Pipeline nesting”](#pipeline-nesting) Operators can contain entire subpipelines that execute based on the operator’s semantics. For example, the [`every`](/reference/operators/every) operator executes its subpipeline at regular intervals: ```tql every 1h { from_http "api.example.com" select domain, risk context::update "domains", key=domain, value=risk } ``` You define subpipelines syntactically within a block of curly braces (`{}`). Some operators require that you define a closed (void-to-void) pipeline, whereas others exhibit parsing (bytes-to-events) or printing (events-to-bytes) semantics. ## Comments [Section titled “Comments”](#comments) Comments make implicit choices and assumptions explicit. They have no semantic effect and the compiler ignores them during parsing. TQL features C-style comments, both single and multi-line. ### Single-line comments [Section titled “Single-line comments”](#single-line-comments) Use a double slash (`//`) to comment until the end of the line. Here’s an example where a comment spans a full line: ```tql // the app only supports lower-case user names let $user = "jane" ``` Here’s an example where a comment starts in the middle of a line: ```tql let $users = [ "jane", // NB: also admin! "john", // Been here since day 1. ] ``` ### Multi-line comments [Section titled “Multi-line comments”](#multi-line-comments) Use a slash-star (`/*`) to start a multi-line comment and a star-slash (`*/`) to end it. Here’s an example where a comment spans multiple lines: ```tql /* * User validation logic * --------------------- * Validate user input against a set of rules. * If any rule fails, the user is rejected. * If all rules pass, the user is accepted. */ let $user = "jane" ``` ## Execution Model [Section titled “Execution Model”](#execution-model) TQL pipelines execute on a streaming engine that processes data incrementally. Understanding the execution model helps you write efficient pipelines and predict performance characteristics. Key execution principles: * **Stream processing by default**: Data flows through operators as it arrives * **Lazy evaluation**: Operations execute only when data flows through them * **Back-pressure handling**: Automatic flow control prevents memory exhaustion * **Network transparency**: Pipelines can span multiple nodes seamlessly ### Streaming vs blocking [Section titled “Streaming vs blocking”](#streaming-vs-blocking) Understanding operator behavior helps write efficient pipelines: **Streaming operators** process events incrementally: * [`where`](/reference/operators/where): Filters one event at a time * [`select`](/reference/operators/select): Transforms fields immediately * [`drop`](/reference/operators/drop): Removes fields as events flow **Blocking operators** need all input before producing output: * [`sort`](/reference/operators/sort): Must see all events to order them * [`summarize`](/reference/operators/summarize): Aggregates across the entire stream * [`reverse`](/reference/operators/reverse): Needs complete input to reverse order  Efficient: streaming operations first: ```tql from "large_file.json" where severity == "critical" // Streaming: reduces data early select relevant_fields // Streaming: drops unnecessary data sort timestamp // Blocking: but on reduced dataset ``` L Less efficient: blocking operation on full data: ```tql from "large_file.json" sort timestamp // Blocking: processes everything where severity == "critical" // Then filters ``` ### Constant vs runtime evaluation [Section titled “Constant vs runtime evaluation”](#constant-vs-runtime-evaluation) Understanding when expressions evaluate helps write efficient pipelines: Constants: evaluated once at pipeline start ```tql let $threshold = 1Ki let $start_time = 2024-01-15T09:00:00 // Would be now() - 1h in real usage let $config = { ports: [80, 443, 8080], networks: [10.0.0.0/8, 192.168.0.0/16], } // Runtime: evaluated per event from {bytes: 2Ki, timestamp: 2024-01-15T09:30:00}, {bytes: 512, timestamp: 2024-01-15T09:45:00}, {bytes: 3Ki, timestamp: 2024-01-15T10:00:00} where bytes > $threshold // Constant comparison where timestamp > $start_time // Constant comparison current_time = 2024-01-15T10:30:00 // Would be now() in real usage age = current_time - timestamp // Runtime calculation ``` ### Network transparency [Section titled “Network transparency”](#network-transparency) TQL pipelines can span network boundaries seamlessly. For example, the [`import`](/reference/operators/import) operator implicitly performs a network connection based on where it runs. If the `tenzir` binary executes the pipeline, the executor establishesa transparent network connection. If the pipeline runs within a node, the executor passes the data directly to the next operator in the same process.

# Statements

TQL programs are a sequence of statements. Operator statements perform various actions on data streams. Each operator statement can be thought of as a modular unit that processes data and can be combined with other operators to create complex dataflows. [Statements](/explanations/language/statements) provide control and structure with bindings, operators, assignments, and control-flow primitives. ## Operator [Section titled “Operator”](#operator) Operator statements consist of the operator name, followed by an arbitrary number of arguments. Arguments are delimited by commas and may optionally be enclosed in parentheses. If the last argument is a pipeline expression, the preceding comma can be omitted for brevity. Arguments can be of two kinds: * **Positional**, where the order matters * **Named**, where each argument is explicitly associated with a parameter name. Additionally, arguments can be either: * **Required**, meaning they must be provided * **Optional**, which means they do not necessarily need to be provided, usually meaning they have a default value. Finally, some operators require [constant arguments](/explanations/language/expressions/#constant-expressions), while others can take expressions, which are evaluated per event. ```tql select foo, bar.baz drop qux head 42 sort abs(x) ``` Operators read, transform, and write data: ```tql where src_endpoint.port in $critical_ports ``` Operators have an *upstream* and *downstream* type, which can be: * **void**: No data (used at pipeline boundaries) * **bytes**: Unstructured binary data (files, network streams) * **events**: Structured, typed records (the primary data model) The diagram below illustrates the cross-product of upstream and downstream types: ![Upstream and Downstream Types](/_astro/operator-table.ChkIKyOz_19DKCs.svg) Here are visual examples that illustrate the upstream and downstream operator types. ```tql from "/path/to/file.json" where src_ip in 10.0.0.0/8 to "s3://bucket/dir/file.parquet" ``` This pipeline consists of three operators: ![Operator Composition Example 1](/_astro/operator-composition-example-1.CA4c2kgf_19DKCs.svg) Let’s break it down: 1. [`from`](/reference/operators/from): A void-to-events input operator that reads events from a URI. 2. [`where`](/reference/operators/where): An events-to-events transformation operator that filters events matching a predicate. 3. [`to`](/reference/operators/to): An events-to-void output operator the writes to the specified URI. The [`from`](/reference/operators/from) and [`to`](/reference/operators/to) operators perform a bit “magic” in that they also infer the format of the data being read or written, i.e., JSON due to the `.json` extension and Parquet due to the `.parquet` extension. You can also write the specific operators for these operations yourself: ```tql load_kafka "topic" read_ndjson select host, message write_yaml save_zmq "tcp://1.2.3.4" ``` ![Operator Composition Example 2](/_astro/operator-composition-example-2.C_LbiE3g_19DKCs.svg) Here, we use a separate set of operators that go through bytes explicitly. Let’s break it down as well: 1. [`load_kafka`](/reference/operators/load_kafka): A void-to-events input operator that reads from a Kafka topic. 2. [`read_ndjson`](/reference/operators/read_ndjson): An bytes-to-events transformation operator (aka. *parser*) that reads newline-delimited JSON. 3. [`select`](/reference/operators/select): An events-to-events transformation operator that selects specific fields from events. 4. [`write_yaml`](/reference/operators/write_yaml): An events-to-bytes transformation operator that turns events to YAML foramt. 5. [`save_zmq`](/reference/operators/save_zmq): A bytes-to-void output operator that writes bytes to a ZeroMQ socket. ## Assignment [Section titled “Assignment”](#assignment) An assignment statement in TQL is structured as `<place> = <expression>`, where `<place>` typically refers to a field or item of a list. If the specified place already exists, the assignment will overwrite its current value. If it does not exist, a new field will be created. The `<place>` can also reference a field path. For example, the statement `foo.bar = 42` assigns the value 42 to the field `bar` within the record `foo`. If `foo` is not a record or does not exist before, it will be set to a record containing just the field `bar`. ```tql category_name = "Network Activity" type_uid = class_uid * 100 + activity_id traffic.bytes_out = event.sent_bytes ``` Assignments modify fields: ```tql risk_score = bytes / 1Ki * severity_weight ``` When you write an assignment outside an explicit operator context, it implicitly uses the [`set`](/reference/operators/set) operator: ```tql severity = "high" // ...is actually shorthand for: set severity = "high" ``` This design keeps pipelines concise while maintaining clarity about what’s happening. ## `let` [Section titled “let”](#let) The `let` statement binds a constant to a specific name within the pipeline’s scope. The syntax for a `let` statement is `let $<identifier> = <expression>`. For instance, `let $meaning = 42` creates a constant `$meaning` that holds the value 42. More complex expressions can also be assigned, such as `let $start = now() - 1h`, which binds `$start` to a value representing one hour before the pipeline was started. Constants defined with `let` can be referenced in subsequent statements, including other `let` statements. For example, `let $end = $start + 30min` can be used to define `$end` depending on the value of `$start`. ```tql let $meaning = 42 let $start = now() - 1h let $end = $start + 30min ``` A `let` statement introduces a constant that gets substituted during [expression evaluation](/explanations/language/expressions/#let-substitution). ## `if` [Section titled “if”](#if) The `if` statement is a primitive designed to route data based on a predicate. Its typical usage follows the syntax `if <expression> { … } else { … }`, where two subpipelines are specified within the braces. When its expression evaluates to `true`, the first pipeline processes the event. Conversely, when it evaluates to `false`, it is routed through the second one. After the `if` statement the event flow from both pipelines is joined together. The `else` clause can be omitted, resulting in the syntax `if <expression> { … }`, which has the same behavior as `if <expression> { … } else {}`. Additionally, the `else` keyword can be followed by another `if` statement, allowing for chained `if` statements. This chaining can be repeated, enabling complex conditional logic to be implemented. ```tql if score < 100 { severity = "low" drop details } else if score < 200 { severity = "medium" } else { severity = "high" } ``` The `if` statement allows for branching into different statements: ```tql if src_ip.is_private() { zone = "internal" } else { zone = "external" } ```

# Type System

TQL balances type safety with practical flexibility: * **Strong typing prevents errors**: Can’t accidentally compare IPs as strings * **Automatic inference**: Types detected from data, no declarations needed * **Null semantics**: Every type is nullable (real data has gaps) * **Domain operations**: Types come with relevant methods Type safety in action ```tql where src_ip in 10.0.0.0/8 // Type-checked subnet membership where duration > 5min // Type-checked duration comparison where timestamp.hour() >= 9 // Extract hour from timestamp where severity?.to_upper() == "HIGH" // Safe navigation with type conversion ``` ## Available Types [Section titled “Available Types”](#available-types) The diagram below illustrates the type system at a glance: ![Type System](/_astro/type-system.BJr7NUfp_19DKCs.svg) Tenzir’s type system is a superset of JSON: Every valid JSON object is a valid Tenzir value, but there also additional types available, such as `ip` and `subnet`. ### Basic Types [Section titled “Basic Types”](#basic-types) Basic types are stateless types with a static structure. The following basic types exist: | Type | Description | Example Expression Literal | | ---------- | ------------------------------------- | -------------------------------------- | | `null` | Denotes an absent or invalid value | `null` | | `bool` | A boolean value | `true`, `false` | | `int64` | A 64-bit signed integer | `42`, `-100`, `1k`, `2Ki` | | `uint64` | A 64-bit unsigned integer | `42`, `100`, `1M`, `2Gi` | | `double` | A 64-bit double (IEEE 754) | `3.14`, `-0.5`, `1.23e-4`, `2.5k` | | `duration` | A time span (nanosecond granularity) | `5s`, `10min`, `1h`, `2d`, `100ms` | | `time` | A time point (nanosecond granularity) | `2024-01-15T10:30:00`, `2024-01-15` | | `string` | A UTF-8 encoded string | `"hello"`, `"world"`, `r"C:\path"` | | `blob` | An arbitrary sequence of bytes | `b"\x00\x01\x02"`, `b"raw bytes"` | | `ip` | An IPv4 or IPv6 address | `192.168.1.1`, `::1`, `fe80::1` | | `subnet` | An IPv4 or IPv6 subnet | `10.0.0.0/8`, `192.168.0.0/16`, `::/0` | | `secret` | A secret value | `secret("API_KEY")` | #### Secrets [Section titled “Secrets”](#secrets) The `secret` type is a special type created by the [`secret`](/reference/functions/secret) function. Secrets can only be used as arguments for operators that accept them and only support a limited set of operations, such as concatenation. See the [explanation page for secrets](/explanations/secrets) for more details. ### Complex Types [Section titled “Complex Types”](#complex-types) Complex types are stateful types that carry additional runtime information. #### List [Section titled “List”](#list) The `list` type is an ordered sequence of values with a fixed element type. Lists have zero or more elements. #### Record [Section titled “Record”](#record) The `record` type consists of an ordered sequence *fields*, each of which have a name and type. Records must have at least one field. The field name is an arbitrary UTF-8 string. The field type is any Tenzir type. ## Optionality [Section titled “Optionality”](#optionality) All types are optional in that there exists an additional `null` data point in every value domain. Consequently, Tenzir does not have a special type to indicate optionality. ## Attributes [Section titled “Attributes”](#attributes) Every type has zero or more **attributes** that are free-form key-value pairs to enrich types with custom semantics. ## Why TQL has more types [Section titled “Why TQL has more types”](#why-tql-has-more-types) TQL extends JSON’s type system because **security and network data has specific patterns** that generic JSON cannot express efficiently or safely. Example: ```tql from { // JSON-compatible types event_id: 42, // number → int64 is_alert: true, // bool message: "Connection established", // string tags: ["network", "established"], // array → list metadata: { // object → record source: "firewall", version: 2 }, // TQL-specific types for security/network data src_ip: 192.168.1.100, // IP address network: 192.168.1.0/24, // subnet timestamp: 2024-01-15T10:30:00, // time point duration: 250ms, // duration api_key: secret("sk_live_..."), // secret (never logged) packet_data: b"\x00\x01\x02" // blob (binary data) } ``` ### Why IP address types? [Section titled “Why IP address types?”](#why-ip-address-types) IP addresses aren’t just strings—they have structure and semantics: ✅ Native IP type operations: ```tql where src_ip in 10.0.0.0/8 // Clear, type-safe subnet check where src_ip.is_private() // Built-in IP operations where src_ip > 192.168.1.1 // Ordered comparison ``` ❌ Without IP type (plain strings): ```tql where src_ip.starts_with("10.") // Error-prone, doesn't handle all cases where regex_match(src_ip, "^(10\\.|192\\.168\\.|...)") // Complex and slow // No way to do proper IP comparisons or subnet matching ``` ### Why duration and time types? [Section titled “Why duration and time types?”](#why-duration-and-time-types) Time calculations are fundamental to log analysis: ✅ Native time type operations: ```tql where timestamp > now() - 1h // Intuitive time arithmetic where response_time > 500ms // Clear units let $window = 5min // Self-documenting ``` ❌ Without time types (using milliseconds): ```tql where timestamp_ms > current_time_ms - 3600000 // What unit is this? where response_time_ms > 500 // Milliseconds? Seconds? let $window = 300000 // Five minutes... or is it? ``` ### Why subnet types? [Section titled “Why subnet types?”](#why-subnet-types) Network segmentation is core to security analysis: ✅ Subnet type operations: ```tql let $internal = 10.0.0.0/8 let $dmz = 192.168.100.0/24 where src_ip in $internal and dst_ip not in $internal // Outbound traffic ``` ❌ Without subnet type: ```tql // Would need complex IP range calculations and bit manipulation // Error-prone and hard to maintain ``` ### Why binary blob types? [Section titled “Why binary blob types?”](#why-binary-blob-types) Security and network data often contains raw binary content that needs special handling: ✅ Blob type for binary data: ```tql // Handle packet captures, certificates, encrypted payloads let $open_packet = b"\x00\x01\x02\x03" // Blob literals packet_data = decode_base64(encoded_packet) // Returns blob where packet_data != $open_packet // Comparing bytes directly payload_hex = encode_hex(packet_data) // blob → hex string hash = hash_sha256(packet_data) // Direct hashing of binary ``` ❌ Without blob type (using strings): ```tql // Binary data corrupts when treated as text packet_data = "\xFF\xFE\x00\x00" // Invalid UTF-8 sequence // No safe way to handle non-UTF8 sequences // Length calculations are wrong for multi-byte encodings ``` Blobs preserve exact byte sequences for forensics, packet analysis, and cryptographic operations. ### Why secret types? [Section titled “Why secret types?”](#why-secret-types) Credentials and sensitive data need protection from accidental exposure: ✅ Secret type for sensitive values: ```tql // Secrets are never logged or displayed let $api_key = secret("API_TOKEN") auth_header = f"Bearer {$api_key}" // Use secrets without exposing them http "api.example.com", headers={Auth: $api_key} // Use secret safely to "debug.json" // Secret value is not written to file ``` ❌ Without secret type (using strings): ```tql // Dangerous: credentials visible in logs api_key = "sk_live_abc123..." // Shows up in debug output // No protection against exposure to "debug.json" // Oops, secret written to file ``` The secret type ensures sensitive data is never accidentally exposed in logs, outputs, or debugging. ## Comparison to Arrow [Section titled “Comparison to Arrow”](#comparison-to-arrow) All Tenzir types have a lossless mapping to [Arrow](http://arrow.apache.org) types, however, not all Arrow types have a Tenzir equivalent. As a result, it is currently not yet possible to import arbitrary Arrow data. In the future, we plan to extend our support for Arrow-native types and also offer conversion options for seamless data handover. Tenzir has a few domain-specific types that map to Arrow [extension types](https://arrow.apache.org/docs/format/Columnar.html#extension-types). These are currently `enum`, `ip`, and `subnet`. Tenzir and Arrow attach type metadata to different entities: Tenzir attaches metadata to a type instance, whereas Arrow attaches metadata to a schema or record field.

# Packages

A **package** is a 1-click deployable unit that implements a specific use case. It contains pipelines, operators, contexts, examples, and tests. A templating mechanism makes packages customizable for a variety of deployment scenarios. ## Anatomy of a package [Section titled “Anatomy of a package”](#anatomy-of-a-package) A package comes in the form of a directory with the following structure: Let’s discuss each component in detail. ### `examples`: Snippets to run [Section titled “examples: Snippets to run”](#examples-snippets-to-run) The `examples` directory contains self-contained code snippets that demonstrate how to use the package. These snippets exemplify the package’s features and provide runnable TQL code that users can execute after installing the package. You can run these examples with a single click from [app.tenzir.com](https://app.tenzir.com). ### `operators`: User-defined operators (UDOs) [Section titled “operators: User-defined operators (UDOs)”](#operators-user-defined-operators-udos) The `operators` directory contains **user-defined operators (UDOs)**: reusable building blocks that you can use in your pipelines. Tenzir names operators using the convention `<package>::[dirs...]::<basename>`. In the example above, the package defines `pkg::ocsf::foo`, `pkg::ocsf::bar`, and `pkg::map`. You can use these operators in any pipeline: ```tql from_file "sample.json" pkg::ocsf::map // 👈 user-defined operator (UDO) defined in /pkg/ocsf/map.tql to_file "ocsf.json" ``` The `tests/` directory can include tests for these operators. ### `pipelines`: End-to-end deployable TQL [Section titled “pipelines: End-to-end deployable TQL”](#pipelines-end-to-end-deployable-tql) The `pipelines` directory contains fully deployable TQL pipelines. Unlike UDOs, pipelines are complete units that must begin with an [input operator](/explanations/architecture/pipeline) and end with an [output operator](/explanations/architecture/pipeline). These pipelines often use UDOs defined in the same package. You can configure pipelines using frontmatter at the beginning of the TQL file. The following options are available: * `restart-on-error`: Configures automatic restart behavior when the pipeline encounters an error. By default, pipelines stop running and show an error state. This option causes pipelines to restart automatically instead. * Omit the option, or set it to `null` or `false` to disable automatic restarts. * Set it to `true` to enable restarts with a default delay of 1 minute. * Set it to a valid duration to enable restarts with a custom delay. * `disabled`: Set to `true` to disable the pipeline. Defaults to `false`. * `unstoppable`: Set to `true` to make the pipeline run automatically and indefinitely. You cannot pause or stop unstoppable pipelines manually. If they complete, they end up in a failed state. If you enable `restart-on-error`, they restart after the specified duration. Defaults to `false`. Example: ```tql --- restart-on-error: 1m disabled: false unstoppable: true --- // TQL here ``` ### `tests`: Integration tests [Section titled “tests: Integration tests”](#tests-integration-tests) The `tests` directory contains deterministic integration tests, primarily for UDOs. These tests leverage the [Test Framework](/reference/test-framework) to verify that operators behave correctly. ### `package.yaml`: Metadata [Section titled “package.yaml: Metadata”](#packageyaml-metadata) The `package.yaml` file serves as the **package manifest**. It contains metadata about a package and acts as a marker to identify a directory as a package. The file is required for every package. #### Package description [Section titled “Package description”](#package-description) The beginning of `package.yaml` provides descriptive metadata: ```yaml # The unique ID of the package. (required) id: example # The display name of the package and a path to an icon for the package. name: Example package_icon: https://github.com/tenzir.png # The display name of the package author and a path to a profile picture. author: Tenzir author_icon: https://github.com/tenzir.png # A user-facing description of the package. description: | **Lorem ipsum** dolor sit amet, consectetur adipiscing elit. Nullam suscipit lacus felis, ac lacinia nibh pretium ut. Curabitur congue aliquam neque. Vivamus in magna non turpis malesuada volutpat ut a felis. Ut lorem eros, vulputate eget finibus ut, posuere sed leo. Vestibulum porta laoreet venenatis. Curabitur aliquet semper sem, et tincidunt metus cursus at. Nulla dapibus nibh vel faucibus commodo. Sed euismod eu sapien ut dictum. Phasellus tincidunt venenatis semper. ``` #### Inputs [Section titled “Inputs”](#inputs) The `inputs` section contains template variables that Tenzir replaces when you install the package. This allows the package definition to remain independent of the deployed environment. ```yaml # Define user inputs to customize the package installation. inputs: # Every input must have a unique id. refresh-rate: # A user-facing name for the input (required). name: Refresh Rate # A user-facing description of the input. description: | The interval at which we refresh our example context. Defaults to refreshing every second. # An (optional) default value for the input. The input is required if there # is no input value. default: 1s ``` You can reference inputs in pipeline and example definitions, and in context arguments using the syntax `{{ inputs.input-name }}`. Tenzir replaces these references with their configured values when installing the package. For example, with the input configured as above, the pipeline `every {{ inputs.refresh-rate }} { version }` would print the version once per second by default. To write double curly braces literally, use the syntax `{{ '{{' }}` to produce the literal string enclosed inside the single quotes. ### Contexts [Section titled “Contexts”](#contexts) The `contexts` section defines contexts for [enrichment](/explanations/enrichment). Here is an example context definition: ```yaml # Define any number of contexts. contexts: # A unique name for the context that's used in the context::* operators to # refer to the context. example: # The type of the context (required). type: lookup-table # An optional user-facing description of the context. description: | **Lorem ipsum** dolor sit amet, consectetur adipiscing elit. Nullam suscipit lacus felis, ac lacinia nibh pretium ut. Curabitur congue aliquam neque. Vivamus in magna non turpis malesuada volutpat ut a felis. Ut lorem eros, vulputate eget finibus ut, posuere sed leo. Vestibulum porta laoreet venenatis. Curabitur aliquet semper sem, et tincidunt metus cursus at. Nulla dapibus nibh vel faucibus commodo. Sed euismod eu sapien ut dictum. Phasellus tincidunt venenatis semper. # Arguments for creating the context, depending on the type. Refer to the # documentation of the individual context types to see the arguments they # require. Note that changes to these arguments do not apply to any # contexts that were previously created. args: {} # Disables the context. disabled: false ``` ## Configuration during installation [Section titled “Configuration during installation”](#configuration-during-installation) During installation, Tenzir merges the package definition with a configuration object. This can happen in three ways: 1. In the [Tenzir Library](https://app.tenzir.com/library), you provide inputs that Tenzir converts into a `config` object. 2. Using the [`package::add`](/reference/operators/package/add) operator, you construct a `config` record explicitly. 3. Using IaC-style installation, you provide a `config.yaml` next to the `package.yaml` manifest. Refer to the [package installation guide](/guides/basic-usage/install-a-package) for details on how each method works. config.yaml ```yaml # The equivalent of `package::add inputs={...}`. inputs: filename: /opt/example/data.tsv ``` Tenzir replaces [inputs](/explanations/packages/#inputs), such as `from_file "{{ inputs.filename }}"`, with their configured values when installing a package. You must explicitly provide values for inputs that do not have a default value by specifying them in your `config.yaml`: config.yaml ```yaml inputs: filename: /opt/example/data.tsv ```

# Secrets

Operators accept secrets as parameters for sensitive values, such as authentication tokens, passwords, or even URLs. ## Usage in TQL [Section titled “Usage in TQL”](#usage-in-tql) You can use secret values only with operators that accept secrets. Operators generally do not document that they accept a secret, but they will accept secrets where appropriate. You have two ways to pass an argument to an operator that expects a secret. The following examples use the [`to_splunk`](/reference/operators/to_splunk) operator, which expects a HEC-token for authentication: * Provide a plain `string` ([Ad-hoc Secret](#ad-hoc-secrets)): ```tql to_splunk "https://localhost:8088", hec_token="my-plaintext-token" ``` This creates a “secret” containing the string literal `my-plaintext-token`. * Use the [`secret`](/reference/functions/secret) function ([Managed Secret](#managed-secrets)): ```tql to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token") ``` The operator fetches the secret named `splunk-hec-token` to authenticate with the Splunk endpoint. ## The `secret` type [Section titled “The secret type”](#the-secret-type) Tenzir’s [type system](/explanations/language/types) includes secrets as a special type. You can access secrets only with the [`secret`](/reference/functions/secret) function. ### Internals [Section titled “Internals”](#internals) A value of type `secret` contains only the secret’s name, not the secret value itself. When a pipeline operator uses a secret, it resolves the name asynchronously. For ad-hoc secrets created from a string literal, name and value of the secret are identical, so no lookup occurs. ### Supported Operations [Section titled “Supported Operations”](#supported-operations) #### Concatenation [Section titled “Concatenation”](#concatenation) You can concatenate secrets with other secrets or strings using the `+` operator: ```tql auth = "Bearer " + secret("my-secret") url = $base_url + secret("user-name") + ":" + secret("password") ``` #### Format Strings [Section titled “Format Strings”](#format-strings) Secrets can be used in [format strings](/explanations/language/expressions#format-strings-f-strings). Unlike other types, which create strings with values formatted as-if using the `string` function, a format string containing a secret yields a secret. The above concatenations can be written using format strings, like so: ```tql auth = f"Bearer {secret("my-secret")}" url = f"{$base_url}{secret("user-name")}:{secret("password")}" ``` A format string will turn secrets nested in a structured value (`record`, `list`) into the string `"***"`. #### Encoding & Decoding [Section titled “Encoding & Decoding”](#encoding--decoding) In general, Tenzir does not assume that a secret is valid UTF-8, although most operators will make that constraint. Conversely some secret stores may require your secrets to be UTF-8, while some operators expect a binary secret. To bridge this gap, you can base64-decode secrets using the [`decode_base64`](/reference/functions/decode_base64) function and base64-encode them using the [`encode_base64`](/reference/functions/encode_base64) function: ```tql let $binary_secret = secret("my-encoded-secret").decode_base64() ``` You can also encode (or decode) the result of a concatenation or format string, which is useful for some APIs: ```tql let $headers = { auth: f"{secret("user")}:{secret("password")}".encode_base64() } ``` #### Turning Secrets into Strings [Section titled “Turning Secrets into Strings”](#turning-secrets-into-strings) You cannot turn a secret into a string. Any such attempt will simply produce the string `"***"`. #### Python [Section titled “Python”](#python) Since secrets can also be values in a pipeline, they can also be passed *through* the [`python`](/reference/operators/python) operator. You must not modify a secret in the [`python`](/reference/operators/python) operator. ## Ad-hoc Secrets [Section titled “Ad-hoc Secrets”](#ad-hoc-secrets) **Ad-hoc secrets** are secrets implicitly created from a `string` within [TQL](/explanations/language). This happens when you provide a `string` to an operator that expects a `secret`. Providing plain string values can help when you develop pipelines and do not want to add the secret to the configuration or a secret store. This approach is also useful for arguments that you do not consider a secret, so you don’t have to create a managed secret. However, secrets created from plain `string`s do not enjoy the same security as managed secrets. Their value appears directly in the TQL pipeline definition, as well as in the compiled and executed representation. As such, the Tenzir Node may persist the value. ## Managed Secrets [Section titled “Managed Secrets”](#managed-secrets) Managed secrets are identified by their name and can come from the following sources, in descending precedence: 1. The environment of the Tenzir Node 2. The configuration of the Tenzir Node 3. The Tenzir Platform secret store for the workspace the Tenzir Node belongs to ![Resolution](/_astro/secret-resolution.a_JW2CI2_19DKCs.svg) You use a managed secrets value using the [`secret`](/reference/functions/secret). The Tenzir Node looks up the secret’s actual value only when an operator requires it. It first checks the config, with environment variables taking precedence over configuration file entries. If the secret is not found there, a request is made to the Tenzir Platform. If the value is transferred over any network connection, it additionally be encrypted using [ECIES](https://en.wikipedia.org/wiki/Integrated_Encryption_Scheme) with a one-time, per-secret key. The value remains encrypted throughout the transfer until the final usage site. A `tenzir` client process can use managed secrets only if it is able to connect to a Tenzir Node. ### Configuration Secrets [Section titled “Configuration Secrets”](#configuration-secrets) You can specify secrets in the `tenzir.yaml` config file, under the path `tenzir.secrets`: tenzir.yaml ```yaml tenzir: secrets: # Add your secrets here. geheim: 1528F9F3-FAFA-45B4-BC3C-B755D0E0D9C2 ``` Since you can also set Tenzir’s configuration options as environment variables, you can define secrets in the environment as well. The above secret could also be defined via the environment variable `TENZIR_SECRETS__GEHEIM`. An environment variable takes precedence over an equivalent key in the configuration file. See the [configuration reference](/reference/node/configuration) for more details. Tenzir hides the `tenzir.secrets` section from the [`config()`](/reference/functions/config) function. ### Platform Secrets [Section titled “Platform Secrets”](#platform-secrets) The Tenzir Platform stores a separate set of secrets for every workspace. All Tenzir Nodes connected to that workspace can access these secrets. Read about how to configure the platform secret store in the [guides section](/guides/platform-setup/configure-secret-store#configuring-the-platform-secret-store). #### External Secret Stores [Section titled “External Secret Stores”](#external-secret-stores) You can configure the Tenzir Platform to provide access to secrets stored in an external secret store instead of using it own store. This access is read-only. Read more about how to configure an external secret store in the [guides section](/guides/platform-setup/configure-secret-store#configuring-external-secret-stores). ## Legacy Model [Section titled “Legacy Model”](#legacy-model) You can use the configuration option `tenzir.legacy-secret-model` to change the behavior of the `secret` function so that it returns a `string` instead of a `secret`. When you use the legacy model, you can only use secrets from the Tenzir Node’s configuration. You cannot use secrets from the Tenzir Platform’s secret store. We do not recommend enabling this option. It exists as a transition option and will be deprecated and removed in some future version. ## Security Design [Section titled “Security Design”](#security-design) We designed secrets to protect against *accidentally* compromising the secret value by revealing it in a log file, showing it in a UI element, or committing it to a code repository as part of a pipeline. However, anyone with access to a workspace can access secret values if sufficiently motivated. For example, you could use a simple HTTP API that echoes requests to extract secret values: ```tql from_http "echo.api.com", headers = { leaked: secret("key") }, metadata_field=metadata select metadata.headers.leaked ``` ```tql { leaked: "secret-value" } ``` ### Secrets in Diagnostics [Section titled “Secrets in Diagnostics”](#secrets-in-diagnostics) We take care not to leak managed secrets in our diagnostic messages. However, numerous of our integrations rely on third party libraries. Those libraries may produce error messages which are outside of our control, but that we forward to the user to help understand an issue. As a remedy for this, an operator censors the values of managed secrets it has used in all diagnostics. Please note that we only censor an occurrence of the full value, not parts of it. This means that a third party diagnostic forwarded to the user may still contain part of a secret value.