<SYSTEM>Integrations</SYSTEM>

# Overview

Tenzir integrates with the services from [Amazon Web Services (AWS)](https://aws.amazon.com) listed below. ## Configuration [Section titled “Configuration”](#configuration) To interact with AWS services, you need to provide appropriate credentials. This defaults to using AWS’s [default credentials provider chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). Make sure to configure AWS credentials for the same user account that runs `tenzir` and `tenzir-node`. The AWS CLI creates configuration files for the current user under `~/.aws`, which can only be read by the same user account. The `tenzir-node` systemd unit by default creates a `tenzir` user and runs as that user, meaning that the AWS credentials must also be configured for that user. The directory `~/.aws` must be readable for the `tenzir` user. If a config file `<prefix>/etc/tenzir/plugin/$PLUGIN.yaml` or `~/.config/tenzir/plugin/$PLUGIN.yaml` exists, it is always preferred over the default AWS credentials. Here, `$PLUGIN` is the Tenzir plugin name, such as `s3` or `sqs`. The configuration file must have the following format: ```yaml access-key: your-access-key secret-key: your-secret-key session-token: your-session-token # optional ```

# MSK

[Amazon Managed Streaming for Apache Kafka (Amazon MSK)](https://aws.amazon.com/msk/) is a streaming data service that manages Apache Kafka infrastructure and operations, making it easier for developers and DevOps managers to run Apache Kafka applications and Apache Kafka Connect connectors on AWS without becoming experts in operating Apache Kafka. ![Amazon MSK](/_astro/msk.BWTbNq0e_19DKCs.svg) ## Sending and Receiving [Section titled “Sending and Receiving”](#sending-and-receiving) Tenzir’s Kafka connectors [`load_kafka`](/reference/operators/load_kafka) and [`save_kafka`](/reference/operators/save_kafka) can send and receive events from Amazon MSK Clusters. ## Authentication [Section titled “Authentication”](#authentication) Provisioned MSK Clusters support different authentication mechanisms such as mTLS, SASL/SCRAM, IAM etc. However Serverless MSK instances currently only support IAM Authentication. The [`load_kafka`](/reference/operators/load_kafka) and [`save_kafka`](/reference/operators/save_kafka) operators can authenticate with MSK using AWS IAM by simply specifying the `aws_iam` option with a record of configuration values such as: ```tql load_kafka "kafkaesque-data", aws_iam={region: "eu-west-1"} ``` The above pipeline will try to fetch credentials from [various different locations](/reference/operators/load_kafka#aws_iam--record-optional) including the Instance Metadata Services. This means you can attach a role with the necessary permissions directly to an EC2 instance and Tenzir will automatically pick it up. ### Assuming roles [Section titled “Assuming roles”](#assuming-roles) Roles can also be assumed by giving the `assume_role` parameter to the `aws_iam` option. ```tql save_kafka "topic", aws_iam={ region: "eu-west-1", assume_role: "arn:aws:iam::1234567890:role/my-msk-role" } ``` The above pipeline attempts to fetch temporary credentials from Amazon STS for the given ARN. ### Example [Section titled “Example”](#example) #### Collecting High Severity OCSF events from MSK [Section titled “Collecting High Severity OCSF events from MSK”](#collecting-high-severity-ocsf-events-from-msk) The following pipeline reads OCSF events from MSK, assuming the role referenced by the provided ARN. The incoming data is then filtered for severity and sent to Splunk clusters in a load balanced fashion. ```tql let $endpoints = ["indexer-1-url", "indexer-2-url"] load_kafka "ocsf-events", aws_iam={region: "us-east-2", assume_role: "arn"} read_json where severity_id >= 4 // High and above load_balance $endpoints { to_splunk $endpoints, hec_token=secret("SPLUNK_TOKEN") } ```

# S3

[Amazon Simple Storage Service (S3)](https://aws.amazon.com/s3/) is an object storage service. Tenzir can treat it like a local filesystem to read and write files. ![S3](/_astro/s3.iRFUIJJn_19DKCs.svg) ## Configuration [Section titled “Configuration”](#configuration) Follow the [standard configuration instructions](/integrations/amazon) to authenticate with your AWS credentials. ## Examples [Section titled “Examples”](#examples) ### Write to an S3 bucket [Section titled “Write to an S3 bucket”](#write-to-an-s3-bucket) ```tql from {foo: 42} to "s3://my-bucket/path/to/file.json.gz" ``` ### Read from an S3 bucket [Section titled “Read from an S3 bucket”](#read-from-an-s3-bucket) ```tql from "s3://my-bucket/path/to/file.json.gz" ``` ```tql {foo: 42} ```

# Security Lake

[Amazon Security Lake](https://aws.amazon.com/security-lake/) is a centralized security data lake service that collects and stores security data in the Open Cybersecurity Schema Framework (OCSF) format. ![Amazon security Lake](/_astro/basic.CxkCu6X6_19DKCs.svg) Tenzir sends events to Amazon Security Lake using the [`to_amazon_security_lake`](/reference/operators/to_amazon_security_lake) operator. ## Configuration [Section titled “Configuration”](#configuration) ![Amazon security Lake](/_astro/detailed.CEMIgE0m_19DKCs.svg) The current architectural pattern for Amazon Security Lake requires creating one custom source per OCSF event class. This design ensures clean data organization, with each custom source receiving its own dedicated directory under `/ext` in the S3 bucket. Since each Parquet file must contain records of only one OCSF event class, this one-to-one mapping between custom sources and event classes is the most practical approach. The partition path follows this structure: ```plaintext /ext/{custom-source-name}/region={region}/accountId={accountID}/eventDay={YYYYMMDD}/ ``` This architecture naturally leads to deploying one Tenzir pipeline per custom source. The [`to_amazon_security_lake`](/reference/operators/to_amazon_security_lake) operator handles the partitioning according to this structure automatically. ### Custom Source Setup [Section titled “Custom Source Setup”](#custom-source-setup) Limited Custom Sources Amazon Security Lake supports a maximum of 50 custom sources, but there are over 70 [supported event classes](https://docs.aws.amazon.com/security-lake/latest/userguide/adding-custom-sources.html). You must choose which event classes to prioritize. Future versions may increase this limit. To set up a custom source: 1. Provide a globally unique 20-character source name using the pattern `tnz-ocsf-${class_uid}` (for example, `tnz-ocsf-1001` for File System Activity) 2. Select an OCSF event class (for example, “Network Activity” or “DNS Activity”) 3. Configure your AWS account ID and external ID 4. Set up a service access role for the AWS Glue crawler For detailed instructions, refer to the [AWS documentation on adding custom sources](https://docs.aws.amazon.com/security-lake/latest/userguide/adding-custom-sources.html). #### Automated Custom Source Creation [Section titled “Automated Custom Source Creation”](#automated-custom-source-creation) For streamlined custom source creation, you can use Tenzir’s [security-lake-tools](https://github.com/tenzir/security-lake-tools) to automate the setup process. The easiest way to use this tool is with `uvx` (from the `uv` Python package manager): ```bash # Create a custom source for Network Activity (class_uid 4001) uvx security-lake-tools create-source \ --region eu-west-1 \ --external-id tenzir \ --account-id 123456789012 \ --profile my-aws-profile \ 4001 # List all available OCSF event classes uvx security-lake-tools create-source --list ``` The tool automates: * Custom source creation with proper naming (`tnz-ocsf-${class_uid}` pattern) * IAM role and policy configuration * S3 bucket permissions * AWS Glue crawler setup * OCSF event class validation This approach is particularly useful when setting up multiple custom sources across different OCSF event classes. ### Tenzir Setup [Section titled “Tenzir Setup”](#tenzir-setup) To run Tenzir pipelines that send data to Security Lake, you’ll need a Tenzir node running on AWS. See our guide on [how to deploy a node on AWS](/guides/node-setup/deploy-a-node/#aws) for detailed instructions. Follow the [standard configuration instructions](/integrations/amazon) to authenticate with your AWS credentials. Tenzir supports multiple authentication methods including IAM roles, access keys, and credential profiles. After deployment, create pipelines using this pattern: ```tql let $in = ... let $url = ... let $region = ... let $account_id = ... subscribe $in where @name == "ocsf.http_activity" ocsf::apply to_amazon_security_lake $url, region=$region, account_id=$account_id, timeout=10m ``` ## Examples [Section titled “Examples”](#examples) ### Send OCSF events from a Kafka topic to Security Lake [Section titled “Send OCSF events from a Kafka topic to Security Lake”](#send-ocsf-events-from-a-kafka-topic-to-security-lake) This example assumes: * An Amazon Security Lake instance in the `eu-west-2` region * A custom source named `tnz-ocsf-4001` (Network Activity class UID) * An AWS account ID of `123456789012` ```tql let $s3_uri = "s3://aws-security-data-lake-eu-west-2-lake-abcdefghijklmnopqrstuvwxyz1234/ext/tnz-ocsf-4001/" load_kafka "ocsf_events" read_ndjson where class_uid == ocsf::class_uid("Network Activity") to_amazon_security_lake $s3_uri, region="eu-west-2", accountId="123456789012" ```

# SQS

[Amazon Simple Queuing Service (SQS)](https://aws.amazon.com/sqs/) is a managed message queue for microservices, distributed systems, and serverless applications. Tenzir can interact with SQS by sending messages to and reading messages from SQS queues. ![SQS](/_astro/sqs.CmbjyY_r_19DKCs.svg) When reading from SQS queues, you cannot specify individual messages. Instead, you determine the maximum number of messages you wish to retrieve, up to a limit of 10. If the parameter `poll_interval` is non-zero, the pipeline automatically performs [long polling](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html). This means that you may receive messages in bursts, according to your specified poll interval. Tenzir pipelines that read from an SQS queue automatically send a deletion request after having received the messages. ## Configuration [Section titled “Configuration”](#configuration) Follow the [standard configuration instructions](/integrations/amazon) to authenticate with your AWS credentials. ## Examples [Section titled “Examples”](#examples) ### Send a message to an SQS queue [Section titled “Send a message to an SQS queue”](#send-a-message-to-an-sqs-queue) ```tql from {foo: 42} to "sqs://my-queue" { write_json } ``` ### Receive messages from an SQS queue [Section titled “Receive messages from an SQS queue”](#receive-messages-from-an-sqs-queue) ```tql from "sqs://my-queue", poll_interval=5s { read_json } ``` ```tql {foo: 42} ```

# AMQP

The [Advanced Message Queuing Protocol (AMQP)](https://www.amqp.org/) is an open standard application layer protocol for message-oriented middleware. The diagram below shows the key abstractions and how they relate to a pipeline: ![AMQP Diagram](/_astro/amqp.B-TDw5B5_19DKCs.svg) Tenzir supports sending and receiving messages via AMQP version 0-9-1. ## Examples [Section titled “Examples”](#examples) ### Send events to an AMQP exchange [Section titled “Send events to an AMQP exchange”](#send-events-to-an-amqp-exchange) ```tql from { x: 42, y: "foo", } to "amqp://admin:pass@0.0.0.1:5672/vhost" ``` ### Receive events from an AMQP queue [Section titled “Receive events from an AMQP queue”](#receive-events-from-an-amqp-queue) ```tql from "amqp://admin:pass@0.0.0.1:5672/vhost" ```

# ClickHouse

[ClickHouse](https://clickhouse.com/clickhouse) is an open-source analytical database that enables its users to generate powerful analytics, using SQL queries, in real-time. ![ClickHouse integration](/_astro/clickhouse.DHWXsxpE_19DKCs.svg) ## How Tenzir Connects to ClickHouse [Section titled “How Tenzir Connects to ClickHouse”](#how-tenzir-connects-to-clickhouse) Tenzir connects to ClickHouse over the network using the native ClickHouse TCP protocol using the official [clickhouse-cpp](https://github.com/ClickHouse/clickhouse-cpp) library. Tenzir communicates with ClickHouse via the host and port you specify in the [`to_clickhouse`](/reference/operators/to_clickhouse) operator. This means: * **Network**: Tenzir and ClickHouse can run on the same machine (using `localhost`) or on different machines in the same network. You just need to make sure that Tenzir can reach the ClickHouse server. * **IPC**: There is no direct inter-process communication (IPC) mechanism; all communication uses ClickHouse’s network protocol. * **Co-deployment**: For best performance and security, deploy Tenzir and ClickHouse in the same trusted network or use secure tunnels if needed. ## Setting Up ClickHouse [Section titled “Setting Up ClickHouse”](#setting-up-clickhouse) To get started with ClickHouse, follow the [official quick start guide](https://clickhouse.com/docs/getting-started/quick-start/oss): ### Native Binary [Section titled “Native Binary”](#native-binary) 1. Download the binary: ```sh curl https://clickhouse.com/ | sh ``` 2. Start the server: ```sh ./clickhouse server ``` This downloads the ClickHouse binary and starts the server. You can then connect to ClickHouse at `localhost:9000` (native protocol) or `localhost:8123` (HTTP interface). 3. (Optionally) Start CLI client: ```sh ./clickhouse client ``` With this client, you can now run SQL queries on your ClickHouse server. ### Docker [Section titled “Docker”](#docker) 1. Run Docker: ```sh docker run -d --name clickhouse-server --ulimit nofile=262144:262144 \ -p 9000:9000 -p 8123:8123 clickhouse/clickhouse-server ``` *** You can now connect to ClickHouse at `localhost:9000` (native protocol) or `localhost:8123` (HTTP interface). ## Usage Examples [Section titled “Usage Examples”](#usage-examples) These examples assume that the ClickHouse server is running on the same host as Tenzir and that it allows non-TLS connections (hence using `tls=false` in the pipelines). You can find out more about how to configure TLS on the [`to_clickhouse`](/reference/operators/to_clickhouse) documentation and the [Clickhouse SSL-TLS configuration guide](https://clickhouse.com/docs/guides/sre/configuring-ssl) ### 1. Easy Mode: Automatic table creation [Section titled “1. Easy Mode: Automatic table creation”](#1-easy-mode-automatic-table-creation) Tenzir can automatically create tables in ClickHouse based on the incoming data schema. For example, to ingest OCSF network activity data: ```tql from "ocsf_network_activity.json" ocsf::apply to_clickhouse table="ocsf.network_activity", primary=timestamp, tls=false ``` When creating a table, the [`to_clickhouse`](/reference/operators/to_clickhouse) operator uses the first event to determine the schema. You must take care that there are no untyped nulls in this event, as the operator cannot transmit those. In this example, we use the [`ocsf::apply`](/reference/operators/ocsf/apply) operator, which will automatically align events with the correct OCSF schema, giving all fields the correct types and adding all fields that should be in `ocsf.network_activity`. This ensures that we create a complete table without missing or incorrectly typed columns. You can now query the data in ClickHouse, e.g.: ```sql SELECT median(traffic.bytes_in), median(traffic.bytes_out) FROM ocsf.network_activity GROUP BY * ``` ### 2. Advanced: Explicit Table Creation [Section titled “2. Advanced: Explicit Table Creation”](#2-advanced-explicit-table-creation) For more control, you can create the table in ClickHouse first. Use this approach when you know the full schema of your table, but not all events contain all fields and as such the operator would not create the correct table. 1. Create the table in ClickHouse: ```sql CREATE TABLE my_table ( id Int64, name String, mice_caught Nullable(Int64) ) ENGINE = MergeTree() ORDER BY id; ``` 2. Ingest data from Tenzir: my\_file.csv ```csv id,name,mice_caught 0,Jerry, 1,Tom,0 ``` ```tql from "my_file.csv" to_clickhouse table="my_table", mode="append", tls=false ``` We use the explicit `mode="append"` to ensure that the table already exists. In this example *Jerry*, being a mouse, has no value for `mice_caught`. Since we created a table with the expected type, this is not an issue.

# Elasticsearch

[Elasticsearch](https://www.elastic.co/elasticsearch) is a search and observability suite for unstructured data. Tenzir can send events to Elasticsearch and emulate and Elasticsearch Bulk API endpoint. ![How to send and receive data](/_astro/elasticsearch.G89ivtEj_19DKCs.svg) When sending data to Elasticsearch, Tenzir uses the [Bulk API](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html) and attempts to maximally batch events for throughput, accumulating multiple events before shipping them within a single API call. You can control batching behavior with the `max_content_length` and `send_timeout` options. For more details, see the documentation for the [`to_opensearch`](/reference/operators/to_opensearch) operator. Tenzir can also present an Elasticsearch-compatible REST API via the [`from_opensearch`](/reference/operators/from_opensearch) operator. ## Examples ### Send events to an Elasticsearch index Send an example event to the `main` index: ```tql from {event: "example"} to "elasticsearch://1.2.3.4:9200", action="create", index="main" ``` Instead of treating the entire event as document to be indexed by Elasticsearch, you can designate a nested record as document: ```tql from {category: "qux", doc_id: "XXX", event: {foo: "bar"}} to "elasticsearch://localhost:9200", id=doc_id, doc=event, action="update", index=category ``` The above example updates the document with ID `XXX` with the contents from the nested field `event`. ### Accept data by emulating Elasticsearch Tenzir can act as drop-in replacement for Elasticsearch by accepting data via a Bulk API endpoint. This allows you to point your [Logstash](https://www.elastic.co/logstash) or Beats instances to Tenzir instead. ```tql from "elasticsearch://localhost:9200", keep_actions=true publish "elasticsearch" ``` This pipeline accepts data on port 9200 and publishes all received events on the `elasticsearch` topic for further processing by other pipelines. Setting `keep_actions=true` causes command events to remain in the stream, e.g., like this: ```tql {create:{_index:"filebeat-8.17.3"}} // 👈 command event {"@timestamp":2025-03-31T13:42:28.068Z,log:{offset:1,file:{path:"/mounted/logfile"}},message:"hello",input:{type:"log"},host:{name:"eb21"},agent:{id:"682cfcf4-f251-4576-abcb-6c8bcadfda08",name:"eb21",type:"filebeat",version:"8.17.3",ephemeral_id:"17f74f6e-36f0-4045-93e6-c549874716df"},ecs:{version:"8.0.0"}} {create:{_index:"filebeat-8.17.3"}} // 👈 command event {"@timestamp":2025-03-31T13:42:28.068Z,log:{offset:7,file:{path:"/mounted/logfile"}},message:"this",input:{type:"log"},host:{name:"eb21"},agent:{id:"682cfcf4-f251-4576-abcb-6c8bcadfda08",name:"eb21",type:"filebeat",version:"8.17.3",ephemeral_id:"17f74f6e-36f0-4045-93e6-c549874716df"},ecs:{version:"8.0.0"}} ``` #### Ship data via Filebeat Configure [Filebeat](https://www.elastic.co/beats/filebeat) as follows to ship data to Tenzir: filebeat.yml ```yaml output: elasticsearch: hosts: ["localhost:9200"] ``` Set `hosts` to the endpoint of the Tenzir pipeline accepting data. #### Ship data via Logstash Configure [Logstash](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html) with the `elasticsearch` output plugin to ship data to Tenzir: pipeline.conf ```javascript output { elasticsearch { hosts => "https://localhost:9200" } } ``` Set `hosts` to the endpoint of the Tenzir pipeline accepting data.

# Email

Tenzir supports sending events as email using the [`save_email`](/reference/operators/save_email) operator. To this end, the operator establishes a connection with an SMTP server that sends the message on behalf of Tenzir. ![Pipeline to email](/_astro/email.DwOnOCKn_19DKCs.svg) ## Examples [Section titled “Examples”](#examples) ### Email the Tenzir version as CSV message [Section titled “Email the Tenzir version as CSV message”](#email-the-tenzir-version-as-csv-message) ```tql version write_csv save_email "Example User <user@example.org>" ``` ### Send the email body as MIME part [Section titled “Send the email body as MIME part”](#send-the-email-body-as-mime-part) ```tql version write_json save_email "user@example.org, mime=true ``` This results in an email body of this shape: ```plaintext --------------------------s89ecto6c12ILX7893YOEf Content-Type: application/json Content-Transfer-Encoding: quoted-printable { "version": "4.10.4+ge0a060567b-dirty", "build": "ge0a060567b-dirty", "major": 4, "minor": 10, "patch": 4 } --------------------------s89ecto6c12ILX7893YOEf-- ```

# File

Tenzir supports reading from and writing to files, including non-regular files, such as [Unix domain sockets](https://en.wikipedia.org/wiki/Unix_domain_socket), standard input, standard output, and standard error. ![File](/_astro/file.BdqTXxJ3_19DKCs.svg) When `~` is the first character in the file path, the operator substitutes it with the `$HOME` environment variable. ## Examples [Section titled “Examples”](#examples) ### Read a file [Section titled “Read a file”](#read-a-file) Read from a file and parse it in the format applied by the file extension: ```tql from "/tmp/file.json" ``` The [`from`](/reference/operators/from) operator automatically decompresses the file, if the suffix list contains a [supported compression algorithm](/reference/operators/from#compression): ```tql from "/tmp/file.json.gz" ``` Some operators perform better when the entire file arrives as a single block of bytes, such as the [`yara`](/reference/operators/yara) operator. In this case, passing `mmap=true` runs more efficiently: ```tql from "/sandbox/malware.gz", mmap=true { decompress "gzip" yara "rule.yaml" } ``` ### Follow a file [Section titled “Follow a file”](#follow-a-file) A pipeline typically completes once it reads the end of a file. Pass `follow=true` to disable this behavior and instead wait for new data written to it. This is similar to running `tail -f` on a file. ```plaintext from "/tmp/never-ending-stream.ndjson", follow=true ``` ### Write a file [Section titled “Write a file”](#write-a-file) Write to a file in the format implied by the file extension: ```tql version to "/tmp/tenzir-version.json" ``` The [`to`](/reference/operators/to) operator automatically compresses the file, if the suffix list contains a [supported compression algorithm](/reference/operators/to#compression): ```tql version to "/tmp/tenzir-version.json.bz2" ``` ### Append to a file [Section titled “Append to a file”](#append-to-a-file) In case the file exists and you do not want to overwrite it, pass `append=true` as option: ```tql from {x: 42} to "/tmp/event.csv", append=true ``` ### Read/write a Unix domain socket [Section titled “Read/write a Unix domain socket”](#readwrite-a-unix-domain-socket) Pass `uds=true` to signal that the file is a Unix domain socket: ```tql to "/tmp/socket", uds=true { write_ndjson } ``` When reading from a Unix domain socket, Tenzir automatically figures out whether the file is regular or a socket: ```tql from "/tmp/socket" { read_ndjson } ```

# Fluent Bit

[Fluent Bit](https://fluentbit.io) is a an open source observability pipeline. Tenzir embeds Fluent Bit, exposing all its [inputs](https://docs.fluentbit.io/manual/pipeline/inputs) via [`from_fluent_bit`](/reference/operators/from_fluent_bit) and [outputs](https://docs.fluentbit.io/manual/pipeline/outputs) via [`to_fluent_bit`](/reference/operators/to_fluent_bit) This makes Tenzir effectively a superset of Fluent Bit. ![Fluent Bit Inputs & Outputs](/_astro/fluent-bit.BMlvI90p_19DKCs.svg) Fluent Bit [parsers](https://docs.fluentbit.io/manual/pipeline/parsers) map to Tenzir operators that accept bytes as input and produce events as output. Fluent Bit [filters](https://docs.fluentbit.io/manual/pipeline/filters) correspond to Tenzir operators that perform event-to-event transformations. Tenzir does not expose Fluent Bit parsers and filters, only inputs and output. Internally, Fluent Bit uses [MsgPack](https://msgpack.org/) to encode events whereas Tenzir uses [Arrow](https://arrow.apache.org) record batches. The `fluentbit` source operator transposes MsgPack to Arrow, and the `fluentbit` sink performs the reverse operation. ## Usage [Section titled “Usage”](#usage) An invocation of the `fluent-bit` commandline utility ```bash fluent-bit -o input_plugin -p key1=value1 -p key2=value2 -p… ``` translates to Tenzir’s [`from_fluent_bit`](/reference/operators/from_fluent_bit) operator as follows: ```tql from_fluent_bit "input_plugin", options={key1: value1, key2: value2, …} ``` with the [`to_fluent_bit`](/reference/operators/to_fluent_bit) operator working exactly analogous. ## Examples [Section titled “Examples”](#examples) ### Ingest OpenTelemetry logs, metrics, and traces [Section titled “Ingest OpenTelemetry logs, metrics, and traces”](#ingest-opentelemetry-logs-metrics-and-traces) ```tql from_fluent_bit "opentelemetry" ``` You can then send JSON-encoded log data to a freshly created API endpoint: ```bash curl \ --header "Content-Type: application/json" \ --request POST \ --data '{"resourceLogs":[{"resource":{},"scopeLogs":[{"scope":{},"logRecords":[{"timeUnixNano":"1660296023390371588","body":{"stringValue":"{\"message\":\"dummy\"}"},"traceId":"","spanId":""}]}]}]}' \ http://0.0.0.0:4318/v1/logs ``` ### Imitate a Splunk HEC endpoint [Section titled “Imitate a Splunk HEC endpoint”](#imitate-a-splunk-hec-endpoint) ```tql from_fluent_bit "splunk", options = {port: 8088} ``` ### Imitate an ElasticSearch & OpenSearch Bulk API endpoint [Section titled “Imitate an ElasticSearch & OpenSearch Bulk API endpoint”](#imitate-an-elasticsearch--opensearch-bulk-api-endpoint) This allows you to ingest from beats (e.g., Filebeat, Metricbeat, Winlogbeat). ```tql from_fluent_bit "elasticsearch", options = {port: 9200} ``` ### Send to Datadog [Section titled “Send to Datadog”](#send-to-datadog) ```tql to_fluent_bit "datadog", options = {apikey: "XXX"} ``` ### Send to ElasticSearch [Section titled “Send to ElasticSearch”](#send-to-elasticsearch) ```tql to_fluent_bit "es", options = {host: 192.168.2.3, port: 9200, index: "my_index", type: "my_type"} ```

# FTP

Tenzir supports the [File Transfer Protocol (FTP)](https://en.wikipedia.org/wiki/File_Transfer_Protocol), both downloading and uploading files. ![FTP](/_astro/ftp.zkl0U-rL_19DKCs.svg) FTP consists of two separate TCP connections, one control and one data connection. This can be tricky for some firewalls and may require special attention. ## Examples [Section titled “Examples”](#examples) ### Download a file from an FTP server [Section titled “Download a file from an FTP server”](#download-a-file-from-an-ftp-server) ```tql from "ftp://user:pass@ftp.example.org/path/to/file.json" ``` ### Upload events to an FTP server [Section titled “Upload events to an FTP server”](#upload-events-to-an-ftp-server) ```tql from { x: 42, y: "foo", } to "ftp://user:pass@ftp.example.org/a/b/c/events.json.gz" ```

# Cloud Logging

[Google Cloud Logging](https://cloud.google.com/logging) is Google’s log management solution. Tenzir can send events to Google Cloud Logging. ![Google Cloud Logging](/_astro/cloud-logging.CbMzNYg2_19DKCs.svg) ## Examples [Section titled “Examples”](#examples) ### Send an event to Google Cloud Logging [Section titled “Send an event to Google Cloud Logging”](#send-an-event-to-google-cloud-logging) The easiest way to send data to Cloud Logging is via Google [Applciation Default Credentials (ADC)](https://cloud.google.com/docs/authentication/application-default-credentials). Assuming you have configured your node so that it finds the credentials, you can pipe any data to the [`to_google_cloud_logging`](/reference/operators/to_google_cloud_logging) operator: ```tql from { content: "log message", timestamp: now(), } to_google_cloud_logging name="projects/PROJECT_ID/logs/LOG_ID", resource_type="global" ```

# Cloud Pub/Sub

[Google Cloud Pub/Sub](https://cloud.google.com/pubsub) ingests events for streaming into BigQuery, data lakes, or operational databases. Tenzir can act as a publisher that sends messages to a topic, and as a subscriber that receives messages from a subscription. ![Google Cloud Pub/Sub](/_astro/cloud-pubsub.CbURM6vM_19DKCs.svg) ## Examples [Section titled “Examples”](#examples) ### Publish a message to a topic [Section titled “Publish a message to a topic”](#publish-a-message-to-a-topic) ```tql from {foo: 42} to "gcps://my-project/my-topic" { write_json } ``` ### Receive messages from a subscription [Section titled “Receive messages from a subscription”](#receive-messages-from-a-subscription) ```tql from "gcps://my-project/my-topic" { read_json } ```

# Cloud Storage

[Cloud Storage](https://cloud.google.com/storage) is Google’s object storage service. Tenzir can treat it like a local filesystem to read and write files. ![Google Cloud Storage](/_astro/cloud-storage.IUao_ukw_19DKCs.svg) ## Configuration [Section titled “Configuration”](#configuration) You need to configure appropriate credentials using Google’s [Application Default Credentials](https://google.aip.dev/auth/4110). ## Examples [Section titled “Examples”](#examples) ### Write an event to a file in a bucket [Section titled “Write an event to a file in a bucket”](#write-an-event-to-a-file-in-a-bucket) ```tql from {foo: 42} to "gs://bucket/path/to/file.json" ``` ### Read events from a file in a bucket [Section titled “Read events from a file in a bucket”](#read-events-from-a-file-in-a-bucket) ```tql from "gs://bucket/path/to/file.json" ``` ```tql {foo: 42} ```

# SecOps

[Google Security Operations (SecOps)](https://cloud.google.com/security/products/security-operations) is Google’s security operations platform that enables detection, investigation and response to incidents. Tenzir can send events to Google SecOps using the [unstructured logs ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api#unstructuredlogentries). ![Google Security Operations](/_astro/secops.DocXpfno_19DKCs.svg) ## Examples [Section titled “Examples”](#examples) ### Send an event to Google SecOps [Section titled “Send an event to Google SecOps”](#send-an-event-to-google-secops) ```tql from {log: "31-Mar-2025 01:35:02.187 client 0.0.0.0#4238: query: tenzir.com IN A + (255.255.255.255)"} to_google_secops \ customer_id="00000000-0000-0000-00000000000000000", private_key=secret("my_secops_key"), client_email="somebody@example.com", log_text=log, log_type="BIND_DNS", region="europe" ```

# Graylog

[Graylog](https://graylog.org/) is a log management solution based on top of OpenSearch. Tenzir can send data to and receive data from Graylog.[1](#user-content-fn-1) ![Graylog](/_astro/graylog.DlsP54zt_19DKCs.svg) ## Receive data from Graylog [Section titled “Receive data from Graylog”](#receive-data-from-graylog) To receive data from Graylog with a Tenzir pipeline, you need to configure a new output and setup a stream that sends data to that output. The example below assumes that Graylog sends data in GELF to a TCP endpoint that listens on IP address 1.2.3.4 at port 5678. ### Configure a GELF TCP output [Section titled “Configure a GELF TCP output”](#configure-a-gelf-tcp-output) 1. Navigate to *System/Outputs* in Graylog’s web interface. 2. Click *Manage Outputs*. 3. Select `GELF TCP` as the output type. 4. Configure the output settings: * Specify the target server’s address in the `host` field (e.g., `1.2.3.4`). * Enter the port number for the TCP connection (e.g., `5678`). * Optionally adjust other settings like reconnect delay, queue size, and send buffer size. 5. Save the configuration. Now Graylog will forward messages in GELF format to the specified TCP endpoint. ### Create a Graylog stream [Section titled “Create a Graylog stream”](#create-a-graylog-stream) The newly created output still needs to be connected to a stream to produce data. For example, to route all incoming traffic in Graylog to an output: 1. Go to *Streams* in the Graylog web interface. 2. Create a new stream or edit an existing one. 3. In the stream’s settings, configure it to match all incoming messages. You can do this by setting up a rule that matches all messages or by leaving the rules empty. 4. Once the stream is configured, go to the *Outputs* tab in the stream’s settings. 5. Add the previously configured GELF TCP output to this stream. This setup will direct all messages that arrive in Graylog to the specified output. Adapt your filters for more fine-grained forwarding. ### Test the connection with a Tenzir pipeline [Section titled “Test the connection with a Tenzir pipeline”](#test-the-connection-with-a-tenzir-pipeline) Now that Graylog is configured, you can test that data is flowing using the following Tenzir pipeline: ```tql from "tcp://1.2.3.4:5678" { read_gelf } ``` This pipelines opens a listening socket at IP address 1.2.3.4 at port 5678 via [`from`](/reference/operators/from) and then spawns a nested pipeline per accepted connection, each of which reads a stream of GELF messages using [`read_gelf`](/reference/operators/read_gelf). Graylog will connect to this socket, based on the reconnect interval that you configured in the output (by default 500ms). Now that data is flowing, you can decide what to do with the Graylog data, e.g., make available the data on an topic using [`publish`](/reference/operators/publish): ```tql from "tcp://1.2.3.4:5678" { read_gelf } publish "graylog" ``` ## Footnotes [Section titled “Footnotes”](#footnote-label) 1. This guide focuses currently focuses only on receiving data to Graylog, although it’s already possible to send data to Graylog. [↩](#user-content-fnref-1)

# HTTP

Tenzir supports HTTP and HTTPS, both as sender and receiver. When retrieving data from an API or website, you prepare your HTTP request and get back the HTTP response body as your pipeline data: ![HTTP from](/_astro/http-from.CK3DDVvm_19DKCs.svg) When sending data from a pipeline to an API or website, the events in the pipeline make up the HTTP request body. If the HTTP status code is not 2\*\*, you will get a warning. ![HTTP from](/_astro/http-to.DgFU5GZf_19DKCs.svg) In both cases, you can only provide static header data. ## Examples [Section titled “Examples”](#examples) ### Perform a GET request with URL parameters [Section titled “Perform a GET request with URL parameters”](#perform-a-get-request-with-url-parameters) ```tql from "http://example.com:8888/api", method="GET", params={query: "tenzir"} ``` ### Perform a POST request with JSON body [Section titled “Perform a POST request with JSON body”](#perform-a-post-request-with-json-body) ```tql from "http://example.com:8888/api", method="POST", data={query: "tenzir"} ``` ### Call a webhook API with pipeline data [Section titled “Call a webhook API with pipeline data”](#call-a-webhook-api-with-pipeline-data) ```tql from { x: 42, y: "foo", } to "http://example.com:8888/api" ```

# Kafka

[Apache Kafka](https://kafka.apache.org) is a distributed open-source message broker. The Tenzir integration can publish (send messages to a topic) or subscribe (receive) messages from a topic. ![Kafka Diagram](/_astro/kafka.C4MFfO6p_19DKCs.svg) Internally, we use Confluent’s official [librdkafka](https://github.com/confluentinc/librdkafka) library, which gives us full control in passing options. ## Examples [Section titled “Examples”](#examples) ### Send events to a Kafka topic [Section titled “Send events to a Kafka topic”](#send-events-to-a-kafka-topic) ```tql from { x: 42, y: "foo", } to "kafka://topic" { write_ndjson } ``` ### Subscribe to a topic [Section titled “Subscribe to a topic”](#subscribe-to-a-topic) The `offset` option controls where to start reading: ```tql from "kafka://topic", offset="beginning" { read_ndjson } ``` Other values are `"end"` to read at the last offset, `"stored"` to read at the stored offset, a positive integer representing an absolute offset, or a negative integer representing a relative offset counting from the end.

# Azure Blob Storage

[Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs) is Azure’s object storage service. Tenzir can treat it like a local filesystem to read and write files. ![Azure Blob Storage](/_astro/azure-blob-storage.Sc3L-GkA_19DKCs.svg) ## Examples [Section titled “Examples”](#examples) ### Write an event to a file in a container [Section titled “Write an event to a file in a container”](#write-an-event-to-a-file-in-a-container) ```tql from {foo: 42} to "abfss://user@container/path/to/file.json" ``` ### Read events from a file in a container [Section titled “Read events from a file in a container”](#read-events-from-a-file-in-a-container) ```tql from "abfss://user@container/path/to/file.json" ``` ```tql {foo: 42} ```

# Azure Log Analytics

Azure Monitor is Microsoft’s cloud solution for collecting and analyzing logs and system events. Azure Log Analytics is a part of Monitor and comes with an [Logs Ingestion API](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview) for sending data to tables within a [Log Analytics workspace](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview) that which is a unique environment for log data, such as from [Microsoft Sentinel](https://learn.microsoft.com/en-us/azure/sentinel/overview?tabs=azure-portal) and [Microsoft Defender for Cloud](https://learn.microsoft.com/en-us/azure/defender-for-cloud/defender-for-cloud-introduction). Log Anlaytics tables are either pre-defined [standard tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview#supported-tables) that follow a given schema, or user-defined [custom tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/create-custom-table#create-a-custom-table). The diagram below illustrates the key components involved when sending data to a Log Analytics table: ![Log Ingestion Workflow](/_astro/azure-log-analytics.HULrdTJf_19DKCs.svg) The [Data Collection Endpoint (DCE)](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-collection-endpoint-overview) is an authenticated HTTPS API endpoint that accepts HTTP POST requests with events encoded as JSON arrays in the request body. The [Data Collection Rule (DCR)](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-collection-rule-overview) offers optional transformation of arriving data and routes the data to a Log Analytics table. ## Examples [Section titled “Examples”](#examples) ### Send logs to custom table [Section titled “Send logs to custom table”](#send-logs-to-custom-table) Let’s assume that you have the following CSV file that you want to send to a custom table: users.csv ```csv user,age Alice,42 Bob,43 Charlie,44 ``` Assuming you have already [created a custom table](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/create-custom-table#create-a-custom-table) called `Custom-Users`, you can send this file to the table using the [`to_azure_log_analytics`](/reference/operators/to_azure_log_analytics) operator: ```tql load_file "users.csv" read_csv to_azure_log_analytics \ tenant_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx", client_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx", client_secret="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", dce="https://my-dce.westeurope-1.ingest.monitor.azure.com", dcr="dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", table="Custom-Users" ```

# Defender

[Microsoft Defender](https://learn.microsoft.com/en-us/defender-xdr/microsoft-365-defender-portal) offers protection, detection, investigation, and response to threats. Defender comes in multiple editions, [Defender for Office 365](https://learn.microsoft.com/en-us/defender-office-365/mdo-about), [Defender for Endpoint](https://learn.microsoft.com/en-us/defender-endpoint/), [Defender for IoT](https://learn.microsoft.com/en-us/defender-for-iot/microsoft-defender-iot), [Defender for Identity](https://learn.microsoft.com/en-us/defender-for-identity/what-is), and [Defender for Cloud](https://learn.microsoft.com/en-us/defender-xdr/microsoft-365-security-center-defender-cloud). All Defender products can stream events in real time to Tenzir using [Azure Event Hubs](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about). ![Microsoft Defender](/_astro/defender.0xY2bvqA_19DKCs.svg) ## Requirements and Setup [Section titled “Requirements and Setup”](#requirements-and-setup) ### Azure Event Hub & Kafka [Section titled “Azure Event Hub & Kafka”](#azure-event-hub--kafka) To stream security events from Defender in realtime, you can use Azure Event Hub, which provides a Kafka endpoint starting at the Standard tier. Make sure to enable *Kafka Surface* after the Event Hub setup. ### Microsoft Security Center [Section titled “Microsoft Security Center”](#microsoft-security-center) In Microsoft Security Center, configure Streaming under `System -> Settings -> Microsoft Defender XDR -> General -> Streaming API`. Add a new Streaming API for the target Event Hub and enable all event types that you want to collect. ## Examples [Section titled “Examples”](#examples) ### Process Defender events with a pipeline [Section titled “Process Defender events with a pipeline”](#process-defender-events-with-a-pipeline) Tenzir’s [Kafka integration](/integrations/kafka) allows for seamless consumption of Defender events. In the following pipeline, replace all strings starting with `YOUR_`with the configuration values in Azure under `Event Hub Namespace -> Settings -> Shared access policies -> (Your policy)`. ```tql from "kafka://YOUR_EVENT_HUB_NAME", options = { "bootstrap.servers": "YOUR_EVENT_HUB_NAME.servicebus.windows.net:9093", "security.protocol": "SASL_SSL", "sasl.mechanism": "PLAIN", "sasl.username": "$ConnectionString", "sasl.password": "YOUR_CONNECTION_STRING" // Connection string-primary key } { read_json } ``` After replacing the configuration values, your pipeline may look like this: ```tql from "kafka://tenzir-defender-event-hub", options = { "bootstrap.servers": "tenzir-defender-event-hub.servicebus.windows.net:9093", "security.protocol": "SASL_SSL", "sasl.mechanism": "PLAIN", "sasl.username": "$ConnectionString", "sasl.password": "Endpoint=sb://tenzir-defender-event-hub.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=SECRET123456" } { read_json } ``` ```tql { records: [ { time: "2024-12-04T13:38:20.360851", tenantId: "40431729-d276-4582-abb4-01e21c8b58fe", operationName: "Publish", category: "AdvancedHunting-IdentityLogonEvents", _TimeReceivedBySvc: "2024-12-04T13:36:26.632556", properties: { ActionType: "LogonFailed", LogonType: "Failed logon", Protocol: "Ntlm", AccountDisplayName: null, AccountUpn: null, AccountName: "elias", AccountDomain: "tenzir.com", AccountSid: null, AccountObjectId: null, IPAddress: null, Location: null, DeviceName: "WIN-P3MCS4024KP", OSPlatform: null, DeviceType: null, ISP: null, DestinationDeviceName: "ad-test.tenzir.com", TargetDeviceName: null, FailureReason: "UnknownUser", Port: null, DestinationPort: null, DestinationIPAddress: null, TargetAccountDisplayName: null, AdditionalFields: { Count: "1", Category: "Initial Access", AttackTechniques: "Valid Accounts (T1078), Domain Accounts (T1078.002)", SourceAccountName: "tenzir.com\\elias", SourceComputerOperatingSystemType: "unknown", DestinationComputerObjectGuid: "793e9b90-9eef-4620-aaa2-442a22f81321", DestinationComputerOperatingSystem: "windows server 2022 datacenter", DestinationComputerOperatingSystemVersion: "10.0 (20348)", DestinationComputerOperatingSystemType: "windows", SourceComputerId: "computer win-p3mcs4024kp", FROM.DEVICE: "WIN-P3MCS4024KP", TO.DEVICE: "ad-test", ACTOR.DEVICE: "", }, ReportId: "3d359b95-f8d5-4dbd-a64b-7327c92d32f1", Timestamp: "2024-12-04T13:33:19.801823", Application: "Active Directory", }, Tenant: "DefaultTenant", }, ] } ```

# Windows Event Logs

Windows Event Logs are records generated by the Windows operating system and applications that detail system, security, and application-related events for monitoring and troubleshooting purposes. Once Windows Event Logs are flowing in a Tenzir pipeline, you can use any operator to process them. The below examples simply [import all data into a node](/guides/edge-storage/import-into-a-node). ## Collect logs with an agent [Section titled “Collect logs with an agent”](#collect-logs-with-an-agent) Installing a third-party agent to ship logs away from a Windows machine is common way to send events to a remote location. ![Windows Events with Agent](/_astro/windows-events-with-agent.9XvCYTpA_19DKCs.svg) Regardless of the concrete agent you are using for shipping, the high-level setup is always the same: the agent sends events in a push-based to a Tenzir pipeline. ### Winlogbeat [Section titled “Winlogbeat”](#winlogbeat) [Winlogbeat](https://www.elastic.co/beats/winlogbeat) is Elastic’s log shipper to get Windows Event Logs out of Windows machines into the Elastic stack. #### Configure Winlogbeat [Section titled “Configure Winlogbeat”](#configure-winlogbeat) After [installing Winlogbeat](https://www.elastic.co/guide/en/beats/winlogbeat/current/winlogbeat-installation-configuration.html), create a configuration: winlogbeat.yml ```yaml # Choose your channels. winlogbeat.event_logs: - name: Application - name: System - name: Security - name: ForwardedEvents - name: Windows PowerShell - name: Microsoft-Windows-Sysmon/Operational - name: Microsoft-Windows-PowerShell/Operational - name: Microsoft-Windows-Windows Defender/Operational - name: Microsoft-Windows-TaskScheduler/Operational - name: Microsoft-Windows-TerminalServices-LocalSessionManager/Operational - name: Microsoft-Windows-TerminalServices-RDPClient/Operational # Send data to a Tenzir pipeline with an ElasticSearch source. output.elasticsearch: hosts: ["https://10.0.0.1:9200"] username: "$USER" password: "$PASSWORD" ssl: enabled: true certificate_authorities: [C:\Program Files\Winlogbeat\ca.crt] # PEM format certificate: C:\Program Files\Winlogbeat\tenzir.crt key: C:\Program Files\Winlogbeat\beat-win10\tenzir.key ``` #### Start Winlogbeat as a service [Section titled “Start Winlogbeat as a service”](#start-winlogbeat-as-a-service) After completing your configuration, start the Winlogbeat service: ```plaintext C:\Program Files\Winlogbeat> Start-Service winlogbeat ``` #### Run a Tenzir pipeline [Section titled “Run a Tenzir pipeline”](#run-a-tenzir-pipeline) Now consume the data via a Tenzir pipeline using the [Elasticsearch](/integrations/elasticsearch)/[OpenSearch](/integrations/opensearch) integration that mimics a bulk ingest endpoint: ```tql from "elasticsearch://10.0.0.1:92000" \ tls=true, certfile="server.crt", keyfile="private.key" import ``` ### Fluent Bit [Section titled “Fluent Bit”](#fluent-bit) Since Tenzir has native Fluent Bit support, collecting logs via the Fluent Bit agent is a simple approach. #### Configure Fluent Bit [Section titled “Configure Fluent Bit”](#configure-fluent-bit) First, install Fluent Bit on Windows according to the [official instructions](https://docs.fluentbit.io/manual/installation/windows). Then create a [YAML configuration](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/yaml/configuration-file) to send Windows Event Logs out via the [Forward output](https://docs.fluentbit.io/manual/pipeline/outputs/forward), which encodes the events using Fluent Bit’s MsgPack-based wire format: ```yaml input: - name: winevtlog channels: Setup,Windows PowerShell interval_sec: 1 db: winevtlog.sqlite output: - name: forward match: "*" host: 10.0.0.1 ``` Adapt `input.channels` according to the Event Log channels you would like Fluent Bit to monitor. #### Run a Tenzir pipeline [Section titled “Run a Tenzir pipeline”](#run-a-tenzir-pipeline-1) Use the [`from_fluent_bit`](/reference/operators/from_fluent_bit) source operator with the [Forward input](https://docs.fluentbit.io/manual/pipeline/inputs/forward): ```tql from_fluent_bit "forward", options={ listen: 10.0.0.1, } import ``` Ensure that the `listen` parameter matches the `host` value in your Fluent Bit configuration. #### Test the configuration [Section titled “Test the configuration”](#test-the-configuration) Test the setup by running the Fluent Bit command line utility: ```plaintext C:\Program Files\fluent-bit\bin\fluent-bit.exe -c \fluent-bit\conf\fluent-bit.yaml ``` #### Deploy Fluent Bit as a service [Section titled “Deploy Fluent Bit as a service”](#deploy-fluent-bit-as-a-service) To make the setup permanent, [run Fluent Bit as a service](https://docs.fluentbit.io/manual/installation/windows#windows-service-support). Create the service: ```plaintext sc.exe create fluent-bit binpath= "\fluent-bit\bin\fluent-bit.exe -c \fluent-bit\conf\fluent-bit.yaml" ``` Start and check the service: ```plaintext sc.exe start fluent-bit sc.exe query fluent-bit ``` Start the service at boot: ```plaintext sc.exe config fluent-bit start= auto ``` ### NXLog [Section titled “NXLog”](#nxlog) The NXLog agent collects Windows Event Logs and offers numerous [output modules](https://docs.nxlog.co/refman/current/om/index.html). You have several options with Tenzir. We use the JSON extension to format the data in all examples. #### Ship logs via TCP [Section titled “Ship logs via TCP”](#ship-logs-via-tcp) To send logs straight to a TCP socket, use the [TCP output module](https://docs.nxlog.co/refman/current/om/tcp.html) with the following configuration: ```plaintext <Extension json> Module xm_json </Extension> <Output tcp> Module om_tcp Host 10.0.0.1:1514 Exec to_json(); </Output> ``` Import the logs via TCP: ```tql from "tcp://10.0.0.1:1514" { read_json } import ``` #### Ship logs via TLS/SSL [Section titled “Ship logs via TLS/SSL”](#ship-logs-via-tlsssl) For an encrypted connection, use the [SSL output module](https://docs.nxlog.co/refman/current/om/ssl.html) with the following configuration: ```plaintext <Extension json> Module xm_json </Extension> <Output ssl> Module om_ssl Host example.com:23456 CAFile %CERTDIR%/ca.pem CertFile %CERTDIR%/client-cert.pem CertKeyFile %CERTDIR%/client-key.pem KeyPass secret AllowUntrusted TRUE OutputType Binary Exec to_json(); </Output> ``` Import the logs via TCP: ```tql load_tcp "127.0.0.1:4000", tls=true, certfile="key_and_cert.pem", keyfile="key_and_cert.pem" { read_json } import ``` #### Ship logs via Kafka [Section titled “Ship logs via Kafka”](#ship-logs-via-kafka) The [Kafka output module](https://docs.nxlog.co/refman/current/om/kafka.html) publishes to a Kafka topic that Tenzir can read from. Use the following output configuration to publish to the `nxlog` topic: ```plaintext <Output out> Module om_kafka BrokerList localhost:9092 Topic nxlog LogqueueSize 100000 Partition 0 Protocol ssl CAFile %CERTDIR%/ca.pem CertFile %CERTDIR%/client-cert.pem CertKeyFile %CERTDIR%/client-key.pem KeyPass thisisasecret </Output> ``` Then use our [Kafka integration](/integrations/kafka) to read from the topic: ```tql from "kafka://nxlog" { read_json } import ``` ## Collect logs via WEF & WEC [Section titled “Collect logs via WEF & WEC”](#collect-logs-via-wef--wec) Instead of deploying an agent on a Windows endpoint, you can also use native facilities to collect logs centrally. To this end, Windows comes with a **Windows Event Forwarding (WEF)** mechanism on the endpoints that uses standard *Windows Remote Management (WinRM)* protocols to transmit events. The **Windows Event Collector (WEC)** is the service running on a server that receives the events sent by clients through WEF. The WEC aggregates these logs and makes them available for review and analysis. Administrators can create subscriptions on the WEC that define which events to collect from which clients, using criteria such as event IDs, keywords, or log levels. The diagram below illustrates a typical setup: ![WEF & WEC](/_astro/windows-events-wef-wec.xYR9mcD5_19DKCs.svg) On the WEC, you typically ship the collected logs away using a third-party agent, as described above. Read below on using [OpenWEC](#collect-logs-via-openwec) as an agent-free alternative. The following configuration steps are heavily inspired by [SEKOIA](https://docs.sekoia.io/xdr/features/collect/integrations/endpoint/windows/#windows-event-forwarder-to-windows-event-collector-to-a-concentrator)’s instructions. ### Setup the Window Event Collector (WEC) [Section titled “Setup the Window Event Collector (WEC)”](#setup-the-window-event-collector-wec) We begin with setting up the WEC prior to connecting the client to it. #### Setup Windows Remote Management (WinRM) [Section titled “Setup Windows Remote Management (WinRM)”](#setup-windows-remote-management-winrm) Configure WinRM as follows: ```cmd winrm qc -q ``` The argument `qc` stands for “quick configuration” to perform a basic configuration of WinRM with default settings. This configuration includes starting the WinRM service, setting it to start automatically with the system, creating a listener on HTTP to accept WS-Management protocol requests, and configuring the Windows Firewall to allow WinRM traffic. The `-q` flag means “quiet mode” to avoid prompting you for any input or confirmation, i.e., it makes the process non-interactive. :::caution Unencrypted HTTP This command sets up WinRM to listen on HTTP, which is not encrypted. For a secure production environment, it’s advisable to configure WinRM to use HTTPS, which requires additional steps, including setting up an appropriate server certificate for encryption. ::: #### Enable the Event Collector service [Section titled “Enable the Event Collector service”](#enable-the-event-collector-service) Use the `wecutil` command to perform the necessary steps to set up the Windows Event Collector service: ```cmd wecutil qc /q ``` As above, `qc` stands for “quick configuration” and `/q` for “quiet”. The setup includes actions such as configuring the service to start automatically and ensuring that the service is in a state ready to create and manage event subscriptions. #### Create a subscription file [Section titled “Create a subscription file”](#create-a-subscription-file) Create a new file with the following contents: DC\_SUBSCRIPTION.xml ```xml <?xml version="1.0" encoding="UTF-8"?> <Subscription xmlns="http://schemas.microsoft.com/2006/03/windows/events/subscription"> <!-- Name of subscription --> <SubscriptionId>DC_SUBSCRIPTION</SubscriptionId> <!-- Push mode (DC to WEC) --> <SubscriptionType>SourceInitiated</SubscriptionType> <Description>Source Initiated Subscription from DC_SUBSCRIPTION</Description> <!-- Subscription is active --> <Enabled>true</Enabled> <Uri>http://schemas.microsoft.com/wbem/wsman/1/windows/EventLog</Uri> <!-- This mode ensures that events are delivered with minimal delay --> <!-- It is an appropriate choice if you are collecting alerts or critical events --> <!-- It uses push delivery mode and sets a batch timeout of 30 seconds --> <ConfigurationMode>MinLatency</ConfigurationMode> <!-- Event log to retrieved --> <Query> <![CDATA[ <QueryList> <Query Id="0"> <Select Path="Application">*</Select> <Select Path="Security">*</Select> <Select Path="System">*</Select> </Query> </QueryList> ]]> </Query> <!-- Collect events generated since the subscription (not oldest) --> <ReadExistingEvents>false</ReadExistingEvents> <!-- Protocol and port used (DC to WEC) --> <TransportName>http</TransportName> <!-- Mandatory value (https://www-01.ibm.com/support/docview.wss?crawler=1&uid=swg1IV71375) --> <ContentFormat>RenderedText</ContentFormat> <Locale Language="en-US"/> <!-- Target Event log on WEC --> <LogFile>ForwardedEvents</LogFile> <!-- Define which domain computers are allowed or not to initiate subscriptions --> <!-- This exemple grants members of the Domain Computers domain group, as well as the local Network Service group (for local forwarder) --> <AllowedSourceDomainComputers>O:NSG:NSD:(A;;GA;;;DC)(A;;GA;;;NS)</AllowedSourceDomainComputers> </Subscription> ``` Key elements are: * `SubscriptionId`: Give a unique name to your subscription. * `SubscriptionType`: Choose between `SourceInitiated` (push) or `CollectorInitiated` (pull). * `Description`: Provide a meaningful description. * `Query`: Modify the event log query to specify which events to collect. * `LogFile`: Define the destination log file on the collector. * `AllowedSourceDomainComputers`: Adjust the [SDDL](https://learn.microsoft.com/en-gb/windows/win32/secauthz/security-descriptor-definition-language?redirectedfrom=MSDN) string to specify which computers can forward events. There are [several](https://github.com/palantir/windows-event-forwarding/tree/master/wef-subscriptions) [GitHub](https://github.com/nsacyber/Event-Forwarding-Guidance/tree/master/Subscriptions/samples) [repositories](https://github.com/mdecrevoisier/Windows-WEC-server_auto-deploy/tree/master/windows-subscriptions) out there with ideas for additional subscriptions. #### Activate the subscription [Section titled “Activate the subscription”](#activate-the-subscription) Now that the collector is running, activate the subscription: ```cmd wecutil cs "<FILE_PATH>\DC_SUBSCRIPTION.xml" ``` The `cs` subcommand stands for “create subscription” and creates a subscription according to the file passed as the next argument. #### Verify the subscription [Section titled “Verify the subscription”](#verify-the-subscription) Finally, verify that the subscription is active: ```cmd wecutil gr DC_SUBSCRIPTION ``` The `gr` subcommand stands for “get runtime status” and displays the subscription status for the ID `DC_SUBSCRIPTION`, which corresponds to the `<SubscriptionId>` XML tag in the configuration file. ### Setup Windows Event Forwarding (WEF) [Section titled “Setup Windows Event Forwarding (WEF)”](#setup-windows-event-forwarding-wef) After you completed the server-side configuration, now configure the machines that should log to the WEC. #### Setup Windows Remote Management (WinRM) [Section titled “Setup Windows Remote Management (WinRM)”](#setup-windows-remote-management-winrm-1) Active WinRM as follows: ```cmd winrm qc -q \ ``` This step is identical to the WinRM configuration on the WEC. #### Change local group policy settings [Section titled “Change local group policy settings”](#change-local-group-policy-settings) Use the Local Group Policy Editor (`gpedit.msc`) to navigate to the `Computer Configuration\Administrative Templates\Windows Components\Event Forwarding` path. Here, you’ll need to open the policy named “Configure the server address, refresh interval, and issuer certificate authority of a target Subscription Manager.” In the policy settings, enable the policy and click the “Show…” button for SubscriptionManagers. Enter the server details for your WEC: ```plaintext Server=http://WEC_FQDN:5985/wsman/SubscriptionManager/WEC,Refresh=60 ``` Replace `WEC_FQDN` with the actual FQDN of your WEC. #### Apply the local group policy [Section titled “Apply the local group policy”](#apply-the-local-group-policy) Refresh the Local Group Policy settings and apply the changes by running: ```plaintext gpupdate /force ``` On the WEC, now [verify that the machine forwards events](#verify-the-subscription). ## Collect logs via OpenWEC [Section titled “Collect logs via OpenWEC”](#collect-logs-via-openwec) Instead of natively running a WEC on a Windows machine, you can also run the third-party implementation [OpenWEC](https://github.com/cea-sec/openwec). ![OpenWEC](/_astro/windows-events-openwec.o1DLW_jW_19DKCs.svg) From a functional perspective, this setup is identical to running a native WEC, but it does not require an additional agent at the WEC. In addition, OpenWEC can be scaled redundantly for high availability setups. ### Setup OpenWEC [Section titled “Setup OpenWEC”](#setup-openwec) Refer to the [OpenWEC getting started guide](https://github.com/cea-sec/openwec/blob/main/doc/getting_started.md) for setup instructions. For running OpenWEC on non-Windows machines that are likely not joined to a Windows domain, it is most useful to configure TLS client authentication. The OpenWEC documentation (see above) recommends to use [a script collection from NXLog](https://gitlab.com/nxlog-public/contrib/-/tree/master/windows-event-forwarding) that creates keys/certificates that can immediately be used to configure both Windows clients and the OpenWEC collector. Make sure to pay attention to specifying the correct hostnames for the sending and receiving machines. ```bash git clone https://gitlab.com/nxlog-public/contrib cd contrib/windows-event-forwarding ./genca.sh myca ./gencert-client.sh myclient.domain.com ./gencert-server.sh openwec.domain.com ``` Use the following for the Subscription Manager string: ```plaintext Server=HTTPS://openwec.domain.com:5985/wsman/,Refresh=14400,IssuerCA=6605742C5400141B76A747E19EA585E29B09F017 ``` The string in the last line can be used to configure the Windows Event Forwarding subscription manager on the sending side as described in the section above (“Change local group policy settings”). While you’re at it, also import the `client.pfx` and `ca-cert.pem` into the corresponding stores (see [the documentation](https://github.com/cea-sec/openwec/blob/main/doc/tls.md#client-configuration)). Then, configure the OpenWEC server as follows (assuming the output files are in `/etc`, the directory `/var/db/openwec` exists and it is writable by the current user): openwec.conf.toml ```toml [server] [logging] verbosity = "info" [database] type = "SQLite" path = "/var/db/openwec/openwec.sqlite" [[collectors]] listen_address = "0.0.0.0" hostname = "openwec.domain.com" [collectors.authentication] type = "Tls" ca_certificate = "/etc/ca-cert.pem" server_certificate = "/etc/server-cert.pem" server_private_key = "/etc/server-key.pem" ``` Create the database (only needs to be done once): ```bash openwec -c openwec.conf.toml db init ``` Start the server: ```bash openwecd -c openwec.conf.toml ``` ```plaintext 2024-01-30T13:59:26.295792509+01:00 INFO server - Server settings: Server { db_sync_interval: None, flush_heartbeats_interval: None, heartbeats_queue_size: None, node_name: None, keytab: None, tcp_keepalive_time: None, tcp_keepalive_intvl: None, tcp_keepalive_probes: None } 2024-01-30T13:59:26.295947557+01:00 INFO server::subscription - reload_subscriptions task started 2024-01-30T13:59:26.296046314+01:00 INFO server::heartbeat - Heartbeat task started 2024-01-30T13:59:26.297503212+01:00 WARN server::subscription - There are no active subscriptions! 2024-01-30T13:59:26.306151854+01:00 INFO server::tls - Loaded TLS configuration with server certificate /etc/server-cert.pem 2024-01-30T13:59:26.309876793+01:00 INFO server - Server listenning on 0.0.0.0:5985 ``` It might make sense to ensure that the server is started and kept up via some automated means, like systemd. Then, while the server is running, create a subscription in OpenWEC for the desired channels. For example, to match the subscription from the example above, create an XML file like this: subscription.xml ```xml <QueryList> <Query Id="0"> <Select Path="Application">*</Select> <Select Path="Security">*</Select> <Select Path="System">*</Select> </Query> </QueryList> ``` Pass this file to `openwec` to create a subscription, e.g., with name `DC_SUBSCRIPTION`: ```bash openwec subscriptions new DC_SUBSCRIPTION subscription.xml ``` For the new subscription, configure JSON over TCP as [output](https://github.com/cea-sec/openwec/blob/main/doc/outputs.md): ```bash openwec subscriptions edit DC_SUBSCRIPTION outputs add --format json tcp 10.0.0.1 1514 ``` Finally, enable the subscription: ```bash openwec subscriptions enable DC_SUBSCRIPTION ``` That’s it! You should now be able read the Windows event logs in JSON format by spinning up a server that listens at `tcp://10.0.0.1:1514`. ### Run a Tenzir pipeline [Section titled “Run a Tenzir pipeline”](#run-a-tenzir-pipeline-2) Accept the logs sent with the configuration above into Tenzir via [TCP](/integrations/tcp): ```tql from "tcp://10.0.0.1:1514" { read_json } publish "wec" ``` ## Increase visibility with Sysmon [Section titled “Increase visibility with Sysmon”](#increase-visibility-with-sysmon) [Sysmon](https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon) (System Monitor) is a Windows system service and device driver that, once installed on a system, remains resident across system reboots to monitor and log system activity to the Windows event log. Key features include: * **Process creation tracking**: Logs details of new processes. * **Network connection monitoring**: Records incoming and outgoing network connections. * **File creation time changes**: Tracks changes to file creation times. * **Driver and image load monitoring**: Logs loading of drivers and DLL files. * **Registry tracking**: Monitors changes to the Windows registry. ### Download and extract Sysmon via Powershell [Section titled “Download and extract Sysmon via Powershell”](#download-and-extract-sysmon-via-powershell) Download Sysmon: ```ps Invoke-WebRequest -Uri "https://download.sysinternals.com/files/Sysmon.zip" -OutFile "Sysmon.zip" ``` Extract the archive: ```plaintext Expand-Archive -Path Sysmon.zip -DestinationPath Sysmon ``` ### Choose a Symon configuration [Section titled “Choose a Symon configuration”](#choose-a-symon-configuration) Choose a suitable Sysmon configuration, e.g., from [Florian Roth](https://github.com/Neo23x0/sysmon-config/) or [SwiftOnSecurity](https://github.com/SwiftOnSecurity/sysmon-config): ```ps Invoke-WebRequest -Uri "https://raw.githubusercontent.com/Neo23x0/sysmon-config/master/sysmonconfig-export.xml" -OutFile "sysmonconfig-export.xml" ``` ### Install Symon with a configuration [Section titled “Install Symon with a configuration”](#install-symon-with-a-configuration) ```ps .\Sysmon64.exe -accepteula -i sysmonconfig-export.xml ``` Now use any of the above techniques to collect event logs through the channel `Microsoft-Windows-Sysmon/Operational`.

# Network Interface

Tenzir supports reading packets from a network interface card (NIC). The [`load_nic`](/reference/operators/load_nic) produces a stream of bytes in PCAP file format: ![Packet pipeline](/_astro/nic.RqhazQst_19DKCs.svg) We designed `load_nic` such that it produces a byte stream in the form of a PCAP file. That is, when the pipeline starts, it first produces a file header, followed by chunks of packets. This creates a byte stream that is wire-compatible with the PCAP format, allowing you to exchange `load_nic` with [`load_file`](/reference/operators/load_file) and It Just Works™. ## Examples [Section titled “Examples”](#examples) ### List active network interfaces [Section titled “List active network interfaces”](#list-active-network-interfaces) If you don’t know what interface to read from, use the [`nics`](/reference/operators/nics) operator to identify suitable candidates: ```tql nics select name, addresses, up where up ``` ```tql { name: "eth0", addresses: [ "169.254.172.2", "fe80::6471:53ff:fe5f:a8cc", ], up: true, } { name: "eth1", addresses: [ "10.0.101.13", "fe80::f7:75ff:fe66:94e5", ], up: true, } { name: "lo", addresses: [ "127.0.0.1", "::1", ], up: true, } ``` ### Read packets from a network interface [Section titled “Read packets from a network interface”](#read-packets-from-a-network-interface) Load packets from `eth0` and parse them as PCAP: ```tql load_nic "eth0" read_pcap head 3 ``` ```tql { linktype: 1, timestamp: "2021-11-17T13:32:43.237882", captured_packet_length: 74, original_packet_length: 74, data: "ABY88f1tZJ7zvttmCABFAAA8inQAADQGN+yADoaqxkf3W+B8AFDc3z7hAAAAAKACchATrQAAAgQFtAQCCApMw7SVAAAAAAEDAwc=", } { linktype: 1, timestamp: "2021-11-17T13:32:43.237939", captured_packet_length: 74, original_packet_length: 74, data: "ZJ7zvttmABY88f1tCABFAAA8AABAAEAGdmDGR/dbgA6GqgBQ4HzXXzhD3N8+4qAS/ohsJAAAAgQFtAQCCAqjGGhDTMO0lQEDAwc=", } { linktype: 1, timestamp: "2021-11-17T13:32:43.249425", captured_packet_length: 66, original_packet_length: 66, data: "ABY88f1tZJ7zvttmCABFAAA0inUAADQGN/OADoaqxkf3W+B8AFDc3z7i1184RIAQAOWYkQAAAQEICkzDtJijGGhD", } ``` ### Decapsulate packets [Section titled “Decapsulate packets”](#decapsulate-packets) After you have structured data in the form of PCAP events, you can use the [`decapsulate`](/reference/functions/decapsulate) function to decode the binary `data`: ```tql load_nic "eth0" read_pcap select packet = decapsulate(this) head 1 ``` ```tql { packet: { ether: { src: "64-9E-F3-BE-DB-66", dst: "00-16-3C-F1-FD-6D", type: 2048, }, ip: { src: "128.14.134.170", dst: "198.71.247.91", type: 6, }, tcp: { src_port: 57468, dst_port: 80, }, community_id: "1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=", } ``` Decapsulation automatically computes a [Community ID](https://github.com/corelight/community-id-spec) for correlation in the `community_id` field. You could also use the [`community_id`](/reference/functions/community_id) function to compute this value manually for different events.

# OpenSearch

[OpenSearch](https://opensearch.org) is a search and observability suite for unstructured data. Tenzir can send events to OpenSearch and emulate and OpenSearch Bulk API endpoint. ![How to send and receive data](/_astro/opensearch.dpUdbHQj_19DKCs.svg) When sending data to OpenSearch, Tenzir uses the [Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/) and attempts to maximally batch events for throughput, accumulating multiple events before shipping them within a single API call. You can control batching behavior with the `max_content_length` and `send_timeout` options. For more details, see the documentation for the [`to_opensearch`](/reference/operators/to_opensearch) operator. Tenzir can also present an OpenSearch-compatible REST API via the [`from_opensearch`](/reference/operators/from_opensearch) operator. ## Examples ### Send events to an OpenSearch index Send an example event to the `main` index: ```tql from {event: "example"} to "opensearch://1.2.3.4:9200", action="create", index="main" ``` Instead of treating the entire event as document to be indexed by OpenSearch, you can designate a nested record as document: ```tql from {category: "qux", doc_id: "XXX", event: {foo: "bar"}} to "opensearch://localhost:9200", id=doc_id, doc=event, action="update", index=category ``` The above example updates the document with ID `XXX` with the contents from the nested field `event`. ### Accept data by emulating OpenSearch Tenzir can act as drop-in replacement for OpenSearch by accepting data via a Bulk API endpoint. This allows you to point your [Logstash](https://opensearch.org/docs/latest/tools/logstash/index/) or Beats instances to Tenzir instead. ```tql from "opensearch://localhost:9200", keep_actions=true publish "opensearch" ``` This pipeline accepts data on port 9200 and publishes all received events on the `opensearch` topic for further processing by other pipelines. Setting `keep_actions=true` causes command events to remain in the stream, e.g., like this: ```tql {create:{_index:"filebeat-8.17.3"}} // 👈 command event {"@timestamp":2025-03-31T13:42:28.068Z,log:{offset:1,file:{path:"/mounted/logfile"}},message:"hello",input:{type:"log"},host:{name:"eb21"},agent:{id:"682cfcf4-f251-4576-abcb-6c8bcadfda08",name:"eb21",type:"filebeat",version:"8.17.3",ephemeral_id:"17f74f6e-36f0-4045-93e6-c549874716df"},ecs:{version:"8.0.0"}} {create:{_index:"filebeat-8.17.3"}} // 👈 command event {"@timestamp":2025-03-31T13:42:28.068Z,log:{offset:7,file:{path:"/mounted/logfile"}},message:"this",input:{type:"log"},host:{name:"eb21"},agent:{id:"682cfcf4-f251-4576-abcb-6c8bcadfda08",name:"eb21",type:"filebeat",version:"8.17.3",ephemeral_id:"17f74f6e-36f0-4045-93e6-c549874716df"},ecs:{version:"8.0.0"}} ``` #### Ship data via Filebeat Configure [Filebeat](https://www.elastic.co/beats/filebeat) as follows to ship data to Tenzir: filebeat.yml ```yaml output: elasticsearch: hosts: ["localhost:9200"] ``` Set `hosts` to the endpoint of the Tenzir pipeline accepting data. #### Ship data via Logstash Configure [Logstash](https://opensearch.org/docs/latest/tools/logstash/ship-to-opensearch/) with the `opensearch` output plugin to ship data to Tenzir: pipeline.conf ```javascript output { opensearch { hosts => "https://localhost:9200" } } ``` Set `hosts` to the endpoint of the Tenzir pipeline accepting data.

# SentinelOne Data Lake

[SentinelOne](https://www.sentinelone.com) is a cybersecurity platform that provides endpoint protection and threat detection. The SentinelOne [Singularity Data Lake](https://www.sentinelone.com/products/singularity-data-lake/) allows you to store and analyze security events at scale. Tenzir can send structured security events to the SentinelOne Data Lake via its REST API. ![SentinelOne Data Lake](/_astro/sentinelone-data-lake.UmtBpY1y_19DKCs.svg) The operator provides special handling for OCSF events. If it detects that the input event in OCSF, it will automatically map timestamp and severity fields to the corresponding SentinelOne Data Lake fields. ## Examples [Section titled “Examples”](#examples) ### Send events to SentinelOne Data Lake [Section titled “Send events to SentinelOne Data Lake”](#send-events-to-sentinelone-data-lake) To send events from a pipeline to SentinelOne Data Lake, use the [`to_sentinelone_data_lake`](/reference/operators/to_sentinelone_data_lake) operator: ```tql subscribe "suricata" where @name == "suricata.alert" to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("SENTINELONE_TOKEN") ``` Replace `https://ingest.eu1.sentinelone.net` with your conigured SentinelOne Data Lake ingest URL and configure the `SENTINELONE_TOKEN` secret with your bearer token. ### Send events with additional session information [Section titled “Send events with additional session information”](#send-events-with-additional-session-information) You can include additional session information that identifies the source of the events: ```tql subscribe "network-events" to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("SENTINELONE_TOKEN"), session_info={ serverHost: "Tenzir Node 01", serverType: "Tenzir Node", region: "US East" } ``` ### Send OCSF events [Section titled “Send OCSF events”](#send-ocsf-events) If the datastream input is valid OCSF, the operator will automatically extract timestamp and severity fields and map them to the corresponding SentinelOne Data Lake fields `ts` and `sev`: ```tql subscribe "ocsf" where severity_id >= 4 // High and Critical events only to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("SENTINELONE_TOKEN"), session_info={serverHost: "Security Gateway"} ``` ### Send unstructured data [Section titled “Send unstructured data”](#send-unstructured-data) You can also use the operator to send unstructured data and let SentinelOne parse it. Simply give the operator a `message` field as input and specify a `parser` in the `session_info` argument: ```tql select message = this.print_ndjson(); // Format the entire event as JSON to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("sentinelone-token"), session_info={ serverHost: "Node 42", parser: "json", // Have SentinelOne parse the data } ``` In this example, we are formatting the entire event as JSON and sending it as the message field. The SentinelOne `json` parser will then parse the event again.

# Snowflake

[Snowflake](https://snowflake.com) is a multi-cloud data warehouse. Tenzir can send events from a pipeline to [Snowflake databases](https://docs.snowflake.com/en/sql-reference/ddl-database). ![Snowflake](/_astro/snowflake.B5IsBZ0A_19DKCs.svg) Use the [`to_snowflake`](/reference/operators/to_snowflake) output operator at the end of a pipeline to send events to a specific table. ## Examples [Section titled “Examples”](#examples) ### Send data to a Snowflake database [Section titled “Send data to a Snowflake database”](#send-data-to-a-snowflake-database) ```tql from {foo: 42, bar: true} to_snowflake \ account_identifier="asldyuf-xgb47555", user_name="tenzir_user", password="password1234", database="MY_DB", schema="MY_SCHEMA", table="TENZIR" ```

# Splunk

[Splunk](https://splunk.com) is a SIEM solution for storing and processing logs. Tenzir can send data to Splunk via HEC. ![Splunk](/_astro/splunk.CB0WFkHp_19DKCs.svg) ## Examples [Section titled “Examples”](#examples) ### Send data to an existing HEC endpoint [Section titled “Send data to an existing HEC endpoint”](#send-data-to-an-existing-hec-endpoint) To send data from a pipeline to a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector) endpoint, use the [`to_splunk`](/reference/operators/to_splunk) operator. For example, deploy the following pipeline to forward [Suricata](/integrations/suricata) alerts to Splunk: ```tql subscribe "suricata" where @name == "suricata.alert" to_splunk "https://1.2.3.4:8088", hec_token="TOKEN", tls_no_verify=true ``` Replace `1.2.3.4` with the IP address of your Splunk host and `TOKEN` with your HEC token. For more details, see the documentation for the [`to_splunk`](/reference/operators/to_splunk) operator. ### Spawn a HEC endpoint as pipeline source [Section titled “Spawn a HEC endpoint as pipeline source”](#spawn-a-hec-endpoint-as-pipeline-source) To send data to a Tenzir pipeline instead of Splunk, you can open a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector) endpoint using the [`from_fluent_bit`](/reference/operators/from_fluent_bit) source operator. For example, to onboard all data into a Tenzir node instead of Splunk, point your data source to the IP address of the Tenzir node at port 9880 by deploying this pipeline: ```tql from_fluent_bit "splunk", options={ splunk_token: "TOKEN", } publish "splunk" ``` Replace `TOKEN` with the Splunk token configured at your data source. To listen on a different IP address, e.g., 1.2.3.4 add `listen: 1.2.3.4` to the `options` argument. For more details, read the official [Fluent Bit documentation of the Splunk input](https://docs.fluentbit.io/manual/pipeline/inputs/splunk). ## Example Co-Deployment [Section titled “Example Co-Deployment”](#example-co-deployment) To test Splunk and Tenzir together, use the following [Docker Compose](https://docs.docker.com/compose/) setup. ### Setup the containers [Section titled “Setup the containers”](#setup-the-containers) docker-compose.yaml ```yaml version: "3.9" services: splunk: image: ${SPLUNK_IMAGE:-splunk/splunk:latest} platform: linux/amd64 container_name: splunk environment: - SPLUNK_START_ARGS=--accept-license - SPLUNK_HEC_TOKEN=abcd1234 - SPLUNK_PASSWORD=tenzir123 ports: - 8000:8000 - 8088:8088 tenzir-node: container_name: "Demo" image: tenzir/tenzir:latest pull_policy: always environment: - TENZIR_PLUGINS__PLATFORM__CONTROL_ENDPOINT=wss://ws.tenzir.app/production - TENZIR_PLUGINS__PLATFORM__API_KEY=<PLATFORM_API_KEY> - TENZIR_PLUGINS__PLATFORM__TENANT_ID=<PLATFORM_TENANT_ID> - TENZIR_ENDPOINT=tenzir-node:5158 entrypoint: - tenzir-node volumes: - tenzir-node:/var/lib/tenzir/ - tenzir-node:/var/log/tenzir/ tenzir: image: tenzir/tenzir:latest pull_policy: never profiles: - donotstart depends_on: - tenzir-node environment: - TENZIR_ENDPOINT=tenzir-node:5158 volumes: tenzir-node: driver: local ``` ### Configure Splunk [Section titled “Configure Splunk”](#configure-splunk) After you spun up the containers, configure Splunk as follows: 1. Go to `http://localhost:8000` and login with `admin`:`tenzir123` 2. Navigate to *Add data* → *Monitor* → *HTTP Event Collector* 3. Configure the event collector: * Name: Tenzir * Click *Next* * Copy the token * Keep *Start searching*

# Suricata

[Suricata](https://suricata.io/) is network monitor with a rule matching engine to detect threats. Use Tenzir to acquire, process, and store Suricata logs. ![Suricata](/_astro/suricata.DtM9yz2q_19DKCs.svg) ## Examples [Section titled “Examples”](#examples) ### Ingest EVE JSON logs into a node [Section titled “Ingest EVE JSON logs into a node”](#ingest-eve-json-logs-into-a-node) [EVE JSON](https://docs.suricata.io/en/latest/output/eve/eve-json-output.html) is the log format in which Suricata generates events. A typical Suricata configuration looks like this: suricata.yaml ```yaml outputs: # Extensible Event Format (nicknamed EVE) event log in JSON format - eve-log: enabled: yes filetype: regular #regular|syslog|unix_dgram|unix_stream|redis filename: eve.json ``` The `filetype` setting determines how you’d process the log file and defaults to `regular`. Onboard Suricata EVE JSON logs via the [`read_suricata`](/reference/operators/read_suricata) operator as follows: ```tql load_file "/path/to/eve.json" read_suricata publish "suricata" ``` If your set `filetype` to `unix_stream`, you need to create a Unix domain socket first, e.g., like this: ```bash nc -U -l /tmp/eve.socket ``` Then use the same pipeline as above; Tenzir automatically detects the file type.

# Syslog

Tenzir supports parsing and emitting Syslog messages across multiple transport protocols, including both UDP and TCP. This enables seamless integration with Syslog-based systems for ingesting or exporting logs. ![Syslog](/_astro/syslog.B1nQyzBj_19DKCs.svg) Syslog support in Tenzir is powered by two components: * [`read_syslog`](/reference/operators/read_syslog): a parser that turns unstructured Syslog messages into structured events. * [`write_syslog`](/reference/operators/write_syslog): a printer that transforms structured events into compliant Syslog messages. Together, these building blocks enable round-trip Syslog processing. ## Examples [Section titled “Examples”](#examples) ### Create a Syslog Server [Section titled “Create a Syslog Server”](#create-a-syslog-server) To receive Syslog messages on a UDP socket, use `from` with [`read_syslog`](/reference/operators/read_syslog): ```tql from "udp://0.0.0.0:514", insert_newlines=true { read_syslog } publish "syslog" ``` To use TCP instead of UDP, change the scheme and omit the `insert_newlines` option: ```tql from "tcp://0.0.0.0:514" { read_syslog } publish "syslog" ``` ### Parsing CEF, LEEF, or JSON Payloads [Section titled “Parsing CEF, LEEF, or JSON Payloads”](#parsing-cef-leef-or-json-payloads) If your Syslog messages embed structured formats like CEF, LEEF, or JSON, you can follow up with an additional parser. For example, assume you have a Syslog message that includes CEF: ```txt Nov 13 16:00:02 host123 FOO: CEF:0|FORCEPOINT|Firewall|6.6.1|78002|TLS connection state|0|deviceExternalId=Master FW node 1 dvc=10.1.1.40 dvchost=10.1.1.40 msg=TLS: Couldn't establish TLS connection (11, N/A) deviceFacility=Management rt=Jan 17 2020 08:52:09 ``` Why you throw [`read_syslog`](/reference/operators/read_syslog) at this line, you’ll get this output: sample.syslog ```tql { facility: null, severity: null, timestamp: "Nov 13 16:00:02", hostname: "host123", app_name: "FOO", process_id: null, content: "CEF:0|FORCEPOINT|Firewall|6.6.1|78002|TLS connection state|0|deviceExternalId=Master FW node 1 dvc=10.1.1.40 dvchost=10.1.1.40 msg=TLS: Couldn't establish TLS connection (11, N/A) deviceFacility=Management rt=Jan 17 2020 08:52:09", } ``` Note that the `content` field is just a big string. Parse it with [`parse_cef`](/reference/functions/parse_cef): ```tql load_file "/tmp/sample.syslog" read_syslog content = content.parse_cef() ``` This yields the following structured output: ```tql { facility: null, severity: null, timestamp: "Nov 13 16:00:02", hostname: "host123", app_name: "FOO", process_id: null, content: { cef_version: 0, device_vendor: "FORCEPOINT", device_product: "Firewall", device_version: "6.6.1", signature_id: "78002", name: "TLS connection state", severity: "0", extension: { deviceExternalId: "Master FW node 1", dvc: 10.1.1.40, dvchost: 10.1.1.40, msg: "TLS: Couldn't establish TLS connection (11, N/A)", deviceFacility: "Management", rt: "Jan 17 2020 08:52:09", }, }, } ``` ### Handling Multi-line Syslog Messages [Section titled “Handling Multi-line Syslog Messages”](#handling-multi-line-syslog-messages) Tenzir’s Syslog parser supports multi-line messages using a heuristic: 1. Split the input at newlines. 2. Try parsing the next line as a new Syslog message. 3. If successful, treat it as a new message. 4. If parsing fails, append the line to the current message and repeat. This allows ingesting logs with stack traces or other verbose content correctly. ## Emit Events as Syslog [Section titled “Emit Events as Syslog”](#emit-events-as-syslog) Tenzir also supports **creating** Syslog messages from structured events via [`write_syslog`](/reference/operators/write_syslog). Here’s a basic example that emits a single Syslog line over UDP: ```tql from { facility: 3, severity: 6, timestamp: 2020-03-02T18:44:46, hostname: "parallels-Parallels-Virtual-Platform", app_name: "packagekitd", process_id: "1370", message_id: "", structured_data: {}, message: " PARENT process running...", } write_syslog save_udp "1.2.3.4:514" ``` This pipeline sends the following RFC 5424-formatted message to `1.2.3.4:514/udp`: ```txt <30>1 2020-03-02T18:44:46.000000Z parallels-Parallels-Virtual-Platform packagekitd 1370 - - PARENT process running... ``` ### Example with Structured Data [Section titled “Example with Structured Data”](#example-with-structured-data) Here is a richer event with structured Syslog fields. Let’s create a Syslog event from it: ```tql from { facility: 20, severity: 5, version: 8, timestamp: 2003-10-11T22:14:15, hostname: "mymachineexamplecom", app_name: "evntslog", process_id: "", message_id: "ID47", structured_data: { "exampleSDID@32473": { iut: 5, eventSource: "Applic\\ation", eventID: 1011, }, "examplePriority@32473": { class: "high", }, }, message: null, } write_syslog ``` Output: ```txt <165>1 2003-10-11T22:14:15.000000Z mymachineexamplecom evntslog - ID47 [exampleSDID@32473 iut="5" eventSource="Applic\\ation" eventID="1011"][examplePriority@32473 class="high"] ``` The [`write_syslog`](/reference/operators/write_syslog) operator converts the `structured_data` field into a valid [RFC 5424](https://datatracker.ietf.org/doc/html/rfc5424) structured block.

# TCP

The [Transmission Control Protocol (TCP)](https://en.wikipedia.org/wiki/Transmission_Control_Protocol) offers a bi-directional byte stream between applications that communicate via IP. Tenzir supports writing to and reading from TCP sockets, both in server (listening) and client (connect) mode. ![TCP](/_astro/tcp.C0uV4vEi_19DKCs.svg) Use the IP address `0.0.0.0` to listen on all available network interfaces. ## SSL/TLS [Section titled “SSL/TLS”](#ssltls) To enable TLS, use `tls=true`. You can optionally pass a PEM-encoded certificate and private key via the `certfile` and `keyfile` options. For testing purposes, you can quickly generate a self-signed certificate as follows: ```bash openssl req -x509 -newkey rsa:2048 -keyout key_and_cert.pem -out key_and_cert.pem -days 365 -nodes ``` An easy way to test a TLS connection is to try connecting via OpenSSL: ```bash openssl s_client 127.0.0.1:443 ``` ## Examples [Section titled “Examples”](#examples) ### Read data by connecting to a remote TCP server [Section titled “Read data by connecting to a remote TCP server”](#read-data-by-connecting-to-a-remote-tcp-server) ```tql from "tcp://127.0.0.1:443", connect=true { read_json } ``` ### Read data by listen on localhost with TLS enabled [Section titled “Read data by listen on localhost with TLS enabled”](#read-data-by-listen-on-localhost-with-tls-enabled) ```tql from "tcp://127.0.0.1:443", tls=true, certfile="cert.pem", keyfile="key.pem" { read_json } ```

# UDP

The [User Datagram Protocol (UDP)](https://en.wikipedia.org/wiki/User_Datagram_Protocol) is a connection-less protocol to send messages on an IP network. Tenzir supports writing to and reading from UDP sockets, both in server (listening) and client (connect) mode. ![UDP](/_astro/udp.BzerWlJj_19DKCs.svg) Use the IP address `0.0.0.0` to listen on all available network interfaces. ## Examples [Section titled “Examples”](#examples) ### Accept Syslog messages over UDP [Section titled “Accept Syslog messages over UDP”](#accept-syslog-messages-over-udp) ```tql from "udp://127.0.0.1:541", insert_newlines=true { read_syslog } ``` ### Send events to a UDP socket [Section titled “Send events to a UDP socket”](#send-events-to-a-udp-socket) ```tql from {message: "Tenzir"} to "udp://1.2.3.4:8080" { write_ndjson } ```

# Velociraptor

[Velociraptor](https://docs.velociraptor.app) is a digital forensics and incident response (DFIR) tool for interrogating endpoints. Use Tenzir to conveniently speak with a Velociraptor server over the [gRPC API](https://docs.velociraptor.app/docs/server_automation/server_api/). ![Velociraptor](/_astro/velociraptor.3H76nfIW_19DKCs.svg) ## Create a TLS certificate to communicate with Velociraptor [Section titled “Create a TLS certificate to communicate with Velociraptor”](#create-a-tls-certificate-to-communicate-with-velociraptor) The `velociraptor` acts as client and establishes a connection to a Velociraptor server via gRPC. All Velociraptor client-to-server communication is mutually authenticated and encrypted via TLS certificates. This means you must provide a client-side certificate, which you can generate as follows. (Velociraptor ships as a static binary that we refer to as `velociraptor-binary` here.) 1. Create a server configuration `server.yaml`: ```bash velociraptor-binary config generate > server.yaml ``` 2. Create an API client: ```bash velociraptor-binary -c server.yaml config api_client --name tenzir client.yaml ``` Copy the generated `client.yaml` to your Tenzir [plugin configuration](/reference/node/configuration) directory as `velociraptor.yaml` so that the operator can find it: ```bash cp client.yaml /etc/tenzir/plugin/velociraptor.yaml ``` 3. Create a user (e.g., an admin named `tenzir`): ```bash velociraptor-binary -v -c server.yaml user add --role administrator tenzir ``` 4. Run the frontend with the server configuration: ```bash velociraptor-binary -c server.yaml frontend ``` ## Examples [Section titled “Examples”](#examples) ### Run raw VQL [Section titled “Run raw VQL”](#run-raw-vql) After you have created a TLS certificate, you can use the [`from_velociraptor`](/reference/operators/from_velociraptor) operator to execute a [Velociraptor Query Language (VQL)](https://docs.velociraptor.app/docs/vql/) query: ```tql from_velociraptor query="select * from pslist()" select Name, Pid, PPid, CommandLine where Name == "remotemanagement" ``` ### Subscribe to forensic artifacts [Section titled “Subscribe to forensic artifacts”](#subscribe-to-forensic-artifacts) You can also hunt for forensic artifacts, such as dropped files or specific entries in the Windows registry, on assets connected to your Velociraptor server. Every time a client reports back on an artifact that matches a given Regex, e.g., `Windows` or `Windows.Sys.StartupItems`, the Velociraptor server sends the result into the pipeline. For example, run this pipeline to subscribe to an artifact collection of Windows startup items and import them into a node: ```tql from_velociraptor subscribe="Windows.Sys.StartupItems" import ```

# Zeek

The [Zeek](https://zeek.org) network monitor translates raw packets into structured logs. Tenzir supports various Zeek use cases, such as continuous ingestion, ad-hoc log file processing, and even generating Zeek logs. ![Zeek](/_astro/zeek.dLhB74jT_19DKCs.svg) Zeek logs come in three forms in practice, all of which Tenzir can parse natively: 1. Tab-Separated Values (TSV) with a custom header. 2. One NDJSON file for all log types combined (aka. *JSON Streaming*) 3. One NDJSON file per log type. ## Examples [Section titled “Examples”](#examples) ### Ingest logs into a node [Section titled “Ingest logs into a node”](#ingest-logs-into-a-node) To ingest Zeek logs into a Tenzir node, you have multiple options. #### Easy-button import with the official Zeek package [Section titled “Easy-button import with the official Zeek package”](#easy-button-import-with-the-official-zeek-package) Our official [Zeek package](https://github.com/tenzir/zeek-tenzir) makes it easy to ship your Zeek logs to a Tenzir node. Install the package first: ```bash zkg install zeek-tenzir ``` Then add this to your `$PREFIX/share/zeek/site/local.zeek` to send all logs that Zeek produces to a Tenzir node: ```plaintext @load tenzir/import # Uncomment to keep the original Zeek logs. # redef Tenzir::delete_after_postprocesing=F; ``` For ad-hoc command line processing you can also pass `tenzir/import` to a Zeek invocation: ```bash # Ship logs to it and delete the original files. zeek -r trace.pcap tenzir/import # Ship logs to it and keep the original files. zeek -r trace.pcap tenzir/import Tenzir::delete_after_postprocesing=F ``` #### Run an import pipeline when rotating logs [Section titled “Run an import pipeline when rotating logs”](#run-an-import-pipeline-when-rotating-logs) If you cannot use our Zeek package, it is still possible to let Zeek trigger an import pipeline upon rotation. Zeek’s [logging framework](https://docs.zeek.org/en/master/frameworks/logging.html) can execute a shell script whenever it rotates a log file. This requires setting `Log::default_rotation_interval` to a non-zero value. The default of `0 secs` means that log rotation is disabled. Add this to `$PREFIX/share/zeek/site/local.zeek`, which is the place for local configuration changes: ```plaintext redef Log::default_rotation_interval = 1 day; ``` Then redefine [`Log::default_rotation_postprocessor_cmd`](https://docs.zeek.org/en/master/scripts/base/frameworks/logging/main.zeek.html#id-Log::default_rotation_postprocessor_cmd) to point to your shell script, e.g., `/usr/local/bin/ingest`: ```plaintext redef Log::default_rotation_postprocessor_cmd=/usr/local/bin/ingest; ``` Here is an example `ingest` script that imports all files into a Tenzir node: ingest ```bash #!/bin/sh file_name="$1" base_name="$2" from="$3" to="$4" terminating="$5" writer="$6" if [ "$writer" = "ascii" ]; then read="read_zeek_tsv" elif [ "$writer" = "json" ]; then read="read_zeek_json" else echo "unsupported Zeek writer: $writer" exit 1 fi pipeline="load_file \"$file_name\" | $read | import" tenzir "$pipeline" ``` Our blog post [Native Zeek Log Rotation & Shipping](https://tenzir.com/blog/native-zeek-log-rotation-and-shipping) provides further details on this method. ### Run Zeek on a packet pipeline [Section titled “Run Zeek on a packet pipeline”](#run-zeek-on-a-packet-pipeline) You can run Zeek on a pipeline of PCAP packets and continue processing the logs in the same pipeline. A stock Tenzir installation comes with a user-defined `zeek` operator that looks as follows: tenzir.yaml ```yaml tenzir: operators: zeek: | shell "eval \"$(zkg env)\" && zeek -r - LogAscii::output_to_stdout=T JSONStreaming::disable_default_logs=T JSONStreaming::enable_log_rotation=F json-streaming-logs" read_zeek_json ``` This allows you run Zeek on a packet trace as follows: ```bash tenzir 'load_file "/path/to/trace.pcap" | zeek' ``` You can also perform more elaborate packet filtering after light-weight [decapsulation](/reference/functions/decapsulate): ```bash tenzir 'load_file "/path/to/trace.pcap" read_pcap this = decapsulate(this) where ip.src in 10.0.0.0/8 || community == "1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=" write_pcap zeek' ``` ### Process logs with a pipeline on the command line [Section titled “Process logs with a pipeline on the command line”](#process-logs-with-a-pipeline-on-the-command-line) Zeek ships with a helper utility `zeek-cut` that operators on Zeek’s tab-separated logs. For example, to extract the host pairs from a conn log: ```bash zeek-cut id.orig_h id.resp_h < conn.log ``` The list of arguments to `zeek-cut` are the column names of the log. The [`select`](/reference/operators/select) operator performs the equivalent in Tenzir after we parse the logs as Zeek TSV: ```bash tenzir 'read_zeek_tsv | select id.orig_h id.resp_h' < conn.log ``` Since pipelines are *multi-schema* and the Zeek TSV parser is aware of log boundaries, you can also concatenate logs of various types: ```bash cat *.log | tenzir 'read_zeek_tsv | select id.orig_h id.resp_h' ``` ### Generate Zeek TSV from arbitrary data [Section titled “Generate Zeek TSV from arbitrary data”](#generate-zeek-tsv-from-arbitrary-data) You can render any data as Zeek TSV log using [`write_zeek_tsv`](/reference/operators/write_zeek_tsv): For example, this is how you create a filtered version of a Zeek conn.log: ```tql subscribe "zeek" where @name == "zeek.conn" where duration > 2s and id.orig_p != 80 write_zeek_tsv save_file "filtered_conn.log" ```

# ZeroMQ

[ZeroMQ](https://zeromq.org/) (0mq) is a light-weight messaging framework with various socket types. Tenzir supports writing to [PUB sockets](https://zeromq.org/socket-api/#pub-socket) and reading from [SUB sockets](https://zeromq.org/socket-api/#sub-socket), both in server (listening) and client (connect) mode. ![ZeroMQ](/_astro/zeromq.CtgcNXCM_19DKCs.svg) Use the IP address `0.0.0.0` to listen on all available network interfaces. Because ZeroMQ is entirely asynchronous, publishers send messages even when no subscriber is present. This can lead to lost messages when the publisher begins operating before the subscriber. To avoid data loss due to such races, pass `monitor=true` to activate message buffering until at least one remote peer has connected. ## Examples [Section titled “Examples”](#examples) ### Accept Syslog messages over ZeroMQ [Section titled “Accept Syslog messages over ZeroMQ”](#accept-syslog-messages-over-zeromq) ```tql from "zmq://127.0.0.1:541" { read_syslog } ``` ### Send events to a ZeroMQ socket [Section titled “Send events to a ZeroMQ socket”](#send-events-to-a-zeromq-socket) ```tql from {message: "Tenzir"} to "zmq://1.2.3.4:8080" { write_ndjson } ```

# Zscaler

Zscaler’s [Nanolog Streaming Service (NSS)](https://help.zscaler.com/zia/understanding-nanolog-streaming-service) is a family of products that enable Zscaler cloud communication with third-party security solution devices for exchanging event logs. You can either use Zscaler’s Cloud NSS or deploy an on-prem NSS server to obtain the logs. Tenzir can receive Zscaler logs in either case. ![Zscaler NSS](/_astro/zscaler.CbIXkL3W_19DKCs.svg) ## Use a Cloud NSS Feed to send events to Tenzir [Section titled “Use a Cloud NSS Feed to send events to Tenzir”](#use-a-cloud-nss-feed-to-send-events-to-tenzir) ### Configure Tenzir [Section titled “Configure Tenzir”](#configure-tenzir) First, spin up a Tenzir pipeline that mimics a [Splunk HEC endpoint](/integrations/splunk): ```tql from_fluent_bit "splunk", options={ listen: 0.0.0.0, port: 8088, splunk_token: YOUR_TOKEN, } publish "zscaler" ``` In the above example, the pipeline uses `0.0.0.0` to listen on all IP addresses available. ### Create a Cloud NSS Feed [Section titled “Create a Cloud NSS Feed”](#create-a-cloud-nss-feed) Perform the following steps to create a Cloud NSS feed: 1. Log in to your ZIA Admin Portal and go to *Administration → Nanolog Streaming Service*. 2. Select the *Cloud NSS Feeds* tab. 3. Click *Add Cloud NSS Feed*. In the new dialog, configure the following options: * **Feed Name**: Enter a name, e.g., `Tenzir ZIA Logs` * **NSS Type**: NSS for Web * **Status**: Enabled * **SIEM Rate**: Unlimited * **SIEM Type**: Other * **OAuth 2.0 Authentication**: disabled * **Max Batch Size**: 16 KB * **API URL**: Enter the URL that identifies the Tenzir pipeline where the Splunk HEC endpoint is listening, e.g., <https://1.2.3.4:8080/services/collector>. * **HTTP Headers**: Add your token from the Tenzir pipeline enable compression with the following two headers. * `Authorization`: `YOUR_TOKEN` * `Content-Encoding`: `gzip` * **Feed Output Type**: JSON * **JSON Array Notation**: disabled * **Feed Escape Character**: `,\"` After a web, firewall, or DNS feed has been configured, activate the changes as needed and test the feed. To test the connection: 1. Go to *Administration* → *Nanolog Streaming Service* → *Cloud NSS Feeds*. 2. Click the *Cloud icon*. This sends a test message to the Tenzir receiver. ## Use an NSS Feed to send events to Tenzir [Section titled “Use an NSS Feed to send events to Tenzir”](#use-an-nss-feed-to-send-events-to-tenzir) The on-prem NSS server VM pulls logs from the Zscaler cloud and pushes them to Tenzir via Syslog over TCP. The NSS server also buffers logs if the TCP connection goes down. Unencrypted Communication Note that the forwarding TCP connection is *unencrypted*, as the assumption is that the NSS server is on premises next to the receiver. ### Configure Tenzir [Section titled “Configure Tenzir”](#configure-tenzir-1) Spin up a Tenzir pipeline that accepts [TCP](/integrations/tcp) connection and parses [Syslog](/integrations/syslog): ```tql from "tcp://0.0.0.0:514" { read_syslog } publish "zscaler" ``` In the above example, the pipeline uses `0.0.0.0` to listen on all IP addresses available. Depending on how your NSS Feed is configured, you can futher dissect the opaque Syslog message into a structured record. ### Create an NSS Feed [Section titled “Create an NSS Feed”](#create-an-nss-feed) First, [deploy an NSS server](https://help.zscaler.com/zia/adding-nss-servers). Then log into ZIA using your administrator account: 1. Go to *Administration* → *Cloud Configuration* → *Nanolog Streaming Service*. 2. Verify that the NSS State is Healthy. Deploy a new NSS server by following the steps at [NSS Deployment Guides](https://help.zscaler.com/zia/nanolog-streaming-service). 3. Click the *NSS Feeds* tab, and then click *Add NSS Feed*. Follow the [official documentation to add an NSS feed](https://help.zscaler.com/zia/adding-nss-feeds). You can add up to 16 NSS feeds per NSS server. In the new dialog, configure the following options: * **Feed Name**: Enter a name, e.g., `Tenzir ZIA Logs` * **NSS Server**: Choose your server * **Status**: Enabled * **SIEM Destination Type**: FQDN * **SIEM IP Address**: Enter the address of your Tenzir Node * **SIEM TCP Port**: Enter the port of your pipeline * **SIEM Rate**: Unlimited * **Feed Output Type**: JSON * **Feed Escape Character**: `,\"`