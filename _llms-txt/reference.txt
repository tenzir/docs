<SYSTEM>Reference</SYSTEM>

# Functions

Functions appear in [expressions](/explanations/language/expressions) and take positional and/or named arguments, producing a value as a result of their computation. Function signatures have the following notation: ```tql f(arg1:<type>, arg2=<type>, [arg3=type]) -> <type> ``` * `arg:<type>`: positional argument * `arg=<type>`: named argument * `[arg=type]`: optional (named) argument * `-> <type>`: function return type ## Aggregation [Section titled â€œAggregationâ€](#aggregation) ### [all](/reference/functions/all) [â†’](/reference/functions/all) Computes the conjunction (AND) of all grouped boolean values. ```tql all([true,true,false]) ``` ### [any](/reference/functions/any) [â†’](/reference/functions/any) Computes the disjunction (OR) of all grouped boolean values. ```tql any([true,false,true]) ``` ### [collect](/reference/functions/collect) [â†’](/reference/functions/collect) Creates a list of all non-null grouped values, preserving duplicates. ```tql collect([1,2,2,3]) ``` ### [count](/reference/functions/count) [â†’](/reference/functions/count) Counts the events or non-null grouped values. ```tql count([1,2,null]) ``` ### [count\_distinct](/reference/functions/count_distinct) [â†’](/reference/functions/count_distinct) Counts all distinct non-null grouped values. ```tql count_distinct([1,2,2,3]) ``` ### [count\_if](/reference/functions/count_if) [â†’](/reference/functions/count_if) Counts the events or non-null grouped values matching a given predicate. ```tql count_if([1,2,null], x => x > 1) ``` ### [distinct](/reference/functions/distinct) [â†’](/reference/functions/distinct) Creates a sorted list without duplicates of non-null grouped values. ```tql distinct([1,2,2,3]) ``` ### [entropy](/reference/functions/entropy) [â†’](/reference/functions/entropy) Computes the Shannon entropy of all grouped values. ```tql entropy([1,1,2,3]) ``` ### [first](/reference/functions/first) [â†’](/reference/functions/first) Takes the first non-null grouped value. ```tql first([null,2,3]) ``` ### [last](/reference/functions/last) [â†’](/reference/functions/last) Takes the last non-null grouped value. ```tql last([1,2,null]) ``` ### [max](/reference/functions/max) [â†’](/reference/functions/max) Computes the maximum of all grouped values. ```tql max([1,2,3]) ``` ### [mean](/reference/functions/mean) [â†’](/reference/functions/mean) Computes the mean of all grouped values. ```tql mean([1,2,3]) ``` ### [median](/reference/functions/median) [â†’](/reference/functions/median) Computes the approximate median of all grouped values using a t-digest algorithm. ```tql median([1,2,3,4]) ``` ### [min](/reference/functions/min) [â†’](/reference/functions/min) Computes the minimum of all grouped values. ```tql min([1,2,3]) ``` ### [mode](/reference/functions/mode) [â†’](/reference/functions/mode) Takes the most common non-null grouped value. ```tql mode([1,1,2,3]) ``` ### [otherwise](/reference/functions/otherwise) [â†’](/reference/functions/otherwise) Returns a `fallback` value if `primary` is `null`. ```tql x.otherwise(0) ``` ### [quantile](/reference/functions/quantile) [â†’](/reference/functions/quantile) Computes the specified quantile of all grouped values. ```tql quantile([1,2,3,4], q=0.5) ``` ### [stddev](/reference/functions/stddev) [â†’](/reference/functions/stddev) Computes the standard deviation of all grouped values. ```tql stddev([1,2,3]) ``` ### [sum](/reference/functions/sum) [â†’](/reference/functions/sum) Computes the sum of all values. ```tql sum([1,2,3]) ``` ### [value\_counts](/reference/functions/value_counts) [â†’](/reference/functions/value_counts) Returns a list of all grouped values alongside their frequency. ```tql value_counts([1,2,2,3]) ``` ### [variance](/reference/functions/variance) [â†’](/reference/functions/variance) Computes the variance of all grouped values. ```tql variance([1,2,3]) ``` ## Bit Operations [Section titled â€œBit Operationsâ€](#bit-operations) ### [bit\_and](/reference/functions/bit_and) [â†’](/reference/functions/bit_and) Computes the bit-wise AND of its arguments. ```tql bit_and(lhs, rhs) ``` ### [bit\_not](/reference/functions/bit_not) [â†’](/reference/functions/bit_not) Computes the bit-wise NOT of its argument. ```tql bit_not(x) ``` ### [bit\_or](/reference/functions/bit_or) [â†’](/reference/functions/bit_or) Computes the bit-wise OR of its arguments. ```tql bit_or(lhs, rhs) ``` ### [bit\_xor](/reference/functions/bit_xor) [â†’](/reference/functions/bit_xor) Computes the bit-wise XOR of its arguments. ```tql bit_xor(lhs, rhs) ``` ### [shift\_left](/reference/functions/shift_left) [â†’](/reference/functions/shift_left) Performs a bit-wise left shift. ```tql shift_left(lhs, rhs) ``` ### [shift\_right](/reference/functions/shift_right) [â†’](/reference/functions/shift_right) Performs a bit-wise right shift. ```tql shift_right(lhs, rhs) ``` ## Decoding [Section titled â€œDecodingâ€](#decoding) ### [decode\_base64](/reference/functions/decode_base64) [â†’](/reference/functions/decode_base64) Decodes bytes as Base64. ```tql decode_base64("VGVuemly") ``` ### [decode\_hex](/reference/functions/decode_hex) [â†’](/reference/functions/decode_hex) Decodes bytes from their hexadecimal representation. ```tql decode_hex("4e6f6E6365") ``` ### [decode\_url](/reference/functions/decode_url) [â†’](/reference/functions/decode_url) Decodes URL encoded strings. ```tql decode_url("Hello%20World") ``` ## Encoding [Section titled â€œEncodingâ€](#encoding) ### [encode\_base64](/reference/functions/encode_base64) [â†’](/reference/functions/encode_base64) Encodes bytes as Base64. ```tql encode_base64("Tenzir") ``` ### [encode\_hex](/reference/functions/encode_hex) [â†’](/reference/functions/encode_hex) Encodes bytes into their hexadecimal representation. ```tql encode_hex("Tenzir") ``` ### [encode\_url](/reference/functions/encode_url) [â†’](/reference/functions/encode_url) Encodes strings using URL encoding. ```tql encode_url("Hello World") ``` ## Hashing [Section titled â€œHashingâ€](#hashing) ### [hash\_md5](/reference/functions/hash_md5) [â†’](/reference/functions/hash_md5) Computes an MD5 hash digest. ```tql hash_md5("foo") ``` ### [hash\_sha1](/reference/functions/hash_sha1) [â†’](/reference/functions/hash_sha1) Computes a SHA-1 hash digest. ```tql hash_sha1("foo") ``` ### [hash\_sha224](/reference/functions/hash_sha224) [â†’](/reference/functions/hash_sha224) Computes a SHA-224 hash digest. ```tql hash_sha224("foo") ``` ### [hash\_sha256](/reference/functions/hash_sha256) [â†’](/reference/functions/hash_sha256) Computes a SHA-256 hash digest. ```tql hash_sha256("foo") ``` ### [hash\_sha384](/reference/functions/hash_sha384) [â†’](/reference/functions/hash_sha384) Computes a SHA-384 hash digest. ```tql hash_sha384("foo") ``` ### [hash\_sha512](/reference/functions/hash_sha512) [â†’](/reference/functions/hash_sha512) Computes a SHA-512 hash digest. ```tql hash_sha512("foo") ``` ### [hash\_xxh3](/reference/functions/hash_xxh3) [â†’](/reference/functions/hash_xxh3) Computes an XXH3 hash digest. ```tql hash_xxh3("foo") ``` ## IP [Section titled â€œIPâ€](#ip) ### [ip\_category](/reference/functions/ip_category) [â†’](/reference/functions/ip_category) Returns the type classification of an IP address. ```tql ip_category(8.8.8.8) ``` ### [is\_global](/reference/functions/is_global) [â†’](/reference/functions/is_global) Checks whether an IP address is a global address. ```tql is_global(8.8.8.8) ``` ### [is\_link\_local](/reference/functions/is_link_local) [â†’](/reference/functions/is_link_local) Checks whether an IP address is a link-local address. ```tql is_link_local(169.254.1.1) ``` ### [is\_loopback](/reference/functions/is_loopback) [â†’](/reference/functions/is_loopback) Checks whether an IP address is a loopback address. ```tql is_loopback(127.0.0.1) ``` ### [is\_multicast](/reference/functions/is_multicast) [â†’](/reference/functions/is_multicast) Checks whether an IP address is a multicast address. ```tql is_multicast(224.0.0.1) ``` ### [is\_private](/reference/functions/is_private) [â†’](/reference/functions/is_private) Checks whether an IP address is a private address. ```tql is_private(192.168.1.1) ``` ### [is\_v4](/reference/functions/is_v4) [â†’](/reference/functions/is_v4) Checks whether an IP address has version number 4. ```tql is_v4(1.2.3.4) ``` ### [is\_v6](/reference/functions/is_v6) [â†’](/reference/functions/is_v6) Checks whether an IP address has version number 6. ```tql is_v6(::1) ``` ### [network](/reference/functions/network) [â†’](/reference/functions/network) Retrieves the network address of a subnet. ```tql 10.0.0.0/8.network() ``` ## List [Section titled â€œListâ€](#list) ### [add](/reference/functions/add) [â†’](/reference/functions/add) Adds an element into a list if it doesn't already exist (set-insertion). ```tql xs.add(y) ``` ### [append](/reference/functions/append) [â†’](/reference/functions/append) Inserts an element at the back of a list. ```tql xs.append(y) ``` ### [concatenate](/reference/functions/concatenate) [â†’](/reference/functions/concatenate) Merges two lists. ```tql concatenate(xs, ys) ``` ### [get](/reference/functions/get) [â†’](/reference/functions/get) Gets a field from a record or an element from a list ```tql xs.get(index, fallback) ``` ### [length](/reference/functions/length) [â†’](/reference/functions/length) Retrieves the length of a list. ```tql [1,2,3].length() ``` ### [map](/reference/functions/map) [â†’](/reference/functions/map) Maps each list element to an expression. ```tql xs.map(x => x + 3) ``` ### [prepend](/reference/functions/prepend) [â†’](/reference/functions/prepend) Inserts an element at the start of a list. ```tql xs.prepend(y) ``` ### [remove](/reference/functions/remove) [â†’](/reference/functions/remove) Removes all occurrences of an element from a list. ```tql xs.remove(y) ``` ### [sort](/reference/functions/sort) [â†’](/reference/functions/sort) Sorts lists and record fields. ```tql xs.sort() ``` ### [where](/reference/functions/where) [â†’](/reference/functions/where) Filters list elements based on a predicate. ```tql xs.where(x => x > 5) ``` ### [zip](/reference/functions/zip) [â†’](/reference/functions/zip) Combines two lists into a list of pairs. ```tql zip(xs, ys) ``` ## Math [Section titled â€œMathâ€](#math) ### [abs](/reference/functions/abs) [â†’](/reference/functions/abs) Returns the absolute value. ```tql abs(-42) ``` ### [ceil](/reference/functions/ceil) [â†’](/reference/functions/ceil) Computes the ceiling of a number or a time/duration with a specified unit. ```tql ceil(4.2) ``` ### [floor](/reference/functions/floor) [â†’](/reference/functions/floor) Computes the floor of a number or a time/duration with a specified unit. ```tql floor(4.8) ``` ### [round](/reference/functions/round) [â†’](/reference/functions/round) Rounds a number or a time/duration with a specified unit. ```tql round(4.6) ``` ### [sqrt](/reference/functions/sqrt) [â†’](/reference/functions/sqrt) Computes the square root of a number. ```tql sqrt(49) ``` ## Networking [Section titled â€œNetworkingâ€](#networking) ### [community\_id](/reference/functions/community_id) [â†’](/reference/functions/community_id) Computes the Community ID for a network connection/flow. ```tql community_id(src_ip=1.2.3.4, dst_ip=4.5.6.7, proto="tcp") ``` ### [decapsulate](/reference/functions/decapsulate) [â†’](/reference/functions/decapsulate) Decapsulates packet data at link, network, and transport layer. ```tql decapsulate(this) ``` ### [encrypt\_cryptopan](/reference/functions/encrypt_cryptopan) [â†’](/reference/functions/encrypt_cryptopan) Encrypts an IP address via Crypto-PAn. ```tql encrypt_cryptopan(1.2.3.4) ``` ## OCSF [Section titled â€œOCSFâ€](#ocsf) ### [ocsf::category\_name](/reference/functions/ocsf/category_name) [â†’](/reference/functions/ocsf/category_name) Returns the `category_name` for a given `category_uid`. ```tql ocsf::category_name(2) ``` ### [ocsf::category\_uid](/reference/functions/ocsf/category_uid) [â†’](/reference/functions/ocsf/category_uid) Returns the `category_uid` for a given `category_name`. ```tql ocsf::category_uid("Findings") ``` ### [ocsf::class\_name](/reference/functions/ocsf/class_name) [â†’](/reference/functions/ocsf/class_name) Returns the `class_name` for a given `class_uid`. ```tql ocsf::class_name(4003) ``` ### [ocsf::class\_uid](/reference/functions/ocsf/class_uid) [â†’](/reference/functions/ocsf/class_uid) Returns the `class_uid` for a given `class_name`. ```tql ocsf::class_uid("DNS Activity") ``` ### [ocsf::type\_name](/reference/functions/ocsf/type_name) [â†’](/reference/functions/ocsf/type_name) Returns the `type_name` for a given `type_uid`. ```tql ocsf::type_name(400704) ``` ### [ocsf::type\_uid](/reference/functions/ocsf/type_uid) [â†’](/reference/functions/ocsf/type_uid) Returns the `type_uid` for a given `type_name`. ```tql ocsf::type_uid("SSH Activity: Fail") ``` ## Parsing [Section titled â€œParsingâ€](#parsing) ### [parse\_cef](/reference/functions/parse_cef) [â†’](/reference/functions/parse_cef) Parses a string as a CEF message ```tql string.parse_cef() ``` ### [parse\_csv](/reference/functions/parse_csv) [â†’](/reference/functions/parse_csv) Parses a string as CSV (Comma-Separated Values). ```tql string.parse_csv(header=["a","b"]) ``` ### [parse\_grok](/reference/functions/parse_grok) [â†’](/reference/functions/parse_grok) Parses a string according to a grok pattern. ```tql string.parse_grok("%{IP:client} â€¦") ``` ### [parse\_json](/reference/functions/parse_json) [â†’](/reference/functions/parse_json) Parses a string as a JSON value. ```tql string.parse_json() ``` ### [parse\_kv](/reference/functions/parse_kv) [â†’](/reference/functions/parse_kv) Parses a string as key-value pairs. ```tql string.parse_kv() ``` ### [parse\_leef](/reference/functions/parse_leef) [â†’](/reference/functions/parse_leef) Parses a string as a LEEF message ```tql string.parse_leef() ``` ### [parse\_ssv](/reference/functions/parse_ssv) [â†’](/reference/functions/parse_ssv) Parses a string as space separated values. ```tql string.parse_ssv(header=["a","b"]) ``` ### [parse\_syslog](/reference/functions/parse_syslog) [â†’](/reference/functions/parse_syslog) Parses a string as a Syslog message. ```tql string.parse_syslog() ``` ### [parse\_tsv](/reference/functions/parse_tsv) [â†’](/reference/functions/parse_tsv) Parses a string as tab separated values. ```tql string.parse_tsv(header=["a","b"]) ``` ### [parse\_xsv](/reference/functions/parse_xsv) [â†’](/reference/functions/parse_xsv) Parses a string as delimiter separated values. ```tql string.parse_xsv(",", ";", "", header=["a","b"]) ``` ### [parse\_yaml](/reference/functions/parse_yaml) [â†’](/reference/functions/parse_yaml) Parses a string as a YAML value. ```tql string.parse_yaml() ``` ## Printing [Section titled â€œPrintingâ€](#printing) ### [print\_cef](/reference/functions/print_cef) [â†’](/reference/functions/print_cef) Prints records as Common Event Format (CEF) messages ```tql extension.print_cef(cef_version="0", device_vendor="Tenzir", device_product="Tenzir Node", device_version="5.5.0", signature_id=id, name="description", severity="7") ``` ### [print\_csv](/reference/functions/print_csv) [â†’](/reference/functions/print_csv) Prints a record as a comma-separated string of values. ```tql record.print_csv() ``` ### [print\_json](/reference/functions/print_json) [â†’](/reference/functions/print_json) Transforms a value into a JSON string. ```tql record.print_json() ``` ### [print\_kv](/reference/functions/print_kv) [â†’](/reference/functions/print_kv) Prints records in a key-value format. ```tql record.print_kv() ``` ### [print\_leef](/reference/functions/print_leef) [â†’](/reference/functions/print_leef) Prints records as LEEF messages ```tql attributes.print_leef(vendor="Tenzir",product_name="Tenzir Node", product_name="5.5.0",event_class_id=id) ``` ### [print\_ndjson](/reference/functions/print_ndjson) [â†’](/reference/functions/print_ndjson) Transforms a value into a single-line JSON string. ```tql record.print_ndjson() ``` ### [print\_ssv](/reference/functions/print_ssv) [â†’](/reference/functions/print_ssv) Prints a record as a space-separated string of values. ```tql record.print_ssv() ``` ### [print\_tsv](/reference/functions/print_tsv) [â†’](/reference/functions/print_tsv) Prints a record as a tab-separated string of values. ```tql record.print_tsv() ``` ### [print\_xsv](/reference/functions/print_xsv) [â†’](/reference/functions/print_xsv) Prints a record as a delimited sequence of values. ```tql record.print_tsv() ``` ### [print\_yaml](/reference/functions/print_yaml) [â†’](/reference/functions/print_yaml) Prints a value as a YAML document. ```tql record.print_yaml() ``` ## Record [Section titled â€œRecordâ€](#record) ### [get](/reference/functions/get) [â†’](/reference/functions/get) Gets a field from a record or an element from a list ```tql xs.get(index, fallback) ``` ### [has](/reference/functions/has) [â†’](/reference/functions/has) Checks whether a record has a specified field. ```tql record.has("field") ``` ### [keys](/reference/functions/keys) [â†’](/reference/functions/keys) Retrieves a list of field names from a record. ```tql record.keys() ``` ### [merge](/reference/functions/merge) [â†’](/reference/functions/merge) Combines two records into a single record by merging their fields. ```tql merge(foo, bar) ``` ### [sort](/reference/functions/sort) [â†’](/reference/functions/sort) Sorts lists and record fields. ```tql xs.sort() ``` ## Runtime [Section titled â€œRuntimeâ€](#runtime) ### [config](/reference/functions/config) [â†’](/reference/functions/config) Reads Tenzir's configuration file. ```tql config() ``` ### [env](/reference/functions/env) [â†’](/reference/functions/env) Reads an environment variable. ```tql env("PATH") ``` ### [secret](/reference/functions/secret) [â†’](/reference/functions/secret) Use the value of a secret. ```tql secret("KEY") ``` ## Subnet [Section titled â€œSubnetâ€](#subnet) ### [network](/reference/functions/network) [â†’](/reference/functions/network) Retrieves the network address of a subnet. ```tql 10.0.0.0/8.network() ``` ## Time & Date [Section titled â€œTime & Dateâ€](#time--date) ### [count\_days](/reference/functions/count_days) [â†’](/reference/functions/count_days) Counts the number of `days` in a duration. ```tql count_days(100d) ``` ### [count\_hours](/reference/functions/count_hours) [â†’](/reference/functions/count_hours) Counts the number of `hours` in a duration. ```tql count_hours(100d) ``` ### [count\_microseconds](/reference/functions/count_microseconds) [â†’](/reference/functions/count_microseconds) Counts the number of `microseconds` in a duration. ```tql count_microseconds(100d) ``` ### [count\_milliseconds](/reference/functions/count_milliseconds) [â†’](/reference/functions/count_milliseconds) Counts the number of `milliseconds` in a duration. ```tql count_milliseconds(100d) ``` ### [count\_minutes](/reference/functions/count_minutes) [â†’](/reference/functions/count_minutes) Counts the number of `minutes` in a duration. ```tql count_minutes(100d) ``` ### [count\_months](/reference/functions/count_months) [â†’](/reference/functions/count_months) Counts the number of `months` in a duration. ```tql count_months(100d) ``` ### [count\_nanoseconds](/reference/functions/count_nanoseconds) [â†’](/reference/functions/count_nanoseconds) Counts the number of `nanoseconds` in a duration. ```tql count_nanoseconds(100d) ``` ### [count\_seconds](/reference/functions/count_seconds) [â†’](/reference/functions/count_seconds) Counts the number of `seconds` in a duration. ```tql count_seconds(100d) ``` ### [count\_weeks](/reference/functions/count_weeks) [â†’](/reference/functions/count_weeks) Counts the number of `weeks` in a duration. ```tql count_weeks(100d) ``` ### [count\_years](/reference/functions/count_years) [â†’](/reference/functions/count_years) Counts the number of `years` in a duration. ```tql count_years(100d) ``` ### [day](/reference/functions/day) [â†’](/reference/functions/day) Extracts the day component from a timestamp. ```tql ts.day() ``` ### [days](/reference/functions/days) [â†’](/reference/functions/days) Converts a number to equivalent days. ```tql days(100) ``` ### [format\_time](/reference/functions/format_time) [â†’](/reference/functions/format_time) Formats a time into a string that follows a specific format. ```tql ts.format_time("%d/ %m/%Y") ``` ### [from\_epoch](/reference/functions/from_epoch) [â†’](/reference/functions/from_epoch) Interprets a duration as Unix time. ```tql from_epoch(time_ms * 1ms) ``` ### [hour](/reference/functions/hour) [â†’](/reference/functions/hour) Extracts the hour component from a timestamp. ```tql ts.hour() ``` ### [hours](/reference/functions/hours) [â†’](/reference/functions/hours) Converts a number to equivalent hours. ```tql hours(100) ``` ### [microseconds](/reference/functions/microseconds) [â†’](/reference/functions/microseconds) Converts a number to equivalent microseconds. ```tql microseconds(100) ``` ### [milliseconds](/reference/functions/milliseconds) [â†’](/reference/functions/milliseconds) Converts a number to equivalent milliseconds. ```tql milliseconds(100) ``` ### [minute](/reference/functions/minute) [â†’](/reference/functions/minute) Extracts the minute component from a timestamp. ```tql ts.minute() ``` ### [minutes](/reference/functions/minutes) [â†’](/reference/functions/minutes) Converts a number to equivalent minutes. ```tql minutes(100) ``` ### [month](/reference/functions/month) [â†’](/reference/functions/month) Extracts the month component from a timestamp. ```tql ts.month() ``` ### [months](/reference/functions/months) [â†’](/reference/functions/months) Converts a number to equivalent months. ```tql months(100) ``` ### [nanoseconds](/reference/functions/nanoseconds) [â†’](/reference/functions/nanoseconds) Converts a number to equivalent nanoseconds. ```tql nanoseconds(100) ``` ### [now](/reference/functions/now) [â†’](/reference/functions/now) Gets the current wallclock time. ```tql now() ``` ### [parse\_time](/reference/functions/parse_time) [â†’](/reference/functions/parse_time) Parses a time from a string that follows a specific format. ```tql "10/11/2012".parse_time("%d/%m/%Y") ``` ### [second](/reference/functions/second) [â†’](/reference/functions/second) Extracts the second component from a timestamp with subsecond precision. ```tql ts.second() ``` ### [seconds](/reference/functions/seconds) [â†’](/reference/functions/seconds) Converts a number to equivalent seconds. ```tql seconds(100) ``` ### [since\_epoch](/reference/functions/since_epoch) [â†’](/reference/functions/since_epoch) Interprets a time value as duration since the Unix epoch. ```tql since_epoch(2021-02-24) ``` ### [weeks](/reference/functions/weeks) [â†’](/reference/functions/weeks) Converts a number to equivalent weeks. ```tql weeks(100) ``` ### [year](/reference/functions/year) [â†’](/reference/functions/year) Extracts the year component from a timestamp. ```tql ts.year() ``` ### [years](/reference/functions/years) [â†’](/reference/functions/years) Converts a number to equivalent years. ```tql years(100) ``` ## Utility [Section titled â€œUtilityâ€](#utility) ### [contains](/reference/functions/contains) [â†’](/reference/functions/contains) Searches for a value within data structures recursively. ```tql this.contains("value") ``` ### [contains\_null](/reference/functions/contains_null) [â†’](/reference/functions/contains_null) Checks whether the input contains any `null` values. ```tql {x: 1, y: null}.contains_null() == true ``` ### [is\_empty](/reference/functions/is_empty) [â†’](/reference/functions/is_empty) Checks whether a value is empty. ```tql "".is_empty() ``` ### [random](/reference/functions/random) [â†’](/reference/functions/random) Generates a random number in *\[0,1]*. ```tql random() ``` ### [uuid](/reference/functions/uuid) [â†’](/reference/functions/uuid) Generates a Universally Unique Identifier (UUID) string. ```tql uuid() ``` ## String [Section titled â€œStringâ€](#string) ### Filesystem [Section titled â€œFilesystemâ€](#filesystem) ### [file\_contents](/reference/functions/file_contents) [â†’](/reference/functions/file_contents) Reads a file's contents. ```tql file_contents("/path/to/file") ``` ### [file\_name](/reference/functions/file_name) [â†’](/reference/functions/file_name) Extracts the file name from a file path. ```tql file_name("/path/to/log.json") ``` ### [parent\_dir](/reference/functions/parent_dir) [â†’](/reference/functions/parent_dir) Extracts the parent directory from a file path. ```tql parent_dir("/path/to/log.json") ``` ### Inspection [Section titled â€œInspectionâ€](#inspection) ### [ends\_with](/reference/functions/ends_with) [â†’](/reference/functions/ends_with) Checks if a string ends with a specified substring. ```tql "hello".ends_with("lo") ``` ### [is\_alnum](/reference/functions/is_alnum) [â†’](/reference/functions/is_alnum) Checks if a string is alphanumeric. ```tql "hello123".is_alnum() ``` ### [is\_alpha](/reference/functions/is_alpha) [â†’](/reference/functions/is_alpha) Checks if a string contains only alphabetic characters. ```tql "hello".is_alpha() ``` ### [is\_lower](/reference/functions/is_lower) [â†’](/reference/functions/is_lower) Checks if a string is in lowercase. ```tql "hello".is_lower() ``` ### [is\_numeric](/reference/functions/is_numeric) [â†’](/reference/functions/is_numeric) Checks if a string contains only numeric characters. ```tql "1234".is_numeric() ``` ### [is\_printable](/reference/functions/is_printable) [â†’](/reference/functions/is_printable) Checks if a string contains only printable characters. ```tql "hello".is_printable() ``` ### [is\_title](/reference/functions/is_title) [â†’](/reference/functions/is_title) Checks if a string follows title case. ```tql "Hello World".is_title() ``` ### [is\_upper](/reference/functions/is_upper) [â†’](/reference/functions/is_upper) Checks if a string is in uppercase. ```tql "HELLO".is_upper() ``` ### [length\_bytes](/reference/functions/length_bytes) [â†’](/reference/functions/length_bytes) Returns the length of a string in bytes. ```tql "hello".length_bytes() ``` ### [length\_chars](/reference/functions/length_chars) [â†’](/reference/functions/length_chars) Returns the length of a string in characters. ```tql "hello".length_chars() ``` ### [match\_regex](/reference/functions/match_regex) [â†’](/reference/functions/match_regex) Checks if a string partially matches a regular expression. ```tql "Hi".match_regex("[Hh]i") ``` ### [slice](/reference/functions/slice) [â†’](/reference/functions/slice) Slices a string with offsets and strides. ```tql "Hi".slice(begin=2, stride=4) ``` ### [starts\_with](/reference/functions/starts_with) [â†’](/reference/functions/starts_with) Checks if a string starts with a specified substring. ```tql "hello".starts_with("he") ``` ### Transformation [Section titled â€œTransformationâ€](#transformation) ### [capitalize](/reference/functions/capitalize) [â†’](/reference/functions/capitalize) Capitalizes the first character of a string. ```tql "hello".capitalize() ``` ### [join](/reference/functions/join) [â†’](/reference/functions/join) Joins a list of strings into a single string using a separator. ```tql join(["a", "b", "c"], ",") ``` ### [pad\_end](/reference/functions/pad_end) [â†’](/reference/functions/pad_end) Pads a string at the end to a specified length. ```tql "hello".pad_end(10) ``` ### [pad\_start](/reference/functions/pad_start) [â†’](/reference/functions/pad_start) Pads a string at the start to a specified length. ```tql "hello".pad_start(10) ``` ### [replace](/reference/functions/replace) [â†’](/reference/functions/replace) Replaces characters within a string. ```tql "hello".replace("o", "a") ``` ### [replace\_regex](/reference/functions/replace_regex) [â†’](/reference/functions/replace_regex) Replaces characters within a string based on a regular expression. ```tql "hello".replace("l+o", "y") ``` ### [reverse](/reference/functions/reverse) [â†’](/reference/functions/reverse) Reverses the characters of a string. ```tql "hello".reverse() ``` ### [split](/reference/functions/split) [â†’](/reference/functions/split) Splits a string into substrings. ```tql split("a,b,c", ",") ``` ### [split\_regex](/reference/functions/split_regex) [â†’](/reference/functions/split_regex) Splits a string into substrings with a regex. ```tql split_regex("a1b2c", r"\d") ``` ### [to\_lower](/reference/functions/to_lower) [â†’](/reference/functions/to_lower) Converts a string to lowercase. ```tql "HELLO".to_lower() ``` ### [to\_title](/reference/functions/to_title) [â†’](/reference/functions/to_title) Converts a string to title case. ```tql "hello world".to_title() ``` ### [to\_upper](/reference/functions/to_upper) [â†’](/reference/functions/to_upper) Converts a string to uppercase. ```tql "hello".to_upper() ``` ### [trim](/reference/functions/trim) [â†’](/reference/functions/trim) Trims whitespace or specified characters from both ends of a string. ```tql " hello ".trim() ``` ### [trim\_end](/reference/functions/trim_end) [â†’](/reference/functions/trim_end) Trims whitespace or specified characters from the end of a string. ```tql "hello ".trim_end() ``` ### [trim\_start](/reference/functions/trim_start) [â†’](/reference/functions/trim_start) Trims whitespace or specified characters from the start of a string. ```tql " hello".trim_start() ``` ## Type System [Section titled â€œType Systemâ€](#type-system) ### Conversion [Section titled â€œConversionâ€](#conversion) ### [duration](/reference/functions/duration) [â†’](/reference/functions/duration) Casts an expression to a duration value. ```tql duration("1.34w") ``` ### [float](/reference/functions/float) [â†’](/reference/functions/float) Casts an expression to a float. ```tql float(42) ``` ### [int](/reference/functions/int) [â†’](/reference/functions/int) Casts an expression to an integer. ```tql int(-4.2) ``` ### [ip](/reference/functions/ip) [â†’](/reference/functions/ip) Casts an expression to an IP address. ```tql ip("1.2.3.4") ``` ### [string](/reference/functions/string) [â†’](/reference/functions/string) Casts an expression to a string. ```tql string(1.2.3.4) ``` ### [subnet](/reference/functions/subnet) [â†’](/reference/functions/subnet) Casts an expression to a subnet value. ```tql subnet("1.2.3.4/16") ``` ### [time](/reference/functions/time) [â†’](/reference/functions/time) Casts an expression to a time value. ```tql time("2020-03-15") ``` ### [uint](/reference/functions/uint) [â†’](/reference/functions/uint) Casts an expression to an unsigned integer. ```tql uint(4.2) ``` ### Introspection [Section titled â€œIntrospectionâ€](#introspection) ### [type\_id](/reference/functions/type_id) [â†’](/reference/functions/type_id) Retrieves the type id of an expression. ```tql type_id(1 + 3.2) ``` ### [type\_of](/reference/functions/type_of) [â†’](/reference/functions/type_of) Retrieves the type definition of an expression. ```tql type_of(this) ``` ### Transposition [Section titled â€œTranspositionâ€](#transposition) ### [flatten](/reference/functions/flatten) [â†’](/reference/functions/flatten) Flattens nested data. ```tql flatten(this) ``` ### [unflatten](/reference/functions/unflatten) [â†’](/reference/functions/unflatten) Unflattens nested data. ```tql unflatten(this) ```

# abs

Returns the absolute value. ```tql abs(x:number) -> number abs(x:duration) -> duration ``` ## Description [Section titled â€œDescriptionâ€](#description) This function returns the [absolute value](https://en.wikipedia.org/wiki/Absolute_value) for a number or a duration. ### `x: duration|number` [Section titled â€œx: duration|numberâ€](#x-durationnumber) The value to compute absolute value for. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from {x: -13.3} x = x.abs() ``` ```tql {x: 13.3} ```

# add

Adds an element into a list if it doesnâ€™t already exist (set-insertion). ```tql add(xs:list, x:any) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `add` function returns the list `xs` with `x` added at the end, but only if `x` is not already present in the list. This performs a set-insertion operation, ensuring no duplicate values in the resulting list. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The list to add to. ### `x: any` [Section titled â€œx: anyâ€](#x-any) An element to add to the list. If this is of a type incompatible with the list, it will be considered as `null`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Add a new element to a list [Section titled â€œAdd a new element to a listâ€](#add-a-new-element-to-a-list) ```tql from {xs: [1, 2, 3]} xs = xs.add(4) ``` ```tql {xs: [1, 2, 3, 4]} ``` ### Try to add an existing element [Section titled â€œTry to add an existing elementâ€](#try-to-add-an-existing-element) ```tql from {xs: [1, 2, 3]} xs = xs.add(2) ``` ```tql {xs: [1, 2, 3]} ``` ### Add to an empty list [Section titled â€œAdd to an empty listâ€](#add-to-an-empty-list) ```tql from {xs: []} xs = xs.add("hello") ``` ```tql {xs: ["hello"]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`append`](/reference/functions/append), [`prepend`](/reference/functions/prepend), [`remove`](/reference/functions/remove) [`distinct`](/reference/functions/distinct)

# all

Computes the conjunction (AND) of all grouped boolean values. ```tql all(xs:list) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `all` function returns `true` if all values in `xs` are `true` and `false` otherwise. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) A list of boolean values. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if all values are true [Section titled â€œCheck if all values are trueâ€](#check-if-all-values-are-true) ```tql from {x: true}, {x: true}, {x: false} summarize result=all(x) ``` ```tql {result: false} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`any`](/reference/functions/any)

# any

Computes the disjunction (OR) of all grouped boolean values. ```tql any(xs:list) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `any` function returns `true` if any value in `xs` is `true` and `false` otherwise. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) A list of boolean values. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if any value is true [Section titled â€œCheck if any value is trueâ€](#check-if-any-value-is-true) ```tql from {x: false}, {x: false}, {x: true} summarize result=any(x) ``` ```tql {result: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`all`](/reference/functions/all)

# append

Inserts an element at the back of a list. ```tql append(xs:list, x:any) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `append` function returns the list `xs` with `x` inserted at the end. The expression `xs.append(x)` is equivalent to `[...xs, x]`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Append a number to a list [Section titled â€œAppend a number to a listâ€](#append-a-number-to-a-list) ```tql from {xs: [1, 2]} xs = xs.append(3) ``` ```tql {xs: [1, 2, 3]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`add`](/reference/functions/add), [`concatenate`](/reference/functions/concatenate), [`prepend`](/reference/functions/prepend), [`remove`](/reference/functions/remove)

# bit_and

Computes the bit-wise AND of its arguments. ```tql bit_and(lhs:number, rhs:number) -> number ``` ## Description [Section titled â€œDescriptionâ€](#description) The `bit_and` function computes the bit-wise AND of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers. ### `lhs: number` [Section titled â€œlhs: numberâ€](#lhs-number) The left-hand side operand. ### `rhs: number` [Section titled â€œrhs: numberâ€](#rhs-number) The right-hand side operand. ## Examples [Section titled â€œExamplesâ€](#examples) ### Perform bit-wise AND on integers [Section titled â€œPerform bit-wise AND on integersâ€](#perform-bit-wise-and-on-integers) ```tql from {x: bit_and(5, 3)} ``` ```tql {x: 1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_not

Computes the bit-wise NOT of its argument. ```tql bit_not(x:number) -> number ``` ## Description [Section titled â€œDescriptionâ€](#description) The `bit_not` function computes the bit-wise NOT of `x`. The operation inverts each bit in the binary representation of the number. ### `x: number` [Section titled â€œx: numberâ€](#x-number) The number to perform bit-wise NOT on. ## Examples [Section titled â€œExamplesâ€](#examples) ### Perform bit-wise NOT on an integer [Section titled â€œPerform bit-wise NOT on an integerâ€](#perform-bit-wise-not-on-an-integer) ```tql from {x: bit_not(5)} ``` ```tql {x: -6} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_or

Computes the bit-wise OR of its arguments. ```tql bit_or(lhs:number, rhs:number) -> number ``` ## Description [Section titled â€œDescriptionâ€](#description) The `bit_or` function computes the bit-wise OR of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers. ### `lhs: number` [Section titled â€œlhs: numberâ€](#lhs-number) The left-hand side operand. ### `rhs: number` [Section titled â€œrhs: numberâ€](#rhs-number) The right-hand side operand. ## Examples [Section titled â€œExamplesâ€](#examples) ### Perform bit-wise OR on integers [Section titled â€œPerform bit-wise OR on integersâ€](#perform-bit-wise-or-on-integers) ```tql from {x: bit_or(5, 3)} ``` ```tql {x: 7} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`bit_and`](/reference/functions/bit_and), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_xor

Computes the bit-wise XOR of its arguments. ```tql bit_xor(lhs:number, rhs:number) -> number ``` ## Description [Section titled â€œDescriptionâ€](#description) The `bit_xor` function computes the bit-wise XOR (exclusive OR) of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers. ### `lhs: number` [Section titled â€œlhs: numberâ€](#lhs-number) The left-hand side operand. ### `rhs: number` [Section titled â€œrhs: numberâ€](#rhs-number) The right-hand side operand. ## Examples [Section titled â€œExamplesâ€](#examples) ### Perform bit-wise XOR on integers [Section titled â€œPerform bit-wise XOR on integersâ€](#perform-bit-wise-xor-on-integers) ```tql from {x: bit_xor(5, 3)} ``` ```tql {x: 6} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# capitalize

Capitalizes the first character of a string. ```tql capitalize(x:string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `capitalize` function returns the input string with the first character converted to uppercase and the rest to lowercase. ## Examples [Section titled â€œExamplesâ€](#examples) ### Capitalize a lowercase string [Section titled â€œCapitalize a lowercase stringâ€](#capitalize-a-lowercase-string) ```tql from {x: "hello world".capitalize()} ``` ```tql {x: "Hello world"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`to_upper`](/reference/functions/to_upper), [`to_lower`](/reference/functions/to_lower), [`to_title`](/reference/functions/to_title)

# ceil

Computes the ceiling of a number or a time/duration with a specified unit. ```tql ceil(x:number) ceil(x:time, unit:duration) ceil(x:duration, unit:duration) ``` ## Description [Section titled â€œDescriptionâ€](#description) The `ceil` function takes the [ceiling](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions) of a number `x`. For time and duration values, use the second `unit` argument to define the rounding unit. ## Examples [Section titled â€œExamplesâ€](#examples) ### Take the ceiling of integers [Section titled â€œTake the ceiling of integersâ€](#take-the-ceiling-of-integers) ```tql from { x: ceil(3.4), y: ceil(3.5), z: ceil(-3.4), } ``` ```tql { x: 4, y: 4, z: -3, } ``` ### Round time and duration values up to a unit [Section titled â€œRound time and duration values up to a unitâ€](#round-time-and-duration-values-up-to-a-unit) ```tql from { x: ceil(2024-02-24, 1y), y: ceil(10m, 1h) } ``` ```tql { x: 2025-01-01, y: 1h, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`floor`](/reference/functions/floor), [`round`](/reference/functions/round)

# collect

Creates a list of all non-null grouped values, preserving duplicates. ```tql collect(xs:list) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `collect` function returns a list of all non-null values in `xs`, including duplicates. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to collect. ## Examples [Section titled â€œExamplesâ€](#examples) ### Collect values into a list [Section titled â€œCollect values into a listâ€](#collect-values-into-a-list) ```tql from {x: 1}, {x: 2}, {x: 2}, {x: 3} summarize values=collect(x) ``` ```tql {values: [1, 2, 2, 3]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`distinct`](/reference/functions/distinct), [`sum`](/reference/functions/sum)

# community_id

Computes the Community ID for a network connection/flow. ```tql community_id(src_ip=ip, dst_ip=ip, proto=string, [src_port=int, dst_port=int, seed=int]) -> str ``` ## Description [Section titled â€œDescriptionâ€](#description) The `community_id` function computes a unique hash digest of a network connection according to the [Community ID](https://github.com/corelight/community-id-spec) spec. The digest is useful for pivoting between multiple events that belong to the same connection. The `src_ip` and `dst_ip` parameters are required. The `proto` string is also required and must be `tcp`, `udp`, `icmp` or `icmp6`. `src_port` and `dst_port` may only be specified if the other one is. `seed` can be used to set the initial hashing seed. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute a Community ID from a flow 5-tuple [Section titled â€œCompute a Community ID from a flow 5-tupleâ€](#compute-a-community-id-from-a-flow-5-tuple) ```tql from { x: community_id(src_ip=1.2.3.4, src_port=4584, dst_ip=43.3.132.3, dst_port=3483, proto="tcp") } ``` ```tql {x: "1:koNcqhFRD5kb254ZrLsdv630jCM="} ``` ### Compute a Community ID from a host pair [Section titled â€œCompute a Community ID from a host pairâ€](#compute-a-community-id-from-a-host-pair) Because source and destination port are optional, it suffices to provide two IP addreses to compute a valid Community ID. ```tql from {x: community_id(src_ip=1.2.3.4, dst_ip=43.3.132.3, proto="udp")} ``` ```tql {x: "1:7TrrMeH98PrUKC0ySu3RNmpUr48="} ```

# concatenate

Merges two lists. ```tql concatenate(xs:list, ys:list) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `concatenate` function returns a list containing all elements from the lists `xs` and `ys` in order. The expression `concatenate(xs, ys)` is equivalent to `[...xs, ...ys]`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Concatenate two lists [Section titled â€œConcatenate two listsâ€](#concatenate-two-lists) ```tql from {xs: [1, 2], ys: [3, 4]} zs = concatenate(xs, ys) ``` ```tql { xs: [1, 2], ys: [3, 4], zs: [1, 2, 3, 4] } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`append`](/reference/functions/append), [`merge`](/reference/functions/merge), [`prepend`](/reference/functions/prepend), [`zip`](/reference/functions/zip)

# config

Reads Tenzirâ€™s configuration file. ```tql config() -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `config` function retrieves Tenzirâ€™s configuration, including values from various `tenzir.yaml` files, plugin-specific configuration files, environment variables, and command-line options. Note that the `tenzir.secrets`, `tenzir.token` and `caf` options are omitted from the returned record. The former to avoid leaking secrets, the latter as it only contains internal performance-related that are developer-facing and should not be relied upon within TQL. ## Examples [Section titled â€œExamplesâ€](#examples) ### Provide a name mapping in the config file [Section titled â€œProvide a name mapping in the config fileâ€](#provide-a-name-mapping-in-the-config-file) /opt/tenzir/etc/tenzir/tenzir.yaml ```yaml flags: de: ğŸ‡©ğŸ‡ª us: ğŸ‡ºğŸ‡¸ ``` ```tql let $flags = config().flags from ( {country: "de"}, {country: "us"}, {country: "uk"}, ) select flag = $flags.get(country, "unknown") ``` ```tql {flag: "ğŸ‡©ğŸ‡ª"} {flag: "ğŸ‡ºğŸ‡¸"} {flag: "unknown"} ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`env`](/reference/functions/env), [`secret`](/reference/functions/secret)

# contains

Searches for a value within data structures recursively. ```tql contains(input:any, target:any, [exact:bool]) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `contains` function returns `true` if the `target` value is found anywhere within the `input` data structure, and `false` otherwise. The search is performed recursively, meaning it will look inside nested records, lists, and other compound data structures. By default, strings match via substring search and subnets use containment checks. When `exact` is set to `true`, only exact matches are considered. ### `input: any` [Section titled â€œinput: anyâ€](#input-any) The data structure to search within. Can be any type including primitives, records, lists, and nested structures. ### `target: any` [Section titled â€œtarget: anyâ€](#target-any) The value to search for. Cannot be a list or record. ### `exact: bool` (optional) [Section titled â€œexact: bool (optional)â€](#exact-bool-optional) Controls the matching behavior: * When `false` (default): strings match via substring search, and subnets/IPs use containment checks * When `true`: only exact equality matches are considered ## Examples [Section titled â€œExamplesâ€](#examples) ### Search within records [Section titled â€œSearch within recordsâ€](#search-within-records) ```tql from {name: "Alice", age: 30, active: true} found_alice = contains(this, "Alice") found_bob = contains(this, "Bob") found_30 = contains(this, 30) ``` ```tql { name: "Alice", age: 30, active: true, found_alice: true, found_bob: false, found_30: true, } ``` ### Search within nested structures [Section titled â€œSearch within nested structuresâ€](#search-within-nested-structures) ```tql from {user: {profile: {name: "John", settings: {theme: "dark"}}}} found_john = contains(this, "John") found_theme = contains(user, "dark") found_missing = contains(this, "light") ``` ```tql { user: { profile: { name: "John", settings: { theme: "dark", }, }, }, found_john: true, found_theme: true, found_missing: false, } ``` ### Search within lists [Section titled â€œSearch within listsâ€](#search-within-lists) ```tql from {numbers: [1, 2, 3, 42], tags: ["important", "urgent"]} found_42 = contains(numbers, 42) found_important = contains(tags, "important") found_missing = contains(numbers, 99) ``` ```tql { numbers: [1, 2, 3, 42], tags: ["important", "urgent"], found_42: true, found_important: true, found_missing: false, } ``` ### Search with numeric type compatibility [Section titled â€œSearch with numeric type compatibilityâ€](#search-with-numeric-type-compatibility) ```tql from {values: {int_val: 42, uint_val: 42.uint(), double_val: 42.0}} search_int = contains(values, 42) search_uint = contains(values, 42.uint()) search_double = contains(values, 42.0) ``` ```tql { values: { int_val: 42, uint_val: 42, double_val: 42.0, }, search_int: true, search_uint: true, search_double: true, } ``` ### Search in deeply nested structures [Section titled â€œSearch in deeply nested structuresâ€](#search-in-deeply-nested-structures) ```tql from { data: { level1: { level2: { level3: { target: "found" } } } } } deep_search = contains(data, "found") ``` ```tql { data: { level1: { level2: { level3: { target: "found", }, }, }, }, deep_search: true, } ``` ### Substring search in strings [Section titled â€œSubstring search in stringsâ€](#substring-search-in-strings) ```tql from {message: "Hello, World!"} substring_match = contains(message, "World") exact_match = contains(message, "World", exact=true) partial_no_match = contains(message, "Universe") exact_no_match = contains(message, "Hello, World", exact=true) ``` ```tql { message: "Hello, World!", substring_match: true, exact_match: false, partial_no_match: false, exact_no_match: false, } ``` ### Subnet and IP containment [Section titled â€œSubnet and IP containmentâ€](#subnet-and-ip-containment) ```tql from {subnet: 10.0.0.0/8} contains_ip = contains(subnet, 10.1.2.3) contains_subnet = contains(subnet, 10.0.0.0/16) exact_subnet = contains(subnet, 10.0.0.0/8, exact=true) ``` ```tql { subnet: 10.0.0.0/8, contains_ip: true, contains_subnet: true, exact_subnet: true, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`has`](/reference/functions/has), [`match_regex`](/reference/functions/match_regex)

# contains_null

Checks whether the input contains any `null` values. ```tql contains_null(x:any) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `contains_null` function checks if the input contains any `null` values recursively. ### `x: any` [Section titled â€œx: anyâ€](#x-any) The input to check for `null` values. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if list has null values [Section titled â€œCheck if list has null valuesâ€](#check-if-list-has-null-values) ```tql from {x: [{a: 1}, {}]} contains_null = x.contains_null() ``` ```tql { x: [ { a: 1, }, { a: null, }, ], contains_null: true, } ``` ### Check a record with null values [Section titled â€œCheck a record with null valuesâ€](#check-a-record-with-null-values) ```tql from {x: "foo", y: null} contains_null = this.contains_null() ``` ```tql { x: "foo", y: null, contains_null: true, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`has`](/reference/functions/has), [`is_empty`](/reference/functions/is_empty) [`contains`](/reference/functions/contains)

# count

Counts the events or non-null grouped values. ```tql count(xs:list) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `count` function returns the number of non-null values in `xs`. When used without arguments, it counts the total number of events. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to count. ## Examples [Section titled â€œExamplesâ€](#examples) ### Count the number of non-null values [Section titled â€œCount the number of non-null valuesâ€](#count-the-number-of-non-null-values) ```tql from {x: 1}, {x: null}, {x: 2} summarize total=count(x) ``` ```tql {total: 2} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`count_distinct`](/reference/functions/count_distinct)

# count_days

Counts the number of `days` in a duration. ```tql count_days(x:duration) -> float ``` ## Description This function returns the number of days in a duration, i.e., `duration / 1d`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_distinct

Counts all distinct non-null grouped values. ```tql count_distinct(xs:list) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `count_distinct` function returns the number of unique, non-null values in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to count. ## Examples [Section titled â€œExamplesâ€](#examples) ### Count distinct values [Section titled â€œCount distinct valuesâ€](#count-distinct-values) ```tql from {x: 1}, {x: 2}, {x: 2}, {x: 3} summarize unique=count_distinct(x) ``` ```tql {unique: 3} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`count`](/reference/functions/count), [`distinct`](/reference/functions/distinct)

# count_hours

Counts the number of `hours` in a duration. ```tql count_hours(x:duration) -> float ``` ## Description This function returns the number of hours in a duration, i.e., `duration / 1h`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_if

Counts the events or non-null grouped values matching a given predicate. ```tql count_if(xs:list, predicate:any -> bool) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `count_if` function returns the number of non-null values in `xs` that satisfy the given `predicate`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to count. ### `predicate: any -> bool` [Section titled â€œpredicate: any -> boolâ€](#predicate-any---bool) The predicate to apply to each value to check whether it should be counted. ## Examples [Section titled â€œExamplesâ€](#examples) ### Count the number of values greater than 1 [Section titled â€œCount the number of values greater than 1â€](#count-the-number-of-values-greater-than-1) ```tql from {x: 1}, {x: null}, {x: 2} summarize total=x.count_if(x => x > 1) ``` ```tql {total: 1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`count`](/reference/functions/count)

# count_microseconds

Counts the number of `microseconds` in a duration. ```tql count_microseconds(x:duration) -> float ``` ## Description This function returns the number of microseconds in a duration, i.e., `duration / 1us`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_milliseconds

Counts the number of `milliseconds` in a duration. ```tql count_milliseconds(x:duration) -> float ``` ## Description This function returns the number of milliseconds in a duration, i.e., `duration / 1ms`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_minutes

Counts the number of `minutes` in a duration. ```tql count_minutes(x:duration) -> float ``` ## Description This function returns the number of minutes in a duration, i.e., `duration / 1min`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_months

Counts the number of `months` in a duration. ```tql count_months(x:duration) -> float ``` ## Description This function returns the number of months in a duration, i.e., `duration / 1/12 * 1y`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_nanoseconds

Counts the number of `nanoseconds` in a duration. ```tql count_nanoseconds(x:duration) -> int ``` ## Description This function returns the number of nanoseconds in a duration, i.e., `duration / 1ns`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_seconds

Counts the number of `seconds` in a duration. ```tql count_seconds(x:duration) -> float ``` ## Description This function returns the number of seconds in a duration, i.e., `duration / 1s`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_weeks

Counts the number of `weeks` in a duration. ```tql count_weeks(x:duration) -> float ``` ## Description This function returns the number of weeks in a duration, i.e., `duration / 1w`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_years

Counts the number of `years` in a duration. ```tql count_years(x:duration) -> float ``` ## Description This function returns the number of years in a duration, i.e., `duration / 1y`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# day

Extracts the day component from a timestamp. ```tql day(x: time) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `day` function extracts the day component from a timestamp as an integer (1-31). ### `x: time` [Section titled â€œx: timeâ€](#x-time) The timestamp from which to extract the day. ## Examples [Section titled â€œExamplesâ€](#examples) ### Extract the day from a timestamp [Section titled â€œExtract the day from a timestampâ€](#extract-the-day-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } day = ts.day() ``` ```tql { ts: 2024-06-15T14:30:45.123456, day: 15, } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`year`](/reference/functions/year), [`month`](/reference/functions/month), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# days

Converts a number to equivalent days. ```tql days(x:number) -> duration ``` ## Description This function returns days equivalent to a number, i.e., `number * 1d`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# decapsulate

Decapsulates packet data at link, network, and transport layer. ```tql decapsulate(packet:record) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `decapsulate` function decodes binary PCAP packet data by extracting link, network, and transport layer information. The function takes a `packet` record as argument as produced by the [`read_pcap`](/reference/operators/read_pcap) operator, which may look like this: ```tql { linktype: 1, timestamp: 2021-11-17T13:32:43.249525, captured_packet_length: 66, original_packet_length: 66, data: "ZJ7zvttmABY88f1tCABFAAA0LzBAAEAGRzjGR/dbgA6GqgBQ4HzXXzhE3N8/r4AQAfyWoQAAAQEICqMYaE9Mw7SY", } ``` This entire record serves as input to `decapsulate` since the `linktype` determines how to intepret the binary `data` field containing the raw packet data. ### VLAN Tags [Section titled â€œVLAN Tagsâ€](#vlan-tags) The `decapsulate` function also extracts [802.1Q](https://en.wikipedia.org/wiki/IEEE_802.1Q) VLAN tags into a nested `vlan` record, consisting of an `outer` and `inner` field for the respective tags. The value of the VLAN tag corresponds to the 12-bit VLAN identifier (VID). Special values include `0` (frame does not carry a VLAN ID) and `0xFFF` (reserved value; sometimes wildcard match). ## Examples [Section titled â€œExamplesâ€](#examples) ### Decapsulate packets from a PCAP file [Section titled â€œDecapsulate packets from a PCAP fileâ€](#decapsulate-packets-from-a-pcap-file) ```tql from "/path/to/trace.pcap" this = decapsulate(this) ``` ```tql { ether: { src: "00-08-02-1C-47-AE", dst: "20-E5-2A-B6-93-F1", type: 2048, }, ip: { src: 10.12.14.101, dst: 92.119.157.10, type: 6, }, tcp: { src_port: 62589, dst_port: 4443, }, community_id: "1:tSl1HyzM7qS0o3OpbOgxQJYCKCc=", udp: null, icmp: null, } ``` If the trace contains 802.1Q traffic, then the output includes a `vlan` record: ```tql { ether: { src: "00-17-5A-ED-7A-F0", dst: "FF-FF-FF-FF-FF-FF", type: 2048, }, vlan: { outer: 1, inner: 20, }, ip: { src: 192.168.1.1, dst: 255.255.255.255, type: 1, }, icmp: { type: 8, code: 0, }, community_id: "1:1eiKaTUjqP9UT1/1yu/o0frHlCk=", } ```

# decode_base64

Decodes bytes as Base64. ```tql decode_base64(bytes: blob|string) -> blob ``` ## Description [Section titled â€œDescriptionâ€](#description) Decodes bytes as Base64. ### `bytes: blob|string` [Section titled â€œbytes: blob|stringâ€](#bytes-blobstring) The value to decode as Base64. ## Examples [Section titled â€œExamplesâ€](#examples) ### Decode a Base64 encoded string [Section titled â€œDecode a Base64 encoded stringâ€](#decode-a-base64-encoded-string) ```tql from {bytes: "VGVuemly"} decoded = bytes.decode_base64() ``` ```tql {bytes: "VGVuemly", decoded: "Tenzir"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`encode_base64`](/reference/functions/encode_base64)

# decode_hex

Decodes bytes from their hexadecimal representation. ```tql decode_hex(bytes: blob|string) -> blob ``` ## Description [Section titled â€œDescriptionâ€](#description) Decodes bytes from their hexadecimal representation. ### `bytes: blob|string` [Section titled â€œbytes: blob|stringâ€](#bytes-blobstring) The value to decode. ## Examples [Section titled â€œExamplesâ€](#examples) ### Decode a blob from hex [Section titled â€œDecode a blob from hexâ€](#decode-a-blob-from-hex) ```tql from {bytes: "54656E7A6972"} decoded = bytes.decode_hex() ``` ```tql {bytes: "54656E7A6972", decoded: "Tenzir"} ``` ### Decode a mixed-case hex string [Section titled â€œDecode a mixed-case hex stringâ€](#decode-a-mixed-case-hex-string) ```tql from {bytes: "4e6f6E6365"} decoded = bytes.decode_hex() ``` ```tql {bytes: "4e6f6E6365", decoded: "Nonce"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`encode_hex`](/reference/functions/encode_hex)

# decode_url

Decodes URL encoded strings. ```tql decode_url(string: blob|string) -> blob ``` ## Description [Section titled â€œDescriptionâ€](#description) Decodes URL encoded strings or blobs, converting percent-encoded sequences back to their original characters. ### `string: blob|string` [Section titled â€œstring: blob|stringâ€](#string-blobstring) The URL encoded string to decode. ## Examples [Section titled â€œExamplesâ€](#examples) ### Decode a URL encoded string [Section titled â€œDecode a URL encoded stringâ€](#decode-a-url-encoded-string) ```tql from {input: "Hello%20World%20%26%20Special%2FChars%3F"} decoded = input.decode_url() ``` ```tql { input: "Hello%20World%20%26%20Special%2FChars%3F", decoded: "Hello World & Special/Chars?", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`encode_url`](/reference/functions/encode_url)

# distinct

Creates a sorted list without duplicates of non-null grouped values. ```tql distinct(xs:list) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `distinct` function returns a sorted list containing unique, non-null values in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to deduplicate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get distinct values in a list [Section titled â€œGet distinct values in a listâ€](#get-distinct-values-in-a-list) ```tql from {x: 1}, {x: 2}, {x: 2}, {x: 3} summarize unique=distinct(x) ``` ```tql {unique: [1, 2, 3]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`collect`](/reference/functions/collect), [`count_distinct`](/reference/functions/count_distinct), [`value_counts`](/reference/functions/value_counts)

# duration

Casts an expression to a duration value. ```tql duration(x:string) -> duration ``` ## Description [Section titled â€œDescriptionâ€](#description) The `duration` function casts the given string `x` to a duration value. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast a string to a duration [Section titled â€œCast a string to a durationâ€](#cast-a-string-to-a-duration) ```tql from {str: "1ms"} dur = duration(str) ``` ```tql {str: "1ms", dur: 1ms} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [time](/reference/functions/time)

# encode_base64

Encodes bytes as Base64. ```tql encode_base64(bytes: blob|string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) Encodes bytes as Base64. ### `bytes: blob|string` [Section titled â€œbytes: blob|stringâ€](#bytes-blobstring) The value to encode as Base64. ## Examples [Section titled â€œExamplesâ€](#examples) ### Encode a string as Base64 [Section titled â€œEncode a string as Base64â€](#encode-a-string-as-base64) ```tql from {bytes: "Tenzir"} encoded = bytes.encode_base64() ``` ```tql {bytes: "Tenzir", encoded: "VGVuemly"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`decode_base64`](/reference/functions/decode_base64)

# encode_hex

Encodes bytes into their hexadecimal representation. ```tql encode_hex(bytes: blob|string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) Encodes bytes into their hexadecimal representation. ### `bytes: blob|string` [Section titled â€œbytes: blob|stringâ€](#bytes-blobstring) The value to encode. ## Examples [Section titled â€œExamplesâ€](#examples) ### Encode a string to hex [Section titled â€œEncode a string to hexâ€](#encode-a-string-to-hex) ```tql from {bytes: "Tenzir"} encoded = bytes.encode_hex() ``` ```tql {bytes: "Tenzir", encoded: "54656E7A6972"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`decode_hex`](/reference/functions/decode_hex)

# encode_url

Encodes strings using URL encoding. ```tql encode_url(bytes: blob|string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) Encodes strings or blobs using URL encoding, replacing special characters with their percent-encoded equivalents. ### `bytes: blob|string` [Section titled â€œbytes: blob|stringâ€](#bytes-blobstring) The input to URL encode. ## Examples [Section titled â€œExamplesâ€](#examples) ### Encode a string as URL encoded [Section titled â€œEncode a string as URL encodedâ€](#encode-a-string-as-url-encoded) ```tql from {input: "Hello World & Special/Chars?"} encoded = input.encode_url() ``` ```tql { input: "Hello World & Special/Chars?", encoded: "Hello%20World%20%26%20Special%2FChars%3F", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`decode_url`](/reference/functions/decode_url)

# encrypt_cryptopan

Encrypts an IP address via Crypto-PAn. ```tql encrypt_cryptopan(address:ip, [seed=string]) ``` ## Description [Section titled â€œDescriptionâ€](#description) The `encrypt_cryptopan` function encrypts the IP `address` using the [Crypto-PAn](https://en.wikipedia.org/wiki/Crypto-PAn) algorithm. ### `address: ip` [Section titled â€œaddress: ipâ€](#address-ip) The IP address to encrypt. ### `seed = string (optional)` [Section titled â€œseed = string (optional)â€](#seed--string-optional) A 64-byte seed that describes a hexadecimal value. When the seed is shorter than 64 bytes, the function appends zeros to match the size; when it is longer, it truncates the seed. ## Examples [Section titled â€œExamplesâ€](#examples) ### Encrypt IP address fields [Section titled â€œEncrypt IP address fieldsâ€](#encrypt-ip-address-fields) ```tql let $seed = "deadbeef" // use secret() function in practice from { src: encrypt_cryptopan(114.13.11.35, seed=$seed), dst: encrypt_cryptopan(114.56.11.200, seed=$seed), } ``` ```tql { src: 117.179.11.60, dst: 117.135.244.180, } ```

# ends_with

Checks if a string ends with a specified substring. ```tql ends_with(x:string, suffix:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `ends_with` function returns `true` if `x` ends with `suffix` and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string ends with a substring [Section titled â€œCheck if a string ends with a substringâ€](#check-if-a-string-ends-with-a-substring) ```tql from {x: "hello".ends_with("lo")} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`starts_with`](/reference/functions/starts_with)

# entropy

Computes the Shannon entropy of all grouped values. ```tql entropy(xs:list, [normalize=bool]) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `entropy` function calculates the Shannon entropy of the values in `xs`, which measures the amount of uncertainty or randomness in the data. Higher entropy values indicate more randomness, while lower values indicate more predictability. The entropy is calculated as: `H(x) = -sum(p(x[i]) * log(p(x[i])))`, where `p(x[i])` is the probability of each unique value. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ### `normalize: bool (optional)` [Section titled â€œnormalize: bool (optional)â€](#normalize-bool-optional) Optional parameter to normalize the entropy between 0 and 1. When `true`, the entropy is divided by `log(number of unique values)`. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the entropy of values [Section titled â€œCompute the entropy of valuesâ€](#compute-the-entropy-of-values) ```tql from {x: 1}, {x: 1}, {x: 2}, {x: 3} summarize entropy_value=entropy(x) ``` ```tql { entropy_value: 1.0397207708399179, } ``` ### Compute the normalized entropy [Section titled â€œCompute the normalized entropyâ€](#compute-the-normalized-entropy) ```tql from {x: 1}, {x: 1}, {x: 2}, {x: 3} summarize normalized_entropy=entropy(x, normalize=true) ``` ```tql { normalized_entropy: 0.946394630357186, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`mode`](/reference/functions/mode), [`value_counts`](/reference/functions/value_counts)

# env

Reads an environment variable. ```tql env(x:string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `env` function retrieves the value of an environment variable `x`. If the variable does not exist, it returns `null`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read the `PATH` environment variable [Section titled â€œRead the PATH environment variableâ€](#read-the-path-environment-variable) ```tql from {x: env("PATH")} ``` ```tql {x: "/usr/local/bin:/usr/bin:/bin"} ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`config`](/reference/functions/config), [`secret`](/reference/functions/secret)

# file_contents

Reads a fileâ€™s contents. ```tql file_contents(path:string, [binary=bool]) -> blob|string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `file_contents` function reads a fileâ€™s contents. ### `path: string` [Section titled â€œpath: stringâ€](#path-string) Absolute path of file to read. ### `binary = bool (optional)` [Section titled â€œbinary = bool (optional)â€](#binary--bool-optional) Whether to read the file contents as a `blob`, instead of a `string`. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql let $secops_config = file_contents("/path/to/file.json").parse_json() â€¦ to_google_secops client_email=$secops_config.client_email, â€¦ ```

# file_name

Extracts the file name from a file path. ```tql file_name(x:string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `file_name` function returns the file name component of a file path, excluding the parent directories. ## Examples [Section titled â€œExamplesâ€](#examples) ### Extract the file name from a file path [Section titled â€œExtract the file name from a file pathâ€](#extract-the-file-name-from-a-file-path) ```tql from {x: file_name("/path/to/log.json")} ``` ```tql {x: "log.json"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parent_dir`](/reference/functions/parent_dir)

# first

Takes the first non-null grouped value. ```tql first(xs:list) -> any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `first` function returns the first non-null value in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to search. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the first non-null value [Section titled â€œGet the first non-null valueâ€](#get-the-first-non-null-value) ```tql from {x: null}, {x: 2}, {x: 3} summarize first_value=first(x) ``` ```tql {first_value: 2} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`last`](/reference/functions/last)

# flatten

Flattens nested data. ```tql flatten(x:record, separtor=string) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `flatten` function takes a record and performs actions on contained container types: 1. **Records**: Join nested records with a separator (`.` by default). For example, if a field named `x` is a record with fields `a` and `b`, flattening will lift the nested record into the parent scope by creating two new fields `x.a` and `x.b`. 2. **Lists**: Merge nested lists into a single (flat) list. For example, `[[[2]], [[3, 1]], [[4]]]` becomes `[2, 3, 1, 4]`. For records inside lists, `flatten` â€œpushes lists downâ€ into one list per record field. For example, the record ```tql { foo: [ { a: 2, b: 1, }, { a: 4, }, ], } ``` becomes ```tql { "foo.a": [ 2, 4, ], "foo.b": [ 1, null, ], } ``` Lists nested in records that are nested in lists will also be flattened. For example, the record ```tql { foo: [ { a: [ [2, 23], [1,16], ], b: [1], }, { a: [[4]], }, ], } ``` becomes ```tql { "foo.a": [ 2, 23, 1, 16, 4 ], "foo.b": [ 1 ] } ``` As you can see from the above examples, flattening also removes `null` values. ### `x: record` [Section titled â€œx: recordâ€](#x-record) The record you want to flatten. ### `separator: string (optional)` [Section titled â€œseparator: string (optional)â€](#separator-string-optional) The separator to use for joining field names. Defaults to `"."`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Flatten fields with the dot character [Section titled â€œFlatten fields with the dot characterâ€](#flatten-fields-with-the-dot-character) ```tql from { src_ip: 147.32.84.165, src_port: 1141, dest_ip: 147.32.80.9, dest_port: 53, event_type: "dns", dns: { type: "query", id: 553, rrname: "irc.freenode.net", rrtype: "A", tx_id: 0, grouped: { A: [ "tenzir.com", ], }, }, } this = flatten(this) ``` ```tql { src_ip: 147.32.84.165, src_port: 1141, dest_ip: 147.32.80.9, dest_port: 53, event_type: "dns", "dns.type": "query", "dns.id": 553, "dns.rrname": "irc.freenode.net", "dns.rrtype": "A", "dns.tx_id": 0, "dns.grouped.A": ["tenzir.com"], } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`unflatten`](/reference/functions/unflatten)

# float

Casts an expression to a float. ```tql float(x:any) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `float` function converts the given value `x` to a floating-point value. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast an integer to a float [Section titled â€œCast an integer to a floatâ€](#cast-an-integer-to-a-float) ```tql from {x: float(42)} ``` ```tql {x: 42.0} ``` ### Cast a string to a float [Section titled â€œCast a string to a floatâ€](#cast-a-string-to-a-float) ```tql from {x: float("4.2")} ``` ```tql {x: 4.2} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ip`](/reference/functions/ip), [`string`](/reference/functions/string), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [int](/reference/functions/int)

# floor

Computes the floor of a number or a time/duration with a specified unit. ```tql floor(x:number) floor(x:time, unit:duration) floor(x:duration, unit:duration) ``` ## Description [Section titled â€œDescriptionâ€](#description) The `floor` function takes the [floor](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions) of a number `x`. For time and duration values, use the second `unit` argument to define the rounding unit. ## Examples [Section titled â€œExamplesâ€](#examples) ### Take the floor of integers [Section titled â€œTake the floor of integersâ€](#take-the-floor-of-integers) ```tql from { x: floor(3.4), y: floor(3.5), z: floor(-3.4), } ``` ```tql { x: 3, y: 3, z: -4, } ``` ### Round time and duration values down to a unit [Section titled â€œRound time and duration values down to a unitâ€](#round-time-and-duration-values-down-to-a-unit) ```tql from { x: floor(2024-02-24, 1y), y: floor(1h52m, 1h) } ``` ```tql { x: 2024-01-01, y: 1h, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ceil`](/reference/functions/ceil), [`round`](/reference/functions/round)

# format_time

Formats a time into a string that follows a specific format. ```tql format_time(input: time, format: string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `format_time` function formats the given `input` time into a string by using the given `format`. ### `input: time` [Section titled â€œinput: timeâ€](#input-time) The input time for which a string should be constructed. ### `format: string` [Section titled â€œformat: stringâ€](#format-string) The string that specifies the desired output format, for example `"%m-%d-%Y"`. The allowed format specifiers are the same as for `strftime(3)`: | Specifier | Description | Example | | :-------: | :------------------------------------- | :------------------------ | | `%%` | A literal `%` character | `%` | | `%a` | Abbreviated or full weekday name | `Mon`, `Monday` | | `%A` | Equivalent to `%a` | `Mon`, `Monday` | | `%b` | Abbreviated or full month name | `Jan`, `January` | | `%B` | Equivalent to `%b` | `Jan`, `January` | | `%c` | Date and time representation | `Mon Jan 1 12:00:00 2024` | | `%C` | Century as a decimal number | `20` | | `%d` | Day of the month with zero padding | `01`, `31` | | `%D` | Equivalent to `%m/%d/%y` | `01/31/24` | | `%e` | Day of the month with space padding | `1`, `31` | | `%F` | Equivalent to `%Y-%m-%d` | `2024-01-31` | | `%g` | Last two digits of ISO week-based year | `24` | | `%G` | ISO week-based year | `2024` | | `%h` | Equivalent to `%b` | `Jan` | | `%H` | Hour in 24-hour format | `00`, `23` | | `%I` | Hour in 12-hour format | `01`, `12` | | `%j` | Day of year | `001`, `365` | | `%m` | Month number | `01`, `12` | | `%M` | Minutes | `00`, `59` | | `%n` | Newline character | `\n` | | `%p` | AM/PM designation | `AM`, `PM` | | `%r` | 12-hour clock time | `12:00:00 PM` | | `%R` | Equivalent to `%H:%M` | `23:59` | | `%S` | Seconds | `00`, `59` | | `%t` | Tab character | `\t` | | `%T` | Equivalent to `%H:%M:%S` | `23:59:59` | | `%u` | ISO weekday (Monday=1) | `1`, `7` | | `%U` | Week number (Sunday as first day) | `00`, `52` | | `%V` | ISO week number | `01`, `53` | | `%w` | Weekday (Sunday=0) | `0`, `6` | | `%W` | Week number (Monday as first day) | `00`, `52` | | `%x` | Date representation | `01/31/24` | | `%X` | Time representation | `23:59:59` | | `%y` | Year without century | `24` | | `%Y` | Year with century | `2024` | | `%z` | UTC offset | `+0000`, `-0430` | | `%Z` | Time zone abbreviation | `UTC`, `EST` | ## Examples [Section titled â€œExamplesâ€](#examples) ### Format a timestamp [Section titled â€œFormat a timestampâ€](#format-a-timestamp) ```tql from { x: 2024-12-31T12:59:42, } x = x.format_time("%d.%m.%Y") ``` ```tql {x: "31.12.2024"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_time`](/reference/functions/parse_time)

# from_epoch

Interprets a duration as Unix time. ```tql from_epoch(x:duration) -> time ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_epoch` function interprets a duration as [Unix time](https://en.wikipedia.org/wiki/Unix_time). ### `x: duration` [Section titled â€œx: durationâ€](#x-duration) The duration since the Unix epoch, i.e., 00:00:00 UTC on 1 January 1970. ## Examples [Section titled â€œExamplesâ€](#examples) ### Convert an integral Unix time [Section titled â€œConvert an integral Unix timeâ€](#convert-an-integral-unix-time) ```tql from {time: 1736525429} time = from_epoch(time * 1s) ``` ```tql {time: 2025-01-10T16:10:29+00:00} ``` ### Interpret a duration as Unix time [Section titled â€œInterpret a duration as Unix timeâ€](#interpret-a-duration-as-unix-time) ```tql from {x: from_epoch(50y + 12w + 20m)} ``` ```tql {x: 2020-03-13T00:20:00.000000} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`now`](/reference/functions/now), [`since_epoch`](/reference/functions/since_epoch)

# get

Gets a field from a record or an element from a list ```tql get(x:record, field:string, [fallback:any]) -> any get(x:record|list, index:number, [fallback:any]) -> any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `get` function returns the record field with the name `field` or the list element with the index `index`. If `fallback` is provided, the function gracefully returns the fallback value instead of emitting a warning and returning `null`. ### `xs: record|list` [Section titled â€œxs: record|listâ€](#xs-recordlist) A `record` or list you want to access. ### `index: int`/`field: string` [Section titled â€œindex: int/field: stringâ€](#index-intfield-string) An index or field to access. If the functionâ€™s subject `xs` is a `list`, `index` refers to the position in the list. If the subject `xs` is a `record`, `index` refers to the field index. If the subject is a `record`, you can also use the fields name as a `string` to refer to it. If the given `index` or `field` are do not exist in the subject and no `fallback` was provided, a warning will be raised and the function will return `null`. ### `fallback: any (optional)` [Section titled â€œfallback: any (optional)â€](#fallback-any-optional) A fallback value to return if the given `index` or `field` do not exist in the subject. Providing a `fallback` avoids a warning. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the first element of a list, or a fallback value [Section titled â€œGet the first element of a list, or a fallback valueâ€](#get-the-first-element-of-a-list-or-a-fallback-value) ```tql from ( {xs: [1, 2, 3]}, {xs: []}, } select first = xs.get(0, -1) ``` ```tql {first: 1} {first: -1} ``` ### Access a field of a record, or a fallback value [Section titled â€œAccess a field of a record, or a fallback valueâ€](#access-a-field-of-a-record-or-a-fallback-value) ```tql from ( {x: 1, y: 2}, {x: 3}, } select x = this.get("x", -1), y = this.get("y", -1) ``` ```tql {x: 1, y: 2} {x: 3, y: -1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`keys`](/reference/functions/keys)

# has

Checks whether a record has a specified field. ```tql has(x:record, field:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `has` function returns `true` if the record contains the specified field and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a record has a specific field [Section titled â€œCheck if a record has a specific fieldâ€](#check-if-a-record-has-a-specific-field) ```tql from { x: "foo", y: null, } this = { has_x: this.has("x"), has_y: this.has("y"), has_z: this.has("z"), } ``` ```tql { has_x: true, has_y: true, has_z: false, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_empty`](/reference/functions/is_empty), [`keys`](/reference/functions/keys)

# hash_md5

Computes an MD5 hash digest. ```tql hash_md5(x:any, [seed=string]) ``` ## Description [Section titled â€œDescriptionâ€](#description) The `hash` function calculates a hash digest of a given value `x`. ### `x: any` [Section titled â€œx: anyâ€](#x-any) The value to hash. ### `seed = string (optional)` [Section titled â€œseed = string (optional)â€](#seed--string-optional) The seed for the hash. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute an MD5 digest of a string [Section titled â€œCompute an MD5 digest of a stringâ€](#compute-an-md5-digest-of-a-string) ```tql from { x: hash_md5("foo") } ``` ```tql { x: "acbd18db4cc2f85cedef654fccc4a4d8" } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha1

Computes a SHA-1 hash digest. ```tql hash_sha1(x:any, [seed=string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `hash_sha1` function calculates a SHA-1 hash digest for the given value `x`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute a SHA-1 digest of a string [Section titled â€œCompute a SHA-1 digest of a stringâ€](#compute-a-sha-1-digest-of-a-string) ```tql from {x: hash_sha1("foo")} ``` ```tql {x: "0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha224

Computes a SHA-224 hash digest. ```tql hash_sha224(x:any, [seed=string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `hash_sha224` function calculates a SHA-224 hash digest for the given value `x`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute a SHA-224 digest of a string [Section titled â€œCompute a SHA-224 digest of a stringâ€](#compute-a-sha-224-digest-of-a-string) ```tql from {x: hash_sha224("foo")} ``` ```tql {x: "0808f64e60d58979fcb676c96ec938270dea42445aeefcd3a4e6f8db"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha256

Computes a SHA-256 hash digest. ```tql hash_sha256(x:any, [seed=string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `hash_sha256` function calculates a SHA-256 hash digest for the given value `x`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute a SHA-256 digest of a string [Section titled â€œCompute a SHA-256 digest of a stringâ€](#compute-a-sha-256-digest-of-a-string) ```tql from {x: hash_sha256("foo")} ``` ```tql {x: "2c26b46b68ffc68ff99b453c1d30413413422e6e6c8ee90c3abeac38044e8a8c1b0"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha384

Computes a SHA-384 hash digest. ```tql hash_sha384(x:any, [seed=string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `hash_sha384` function calculates a SHA-384 hash digest for the given value `x`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute a SHA-384 digest of a string [Section titled â€œCompute a SHA-384 digest of a stringâ€](#compute-a-sha-384-digest-of-a-string) ```tql from {x: hash_sha384("foo")} ``` ```tql {x: "98c11ffdfdd540676b1a137cb1a22b2a70350c9a44171d6b1180c6be5cbb2ee3f79d532c8a1dd9ef2e8e08e752a3babb"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha512

Computes a SHA-512 hash digest. ```tql hash_sha512(x:any, [seed=string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `hash_sha512` function calculates a SHA-512 hash digest for the given value `x`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute a SHA-512 digest of a string [Section titled â€œCompute a SHA-512 digest of a stringâ€](#compute-a-sha-512-digest-of-a-string) ```tql from {x: hash_sha512("foo")} ``` ```tql {x: "f7fbba6e0636f890e56fbbf3283e524c6fa3204ae298382d624741d0dc6638326e282c41be5e4254d8820772c5518a2c5a8c0c7f7eda19594a7eb539453e1ed7"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_xxh3

Computes an XXH3 hash digest. ```tql hash_xxh3(x:any, [seed=string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `hash_xxh3` function calculates a 64-bit XXH3 hash digest for the given value `x`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute an XXH3 digest of a string [Section titled â€œCompute an XXH3 digest of a stringâ€](#compute-an-xxh3-digest-of-a-string) ```tql from {x: hash_xxh3("foo")} ``` ```tql {x: "ab6e5f64077e7d8a"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512)

# hour

Extracts the hour component from a timestamp. ```tql hour(x: time) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `hour` function extracts the hour component from a timestamp as an integer (0-23). ### `x: time` [Section titled â€œx: timeâ€](#x-time) The timestamp from which to extract the hour. ## Examples [Section titled â€œExamplesâ€](#examples) ### Extract the hour from a timestamp [Section titled â€œExtract the hour from a timestampâ€](#extract-the-hour-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } hour = ts.hour() ``` ```tql { ts: 2024-06-15T14:30:45.123456, hour: 14, } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# hours

Converts a number to equivalent hours. ```tql hours(x:number) -> duration ``` ## Description This function returns hours equivalent to a number, i.e., `number * 1h`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# int

Casts an expression to an integer. ```tql int(x:number|string, base=int) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `int` function casts the provided value `x` to an integer. Non-integer values are truncated. ### `x: number|string` [Section titled â€œx: number|stringâ€](#x-numberstring) The input to convert. ### `base = int` [Section titled â€œbase = intâ€](#base--int) Base (radix) to parse a string as. Can be `10` or `16`. If `16`, the string inputs may be optionally prefixed by `0x` or `0X`, e.g., `-0x134`. Defaults to `10`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast a floating-point number to an integer [Section titled â€œCast a floating-point number to an integerâ€](#cast-a-floating-point-number-to-an-integer) ```tql from {x: int(4.2)} ``` ```tql {x: 4} ``` ### Convert a string to an integer [Section titled â€œConvert a string to an integerâ€](#convert-a-string-to-an-integer) ```tql from {x: int("42")} ``` ```tql {x: 42} ``` ### Parse a hexadecimal number [Section titled â€œParse a hexadecimal numberâ€](#parse-a-hexadecimal-number) ```tql from {x: int("0x42", base=16)} ``` ```tql {x: 66} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [string](/reference/functions/string)

# ip

Casts an expression to an IP address. ```tql ip(x:string) -> ip ``` ## Description [Section titled â€œDescriptionâ€](#description) The `ip` function casts the provided string `x` to an IP address. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast a string to an IP address [Section titled â€œCast a string to an IP addressâ€](#cast-a-string-to-an-ip-address) ```tql from {x: ip("1.2.3.4")} ``` ```tql {x: 1.2.3.4} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`subnet`](/reference/functions/subnet), [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [int](/reference/functions/int), [string](/reference/functions/string)

# ip_category

Returns the type classification of an IP address. ```tql ip_category(x:ip) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `ip_category` function returns the category classification of a given IP address `x` as a string. The possible return values are: * `"unspecified"` - Unspecified address (0.0.0.0 or ::) * `"loopback"` - Loopback address (127.0.0.0/8 or ::1) * `"link_local"` - Link-local address (169.254.0.0/16 or fe80::/10) * `"multicast"` - Multicast address (224.0.0.0/4 or ff00::/8) * `"broadcast"` - Broadcast address (255.255.255.255, IPv4 only) * `"private"` - Private address (RFC 1918 for IPv4, RFC 4193 for IPv6) * `"global"` - Global (publicly routable) address The function returns the most specific classification that applies to the address. For example, 127.0.0.1 is classified as `"loopback"` rather than just non-global. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get IP address types [Section titled â€œGet IP address typesâ€](#get-ip-address-types) ```tql from { global_ipv4: ip_category(8.8.8.8), private_ipv4: ip_category(192.168.1.1), loopback: ip_category(127.0.0.1), multicast: ip_category(224.0.0.1), link_local: ip_category(169.254.1.1), } ``` ```tql { global_ipv4: "global", private_ipv4: "private", loopback: "loopback", multicast: "multicast", link_local: "link_local", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local)

# is_alnum

Checks if a string is alphanumeric. ```tql is_alnum(x:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_alnum` function returns `true` if `x` contains only alphanumeric characters and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string is alphanumeric [Section titled â€œCheck if a string is alphanumericâ€](#check-if-a-string-is-alphanumeric) ```tql from {x: "hello123".is_alnum()} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_alpha`](/reference/functions/is_alpha), [`is_numeric`](/reference/functions/is_numeric), [`is_printable`](/reference/functions/is_printable)

# is_alpha

Checks if a string contains only alphabetic characters. ```tql is_alpha(x:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_alpha` function returns `true` if `x` contains only alphabetic characters and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string is alphabetic [Section titled â€œCheck if a string is alphabeticâ€](#check-if-a-string-is-alphabetic) ```tql from {x: "hello".is_alpha()} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_alnum`](/reference/functions/is_alnum), [`is_lower`](/reference/functions/is_lower), [`is_numeric`](/reference/functions/is_numeric), [`is_printable`](/reference/functions/is_printable), [`is_upper`](/reference/functions/is_upper)

# is_empty

Checks whether a value is empty. ```tql is_empty(x:string|list|record) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_empty` function returns `true` if the input value is empty and `false` otherwise. ### `x: string|list|record` [Section titled â€œx: string|list|recordâ€](#x-stringlistrecord) The value to check for emptiness. The function works on three types: * **Strings**: Returns `true` for empty strings (`""`) * **Lists**: Returns `true` for empty lists (`[]`) * **Records**: Returns `true` for empty records (`{}`) For `null` values, the function returns `null`. For unsupported types, the function emits a warning and returns `null`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string is empty [Section titled â€œCheck if a string is emptyâ€](#check-if-a-string-is-empty) ```tql from { empty: "".is_empty(), not_empty: "hello".is_empty(), } ``` ```tql { empty: true, not_empty: false, } ``` ### Check if a list is empty [Section titled â€œCheck if a list is emptyâ€](#check-if-a-list-is-empty) ```tql from { empty: [].is_empty(), not_empty: [1, 2, 3].is_empty(), } ``` ```tql { empty: true, not_empty: false, } ``` ### Check if a record is empty [Section titled â€œCheck if a record is emptyâ€](#check-if-a-record-is-empty) ```tql from { empty: {}.is_empty(), not_empty: {a: 1, b: 2}.is_empty(), } ``` ```tql { empty: true, not_empty: false, } ``` ### Null handling [Section titled â€œNull handlingâ€](#null-handling) ```tql from { result: null.is_empty(), } ``` ```tql { result: null, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`length`](/reference/functions/length), [`has`](/reference/functions/has)

# is_global

Checks whether an IP address is a global address. ```tql is_global(x:ip) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_global` function checks whether a given IP address `x` is a global address. A global address is a publicly routable address that is not: * Loopback (127.0.0.0/8 for IPv4, ::1 for IPv6) * Private (RFC 1918 for IPv4, RFC 4193 for IPv6) * Link-local (169.254.0.0/16 for IPv4, fe80::/10 for IPv6) * Multicast (224.0.0.0/4 for IPv4, ff00::/8 for IPv6) * Broadcast (255.255.255.255 for IPv4) * Unspecified (0.0.0.0 for IPv4, :: for IPv6) ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if an IP is global [Section titled â€œCheck if an IP is globalâ€](#check-if-an-ip-is-global) ```tql from { global_ipv4: 8.8.8.8.is_global(), global_ipv6: 2001:4860:4860::8888.is_global(), private: 192.168.1.1.is_global(), loopback: 127.0.0.1.is_global(), link_local: 169.254.1.1.is_global(), } ``` ```tql { global_ipv4: true, global_ipv6: true, private: false, loopback: false, link_local: false, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_link_local

Checks whether an IP address is a link-local address. ```tql is_link_local(x:ip) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_link_local` function checks whether a given IP address `x` is a link-local address. For IPv4, link-local addresses are in the range 169.254.0.0 to 169.254.255.255 (169.254.0.0/16). For IPv6, link-local addresses are in the range fe80::/10. Link-local addresses are used for communication between nodes on the same network segment and are not routable on the internet. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if an IP is link-local [Section titled â€œCheck if an IP is link-localâ€](#check-if-an-ip-is-link-local) ```tql from { ipv4_link_local: 169.254.1.1.is_link_local(), ipv6_link_local: fe80::1.is_link_local(), not_link_local: 192.168.1.1.is_link_local(), } ``` ```tql { ipv4_link_local: true, ipv6_link_local: true, not_link_local: false, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`ip_category`](/reference/functions/ip_category)

# is_loopback

Checks whether an IP address is a loopback address. ```tql is_loopback(x:ip) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_loopback` function checks whether a given IP address `x` is a loopback address. For IPv4, loopback addresses are in the range 127.0.0.0 to 127.255.255.255 (127.0.0.0/8). For IPv6, the loopback address is ::1. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if an IP is loopback [Section titled â€œCheck if an IP is loopbackâ€](#check-if-an-ip-is-loopback) ```tql from { ipv4_loopback: 127.0.0.1.is_loopback(), ipv6_loopback: ::1.is_loopback(), not_loopback: 8.8.8.8.is_loopback(), } ``` ```tql { ipv4_loopback: true, ipv6_loopback: true, not_loopback: false, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_lower

Checks if a string is in lowercase. ```tql is_lower(x:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_lower` function returns `true` if `x` is entirely in lowercase and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string is lowercase [Section titled â€œCheck if a string is lowercaseâ€](#check-if-a-string-is-lowercase) ```tql from {x: "hello".is_lower()} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_alpha`](/reference/functions/is_alpha), [`is_upper`](/reference/functions/is_upper), [`to_lower`](/reference/functions/to_lower)

# is_multicast

Checks whether an IP address is a multicast address. ```tql is_multicast(x:ip) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_multicast` function checks whether a given IP address `x` is a multicast address. For IPv4, multicast addresses are in the range 224.0.0.0 to 239.255.255.255 (224.0.0.0/4). For IPv6, multicast addresses start with the prefix ff00::/8. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if an IP is multicast [Section titled â€œCheck if an IP is multicastâ€](#check-if-an-ip-is-multicast) ```tql from { ipv4_multicast: 224.0.0.1.is_multicast(), ipv6_multicast: ff02::1.is_multicast(), not_multicast: 8.8.8.8.is_multicast(), } ``` ```tql { ipv4_multicast: true, ipv6_multicast: true, not_multicast: false, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_numeric

Checks if a string contains only numeric characters. ```tql is_numeric(x:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_numeric` function returns `true` if `x` contains only numeric characters and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string is numeric [Section titled â€œCheck if a string is numericâ€](#check-if-a-string-is-numeric) ```tql from {x: "1234".is_numeric()} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_alpha`](/reference/functions/is_alpha), [`is_alnum`](/reference/functions/is_alnum)

# is_printable

Checks if a string contains only printable characters. ```tql is_printable(x:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_printable` function returns `true` if `x` contains only printable characters and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string is printable [Section titled â€œCheck if a string is printableâ€](#check-if-a-string-is-printable) ```tql from {x: "hello".is_printable()} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_alnum`](/reference/functions/is_alnum), [`is_alpha`](/reference/functions/is_alpha)

# is_private

Checks whether an IP address is a private address. ```tql is_private(x:ip) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_private` function checks whether a given IP address `x` is a private address according to RFC 1918 (IPv4) and RFC 4193 (IPv6). For IPv4, private addresses are: * 10.0.0.0/8 (10.0.0.0 - 10.255.255.255) * 172.16.0.0/12 (172.16.0.0 - 172.31.255.255) * 192.168.0.0/16 (192.168.0.0 - 192.168.255.255) For IPv6, private addresses are: * fc00::/7 (Unique Local Addresses) Note: Link-local addresses (169.254.0.0/16 for IPv4 and fe80::/10 for IPv6) are **not** considered private addresses by this function. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if an IP is private [Section titled â€œCheck if an IP is privateâ€](#check-if-an-ip-is-private) ```tql from { private_10: 10.0.0.1.is_private(), private_172: 172.16.0.1.is_private(), private_192: 192.168.1.1.is_private(), private_ipv6: fc00::1.is_private(), link_local: 169.254.1.1.is_private(), public: 8.8.8.8.is_private(), } ``` ```tql { private_10: true, private_172: true, private_192: true, private_ipv6: true, link_local: false, public: false, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_title

Checks if a string follows title case. ```tql is_title(x:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_title` function returns `true` if `x` is in title case and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string is in title case [Section titled â€œCheck if a string is in title caseâ€](#check-if-a-string-is-in-title-case) ```tql from {x: "Hello World".is_title()} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`to_title`](/reference/functions/to_title)

# is_upper

Checks if a string is in uppercase. ```tql is_upper(x:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_upper` function returns `true` if `x` is entirely in uppercase; otherwise, it returns `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string is uppercase [Section titled â€œCheck if a string is uppercaseâ€](#check-if-a-string-is-uppercase) ```tql from {x: "HELLO".is_upper()} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_alpha`](/reference/functions/is_alpha), [`is_lower`](/reference/functions/is_lower), [`to_upper`](/reference/functions/to_upper)

# is_v4

Checks whether an IP address has version number 4. ```tql is_v4(x:ip) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `ipv4` function checks whether the version number of a given IP address `x` is 4. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if an IP is IPv4 [Section titled â€œCheck if an IP is IPv4â€](#check-if-an-ip-is-ipv4) ```tql from { x: 1.2.3.4.is_v4(), y: ::1.is_v4(), } ``` ```tql { x: true, y: false, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_v6

Checks whether an IP address has version number 6. ```tql is_v6(x:ip) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `is_v6` function checks whether the version number of a given IP address `x` is 6. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if an IP is IPv6 [Section titled â€œCheck if an IP is IPv6â€](#check-if-an-ip-is-ipv6) ```tql from { x: 1.2.3.4.is_v6(), y: ::1.is_v6(), } ``` ```tql { x: false, y: true, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# join

Joins a list of strings into a single string using a separator. ```tql join(xs:list, [separator:string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `join` function concatenates the elements of the input list `xs` into a single string, separated by the specified `separator`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) A list of strings to join. ### `separator: string (optional)` [Section titled â€œseparator: string (optional)â€](#separator-string-optional) The string used to separate elements in the result. Defaults to `""`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Join a list of strings with a comma [Section titled â€œJoin a list of strings with a commaâ€](#join-a-list-of-strings-with-a-comma) ```tql from {x: join(["a", "b", "c"], "-")} ``` ```tql {x: "a-b-c"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`split`](/reference/functions/split), [`split_regex`](/reference/functions/split_regex)

# keys

Retrieves a list of field names from a record. ```tql keys(x:record) -> list<string> ``` ## Description [Section titled â€œDescriptionâ€](#description) The `keys` function returns a list of strings containing all field names from the input record `x`. ### `x: record` [Section titled â€œx: recordâ€](#x-record) The record whose field names you want to retrieve. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get all field names from a record [Section titled â€œGet all field names from a recordâ€](#get-all-field-names-from-a-record) ```tql from {x: 1, y: "hello", z: true} select field_names = this.keys() ``` ```tql { field_names: ["x", "y", "z"], } ``` ### Use keys to dynamically access record fields [Section titled â€œUse keys to dynamically access record fieldsâ€](#use-keys-to-dynamically-access-record-fields) You can combine `keys` with sorting and element access to dynamically select fields from records. For example, the following collects the values from the element with the first key alphabetically in each record: ```tql from ( {foo: 10, bar: 20, baz: 30}, {foo: 100, bar: 200}, {baz: 300, qux: 400}, {}, ) summarize first_sorted_key = this[this.keys().sort().first()]?.collect() ``` ```tql { first_sorted_key: [20, 200, 300], } ``` ### Use keys to get distribution of available fields [Section titled â€œUse keys to get distribution of available fieldsâ€](#use-keys-to-get-distribution-of-available-fields) ```tql from ( {foo: 10, bar: 20, baz: 30}, {foo: 100, bar: 200}, {baz: 300, qux: 400}, {}, ) select key = this.keys() unroll key top key ``` ```tql {name: "foo", count: 2} {name: "bar", count: 2} {name: "baz", count: 2} {name: "qux", count: 1} ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`has`](/reference/functions/has), [`get`](/reference/functions/get)

# last

Takes the last non-null grouped value. ```tql last(xs:list) -> any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `last` function returns the last non-null value in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to search. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the last non-null value [Section titled â€œGet the last non-null valueâ€](#get-the-last-non-null-value) ```tql from {x: 1}, {x: 2}, {x: null} summarize last_value=last(x) ``` ```tql {last_value: 2} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`first`](/reference/functions/first)

# length

Retrieves the length of a list. ```tql length(xs:list) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `length` function returns the number of elements in the list `xs`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the length of a list [Section titled â€œGet the length of a listâ€](#get-the-length-of-a-list) ```tql from {n: [1, 2, 3].length()} ``` ```tql {n: 3} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`is_empty`](/reference/functions/is_empty), [`length_bytes`](/reference/functions/length_bytes), [`length_chars`](/reference/functions/length_chars)

# length_bytes

Returns the length of a string in bytes. ```tql length_bytes(x:string) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `length_bytes` function returns the byte length of the `x` string. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the byte length of a string [Section titled â€œGet the byte length of a stringâ€](#get-the-byte-length-of-a-string) For ASCII strings, the byte length is the same as the number of characters: ```tql from {x: "hello".length_bytes()} ``` ```tql {x: 5} ``` For Unicode, this may not be the case: ```tql from {x: "ğŸ‘»".length_bytes()} ``` ```tql {x: 4} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`length`](/reference/functions/length), [`length_chars`](/reference/functions/length_chars)

# length_chars

Returns the length of a string in characters. ```tql length_chars(x:string) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `length_chars` function returns the character length of the `x` string. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the character length of a string [Section titled â€œGet the character length of a stringâ€](#get-the-character-length-of-a-string) For ASCII strings, the character length is the same as the number of bytes: ```tql from {x: "hello".length_chars()} ``` ```tql {x: 5} ``` For Unicode, this may not be the case: ```tql from {x: "ğŸ‘»".length_chars()} ``` ```tql {x: 1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`length`](/reference/functions/length), [`length_bytes`](/reference/functions/length_bytes)

# map

Maps each list element to an expression. ```tql map(xs:list, capture:field, any->any) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `map` function applies an expression to each element within a list, returning a list of the same length. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) A list of values. ### `function: any -> any` [Section titled â€œfunction: any -> anyâ€](#function-any---any) A lambda function that is applied to each list element. If the lambda evaluates to different but compatible types for the elements of the list a unification is performed. For example, records are compatible with other records, and the resulting record will have the keys of both. If the lambda evaluates to incompatible types for different elements of the list, the largest possible group of compatible values will be chosen and all other values will be `null`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check a predicate for all members of a list [Section titled â€œCheck a predicate for all members of a listâ€](#check-a-predicate-for-all-members-of-a-list) ```tql from { hosts: [1.2.3.4, 127.0.0.1, 10.0.0.127] } hosts = hosts.map(x => x in 10.0.0.0/8) ``` ```tql { hosts: [false, false, true] } ``` ### Reshape a record inside a list [Section titled â€œReshape a record inside a listâ€](#reshape-a-record-inside-a-list) ```tql from { answers: [ { rdata: 76.76.21.21, rrname: "tenzir.com" } ] } answers = answers.map(x => {hostname: x.rrname, ip: x.rdata}) ``` ```tql { answers: [ { hostname: "tenzir.com", ip: "76.76.21.21", } ] } ``` ### Null values [Section titled â€œNull valuesâ€](#null-values) In the below example, the first entry does not match the given grok pattern, causing `parse_grok` to emit a `null`. `map` will promote `null` values to typed null values, allowing you to still get all valid parts of the list mapped. ```tql let $pattern = "%{WORD:w} %{NUMBER:n}" from { l: ["hello", "world 42"] } l = l.map(str => str.parse_grok($pattern)) ``` ```tql { l: [ null, { w: "world", n: 42, }, ], } ``` ### Incompatible types between elements [Section titled â€œIncompatible types between elementsâ€](#incompatible-types-between-elements) In the below example the list `l` contains three strings. Two of those are JSON objects and one is a JSON list. While all three can be parsed as JSON by `parse_json`, the resulting `record` and `list` are incompatible types. `map` will resolve this by picking the â€œlargest compatible groupâ€, in this case preferring the two `record`s over one `list`. ```tql from { l: [ r#"{ "x": 0 }"#, r#"{ "y": 0 }"#, r#"[ 3 ]"#, ] } l = l.map(str => str.parse_json()) ``` ```tql { l: [ {x: 0, y: null}, {x: null, y: 0}, null, ] } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`where`](/reference/functions/where), [`zip`](/reference/functions/zip)

# match_regex

Checks if a string partially matches a regular expression. ```tql match_regex(input:string, regex:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `match_regex` function returns `true` if `regex` matches a substring of `input`. To check whether the full string matches, you can use `^` and `$` to signify start and end of the string. ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to partially match. ### `regex: string` [Section titled â€œregex: stringâ€](#regex-string) The regular expression try and match. The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `match_regex` at the moment. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check contains a matching substring [Section titled â€œCheck contains a matching substringâ€](#check-contains-a-matching-substring) ```tql from {input: "Hello There World"}, {input: "hi there!"}, {input: "Good Morning" } output = input.match_regex("[T|t]here") ``` ```tql {input: "Hello There World", output: true} {input: "hi there!", output: true} {input: "Good Morning", output: false} ``` ### Check if a string matches fully [Section titled â€œCheck if a string matches fullyâ€](#check-if-a-string-matches-fully) ```tql from {input: "example"}, {input: "Example!"}, {input: "example?" } output = input.match_regex("^[E|e]xample[!]?$") ``` ```tql {input: "example", output: true} {input: "example!", output: true} {input: "example?", output: false} ```

# max

Computes the maximum of all grouped values. ```tql max(xs:list) -> number ``` ## Description [Section titled â€œDescriptionâ€](#description) The `max` function returns the largest numeric value in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Find the maximum value [Section titled â€œFind the maximum valueâ€](#find-the-maximum-value) ```tql from {x: 1}, {x: 2}, {x: 3} summarize max_value=max(x) ``` ```tql {max_value: 3} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`min`](/reference/functions/min), [`mean`](/reference/functions/mean), [`sum`](/reference/functions/sum)

# mean

Computes the mean of all grouped values. ```tql mean(xs:list) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `mean` function returns the average of all numeric values in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to average. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the mean value [Section titled â€œCompute the mean valueâ€](#compute-the-mean-value) ```tql from {x: 1}, {x: 2}, {x: 3} summarize avg=mean(x) ``` ```tql {avg: 2.0} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`max`](/reference/functions/max), [`median`](/reference/functions/median), [`min`](/reference/functions/min), [`quantile`](/reference/functions/quantile), [`stddev`](/reference/functions/stddev), [`sum`](/reference/functions/sum), [`variance`](/reference/functions/variance)

# median

Computes the approximate median of all grouped values using a t-digest algorithm. ```tql median(xs:list) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `median` function returns an approximate median of all numeric values in `xs`, computed with a t-digest algorithm. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the median value [Section titled â€œCompute the median valueâ€](#compute-the-median-value) ```tql from {x: 1}, {x: 2}, {x: 3}, {x: 4} summarize median_value=median(x) ``` ```tql {median_value: 2.5} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`mean`](/reference/functions/mean), [`mode`](/reference/functions/mode), [`quantile`](/reference/functions/quantile)

# merge

Combines two records into a single record by merging their fields. ```tql merge(x: record, y: record) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `merge` function takes two records and returns a new record containing all fields from both records. If both records contain the same field, the value from the second record takes precedence. ## Examples [Section titled â€œExamplesâ€](#examples) ### Basic record merging [Section titled â€œBasic record mergingâ€](#basic-record-merging) ```tql from {x: {a: 1, b: 2}, y: {c: 3, d: 4}} select result = merge(x, y) ``` ```tql { result: { a: 1, b: 2, c: 3, d: 4 } } ``` ### Handling overlapping fields [Section titled â€œHandling overlapping fieldsâ€](#handling-overlapping-fields) When fields exist in both records, the second recordâ€™s values take precedence: ```tql from { r1: {name: "Alice", age: 30}, r2: {name: "Bob", location: "NY"}, } select result = merge(r1, r2) ``` ```tql { result: { name: "Bob", age: 30, location: "NY" } } ``` ### Handling null values [Section titled â€œHandling null valuesâ€](#handling-null-values) If either input is null, the input will be ignored. ```tql from {x: {a: 1}, y: null} select result = merge(x, y) ``` ```tql { result: { a: 1 } } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`concatenate`](/reference/functions/concatenate)

# microseconds

Converts a number to equivalent microseconds. ```tql microseconds(x:number) -> duration ``` ## Description This function returns microseconds equivalent to a number, i.e., `number * 1us`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# milliseconds

Converts a number to equivalent milliseconds. ```tql milliseconds(x:number) -> duration ``` ## Description This function returns milliseconds equivalent to a number, i.e., `number * 1ms`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# min

Computes the minimum of all grouped values. ```tql min(xs:list) -> number ``` ## Description [Section titled â€œDescriptionâ€](#description) The `min` function returns the smallest numeric value in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Find the minimum value [Section titled â€œFind the minimum valueâ€](#find-the-minimum-value) ```tql from {x: 1}, {x: 2}, {x: 3} summarize min_value=min(x) ``` ```tql {min_value: 1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`max`](/reference/functions/max), [`mean`](/reference/functions/mean), [`sum`](/reference/functions/sum)

# minute

Extracts the minute component from a timestamp. ```tql minute(x: time) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `minute` function extracts the minute component from a timestamp as an integer (0-59). ### `x: time` [Section titled â€œx: timeâ€](#x-time) The timestamp from which to extract the minute. ## Examples [Section titled â€œExamplesâ€](#examples) ### Extract the minute from a timestamp [Section titled â€œExtract the minute from a timestampâ€](#extract-the-minute-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } minute = ts.minute() ``` ```tql { ts: 2024-06-15T14:30:45.123456, minute: 30, } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`second`](/reference/functions/second)

# minutes

Converts a number to equivalent minutes. ```tql minutes(x:number) -> duration ``` ## Description This function returns minutes equivalent to a number, i.e., `number * 1min`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# mode

Takes the most common non-null grouped value. ```tql mode(xs:list) -> any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `mode` function returns the most frequently occurring non-null value in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Find the mode of values [Section titled â€œFind the mode of valuesâ€](#find-the-mode-of-values) ```tql from {x: 1}, {x: 1}, {x: 2}, {x: 3} summarize mode_value=mode(x) ``` ```tql {mode_value: 1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`median`](/reference/functions/median), [`value_counts`](/reference/functions/value_counts)

# month

Extracts the month component from a timestamp. ```tql month(x: time) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `month` function extracts the month component from a timestamp as an integer (1-12). ### `x: time` [Section titled â€œx: timeâ€](#x-time) The timestamp from which to extract the month. ## Examples [Section titled â€œExamplesâ€](#examples) ### Extract the month from a timestamp [Section titled â€œExtract the month from a timestampâ€](#extract-the-month-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } month = ts.month() ``` ```tql { ts: 2024-06-15T14:30:45.123456, month: 6, } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`year`](/reference/functions/year), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# months

Converts a number to equivalent months. ```tql months(x:number) -> duration ``` ## Description This function returns months equivalent to a number, i.e., `number * 1/12 * 1y`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# nanoseconds

Converts a number to equivalent nanoseconds. ```tql nanoseconds(x:number) -> duration ``` ## Description This function returns nanoseconds equivalent to a number, i.e., `number * 1ns`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# network

Retrieves the network address of a subnet. ```tql network(x:subnet) -> ip ``` ## Description [Section titled â€œDescriptionâ€](#description) The `network` function returns the network address of a subnet. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the network address of a subnet [Section titled â€œGet the network address of a subnetâ€](#get-the-network-address-of-a-subnet) ```tql from {subnet: 192.168.0.0/16} select ip = subnet.network() ``` ```tql {ip: 192.168.0.0} ```

# now

Gets the current wallclock time. ```tql now() -> time ``` ## Description [Section titled â€œDescriptionâ€](#description) The `now` function returns the current wallclock time. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the time in UTC [Section titled â€œGet the time in UTCâ€](#get-the-time-in-utc) ```tql let $now = now() from { x: $now } ``` ```tql {x: 2024-10-28T13:27:33.957987} ``` ### Compute a field with the current time [Section titled â€œCompute a field with the current timeâ€](#compute-a-field-with-the-current-time) ```tql subscribe "my-topic" select ts=now() ``` ```tql {ts: 2024-10-30T15:03:04.85298} {ts: 2024-10-30T15:03:06.31878} {ts: 2024-10-30T15:03:07.59813} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`from_epoch`](/reference/functions/from_epoch), [`since_epoch`](/reference/functions/since_epoch)

# ocsf::category_name

Returns the `category_name` for a given `category_uid`. ```tql ocsf::category_uid(id:int) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `id: int` [Section titled â€œid: intâ€](#id-int) The `category_uid` for which `category_name` should be returned. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::category_uid`](/reference/functions/ocsf/category_uid)

# ocsf::category_uid

Returns the `category_uid` for a given `category_name`. ```tql ocsf::category_uid(name:string) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `name: string` [Section titled â€œname: stringâ€](#name-string) The `category_name` for which `category_uid` should be returned. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::category_name`](/reference/functions/ocsf/category_name)

# ocsf::class_name

Returns the `class_name` for a given `class_uid`. ```tql ocsf::class_uid(id:int) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `id: int` [Section titled â€œid: intâ€](#id-int) The `class_uid` for which `class_name` should be returned. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::class_uid`](/reference/functions/ocsf/class_uid)

# ocsf::class_uid

Returns the `class_uid` for a given `class_name`. ```tql ocsf::class_uid(name:string) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `name: string` [Section titled â€œname: stringâ€](#name-string) The `class_name` for which `class_uid` should be returned. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::class_name`](/reference/functions/ocsf/class_name)

# ocsf::type_name

Returns the `type_name` for a given `type_uid`. ```tql ocsf::type_name(id:int) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `id: int` [Section titled â€œid: intâ€](#id-int) The `type_uid` for which `type_name` should be returned. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::type_uid`](/reference/functions/ocsf/type_uid)

# ocsf::type_uid

Returns the `type_uid` for a given `type_name`. ```tql ocsf::type_uid(name:string) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `name: string` [Section titled â€œname: stringâ€](#name-string) The `type_name` for which `type_uid` should be returned. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::type_name`](/reference/functions/ocsf/type_name)

# otherwise

Returns a `fallback` value if `primary` is `null`. ```tql otherwise(primary:any, fallback:any) -> any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `otherwise` function evaluates its arguments and replaces `primary` with `fallback` where `primary` would be `null`. ### `primary: any` [Section titled â€œprimary: anyâ€](#primary-any) The expression to return if not `null`. ### `fallback: any` [Section titled â€œfallback: anyâ€](#fallback-any) The expression to return if `primary` evaluates to `null`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Set a default value for a key [Section titled â€œSet a default value for a keyâ€](#set-a-default-value-for-a-key) ```tql from {x: 1}, {x: 2}, {} x = x.otherwise(-1) ``` ```tql {x: 1} {x: 2} {x: -1} ```

# pad_end

Pads a string at the end to a specified length. ```tql pad_end(x:string, length:int, [pad_char:string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `pad_end` function pads the string `x` at the end with `pad_char` (default: space) until it reaches the specified `length`. If the string is already longer than or equal to the specified length, it returns the original string unchanged. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string to pad. ### `length: int` [Section titled â€œlength: intâ€](#length-int) The target length of the resulting string. ### `pad_char: string` [Section titled â€œpad\_char: stringâ€](#pad_char-string) The character to use for padding. Must be a single character. Defaults to a space. Defaults to `" "`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Pad with spaces [Section titled â€œPad with spacesâ€](#pad-with-spaces) ```tql from {x: "hello".pad_end(10)} ``` ```tql {x: "hello "} ``` ### Pad with custom character [Section titled â€œPad with custom characterâ€](#pad-with-custom-character) ```tql from {x: "hello".pad_end(10, ".")} ``` ```tql {x: "hello....."} ``` ### String already long enough [Section titled â€œString already long enoughâ€](#string-already-long-enough) ```tql from {x: "hello world".pad_end(5)} ``` ```tql {x: "hello world"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`pad_start`](/reference/functions/pad_start), [`trim`](/reference/functions/trim), [`trim_end`](/reference/functions/trim_end)

# pad_start

Pads a string at the start to a specified length. ```tql pad_start(x:string, length:int, [pad_char:string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `pad_start` function pads the string `x` at the start with `pad_char` (default: space) until it reaches the specified `length`. If the string is already longer than or equal to the specified length, it returns the original string unchanged. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string to pad. ### `length: int` [Section titled â€œlength: intâ€](#length-int) The target length of the resulting string. ### `pad_char: string` [Section titled â€œpad\_char: stringâ€](#pad_char-string) The character to use for padding. Must be a single character. Defaults to a space. Defaults to `" "`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Pad with spaces [Section titled â€œPad with spacesâ€](#pad-with-spaces) ```tql from {x: "hello".pad_start(10)} ``` ```tql {x: " hello"} ``` ### Pad with custom character [Section titled â€œPad with custom characterâ€](#pad-with-custom-character) ```tql from {x: "42".pad_start(5, "0")} ``` ```tql {x: "00042"} ``` ### String already long enough [Section titled â€œString already long enoughâ€](#string-already-long-enough) ```tql from {x: "hello world".pad_start(5)} ``` ```tql {x: "hello world"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`pad_end`](/reference/functions/pad_end), [`trim`](/reference/functions/trim), [`trim_start`](/reference/functions/trim_start)

# parent_dir

Extracts the parent directory from a file path. ```tql parent_dir(x:string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parent_dir` function returns the parent directory path of the given file path, excluding the file name. ## Examples [Section titled â€œExamplesâ€](#examples) ### Extract the parent directory from a file path [Section titled â€œExtract the parent directory from a file pathâ€](#extract-the-parent-directory-from-a-file-path) ```tql from {x: parent_dir("/path/to/log.json")} ``` ```tql {x: "/path/to"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`file_name`](/reference/functions/file_name)

# parse_cef

Parses a string as a CEF message ```tql parse_cef(input:string, [schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_cef` function parses a string as a CEF message ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from { x: "CEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8|key=value" } y = x.parse_cef() ``` ```tql { cef_version: 0, device_vendor: "Cynet", device_product: "Cynet 360", device_version: "4.5.4.22139", signature_id: "0", name: "Memory Pattern - Cobalt Strike Beacon ReflectiveLoader", severity: "8", extension: { key: "value" } } ``` # See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_cef`](/reference/operators/read_cef), [`print_cef`](/reference/functions/print_cef), [`read_syslog`](/reference/operators/read_syslog)

# parse_csv

Parses a string as CSV (Comma-Separated Values). ```tql parse_csv(input:string, header=list<string>|string, [list_separator=string, null_value=string, auto_expand=bool, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_csv` function parses a string as [CSV](https://en.wikipedia.org/wiki/Comma-separated_values). ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `header = list<string>|string` [Section titled â€œheader = list\<string>|stringâ€](#header--liststringstring) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. ### `list_separator = string (optional)` [Section titled â€œlist\_separator = string (optional)â€](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `;`. ### `null_value = string (optional)` [Section titled â€œnull\_value = string (optional)â€](#null_value--string-optional) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled â€œauto\_expand = bool (optional)â€](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from { input: "1,2,3" } output = input.parse_csv(header=["a","b","c"]) ``` ```tql { input: "1,2,3", output: { a: 1, b: 2, c: 3, }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_csv`](/reference/operators/read_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_tsv`](/reference/functions/parse_tsv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_grok

Parses a string according to a grok pattern. ```tql parse_grok(input:string, pattern:string, [pattern_definitions=record|string, indexed_captures=bool, include_unnamed=bool, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) `parse_grok` uses a regular expression based parser similar to the [Logstash `grok` plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) in Elasticsearch. Tenzir ships with the same built-in patterns as Elasticsearch, found [here](https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns/ecs-v1). In short, `pattern` consists of replacement fields, that look like `%{SYNTAX[:SEMANTIC[:CONVERSION]]}`, where: * `SYNTAX` is a reference to a pattern, either built-in or user-defined through the `pattern_defintions` option. * `SEMANTIC` is an identifier that names the field in the parsed record. * `CONVERSION` is either `infer` (default), `string` (default with `raw=true`), `int`, or `float`. The supported regular expression syntax is the one supported by [Boost.Regex](https://www.boost.org/doc/libs/1_81_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html), which is effectively Perl-compatible. ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `pattern: string` [Section titled â€œpattern: stringâ€](#pattern-string) The `grok` pattern used for matching. Must match the input in its entirety. ### `pattern_definitions = record|string (optional)` [Section titled â€œpattern\_definitions = record|string (optional)â€](#pattern_definitions--recordstring-optional) New pattern definitions to use. This may be a record of the form ```tql { pattern_name: "pattern", â€¦ } ``` For example, the built-in pattern `INT` would be defined as ```tql { INT: "(?:[+-]?(?:[0-9]+))" } ``` Alternatively, this may be a user-defined newline-delimited list of patterns, where a line starts with the pattern name, followed by a space, and the `grok`-pattern for that pattern. For example, the built-in pattern `INT` is defined as follows: ```plaintext INT (?:[+-]?(?:[0-9]+)) ``` ### `indexed_captures = bool (optional)` [Section titled â€œindexed\_captures = bool (optional)â€](#indexed_captures--bool-optional) All subexpression captures are included in the output, with the `SEMANTIC` used as the field name if possible, and the capture index otherwise. ### `include_unnamed = bool (optional)` [Section titled â€œinclude\_unnamed = bool (optional)â€](#include_unnamed--bool-optional) By default, only fields that were given a name with `SEMANTIC`, or with the regular expression named capture syntax `(?<name>...)` are included in the resulting record. With `include_unnamed=true`, replacement fields without a `SEMANTIC` are included in the output, using their `SYNTAX` value as the record field name. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ```tql let $pattern = "%{IP:client} %{WORD} %{URIPATHPARAM:req} %{NUMBER:bytes} %{NUMBER:dur}" from { input: "55.3.244.1 GET /index.html 15824 0.043" } output = input.parse_grok($pattern) output.dur = output.dur * 1s ``` ```tql { input: "55.3.244.1 GET /index.html 15824 0.043", output: { client: 55.3.244.1, req: "/index.html", bytes: 15824, dur: 43.0ms } } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_grok`](/reference/operators/read_grok)

# parse_json

Parses a string as a JSON value. ```tql parse_json(input:string, [schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_json` function parses a string as a JSON value. ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse a JSON record [Section titled â€œParse a JSON recordâ€](#parse-a-json-record) ```tql from { input: r#"{ "a": 42, "b": "text"}"# } output = input.parse_json() ``` ```tql { input: "{ \"a\": 42, \"b\": \"text\"}", output: { a: 42, b: "text" } } ``` ### Parse a JSON list [Section titled â€œParse a JSON listâ€](#parse-a-json-list) ```tql from { input: "[0,1]" } output = input.parse_json() ``` ```tql { input: "[0,1]", output: [0, 1] } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_json`](/reference/operators/read_json)

# parse_kv

Parses a string as key-value pairs. ```tql parse_kv(input:string, [field_split=string, value_split=string, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_kv` function parses a string as key-value pairs. The string is first split into fields according to `field_split`. This can be a regular expression. For example, the input `foo: bar, baz: 42` can be split into `foo: bar` and `baz: 42` with the regular expression `r",\s*"` (a comma, followed by any amount of whitespace) as the field splitter. Note that the matched separators are removed when splitting a string. Afterwards, the extracted fields are split into their key and value by `value_split`, which can again be a regular expression. In our example, `r":\s*"` could be used to split `foo: bar` into the key `foo` and its value `bar`, and similarly `baz: 42` into `baz` and `42`. The result would thus be `{"foo": "bar", "baz": 42}`. If the regex matches multiple substrings, only the first match is used. If no match is found, the â€œfieldâ€ is considered an extension of the previous fields value. The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `parse_kv` at the moment. However, if the regular expression has a capture group, it is assumed that only the content of the capture group shall be used as the separator. This means that unsupported regular expressions such as `(?=foo)bar(?<=baz)` can be effectively expressed as `foo(bar)baz` instead. ### Quoted Values [Section titled â€œQuoted Valuesâ€](#quoted-values) The parser is aware of double-quotes (`"`). If the `field_split` or `value_split` are found within enclosing quotes, they are not considered matches. This means that both the key and the value may be enclosed in double-quotes. For example, given `field_split` `\s*,\s*` and `value_split` `=`, the input ```plaintext "key"="nested = value",key2="value, and more" ``` will parse as ```tql { key: "nested = value", key2: "value, and more", } ``` ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `field_split = string (optional)` [Section titled â€œfield\_split = string (optional)â€](#field_split--string-optional) The regular expression used to separate individual fields. Defaults to `r"\s"`. ### `value_split = string (optional)` [Section titled â€œvalue\_split = string (optional)â€](#value_split--string-optional) The regular expression used to separate a key from its value. Defaults to `"="`. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse comma-separated key-value pairs [Section titled â€œParse comma-separated key-value pairsâ€](#parse-comma-separated-key-value-pairs) ```tql from { input: "surname: John, family_name: Smith, date_of_birth: 1995-05-26" } output = input.parse_kv(field_split=r"\s*,\s*", value_split=r"\s*:\s*") ``` ```tql { input: "surname: John, family_name: Smith, date_of_birth: 1995-05-26", output: { surname: "John", family_name: "Smith", date_of_birth: 1995-05-26, }, } ``` ### Fields without a `value_split` [Section titled â€œFields without a value\_splitâ€](#fields-without-a-value_split) ```tql from { input: "x=1 y=2 z=3 4 5 a=6" } this = { ...input.parse_kv() } ``` ```tql { x: 1, y: 2, z: "3 4 5", a: 6, } ``` # See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_kv`](/reference/operators/read_kv), [`print_kv`](/reference/functions/print_kv)

# parse_leef

Parses a string as a LEEF message ```tql parse_leef(input:string, [schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_leef` function parses a string as a LEEF message ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from { input: "LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1 dst=2.10.20.20 spt=1200" } output = input.parse_leef() ``` ```tql { input: "LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1\tdst=2.10.20.20\tspt=1200", output: { leef_version: "1.0", vendor: "Microsoft", product_name: "MSExchange", product_version: "2016", event_class_id: "15345", attributes: { src: 10.50.1.1, dst: 2.10.20.20, spt: 1200, }, }, } ``` # See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_leef`](/reference/operators/read_leef), [`print_leef`](/reference/functions/print_leef), [`parse_cef`](/reference/functions/parse_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# parse_ssv

Parses a string as space separated values. ```tql parse_ssv(input:string, header=list<string>|string, [list_separator:string, null_value:string, auto_expand=bool, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_ssv` function parses a string as space separated values. ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `header = list<string>|string` [Section titled â€œheader = list\<string>|stringâ€](#header--liststringstring) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. ### `list_separator = string (optional)` [Section titled â€œlist\_separator = string (optional)â€](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `,`. ### `null_value = string (optional)` [Section titled â€œnull\_value = string (optional)â€](#null_value--string-optional) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled â€œauto\_expand = bool (optional)â€](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from { input: "1 2 3" } output = input.parse_ssv(header=["a","b","c"]) ``` ```tql { input: "1 2 3", output: { a: 1, b: 2, c: 3, }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_ssv`](/reference/operators/read_ssv), [`parse_csv`](/reference/functions/parse_csv), [`parse_tsv`](/reference/functions/parse_tsv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_syslog

Parses a string as a Syslog message. ```tql parse_syslog [raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) Parses a string as a [Syslog](https://en.wikipedia.org/wiki/Syslog) message. Tenzir supports reading syslog messages in both the standardized â€œSyslog Protocolâ€ format ([RFC 5424](https://tools.ietf.org/html/rfc5424)), and the older â€œBSD syslog Protocolâ€ format ([RFC 3164](https://tools.ietf.org/html/rfc3164)). ## `input: string` [Section titled â€œinput: stringâ€](#input-string) ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse a RFC5424 syslog string [Section titled â€œParse a RFC5424 syslog stringâ€](#parse-a-rfc5424-syslog-string) ```tql from { input: r#"<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource="Application" eventID="1011"] Event log entry"#} output = input.parse_syslog() ``` ```tql { input: "<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource=\"Application\" eventID=\"1011\"] Event log entry", output: { facility: 20, severity: 5, version: 8, timestamp: 2023-10-11T22:14:15.003Z, hostname: "mymachineexamplecom", app_name: "evntslog", process_id: "1370", message_id: "ID47", structured_data: { "exampleSDID@32473": { eventSource: "Application", eventID: 1011, }, }, message: "Event log entry", }, } ```

# parse_time

Parses a time from a string that follows a specific format. ```tql parse_time(input: string, format: string) -> time ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_time` function matches the given `input` string against the `format` to construct a timestamp. ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The input string from which the timestamp should be extracted. ### `format: string` [Section titled â€œformat: stringâ€](#format-string) The string that specifies the format of `input`, for example `"%m-%d-%Y"`. The allowed format specifiers are the same as for `strptime(3)`: | Specifier | Description | Example | | :-------: | :------------------------------------- | :------------------------ | | `%%` | A literal `%` character | `%` | | `%a` | Abbreviated weekday name | `Mon` | | `%A` | Full weekday name | `Monday` | | `%b` | Abbreviated month name | `Jan` | | `%B` | Full month name | `January` | | `%c` | Date and time representation | `Mon Jan 1 12:00:00 2024` | | `%C` | Century (year divided by 100) | `20` | | `%d` | Day of month with zero padding | `01`, `31` | | `%D` | Equivalent to `%m/%d/%y` | `01/31/24` | | `%e` | Day of month with space padding | `1`, `31` | | `%F` | Equivalent to `%Y-%m-%d` | `2024-01-31` | | `%g` | Last two digits of ISO week-based year | `24` | | `%G` | ISO week-based year | `2024` | | `%h` | Equivalent to `%b` | `Jan` | | `%H` | Hour in 24-hour format | `00`, `23` | | `%I` | Hour in 12-hour format | `01`, `12` | | `%j` | Day of year | `001`, `365` | | `%m` | Month number | `01`, `12` | | `%M` | Minute | `00`, `59` | | `%n` | Newline character | `\n` | | `%p` | AM/PM designation | `AM`, `PM` | | `%r` | 12-hour clock time | `12:00:00 PM` | | `%R` | Equivalent to `%H:%M` | `23:59` | | `%S` | Seconds | `00`, `59` | | `%t` | Tab character | `\t` | | `%T` | Equivalent to `%H:%M:%S` | `23:59:59` | | `%u` | ISO weekday (Monday=1) | `1`, `7` | | `%U` | Week number (Sunday as first day) | `00`, `52` | | `%V` | ISO week number | `01`, `53` | | `%w` | Weekday (Sunday=0) | `0`, `6` | | `%W` | Week number (Monday as first day) | `00`, `52` | | `%x` | Date representation | `01/31/24` | | `%X` | Time representation | `23:59:59` | | `%y` | Year without century | `24` | | `%Y` | Year with century | `2024` | | `%z` | UTC offset | `+0000`, `-0430` | | `%Z` | Time zone abbreviation | `UTC`, `EST` | ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse a timestamp [Section titled â€œParse a timestampâ€](#parse-a-timestamp) ```tql from { x: "2024-12-31+12:59:42", } x = x.parse_time("%Y-%m-%d+%H:%M:%S") ``` ```tql {x: 2024-12-31T12:59:42.000000} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`format_time`](/reference/functions/format_time)

# parse_tsv

Parses a string as tab separated values. ```tql parse_tsv(input:string, header=list<string>|string, [list_separator:string, null_value:string, auto_expand=bool, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_tsv` function parses a string as [TSV](https://en.wikipedia.org/wiki/Tab-separated_values). ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `header = list<string>|string` [Section titled â€œheader = list\<string>|stringâ€](#header--liststringstring) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. ### `list_separator = string (optional)` [Section titled â€œlist\_separator = string (optional)â€](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `,`. ### `null_value = string (optional)` [Section titled â€œnull\_value = string (optional)â€](#null_value--string-optional) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled â€œauto\_expand = bool (optional)â€](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ```tql from { input: "1\t2\t3" } output = input.parse_tsv(header=["a","b","c"]) ``` ```tql { input: "1\t2\t3", output: { a: 1, b: 2, c: 3, }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_tsv`](/reference/operators/read_tsv), [`parse_csv`](/reference/functions/parse_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_xsv

Parses a string as delimiter separated values. ```tql parse_xsv(input:string, field_separator=string, list_separator=string, null_value=string, header=list<string>|string, [auto_expand=bool, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_xsv` function parses a string as [XSV](https://en.wikipedia.org/wiki/Delimiter-separated_values), a generalization of CSV with a more flexible separator specification. The following table lists existing XSV configurations: | Format | Field Separator | List Separator | Null Value | | --------------------------------------- | :-------------: | :------------: | :--------: | | [`csv`](/reference/functions/parse_csv) | `,` | `;` | empty | | [`ssv`](/reference/functions/parse_ssv) | `<space>` | `,` | `-` | | [`tsv`](/reference/functions/parse_tsv) | `\t` | `,` | `-` | ### `header = list<string>|string` [Section titled â€œheader = list\<string>|stringâ€](#header--liststringstring) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. ### `field_separator = string` [Section titled â€œfield\_separator = stringâ€](#field_separator--string) The string separating different fields. ### `list_separator = string` [Section titled â€œlist\_separator = stringâ€](#list_separator--string) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. ### `null_value = string` [Section titled â€œnull\_value = stringâ€](#null_value--string) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled â€œauto\_expand = bool (optional)â€](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from { input: "1,2,3" } output = input.parse_xsv( field_separator=",", list_separator= ";", null_value="", header=["a","b","c"], ) ``` ```tql { input: "1,2,3", output: { a: 1, b: 2, c: 3, }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_xsv`](/reference/operators/read_xsv), [`parse_csv`](/reference/functions/parse_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_tsv`](/reference/functions/parse_tsv)

# parse_yaml

Parses a string as a YAML value. ```tql parse_yaml(input:string, [schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_yaml` function parses a string as a YAML value. ### `input: string` [Section titled â€œinput: stringâ€](#input-string) The string to parse. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse a YAML map containing a list [Section titled â€œParse a YAML map containing a listâ€](#parse-a-yaml-map-containing-a-list) ```tql from { x: r#"yarp: - darp - larp"# } x = x.parse_yaml() ``` ```tql { x: { yarp: [ "darp", "larp", ], }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_yaml`](/reference/operators/read_yaml), [`print_yaml`](/reference/functions/print_yaml)

# prepend

Inserts an element at the start of a list. ```tql prepend(xs:list, x:any) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `prepend` function returns the list `xs` with `x` inserted at the front. The expression `xs.prepend(y)` is equivalent to `[x, ...xs]`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Prepend a number to a list [Section titled â€œPrepend a number to a listâ€](#prepend-a-number-to-a-list) ```tql from {xs: [1, 2]} xs = xs.prepend(3) ``` ```tql {xs: [3, 1, 2]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`add`](/reference/functions/add), [`append`](/reference/functions/append), [`concatenate`](/reference/functions/concatenate), [`remove`](/reference/functions/remove)

# print_cef

Prints records as Common Event Format (CEF) messages ```tql print_cef(extension:record, cef_version=str, device_vendor=str, device_product=str, device_version=str, signature_id=str, name=str, severity=str, [flatten_separator=str, null_value=str]) -> str ``` ## Description [Section titled â€œDescriptionâ€](#description) Prints records as the attributes of a CEF message. ### `extension: record` [Section titled â€œextension: recordâ€](#extension-record) The record to print as the extension of the CEF message ### `cef_version = str` [Section titled â€œcef\_version = strâ€](#cef_version--str) The CEF version in the CEF header. ### `device_vendor = str` [Section titled â€œdevice\_vendor = strâ€](#device_vendor--str) The vendor in the CEF header. ### `device_product = str` [Section titled â€œdevice\_product = strâ€](#device_product--str) The product name in the CEF header. ### `device_version = str` [Section titled â€œdevice\_version = strâ€](#device_version--str) The product version in the CEF header. ### `signature_id = str` [Section titled â€œsignature\_id = strâ€](#signature_id--str) The event (class) ID in the CEF header. ### `name = str` [Section titled â€œname = strâ€](#name--str) The name field in the CEF header, i.e. the human readable description. ### `severity = str` [Section titled â€œseverity = strâ€](#severity--str) The severity in the CEF header. ### `null_value = str (optional)` [Section titled â€œnull\_value = str (optional)â€](#null_value--str-optional) A string to use if any of the values in `extension` are `null`. Defaults to the empty string. ### `flatten_separator = str (optional)` [Section titled â€œflatten\_separator = str (optional)â€](#flatten_separator--str-optional) A string used to flatten nested records in `attributes`. Defaults to `"."`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write a CEF [Section titled â€œWrite a CEFâ€](#write-a-cef) ```tql from { extension: { a: 42, b: "Hello" }, signature_id: "MyCustomSignature", severity: "8" } r = extension.print_cef( cef_version="0", device_vendor="Tenzir", device_product="Tenzir Node", device_version="5.5.0", signature_id=signature_id, severity=severity, name= signature_id + " written by Tenzir" ) select r write_lines ``` ```txt CEF:0|Tenzir|Tenzir Node|5.5.0|MyCustomSignature|MyCustomSignature written by Tenzir|8|a=42 b=Hello ``` ### Upgrade a nested CEF message in Syslog [Section titled â€œUpgrade a nested CEF message in Syslogâ€](#upgrade-a-nested-cef-message-in-syslog) ```tql from "my.log" { read_syslog // produces the expected shape for `write_syslog` } // read the message into a structured form message = message.parse_cef() // re-write the message with modifications message = message.extension.print_cef( cef_version=message.cef_version, device_vendor=message.device_vendor, device_product=message.device_product, device_version=message.device_version, signature_id=signature_id, severity="9" name=message.name ) write_syslog ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_cef`](/reference/functions/parse_cef), [`read_cef`](/reference/operators/read_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# print_csv

Prints a record as a comma-separated string of values. ```tql print_csv(input:record, [list_separator=str, null_value=str]) -> string ``` ## Description The `print_csv` function prints a recordâ€™s values as a comma separated string. ### `input: record` The record you want to print. ### `list_separator = str (optional)` The string separating the elements in list fields. Defaults to `";"`. ### `null_value = str (optional)` The string denoting an absent value. Defaults to `""`. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from { x:1, y:true, z: "String" } output = this.print_csv() ``` ```tql { x: 1, y: true, z: "String", output: "1,true,String", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_csv`](/reference/functions/parse_csv), [`write_csv`](/reference/operators/write_csv)

# print_json

Transforms a value into a JSON string. ```tql print_json(input:any, [strip=bool, color=bool, arrays_of_objects=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms a value into a JSON string. ### `input: any` The value to print as a JSON value. ### `strip = bool (optional)` Enables all `strip_*` options. Defaults to `false`. ### `color = bool (optional)` Colorize the output. Defaults to `false`. ### `strip_null_fields = bool (optional)` Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Print without null fields [Section titled â€œPrint without null fieldsâ€](#print-without-null-fields) ```tql from {x: 0}, {x:null}, {x: { x: 0, y: 1 } }, { x: [0,1,2,] } x = x.print_json(strip_null_fields=true) ``` ```tql { x: "0", } { x: null, } { x: "{\n \"x\": 0,\n \"y\": 1\n}", } { x: "[\n 0,\n 1,\n 2\n]", } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`write_json`](/reference/operators/write_json), [`print_ndjson`](/reference/functions/print_ndjson), [`parse_json`](/reference/functions/parse_json)

# print_kv

Prints records in a key-value format. ```tql print_kv( input:record, [field_separator=str, value_separator=str, list_separator=str, flatten_separator=str, null_value=str] ) -> str ``` ## Description [Section titled â€œDescriptionâ€](#description) Prints records in a Key-Value format. Nested data will be flattend, keys or values containing the given separators will be quoted and the special characters `\n`, `\r`, `\` and `"` will be escaped. ### `input: record` [Section titled â€œinput: recordâ€](#input-record) The record to print as a string. ### `field_separator = str (optional)` [Section titled â€œfield\_separator = str (optional)â€](#field_separator--str-optional) A string that shall separate the key-value pairs. Must not be an empty string. Defaults to `" "`. ### `value_separator = str (optional)` [Section titled â€œvalue\_separator = str (optional)â€](#value_separator--str-optional) A string that shall separate key and value within key-value pair. Must not be an empty string. Defaults to `"="`. ### `list_separator = str (optional)` [Section titled â€œlist\_separator = str (optional)â€](#list_separator--str-optional) Must not be an empty string. Defaults to `","`. ### `flatten_separator = str (optional)` [Section titled â€œflatten\_separator = str (optional)â€](#flatten_separator--str-optional) A string to join the keys of nested records with. For example, given `flatten="."` Defaults to `"."`. ### `null_value = str (optional)` [Section titled â€œnull\_value = str (optional)â€](#null_value--str-optional) A string to represent null values. Defaults to the empty string. ## Examples [Section titled â€œExamplesâ€](#examples) ### Format a record as key-value pair [Section titled â€œFormat a record as key-value pairâ€](#format-a-record-as-key-value-pair) ```tql from { input: {key: "value"} } output = input.print_kv() ``` ```tql { input: { key: "value", }, output: "key=value", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_kv`](/reference/functions/parse_kv), [`write_kv`](/reference/operators/read_kv), [`write_kv`](/reference/operators/write_kv)

# print_leef

Prints records as LEEF messages ```tql print_leef(attributes:record, vendor=str, product_name=str, product_version=str, event_class_id=str, [delimiter=str, null_value=str, flatten_separator=str]) -> str ``` ## Description [Section titled â€œDescriptionâ€](#description) Prints records as the attributes of a [LEEF](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) message. ### `attributes: record` [Section titled â€œattributes: recordâ€](#attributes-record) The record to print as the attributes of a LEEF message ### `vendor = str` [Section titled â€œvendor = strâ€](#vendor--str) The vendor in the LEEF header. ### `product_name = str` [Section titled â€œproduct\_name = strâ€](#product_name--str) The product name in the LEEF header. ### `product_version = str` [Section titled â€œproduct\_version = strâ€](#product_version--str) The product version in the LEEF header. ### `event_class_id = str` [Section titled â€œevent\_class\_id = strâ€](#event_class_id--str) The event (class) ID in the LEEF header. ### `delimiter = str (optional)` [Section titled â€œdelimiter = str (optional)â€](#delimiter--str-optional) This delimiter will be used to separate the key-value pairs in the attributes. It must be a single character. If the chosen delimiter is not `"\t"`, the message will be a LEEF:2.0 message, otherwise it will be LEEF:1.0. Defaults to `"\t"`. ### `null_value = str (optional)` [Section titled â€œnull\_value = str (optional)â€](#null_value--str-optional) A string to use if any of the header values evaluate to null. Defaults to an empty string. ### `flatten_separator = str (optional)` [Section titled â€œflatten\_separator = str (optional)â€](#flatten_separator--str-optional) A string used to flatten nested records in `attributes`. Defaults to `"."`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write a LEEF:1.0 message [Section titled â€œWrite a LEEF:1.0 messageâ€](#write-a-leef10-message) ```tql from { attributes: { a: 42, b: "Hello" }, event_class_id: "critical" } r = attributes.print_leef( vendor="Tenzir", product_name="Tenzir Node", product_version="5.5.0", event_class_id=event_class_id) select r write_lines ``` ```txt LEEF:1.0|Tenzir Node|5.5.0|critical|a=42 b=Hello ``` ### Reformat a nested LEEF message [Section titled â€œReformat a nested LEEF messageâ€](#reformat-a-nested-leef-message) ```tql from "my.log" { read_syslog // produces the expected shape for `write_syslog` } message = message.parse_leef() message = message.attributes.print_leef( vendor=message.vendor, product_name=message.product_name, product_version=message.product_version, event_class_id=message.event_class_id, delimiter="^" ) write_syslog ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_leef`](/reference/functions/parse_leef), [`read_leef`](/reference/operators/read_leef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# print_ndjson

Transforms a value into a single-line JSON string. ```tql print_ndjson(input:any, [strip=bool, color=bool, arrays_of_objects=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms a value into a single-line JSON string. ### `input: any` The value to print as a JSON value. ### `strip = bool (optional)` Enables all `strip_*` options. Defaults to `false`. ### `color = bool (optional)` Colorize the output. Defaults to `false`. ### `strip_null_fields = bool (optional)` Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Print without null fields [Section titled â€œPrint without null fieldsâ€](#print-without-null-fields) ```tql from {x: 0}, {x:null}, {x: {x: 0, y: 1}}, {x: [0,1,2,]} x = x.print_ndjson(strip_null_fields=true) ``` ```tql { x: "0", } { x: null, } { x: "{\"x\": 0, \"y\": 1}", } { x: "[0, 1, 2]", } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`write_ndjson`](/reference/operators/write_ndjson), [`print_json`](/reference/functions/print_json), [`parse_json`](/reference/functions/parse_json)

# print_ssv

Prints a record as a space-separated string of values. ```tql print_ssv(input:record, [list_separator=str, null_value=str]) -> string ``` ## Description The `print_ssv` function prints a recordâ€™s values as a space separated string. ### `input: record` The record you want to print. ### `list_separator = str (optional)` The string separating the elements in list fields. Defaults to `","`. ### `null_value = str (optional)` The string denoting an absent value. Defaults to `"-"`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Print a record as space [Section titled â€œPrint a record as spaceâ€](#print-a-record-as-space) ```tql from {x:1, y:true, z: "String"} output = this.print_ssv() ``` ```tql { x: 1, y: true, z: "String", output: "1 true String", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_ssv`](/reference/functions/parse_ssv), [`write_ssv`](/reference/operators/write_ssv)

# print_tsv

Prints a record as a tab-separated string of values. ```tql print_tsv(input:record, [list_separator=str, null_value=str]) -> string ``` ## Description The `print_tsv` function prints a recordâ€™s values as a tab separated string. ### `input: record` The record you want to print. ### `list_separator = str (optional)` The string separating the elements in list fields. Defaults to `","`. ### `null_value = str (optional)` The string denoting an absent value. Defaults to `"-"`. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from { x:1, y:true, z: "String" } output = this.print_tsv() ``` ```tql { x: 1, y: true, z: "String", output: "1\ttrue\tString", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_tsv`](/reference/functions/parse_tsv), [`write_tsv`](/reference/operators/write_tsv)

# print_xsv

Prints a record as a delimited sequence of values. ```tql print_xsv(input:record, field_separator=str, list_separator=str, null_value=str) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `parse_xsv` function prints a recordâ€™s values as delimiter separated string. The following table lists existing XSV configurations: | Format | Field Separator | List Separator | Null Value | | --------------------------------------- | :-------------: | :------------: | :--------: | | [`csv`](/reference/functions/print_csv) | `,` | `;` | empty | | [`ssv`](/reference/functions/print_ssv) | `<space>` | `,` | `-` | | [`tsv`](/reference/functions/print_tsv) | `\t` | `,` | `-` | ### `field_separator = str` [Section titled â€œfield\_separator = strâ€](#field_separator--str) The string separating different fields. ### `list_separator = str` [Section titled â€œlist\_separator = strâ€](#list_separator--str) The string separating different elements in a list within a single field. ### `null_value = str` [Section titled â€œnull\_value = strâ€](#null_value--str) The string denoting an absent value. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from { x:1, y:true, z: "String", } output = this.print_xsv( field_separator=",", list_separator=";", null_value="null") ``` ```tql { x: 1, y: true, z: "String", output: "1,true,String", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_xsv`](/reference/functions/parse_xsv), [`write_xsv`](/reference/operators/write_xsv)

# print_yaml

Prints a value as a YAML document. ```tql print_yaml( input:any, [include_document_markers=bool] ) ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `input:any` [Section titled â€œinput:anyâ€](#inputany) The value to print as YAML. ### `include_document_markers = bool (optional)` [Section titled â€œinclude\_document\_markers = bool (optional)â€](#include_document_markers--bool-optional) Includes the â€œstart of documentâ€ and â€œend of documentâ€ markers in the result. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from {x: { x: 0, y: 1 } }, { x: [0,1,2,] } x = x.print_yaml() ``` ```tql { x: "x: 0\ny: 1", } { x: "- 0\n- 1\n- 2", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`write_yaml`](/reference/operators/write_yaml), [`parse_yaml`](/reference/functions/parse_yaml)

# quantile

Computes the specified quantile of all grouped values. ```tql quantile(xs:list, q=float) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `quantile` function returns the quantile of all numeric values in `xs`, specified by the argument `q`, which should be a value between 0 and 1. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ### `q: float` [Section titled â€œq: floatâ€](#q-float) The quantile to compute, where `q=0.5` represents the median. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the 0.5 quantile (median) of values [Section titled â€œCompute the 0.5 quantile (median) of valuesâ€](#compute-the-05-quantile-median-of-values) ```tql from {x: 1}, {x: 2}, {x: 3}, {x: 4} summarize median_value=quantile(x, q=0.5) ``` ```tql {median_value: 2.5} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`median`](/reference/functions/median), [`mean`](/reference/functions/mean)

# random

Generates a random number in *\[0,1]*. ```tql random() -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `random` function generates a random number by drawing from a [uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) in the interval *\[0,1]*. ## Examples [Section titled â€œExamplesâ€](#examples) ### Generate a random number [Section titled â€œGenerate a random numberâ€](#generate-a-random-number) ```tql from {x: random()} ``` ```tql {x: 0.19634716885782455} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`uuid`](/reference/functions/uuid)

# remove

Removes all occurrences of an element from a list. ```tql remove(xs:list, x:any) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `remove` function returns the list `xs` with all occurrences of `x` removed. If `x` is not present in the list, the original list is returned unchanged. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) A list to remove elements from. ### `x: any` [Section titled â€œx: anyâ€](#x-any) The value to remove from the list. ## Examples [Section titled â€œExamplesâ€](#examples) ### Remove an element from a list [Section titled â€œRemove an element from a listâ€](#remove-an-element-from-a-list) ```tql from {xs: [1, 2, 3, 2, 4]} xs = xs.remove(2) ``` ```tql {xs: [1, 3, 4]} ``` ### Remove a non-existent element [Section titled â€œRemove a non-existent elementâ€](#remove-a-non-existent-element) ```tql from {xs: [1, 2, 3]} xs = xs.remove(5) ``` ```tql {xs: [1, 2, 3]} ``` ### Remove from a list with strings [Section titled â€œRemove from a list with stringsâ€](#remove-from-a-list-with-strings) ```tql from {xs: ["apple", "banana", "apple", "orange"]} xs = xs.remove("apple") ``` ```tql {xs: ["banana", "orange"]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`add`](/reference/functions/add), [`append`](/reference/functions/append), [`prepend`](/reference/functions/prepend) [`distinct`](/reference/functions/distinct)

# replace

Replaces characters within a string. ```tql replace(x:string, pattern:string, replacement:string, [max=int]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `replace` function returns a new string where occurrences of `pattern` in `x` are replaced with `replacement`, up to `max` times. If `max` is omitted, all occurrences are replaced. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The subject to replace the action on. ### `pattern: string` [Section titled â€œpattern: stringâ€](#pattern-string) The pattern to replace in `x`. ### `replacement: string` [Section titled â€œreplacement: stringâ€](#replacement-string) The replacement value for `pattern`. ### `max = int (optional)` [Section titled â€œmax = int (optional)â€](#max--int-optional) The maximum number of replacements to perform. If the option is not set, all occurrences are replaced. ## Examples [Section titled â€œExamplesâ€](#examples) ### Replace all occurrences of a character [Section titled â€œReplace all occurrences of a characterâ€](#replace-all-occurrences-of-a-character) ```tql from {x: "hello".replace("l", "r")} ``` ```tql {x: "herro"} ``` ### Replace a limited number of occurrences [Section titled â€œReplace a limited number of occurrencesâ€](#replace-a-limited-number-of-occurrences) ```tql from {x: "hello".replace("l", "r", max=1)} ``` ```tql {x: "herlo"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`replace_regex`](/reference/functions/replace_regex), [`replace`](/reference/operators/replace)

# replace_regex

Replaces characters within a string based on a regular expression. ```tql replace_regex(x:string, pattern:string, replacement:string, [max=int]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `replace_regex` function returns a new string where substrings in `x` that match `pattern` are replaced with `replacement`, up to `max` times. If `max` is omitted, all matches are replaced. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The subject to replace the action on. ### `pattern: string` [Section titled â€œpattern: stringâ€](#pattern-string) The pattern (as regular expression) to replace in `x`. ### `replacement: string` [Section titled â€œreplacement: stringâ€](#replacement-string) The replacement value for `pattern`. ### `max = int (optional)` [Section titled â€œmax = int (optional)â€](#max--int-optional) The maximum number of replacements to perform. If the option is not set, all occurrences are replaced. ## Examples [Section titled â€œExamplesâ€](#examples) ### Replace all matches of a regular expression [Section titled â€œReplace all matches of a regular expressionâ€](#replace-all-matches-of-a-regular-expression) ```tql from {x: replace_regex("hello", "l+", "y")} ``` ```tql {x: "heyo"} ``` ### Replace a limited number of matches [Section titled â€œReplace a limited number of matchesâ€](#replace-a-limited-number-of-matches) ```tql from {x: replace_regex("hellolo", "l+", "y", max=1)} ``` ```tql {x: "heyolo"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`replace`](/reference/functions/replace)

# reverse

Reverses the characters of a string. ```tql reverse(x:string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `reverse` function returns a new string with the characters of `x` in reverse order. This function operates on Unicode codepoints, not grapheme clusters. Hence, it will not correctly reverse grapheme clusters composed of multiple codepoints. ## Examples [Section titled â€œExamplesâ€](#examples) ### Reverse a string [Section titled â€œReverse a stringâ€](#reverse-a-string) ```tql from {x: reverse("hello")} ``` ```tql {x: "olleh"} ```

# round

Rounds a number or a time/duration with a specified unit. ```tql round(x:number) round(x:time, unit:duration) round(x:duration, unit:duration) ``` ## Description [Section titled â€œDescriptionâ€](#description) The `round` function rounds a number `x` to an integer. For time and duration values, use the second `unit` argument to define the rounding unit. ## Examples [Section titled â€œExamplesâ€](#examples) ### Round integers [Section titled â€œRound integersâ€](#round-integers) ```tql from { x: round(3.4), y: round(3.5), z: round(-3.4), } ``` ```tql { x: 3, y: 4, z: -3, } ``` ### Round time and duration values [Section titled â€œRound time and duration valuesâ€](#round-time-and-duration-values) ```tql from { x: round(2024-08-23, 1y), y: round(42m, 1h) } ``` ```tql { x: 2025-01-01, y: 1h, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ceil`](/reference/functions/ceil), [`floor`](/reference/functions/floor)

# second

Extracts the second component from a timestamp with subsecond precision. ```tql second(x: time) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `second` function extracts the second component from a timestamp as a floating-point number (0-59.999â€¦) that includes subsecond precision. ### `x: time` [Section titled â€œx: timeâ€](#x-time) The timestamp from which to extract the second. ## Examples [Section titled â€œExamplesâ€](#examples) ### Extract the second from a timestamp [Section titled â€œExtract the second from a timestampâ€](#extract-the-second-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } second = ts.second() ``` ```tql { ts: 2024-06-15T14:30:45.123456, second: 45.123456, } ``` ### Extract only the full second component without subsecond precision [Section titled â€œExtract only the full second component without subsecond precisionâ€](#extract-only-the-full-second-component-without-subsecond-precision) ```tql from { ts: 2024-06-15T14:30:45.123456, } full_second = ts.second().floor() ``` ```tql { ts: 2024-06-15T14:30:45.123456, full_second: 45, } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute)

# seconds

Converts a number to equivalent seconds. ```tql seconds(x:number) -> duration ``` ## Description This function returns seconds equivalent to a number, i.e., `number * 1s`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# secret

Use the value of a secret. ```tql secret(name:string) -> secret ``` ## Description [Section titled â€œDescriptionâ€](#description) An operator accepting a secret will first try and lookup the value in the environment or configuration of the Tenzir Node. A `tenzir` client process can use secrets only if it has a Tenzir Node to connect to. If the secret is not found in the node, a request is made to the Tenzir Platform. Should the platform also not be able to find the secret, an error is raised. See the [explanation page for secrets](/explanations/secrets) for more details. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the secret to use. This must be a constant. ## Legacy Model [Section titled â€œLegacy Modelâ€](#legacy-model) The configuration option `tenzir.legacy-secret-model` changes the behavior of the `secret` function to return a `string` instead of a `secret`. The legacy model only allows using secrets from the Tenzir Nodeâ€™s configuration. No secrets from the Tenzir Platformâ€™s secret store will be available. We do not recommend enabling this option. ## Examples [Section titled â€œExamplesâ€](#examples) ### Using secrets in an operator [Section titled â€œUsing secrets in an operatorâ€](#using-secrets-in-an-operator) ```tql load_tcp "127.0.0.1:4000" { read_ndjson } to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token") ``` ### Secrets are not rendered in output [Section titled â€œSecrets are not rendered in outputâ€](#secrets-are-not-rendered-in-output) ```tql from {x: secret("geheim")} ``` ```tql {x: "***" } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`config`](/reference/functions/config), [`env`](/reference/functions/env)

# shift_left

Performs a bit-wise left shift. ```tql shift_left(lhs:number, rhs:number) -> number ``` ## Description [Section titled â€œDescriptionâ€](#description) The `shift_left` function performs a bit-wise left shift of `lhs` by `rhs` bit positions. Each left shift multiplies the number by 2. ### `lhs: number` [Section titled â€œlhs: numberâ€](#lhs-number) The number to be shifted. ### `rhs: number` [Section titled â€œrhs: numberâ€](#rhs-number) The number of bit positions to shift to the left. ## Examples [Section titled â€œExamplesâ€](#examples) ### Shift bits to the left [Section titled â€œShift bits to the leftâ€](#shift-bits-to-the-left) ```tql from {x: shift_left(5, 2)} ``` ```tql {x: 20} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`shift_right`](/reference/functions/shift_right), [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not)

# shift_right

Performs a bit-wise right shift. ```tql shift_right(lhs:number, rhs:number) -> number ``` ## Description [Section titled â€œDescriptionâ€](#description) The `shift_right` function performs a bit-wise right shift of `lhs` by `rhs` bit positions. Each right shift divides the number by 2, truncating any fractional part. ### `lhs: number` [Section titled â€œlhs: numberâ€](#lhs-number) The number to be shifted. ### `rhs: number` [Section titled â€œrhs: numberâ€](#rhs-number) The number of bit positions to shift to the right. ## Examples [Section titled â€œExamplesâ€](#examples) ### Shift bits to the right [Section titled â€œShift bits to the rightâ€](#shift-bits-to-the-right) ```tql from {x: shift_right(20, 2)} ``` ```tql {x: 5} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`shift_left`](/reference/functions/shift_left), [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not)

# since_epoch

Interprets a time value as duration since the Unix epoch. ```tql since_epoch(x:time) -> duration ``` ## Description [Section titled â€œDescriptionâ€](#description) The `since_epoch` function turns a time value into a duration since the [Unix epoch](https://en.wikipedia.org/wiki/Unix_time), i.e., since 00:00:00 UTC on January 1970. ## Examples [Section titled â€œExamplesâ€](#examples) ### Retrive the Unix time for a given date [Section titled â€œRetrive the Unix time for a given dateâ€](#retrive-the-unix-time-for-a-given-date) ```tql from { x: since_epoch(2021-02-24) } ``` ```tql {x: 18682.0d} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`from_epoch`](/reference/functions/from_epoch), [`now`](/reference/functions/now)

# slice

Slices a string with offsets and strides. ```tql slice(x:string, [begin=int, end=int, stride=int]) ``` ## Description [Section titled â€œDescriptionâ€](#description) The `slice` function takes a string as input and selects parts from it. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string to slice. ### `begin = int (optional)` [Section titled â€œbegin = int (optional)â€](#begin--int-optional) The offset to start slice from. If negative, offset is calculated from the end of string. Defaults to `0`. ### `end = int (optional)` [Section titled â€œend = int (optional)â€](#end--int-optional) The offset to end the slice at. If negative, offset is calculated from the end of string. If unspecified, ends at the `input`â€™s end. ### `stride = int (optional)` [Section titled â€œstride = int (optional)â€](#stride--int-optional) The difference between the current character to take and the next character to take. If negative, characters are chosen in reverse. Defaults to `1`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the first 3 characters of a string [Section titled â€œGet the first 3 characters of a stringâ€](#get-the-first-3-characters-of-a-string) ```tql from {x: "123456789"} x = x.slice(end=3) ``` ```tql {x: "123"} ``` ### Get the 1st, 3rd, and 5th characters [Section titled â€œGet the 1st, 3rd, and 5th charactersâ€](#get-the-1st-3rd-and-5th-characters) ```tql from {x: "1234567890"} x = x.slice(stride=2, end=6) ``` ```tql {x: "135"} ``` ### Select a substring from the 2nd character up to the 8th character [Section titled â€œSelect a substring from the 2nd character up to the 8th characterâ€](#select-a-substring-from-the-2nd-character-up-to-the-8th-character) ```tql from {x: "1234567890"} x = x.slice(begin=1, end=8) ``` ```tql {x: "2345678"} ```

# sort

Sorts lists and record fields. ```tql sort(xs:list|record) -> list|record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `sort` function takes either a list or record as input, ordering lists by value and records by their field name. ### `xs: list|record` [Section titled â€œxs: list|recordâ€](#xs-listrecord) The list or record to sort. ## Examples [Section titled â€œExamplesâ€](#examples) ### Sort values in a list [Section titled â€œSort values in a listâ€](#sort-values-in-a-list) ```tql from {xs: [1, 3, 2]} xs = xs.sort() ``` ```tql {xs: [1, 2, 3]} ``` ### Sort a record by its field names [Section titled â€œSort a record by its field namesâ€](#sort-a-record-by-its-field-names) ```tql from {a: 1, c: 3, b: {y: true, x: false}} this = this.sort() ``` ```tql {a: 1, b: {y: true, x: false}, c: 3} ``` Note that nested records are not automatically sorted. Use `b = b.sort()` to sort it manually.

# split

Splits a string into substrings. ```tql split(x:string, pattern:string, [max:int], [reverse:bool]) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `split` function splits the input string `x` into a list of substrings using the specified `pattern`. Optional arguments allow limiting the number of splits (`max`) and reversing the splitting direction (`reverse`). ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string to split. ### `pattern: string` [Section titled â€œpattern: stringâ€](#pattern-string) The delimiter or pattern used for splitting. ### `max: int (optional)` [Section titled â€œmax: int (optional)â€](#max-int-optional) The maximum number of splits to perform. Defaults to `0`, meaning no limit. ### `reverse: bool (optional)` [Section titled â€œreverse: bool (optional)â€](#reverse-bool-optional) If `true`, splits from the end of the string. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Split a string by a delimiter [Section titled â€œSplit a string by a delimiterâ€](#split-a-string-by-a-delimiter) ```tql from {xs: split("a,b,c", ",")} ``` ```tql {xs: ["a", "b", "c"]} ``` ### Limit the number of splits [Section titled â€œLimit the number of splitsâ€](#limit-the-number-of-splits) ```tql from {xs: split("a-b-c", "-", max=1)} ``` ```tql {xs: ["a", "b-c"]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`split_regex`](/reference/functions/split_regex), [`join`](/reference/functions/join)

# split_regex

Splits a string into substrings with a regex. ```tql split_regex(x:string, pattern:string, [max:int], [reverse:bool]) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `split_regex` function splits the input string `x` into a list of substrings using the specified regular expression `pattern`. Optional arguments allow limiting the number of splits (`max`) and reversing the splitting direction (`reverse`). ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string to split. ### `pattern: string` [Section titled â€œpattern: stringâ€](#pattern-string) The regular expression used for splitting. ### `max: int (optional)` [Section titled â€œmax: int (optional)â€](#max-int-optional) The maximum number of splits to perform. Defaults to `0`, meaning no limit. ### `reverse: bool (optional)` [Section titled â€œreverse: bool (optional)â€](#reverse-bool-optional) If `true`, splits from the end of the string. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Split a string using a regex pattern [Section titled â€œSplit a string using a regex patternâ€](#split-a-string-using-a-regex-pattern) ```tql from {xs: split_regex("a1b2c", r"\d")} ``` ```tql {xs: ["a", "b", "c", ""]} ``` ### Limit the number of splits [Section titled â€œLimit the number of splitsâ€](#limit-the-number-of-splits) ```tql from {xs: split_regex("a1b2c3", r"\d", max=1)} ``` ```tql {xs: ["a", "b2c3"]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`split`](/reference/functions/split), [`join`](/reference/functions/join)

# sqrt

Computes the square root of a number. ```tql sqrt(x:number) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `sqrt` function computes the [square root](https://en.wikipedia.org/wiki/Square_root) of any non-negative number `x`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the square root of an integer [Section titled â€œCompute the square root of an integerâ€](#compute-the-square-root-of-an-integer) ```tql from {x: sqrt(49)} ``` ```tql {x: 7.0} ``` ### Fail to compute the square root of a negative number [Section titled â€œFail to compute the square root of a negative numberâ€](#fail-to-compute-the-square-root-of-a-negative-number) ```tql from {x: sqrt(-1)} ``` ```tql {x: null} ```

# starts_with

Checks if a string starts with a specified substring. ```tql starts_with(x:string, prefix:string) -> bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `starts_with` function returns `true` if `x` starts with `prefix` and `false` otherwise. ## Examples [Section titled â€œExamplesâ€](#examples) ### Check if a string starts with a substring [Section titled â€œCheck if a string starts with a substringâ€](#check-if-a-string-starts-with-a-substring) ```tql from {x: "hello".starts_with("he")} ``` ```tql {x: true} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ends_with`](/reference/functions/ends_with)

# stddev

Computes the standard deviation of all grouped values. ```tql stddev(xs:list) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `stddev` function returns the standard deviation of all numeric values in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the standard deviation of values [Section titled â€œCompute the standard deviation of valuesâ€](#compute-the-standard-deviation-of-values) ```tql from {x: 1}, {x: 2}, {x: 3} summarize stddev_value=stddev(x) ``` ```tql {stddev_value: 0.816} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`variance`](/reference/functions/variance), [`mean`](/reference/functions/mean)

# string

Casts an expression to a string. ```tql string(x:any) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `string` function casts the given value `x` to a string. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast an IP address to a string [Section titled â€œCast an IP address to a stringâ€](#cast-an-ip-address-to-a-string) ```tql from {x: string(1.2.3.4)} ``` ```tql {x: "1.2.3.4"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [int](/reference/functions/int)

# subnet

Casts an expression to a subnet value. ```tql subnet(x:string) -> subnet ``` ## Description [Section titled â€œDescriptionâ€](#description) The `subnet` function casts an expression to a subnet. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string expression to cast. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast a string to a subnet [Section titled â€œCast a string to a subnetâ€](#cast-a-string-to-a-subnet) ```tql from {x: subnet("1.2.3.4/16")} ``` ```tql {x: 1.2.0.0/16} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`int`](/reference/functions/int), [`ip`](/reference/functions/ip), [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [string](/reference/functions/string)

# sum

Computes the sum of all values. ```tql sum(xs:list) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `sum` function computes the total of all number values. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to aggregate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute a sum over a group of events [Section titled â€œCompute a sum over a group of eventsâ€](#compute-a-sum-over-a-group-of-events) ```tql from {x: 1}, {x: 2}, {x: 3} summarize n=sum(x) ``` ```tql {n: 6} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`collect`](/reference/functions/collect), [`max`](/reference/functions/max), [`mean`](/reference/functions/mean), [`min`](/reference/functions/min)

# time

Casts an expression to a time value. ```tql time(x:any) -> time ``` ## Description [Section titled â€œDescriptionâ€](#description) The `time` function casts the given string or number `x` to a time value. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast a string to a time value [Section titled â€œCast a string to a time valueâ€](#cast-a-string-to-a-time-value) ```tql from {x: time("2020-03-15")} ``` ```tql {x: 2020-03-15T00:00:00.000000} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ip`](/reference/functions/ip), [`string`](/reference/functions/string), [`subnet`](/reference/functions/subnet), [`uint`](/reference/functions/uint), [duration](/reference/functions/duration), [float](/reference/functions/float), [int](/reference/functions/int)

# to_lower

Converts a string to lowercase. ```tql to_lower(x:string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_lower` function converts all characters in `x` to lowercase. ## Examples [Section titled â€œExamplesâ€](#examples) ### Convert a string to lowercase [Section titled â€œConvert a string to lowercaseâ€](#convert-a-string-to-lowercase) ```tql from {x: "HELLO".to_lower()} ``` ```tql {x: "hello"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`capitalize`](/reference/functions/capitalize), [`is_lower`](/reference/functions/is_lower), [`to_title`](/reference/functions/to_title), [`to_upper`](/reference/functions/to_upper)

# to_title

Converts a string to title case. ```tql to_title(x:string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_title` function converts all words in `x` to title case. ## Examples [Section titled â€œExamplesâ€](#examples) ### Convert a string to title case [Section titled â€œConvert a string to title caseâ€](#convert-a-string-to-title-case) ```tql from {x: "hello world".to_title()} ``` ```tql {x: "Hello World"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`capitalize`](/reference/functions/capitalize), [`is_title`](/reference/functions/is_title), [`to_lower`](/reference/functions/to_lower), [`to_upper`](/reference/functions/to_upper)

# to_upper

Converts a string to uppercase. ```tql to_upper(x:string) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_upper` function converts all characters in `x` to uppercase. ## Examples [Section titled â€œExamplesâ€](#examples) ### Convert a string to uppercase [Section titled â€œConvert a string to uppercaseâ€](#convert-a-string-to-uppercase) ```tql from {x: "hello".to_upper()} ``` ```tql {x: "HELLO"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`capitalize`](/reference/functions/capitalize), [`is_upper`](/reference/functions/is_upper), [`to_lower`](/reference/functions/to_lower), [`to_title`](/reference/functions/to_title)

# trim

Trims whitespace or specified characters from both ends of a string. ```tql trim(x:string, [chars:string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `trim` function removes characters from both ends of `x`. When called with one argument, it removes leading and trailing whitespace. When called with two arguments, it removes any characters found in `chars` from both ends of the string. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string to trim. ### `chars: string (optional)` [Section titled â€œchars: string (optional)â€](#chars-string-optional) A string where each character represents a character to remove. Any character found in this string will be trimmed from both ends. Defaults to whitespace characters. ## Examples [Section titled â€œExamplesâ€](#examples) ### Trim whitespace from both ends [Section titled â€œTrim whitespace from both endsâ€](#trim-whitespace-from-both-ends) ```tql from {x: " hello ".trim()} ``` ```tql {x: "hello"} ``` ### Trim specific characters [Section titled â€œTrim specific charactersâ€](#trim-specific-characters) ```tql from {x: "/path/to/file/".trim("/")} ``` ```tql {x: "path/to/file"} ``` ### Trim multiple characters [Section titled â€œTrim multiple charactersâ€](#trim-multiple-characters) ```tql from {x: "--hello--world--".trim("-")} ``` ```tql {x: "hello--world"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`trim_start`](/reference/functions/trim_start), [`trim_end`](/reference/functions/trim_end)

# trim_end

Trims whitespace or specified characters from the end of a string. ```tql trim_end(x:string, [chars:string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `trim_end` function removes characters from the end of `x`. When called with one argument, it removes trailing whitespace. When called with two arguments, it removes any characters found in `chars` from the end of the string. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string to trim. ### `chars: string (optional)` [Section titled â€œchars: string (optional)â€](#chars-string-optional) A string where each character represents a character to remove. Any character found in this string will be trimmed from the end. Defaults to whitespace characters. ## Examples [Section titled â€œExamplesâ€](#examples) ### Trim whitespace from the end [Section titled â€œTrim whitespace from the endâ€](#trim-whitespace-from-the-end) ```tql from {x: "hello ".trim_end()} ``` ```tql {x: "hello"} ``` ### Trim specific characters [Section titled â€œTrim specific charactersâ€](#trim-specific-characters) ```tql from {x: "/path/to/file/".trim_end("/")} ``` ```tql {x: "/path/to/file"} ``` ### Trim multiple characters [Section titled â€œTrim multiple charactersâ€](#trim-multiple-characters) ```tql from {x: "hello/-/".trim_end("/-")} ``` ```tql {x: "hello"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`trim`](/reference/functions/trim), [`trim_start`](/reference/functions/trim_start)

# trim_start

Trims whitespace or specified characters from the start of a string. ```tql trim_start(x:string, [chars:string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `trim_start` function removes characters from the beginning of `x`. When called with one argument, it removes leading whitespace. When called with two arguments, it removes any characters found in `chars` from the start of the string. ### `x: string` [Section titled â€œx: stringâ€](#x-string) The string to trim. ### `chars: string (optional)` [Section titled â€œchars: string (optional)â€](#chars-string-optional) A string where each character represents a character to remove. Any character found in this string will be trimmed from the start. Defaults to whitespace characters. ## Examples [Section titled â€œExamplesâ€](#examples) ### Trim whitespace from the start [Section titled â€œTrim whitespace from the startâ€](#trim-whitespace-from-the-start) ```tql from {x: " hello".trim_start()} ``` ```tql {x: "hello"} ``` ### Trim specific characters [Section titled â€œTrim specific charactersâ€](#trim-specific-characters) ```tql from {x: "/path/to/file".trim_start("/")} ``` ```tql {x: "path/to/file"} ``` ### Trim multiple characters [Section titled â€œTrim multiple charactersâ€](#trim-multiple-characters) ```tql from {x: "/-/hello".trim_start("/-")} ``` ```tql {x: "hello"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`trim`](/reference/functions/trim), [`trim_end`](/reference/functions/trim_end)

# type_id

Retrieves the type id of an expression. ```tql type_id(x:any) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `type_id` function returns the type id of the given value `x`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Retrieve the type of a numeric expression [Section titled â€œRetrieve the type of a numeric expressionâ€](#retrieve-the-type-of-a-numeric-expression) ```tql from {x: type_id(1 + 3.2)} ``` ```tql {x: "41615fdb30a38aaf"} ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`type_of`](/reference/functions/type_of)

# type_of

Retrieves the type definition of an expression. ```tql type_of(x:any) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `type_of` function returns the type definition of the given value `x`. Subject to change This function is designed for internal use of the Tenzir Platform and its output format is subject to change without notice. ## Examples [Section titled â€œExamplesâ€](#examples) ### Retrieve the type definition of a schema [Section titled â€œRetrieve the type definition of a schemaâ€](#retrieve-the-type-definition-of-a-schema) ```tql from {x: 1, y: "2"} this = type_of(this) ``` ```tql { name: "tenzir.from", kind: "record", attributes: [], state: { fields: [ { name: "x", type: { name: null, kind: "int64", attributes: [], state: null, }, }, { name: "y", type: { name: null, kind: "string", attributes: [], state: null, }, }, ], }, } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`type_id`](/reference/functions/type_id)

# uint

Casts an expression to an unsigned integer. ```tql uint(x:number|string, base=int) -> uint ``` ## Description [Section titled â€œDescriptionâ€](#description) The `uint` function casts the provided value `x` to an unsigned integer. Non-integer values are truncated. ### `x: number|string` [Section titled â€œx: number|stringâ€](#x-numberstring) The input to convert. ### `base = int` [Section titled â€œbase = intâ€](#base--int) Base (radix) to parse a string as. Can be `10` or `16`. If `16`, the string inputs may be optionally prefixed by `0x` or `0X`, e.g., `0x134`. Defaults to `10`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast a floating-point number to an unsigned integer [Section titled â€œCast a floating-point number to an unsigned integerâ€](#cast-a-floating-point-number-to-an-unsigned-integer) ```tql from {x: uint(4.2)} ``` ```tql {x: 4} ``` ### Parse a hexadecimal number [Section titled â€œParse a hexadecimal numberâ€](#parse-a-hexadecimal-number) ```tql from {x: uint("0x42", base=16)} ``` ```tql {x: 66} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [float](/reference/functions/float), [int](/reference/functions/int), [string](/reference/functions/string)

# unflatten

Unflattens nested data. ```tql unflatten(x:record, [separator=string]) -> record ``` ## Description [Section titled â€œDescriptionâ€](#description) The `unflatten` function creates nested records out of fields whose names include a separator. ### `x: record` [Section titled â€œx: recordâ€](#x-record) The record you want to unflatten. ### `separator: string (optional)` [Section titled â€œseparator: string (optional)â€](#separator-string-optional) The separator to use for splitting field names. Defaults to `"."`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Unflatten fields at the dot character [Section titled â€œUnflatten fields at the dot characterâ€](#unflatten-fields-at-the-dot-character) ```tql // Note the fields in double quotes that are single fields that contain a // literal "." in their field name, as opposed to nested records. from { src_ip: 147.32.84.165, src_port: 1141, dest_ip: 147.32.80.9, dest_port: 53, event_type: "dns", "dns.type": "query", "dns.id": 553, "dns.rrname": "irc.freenode.net", "dns.rrtype": "A", "dns.tx_id": 0, "dns.grouped.A": ["tenzir.com"], } this = unflatten(this) ``` ```tql { src_ip: 147.32.84.165, src_port: 1141, dest_ip: 147.32.80.9, dest_port: 53, event_type: "dns", dns: { type: "query", id: 553, rrname: "irc.freenode.net", rrtype: "A", tx_id: 0, grouped: { A: [ "tenzir.com", ], }, }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`flatten`](/reference/functions/flatten)

# uuid

Generates a Universally Unique Identifier (UUID) string. ```tql uuid([version=string]) -> string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `uuid` function generates a [Universally Unique Identifier (UUID)](https://en.wikipedia.org/wiki/Universally_unique_identifier) string. UUIDs, 128-bit numbers, uniquely identify information in computer systems. This function generates several UUID versions based on relevant standards like [RFC 4122](https://www.rfc-editor.org/rfc/rfc4122.html) and the newer [RFC 9562](https://www.rfc-editor.org/rfc/rfc9562.html) which defines versions 6 and 7. ### `version = string (optional)` [Section titled â€œversion = string (optional)â€](#version--string-optional) Specifies the version of the UUID to generate. If you omit this argument, the function uses `"v4"` by default. It supports the following values: * `"v1"`: Generates a time-based UUID using a timestamp and node ID. * `"v4"`: Generates a randomly generated UUID using a cryptographically strong random number generator (see RFC 4122). **This is the default.** * `"v6"`: Generates a time-based UUID, similar to v1 but reordered for better database index locality and lexical sorting (see RFC 9562). * `"v7"`: Generates a time-based UUID using a Unix timestamp and random bits, designed to be monotonically increasing (suitable for primary keys, see RFC 9562). * `"nil"`: Generates the special â€œnilâ€ UUID, which consists entirely of zeros: `00000000-0000-0000-0000-000000000000`. Defaults to `"v4"`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Generate a random default (v4) UUID [Section titled â€œGenerate a random default (v4) UUIDâ€](#generate-a-random-default-v4-uuid) ```tql from {guid: uuid()} ``` ```tql {guid: "f47ac10b-58cc-4372-a567-0e02b2c3d479"} ``` ### Generate a random version 7 UUID [Section titled â€œGenerate a random version 7 UUIDâ€](#generate-a-random-version-7-uuid) ```tql from {guid: uuid(version="v7")} ``` ```tql {guid: "018ecb4f-abc1-7123-8def-0123456789ab"} ``` ### Generate the nil UUID [Section titled â€œGenerate the nil UUIDâ€](#generate-the-nil-uuid) ```tql from {guid: uuid(version="nil")} ``` ```tql {guid: "00000000-0000-0000-0000-000000000000"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`random`](/reference/functions/random)

# value_counts

Returns a list of all grouped values alongside their frequency. ```tql value_counts(xs:list) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `value_counts` function returns a list of all unique non-null values in `xs` alongside their occurrence count. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get value counts [Section titled â€œGet value countsâ€](#get-value-counts) ```tql from {x: 1}, {x: 2}, {x: 2}, {x: 3} summarize counts=value_counts(x) ``` ```tql {counts: [{value: 1, count: 1}, {value: 2, count: 2}, {value: 3, count: 1}]} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`mode`](/reference/functions/mode), [`distinct`](/reference/functions/distinct)

# variance

Computes the variance of all grouped values. ```tql variance(xs:list) -> float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `variance` function returns the variance of all numeric values in `xs`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) The values to evaluate. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the variance of values [Section titled â€œCompute the variance of valuesâ€](#compute-the-variance-of-values) ```tql from {x: 1}, {x: 2}, {x: 3} summarize variance_value=variance(x) ``` ```tql {variance_value: 0.666} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`stddev`](/reference/functions/stddev), [`mean`](/reference/functions/mean)

# weeks

Converts a number to equivalent weeks. ```tql weeks(x:number) -> duration ``` ## Description This function returns weeks equivalent to a number, i.e., `number * 1w`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# where

Filters list elements based on a predicate. ```tql where(xs:list, any->bool) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `where` function keeps only elements of a list for which a predicate evaluates to `true`. ### `xs: list` [Section titled â€œxs: listâ€](#xs-list) A list of values. ### `function: any -> bool` [Section titled â€œfunction: any -> boolâ€](#function-any---bool) A lambda function that is evaluated for each list element. ## Examples [Section titled â€œExamplesâ€](#examples) ### Keep only elements greater than 3 [Section titled â€œKeep only elements greater than 3â€](#keep-only-elements-greater-than-3) ```tql from { xs: [1, 2, 3, 4, 5] } xs = xs.where(x => x > 3) ``` ```tql { xs: [4, 5] } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`map`](/reference/functions/map)

# year

Extracts the year component from a timestamp. ```tql year(x: time) -> int ``` ## Description [Section titled â€œDescriptionâ€](#description) The `year` function extracts the year component from a timestamp as an integer. ### `x: time` [Section titled â€œx: timeâ€](#x-time) The timestamp from which to extract the year. ## Examples [Section titled â€œExamplesâ€](#examples) ### Extract the year from a timestamp [Section titled â€œExtract the year from a timestampâ€](#extract-the-year-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } year = ts.year() ``` ```tql { ts: 2024-06-15T14:30:45.123456, year: 2024, } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# years

Converts a number to equivalent years. ```tql years(x:number) -> duration ``` ## Description This function returns years equivalent to a number, i.e., `number * 1y`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# zip

Combines two lists into a list of pairs. ```tql zip(xs:list, ys:list) -> list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `zip` function returns a list containing records with two fields `left` and `right`, each containing the respective elements of the input lists. If both lists are null, `zip` returns null. If one of the lists is null or has a mismatching length, missing values are filled in with nulls, using the longer listâ€™s length, and a warning is emitted. ## Examples [Section titled â€œExamplesâ€](#examples) ### Combine two lists [Section titled â€œCombine two listsâ€](#combine-two-lists) ```tql from {xs: [1, 2], ys: [3, 4]} select zs = zip(xs, ys) ``` ```tql { zs: [ {left: 1, right: 3}, {left: 2, right: 4} ] } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`concatenate`](/reference/functions/concatenate), [`map`](/reference/functions/map)

# MCP Server

The [Tenzir MCP Server](https://github.com/tenzir/mcp) enables AI assistants to interact with Tenzir through the [Model Context Protocol](https://modelcontextprotocol.io) (MCP). ## Tools [Section titled â€œToolsâ€](#tools) The MCP server provides several *tools* that your AI agent can call. Work in progress The MCP server is changing rapidly at the moment. Stay tuned for more content here soon!

# Node Configuration

The below example configuration ships with every Tenzir package. Head over to the [explanation of the configuration](/explanations/configuration) for details on how the various settings work. tenzir.yaml ```yaml # This is an example configuration file for Tenzir that shows all available # options. Options in angle brackets have their default value determined at # runtime. # Options that concern Tenzir. tenzir: # The token that is offered when connecting to the Tenzir Platform. # It is used to identify the node and assign it to the correct workspace. # This setting is ignored in the open-source edition of Tenzir, which does # not contain the platform plugin. token: tnz_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX # The host and port to listen at for node-to-node connections in the form # `<host>:<port>`. Host or port may be emitted to use their defaults, which # are localhost and 5158, respectively. Set the port to zero to automatically # choose a port. Set to false to disable exposing an endpoint. endpoint: localhost:5158 # The timeout for connecting to a Tenzir server. Set to 0 seconds to wait # indefinitely. connection-timeout: 5m # The delay between two connection attempts. Set to 0s to try connecting # without retries. connection-retry-delay: 3s # Configure retention policies. retention: # How long to keep metrics for. Set to 0s to disable metrics retention # entirely. # WARNING: A low retention period may negatively impact the usability of # pipeline activity in the Tenzir Platform. #metrics: 7d # How long to keep diagnostics for. Set to 0s to disable diagnostics # retention entirely. # WARNING: A low retention period may negatively impact the usability of # diagnostics in the Tenzir Platform. #diagnostics: 30d # Configure the behavior of the `cache` operator. The Tenzir Platform uses the # cache operator to store and retrieve data efficiently. cache: # Specifies the default lifetime for the `cache` operator. #lifetime: 10min # Specifies an upper bound for the total memory usage in bytes across all # caches in a node. If the memory usage exceeds this limit, the node will # start evicting caches to make room for new data. The node requires a # minimum total cache capacity of 64MiB. #capacity: 1Gi # A certificate file used as the default for operators accepting a `cacert` # option. This will default to an appropriate directory for the system. For # example: # - /etc/ssl/certs/ca-bundle.crt on RedHat # - /etc/ssl/certs/ca-certificates.crt on Ubuntu #cacert: # The file system path used for persistent state. # Defaults to one of the following paths, selecting the first that is # available: # - $STATE_DIRECTORY # - $PWD/tenzir.db #state-directory: # The file system path used for recoverable state. # In a node process, defaults to the first of the following paths that is # available: # - $CACHE_DIRECTORY # - $XDG_CACHE_HOME # - $XDG_HOME_DIR/.cache/tenzir (linux) or $XDG_HOME_DIR/Libraries/caches/tenzir (mac) # - $HOME/.cache/tenzir (linux) or $HOME/Libraries/caches/tenzir (mac) # - $TEMPORARY_DIRECTORY/tenzir-cache-<uid> # To determine $TEMPORARY_DIRECTORY, the values of TMPDIR, TMP, TEMP, TEMPDIR are # checked in that order, and as a last resort "/tmp" is used. # In a client process, this setting is ignored and # `$TEMPORARY_DIRECTORY/tenzir-client-cache-<uid>` is used as cache directory. #cache-directory: # The file system path used for log files. # Defaults to one of the following paths, selecting the first that is # available: # - $LOGS_DIRECTORY/server.log # - <state-directory>/server.log #log-file: # The file system path used for client log files relative to the current # working directory of the client. Note that this is disabled by default. # If not specified no log files are written for clients at all. client-log-file: "client.log" # Format for printing individual log entries to the log-file. # For a list of valid format specifiers, see spdlog format specification # at https://github.com/gabime/spdlog/wiki/3.-Custom-formatting. file-format: "[%Y-%m-%dT%T.%e%z] [%n] [%l] [%s:%#] %v" # Configures the minimum severity of messages written to the log file. # Possible values: quiet, error, warning, info, verbose, debug, trace. # File logging is only available for commands that start a node (e.g., # tenzir-node). The levels above 'verbose' are usually not available in # release builds. file-verbosity: debug # Whether to enable automatic log rotation. If set to false, a new log file # will be created when the size of the current log file exceeds 10 MiB. disable-log-rotation: false # The size limit when a log file should be rotated. log-rotation-threshold: 10MiB # Maximum number of log messages in the logger queue. log-queue-size: 1000000 # The sink type to use for console logging. Possible values: stderr, # syslog, journald. Note that 'journald' can only be selected on linux # systems, and only if Tenzir was built with journald support. # The journald sink is used as default if Tenzir is started as a systemd # service and the service is configured to use the journal for stderr, # otherwise the default is the unstructured stderr sink. #console-sink: stderr/journald # Mode for console log output generation. Automatic renders color only when # writing to a tty. # Possible values: always, automatic, never. (default automatic) console: automatic # Format for printing individual log entries to the console. For a list # of valid format specifiers, see spdlog format specification at # https://github.com/gabime/spdlog/wiki/3.-Custom-formatting. console-format: "%^[%T.%e] %v%$" # Configures the minimum severity of messages written to the console. # For a list of valid log levels, see file-verbosity. console-verbosity: info # List of directories to look for schema files in ascending order of # priority. schema-dirs: [] # Additional directories to load plugins specified using `tenzir.plugins` # from. plugin-dirs: [] # List of paths that contain statically configured packages. # This setting is ignored unless the package manager plugin is enabled. package-dirs: [] # The plugins to load at startup. For relative paths, Tenzir tries to find # the files in the specified `tenzir.plugin-dirs`. The special values # 'bundled' and 'all' enable autoloading of bundled and all plugins # respectively. Note: Add `example` or `/path/to/libtenzir-plugin-example.so` # to load the example plugin. plugins: [] # Names of plugins and builtins to explicitly forbid from being used in # Tenzir. For example, adding `shell` will prohibit use of the `shell` # operator builtin, and adding `kafka` will prohibit use of the `kafka` # connector plugin. disable-plugins: [] # Forbid unsafe location overrides for pipelines with the 'local' and 'remote' # keywords, e.g., remotely reading from a file. no-location-overrides: false # Prevent all pipelines from automatically starting when the node starts. no-autostart: false # Do not move pipeline operators to subprocesses. disable-pipeline-subprocesses: false # The size of an index shard, expressed in number of events. This should # be a power of 2. max-partition-size: 4Mi # Timeout after which the importer forwards events to subscribers like `export # live=true` or `metrics live=true`. Set to 0s for an unbuffered mode. A # higher value increases performance, and a lower value reduces latency. import-buffer-timeout: 1s # Timeout after which an active partition is forcibly flushed, regardless of # its size. active-partition-timeout: 30s # Maximum number of events across all active partitions. This indirectly # controls the maximum memory usage when importing events. max-buffered-events: 12Mi # Automatically rebuild undersized and outdated partitions in the background. # The given number controls how much resources to spend on it. Set to 0 to # disable. automatic-rebuild: 1 # Timeout after which an automatic rebuild is triggered. rebuild-interval: 30min # Zstd compression level applied to the Feather store backend. # zstd-compression-level: <default> # The URL of the control endpoint when connecting to a self-hosted # instance of the Tenzir Platform. platform-control-endpoint: wss://ws.tenzir.app/production # Whether to undermine the security of the TLS connection to the # Tenzir Platform by disabling certificate validation. # Setting this to `true` is strongly discouraged. platform-skip-peer-verification: false # The name to use when connecting to the platform as an ephemeral node. # This setting is ignored unless a workspace token is used to connect to # the platform. Workspace tokens are currently only available for the # Sovereign Edition of the Tenzir Platform. platform-ephemeral-node-name: Ephemeral Node # Control how operator's calculate demand from their upstream operator. Note # that this is an expert feature and should only be changed if you know what # you are doing. The configured values can also be changed per operator by # using the `_tune` operator. demand: # Issue demand only if room for at least this many elements is available. # Must be greater than zero. Values may either be set to a number, or to a # record containing `bytes` and `events` fields with numbers depending on # the operator's input type. min-elements: bytes: 128Ki events: 8Ki # Controls how many elements may be buffered until the operator stops # issuing demand. Must be greater or equal to min-elements. Values may # either be set to a number, or to a record containing `bytes` and `events` # fields with numbers depending on the operator's input type. max-elements: bytes: 4Mi events: 254Ki # Controls how many batches of elements may be buffered until the operator # stops issuing demand. Must be greater than zero. max-batches: 20 # Controls the minimum backoff duration after an operator is detected to be # idle. Must be at least 10ms. min-backoff: 10ms # Controls the maximum backoff duration after an operator is detected to be # idle. Must be at least 10ms. max-backoff: 1s # Controls the growth rate of the backoff duration for operators that # continue to be idle. Must be at least 1.0. Note that setting a growth rate # of 1.0 causes the `max-backoff` duration to be ignored, replacing the # exponential growth with a constant value. backoff-rate: 2.0 # Context configured as part of the configuration that are always available. contexts: # A unique name for the context that's used in the context, enrich, and # lookup operators to refer to the context. indicators: # The type of the context. type: bloom-filter # Arguments for creating the context, depending on the type. Refer to the # documentation of the individual context types to see the arguments they # require. Note that changes to these arguments to not apply to any # contexts that were previously created. arguments: capacity: 1B fp-probability: 0.001 # The `index` key is used to adjust the false-positive rate of # the first-level lookup data structures (called synopses) in the # catalog. The lower the false-positive rate the more space will be # required, so this setting can be used to manually tune the trade-off # of performance vs. space. index: # The default false-positive rate for type synopses. default-fp-rate: 0.01 # rules: # Every rule adjusts the behaviour of Tenzir for a set of targets. # Tenzir creates one synopsis per target. Targets can be either types # or field names. # # fp-rate - false positive rate. Has effect on string and address type # targets # # partition-index - Tenzir will not create dense index when set to false # - targets: [:ip] # fp-rate: 0.01 # The `tenzir-ctl start` command starts a new Tenzir server process. start: # Prints the endpoint for clients when the server is ready to accept # connections. This comes in handy when letting the OS choose an # available random port, i.e., when specifying 0 as port value. print-endpoint: false # Writes the endpoint for clients when the server is ready to accept # connections to the specified destination. This comes in handy when letting # the OS choose an available random port, i.e., when specifying 0 as port # value, and `print-endpoint` is not sufficient. #write-endpoint: /tmp/tenzir-node-endpoint # An ordered list of commands to run inside the node after starting. # As an example, to configure an auto-starting PCAP source that listens # on the interface 'en0' and lives inside the Tenzir node, add `spawn # source pcap -i en0`. # Note that commands are not executed sequentially but in parallel. commands: [] # Triggers removal of old data when the disk budget is exceeded. disk-budget-high: 0GiB # When the budget was exceeded, data is erased until the disk space is # below this value. disk-budget-low: 0GiB # Seconds between successive disk space checks. disk-budget-check-interval: 90 # When erasing, how many partitions to erase in one go before rechecking # the size of the database directory. disk-budget-step-size: 1 # Binary to use for checking the size of the database directory. If left # unset, Tenzir will recursively add up the size of all files in the # database directory to compute the size. Mainly useful for e.g. # compressed filesystem where raw file size is not the correct metric. # Must be the absolute path to an executable file, which will get passed # the database directory as its first and only argument. #disk-budget-check-binary: /opt/tenzir/libexec/tenzir-df-percent.sh # User-defined operators. operators: # The Zeek operator is an example that takes raw bytes in the form of a # PCAP and then parses Zeek's output via the `zeek-json` format to generate # a stream of events. zeek: | shell "zeek -r - LogAscii::output_to_stdout=T JSONStreaming::disable_default_logs=T JSONStreaming::enable_log_rotation=F json-streaming-logs" read_zeek_json # The Suricata operator is analogous to the above Zeek example, with the # difference that we are using Suricata. The commmand line configures # Suricata such that it reads PCAP on stdin and produces EVE JSON logs on # stdout, which we then parse with the `suricata` format. suricata: | shell "suricata -r /dev/stdin --set outputs.1.eve-log.filename=/dev/stdout --set logging.outputs.0.console.enabled=no" read_suricata # In addition to running pipelines interactively, you can also deploy # *Pipelines as Code*. This infrastrucutre-as-code-like method differs from # pipelines run on the command-line or through app.tenzir.com in two ways: # 1. Pipelines deployed as code always start alongside the Tenzir node. # 2. Deletion via the user interface is not allowed for pipelines configured # as code. pipelines: # A unique identifier for the pipeline that's used for metrics, diagnostics, # and API calls interacting with the pipeline. publish-suricata: # An optional user-facing name for the pipeline. Defaults to the id. name: Import Suricata from TCP # The definition of the pipeline. Configured pipelines that fail to start # cause the node to fail to start. definition: | load_tcp "0.0.0.0:34343" { read_suricata schema_only=true } | where event_type != "stats" | publish "suricata" # Pipelines that encounter an error stop running and show an error state. # This option causes pipelines to automatically restart when they # encounter an error instead. The first restart happens immediately, and # subsequent restarts after the configured delay, defaulting to 1 minute. # The following values are valid for this option: # - Omit the option, or set it to null or false to disable. # - Set the option to true to enable with the default delay of 1 minute. # - Set the option to a valid duration to enable with a custom delay. restart-on-error: 1 minute # Pipelines that are unstoppable will run automatically and indefinitely. # They are not able to pause or stop. # If they do complete, they will end up in a failed state. # If `restart-on-error` is enabled, they will restart after the specified # duration. unstoppable: false # Use the legacy secret model. Under this model, the `secret` function yields # plain `string`s and can only look up secrets from the `tenzir.secrets` # section in this config, but not from the Tenzir Platform's secret store. legacy-secret-model: false # Enables the `secret_assert` operator. This operator can be used for our # integration tests and may be useful to test local setups. # Since it theoretically allows for brute-forcing a secret's value, it is # disabled by default. enable-assert-secret-operator: false # Local secrets, defined as key - value pairs. The values must be strings secrets: # my-secret-name: my-secret-value # The below settings are internal to CAF, and aren't checked by Tenzir directly. # Please be careful when changing these options. Note that some CAF options may # be in conflict with Tenzir options, and are only listed here for completeness. caf: # Options affecting the internal scheduler. scheduler: # Accepted alternative: "sharing". policy: stealing # Configures whether the scheduler generates profiling output. enable-profiling: false # Output file for profiler data (only if profiling is enabled). #profiling-output-file: </dev/null> # Measurement resolution in milliseconds (only if profiling is enabled). profiling-resolution: 100ms # Forces a fixed number of threads if set. Defaults to the number of # available CPU cores if starting a Tenzir node, or *2* for client commands. #max-threads: <number of cores> # Maximum number of messages actors can consume in one run. max-throughput: 500 # When using "stealing" as scheduler policy. work-stealing: # Number of zero-sleep-interval polling attempts. aggressive-poll-attempts: 100 # Frequency of steal attempts during aggressive polling. aggressive-steal-interval: 10 # Number of moderately aggressive polling attempts. moderate-poll-attempts: 500 # Frequency of steal attempts during moderate polling. moderate-steal-interval: 5 # Sleep interval between poll attempts. moderate-sleep-duration: 50us # Frequency of steal attempts during relaxed polling. relaxed-steal-interval: 1 # Sleep interval between poll attempts. relaxed-sleep-duration: 10ms stream: # Maximum delay for partial batches. max-batch-delay: 15ms # Selects an implementation for credit computation. # Accepted alternative: "token-based". credit-policy: token-based # When using "size-based" as credit-policy. size-based-policy: # Desired batch size in bytes. bytes-per-batch: 32 # Maximum input buffer size in bytes. buffer-capacity: 256 # Frequency of collecting batch sizes. sampling-rate: 100 # Frequency of re-calibrations. calibration-interval: 1 # Factor for discounting older samples. smoothing-factor: 2.5 # When using "token-based" as credit-policy. token-based-policy: # Number of elements per batch. batch-size: 1 # Max. number of elements in the input buffer. buffer-size: 64 # Collecting metrics can be resource consuming. This section is used for # filtering what should and what should not be collected metrics-filters: # Rules for actor based metrics filtering. actors: # List of selected actors for run-time metrics. includes: [] # List of excluded actors from run-time metrics. excludes: [] # Configure using OpenSSL for node-to-node connections. # NOTE: Use the tenzir.endpoint variable to configure the endpoint. openssl: # Path to the PEM-formatted certificate file. certificate: # Path to the private key file for this node. key: # Passphrase to decrypt the private key. passphrase: # Path to an OpenSSL-style directory of trusted certificates. capath: # Path to a file of concatenated PEM-formatted certificates. cafile: # Colon-separated list of OpenSSL cipher strings to use. cipher-list: ```

# Operators

Tenzir comes with a wide range of built-in pipeline operators. ## Analyze [Section titled â€œAnalyzeâ€](#analyze) ### [rare](/reference/operators/rare) [â†’](/reference/operators/rare) Shows the least common values. ```tql rare auth.token ``` ### [reverse](/reference/operators/reverse) [â†’](/reference/operators/reverse) Reverses the event order. ```tql reverse ``` ### [sort](/reference/operators/sort) [â†’](/reference/operators/sort) Sorts events by the given expressions. ```tql sort name, -abs(transaction) ``` ### [summarize](/reference/operators/summarize) [â†’](/reference/operators/summarize) Groups events and applies aggregate functions to each group. ```tql summarize name, sum(amount) ``` ### [top](/reference/operators/top) [â†’](/reference/operators/top) Shows the most common values. ```tql top user ``` ## Charts [Section titled â€œChartsâ€](#charts) ### [chart\_area](/reference/operators/chart_area) [â†’](/reference/operators/chart_area) Plots events on an area chart. ```tql chart_area â€¦ ``` ### [chart\_bar](/reference/operators/chart_bar) [â†’](/reference/operators/chart_bar) Plots events on an bar chart. ```tql chart_bar â€¦ ``` ### [chart\_line](/reference/operators/chart_line) [â†’](/reference/operators/chart_line) Plots events on an line chart. ```tql chart_line â€¦ ``` ### [chart\_pie](/reference/operators/chart_pie) [â†’](/reference/operators/chart_pie) Plots events on an pie chart. ```tql chart_pie â€¦ ``` ## Connecting Pipelines [Section titled â€œConnecting Pipelinesâ€](#connecting-pipelines) ### [publish](/reference/operators/publish) [â†’](/reference/operators/publish) Publishes events to a channel with a topic. ```tql publish "topic" ``` ### [subscribe](/reference/operators/subscribe) [â†’](/reference/operators/subscribe) Subscribes to events from a channel with a topic. ```tql subscribe "topic" ``` ## Contexts [Section titled â€œContextsâ€](#contexts) ### [context::create\_bloom\_filter](/reference/operators/context/create_bloom_filter) [â†’](/reference/operators/context/create_bloom_filter) Creates a Bloom filter context. ```tql context::create_bloom_filter "ctx", capacity=1Mi, fp_probability=0.01 ``` ### [context::create\_geoip](/reference/operators/context/create_geoip) [â†’](/reference/operators/context/create_geoip) Creates a GeoIP context. ```tql context::create_geoip "ctx", db_path="GeoLite2-City.mmdb" ``` ### [context::create\_lookup\_table](/reference/operators/context/create_lookup_table) [â†’](/reference/operators/context/create_lookup_table) Creates a lookup table context. ```tql context::create_lookup_table "ctx" ``` ### [context::enrich](/reference/operators/context/enrich) [â†’](/reference/operators/context/enrich) Enriches events with data from a context. ```tql context::enrich "ctx", key=x ``` ### [context::erase](/reference/operators/context/erase) [â†’](/reference/operators/context/erase) Removes entries from a context. ```tql context::erase "ctx", key=x ``` ### [context::inspect](/reference/operators/context/inspect) [â†’](/reference/operators/context/inspect) Resets a context. ```tql context::inspect "ctx" ``` ### [context::list](/reference/operators/context/list) [â†’](/reference/operators/context/list) Lists all contexts ```tql context::list ``` ### [context::load](/reference/operators/context/load) [â†’](/reference/operators/context/load) Loads context state. ```tql context::load "ctx" ``` ### [context::remove](/reference/operators/context/remove) [â†’](/reference/operators/context/remove) Deletes a context. ```tql context::remove "ctx" ``` ### [context::reset](/reference/operators/context/reset) [â†’](/reference/operators/context/reset) Resets a context. ```tql context::reset "ctx" ``` ### [context::save](/reference/operators/context/save) [â†’](/reference/operators/context/save) Saves context state. ```tql context::save "ctx" ``` ### [context::update](/reference/operators/context/update) [â†’](/reference/operators/context/update) Updates a context with new data. ```tql context::update "ctx", key=x, value=y ``` ## Detection [Section titled â€œDetectionâ€](#detection) ### [sigma](/reference/operators/sigma) [â†’](/reference/operators/sigma) Filter the input with Sigma rules and output matching events. ```tql sigma "/tmp/rules/" ``` ### [yara](/reference/operators/yara) [â†’](/reference/operators/yara) Executes YARA rules on byte streams. ```tql yara "/path/to/rules", blockwise=true ``` ## Encode & Decode [Section titled â€œEncode & Decodeâ€](#encode--decode) ### [compress](/reference/operators/compress) [â†’](/reference/operators/compress) Compresses a stream of bytes. ```tql compress "zstd" ``` ### [compress\_brotli](/reference/operators/compress_brotli) [â†’](/reference/operators/compress_brotli) Compresses a stream of bytes using Brotli compression. ```tql compress_brotli, level=10 ``` ### [compress\_bz2](/reference/operators/compress_bz2) [â†’](/reference/operators/compress_bz2) Compresses a stream of bytes using bz2 compression. ```tql compress_bz2, level=9 ``` ### [compress\_gzip](/reference/operators/compress_gzip) [â†’](/reference/operators/compress_gzip) Compresses a stream of bytes using gzip compression. ```tql compress_gzip, level=8 ``` ### [compress\_lz4](/reference/operators/compress_lz4) [â†’](/reference/operators/compress_lz4) Compresses a stream of bytes using lz4 compression. ```tql compress_lz4, level=7 ``` ### [compress\_zstd](/reference/operators/compress_zstd) [â†’](/reference/operators/compress_zstd) Compresses a stream of bytes using zstd compression. ```tql compress_zstd, level=6 ``` ### [decompress](/reference/operators/decompress) [â†’](/reference/operators/decompress) Decompresses a stream of bytes. ```tql decompress "gzip" ``` ### [decompress\_brotli](/reference/operators/decompress_brotli) [â†’](/reference/operators/decompress_brotli) Decompresses a stream of bytes in the Brotli format. ```tql decompress_brotli ``` ### [decompress\_bz2](/reference/operators/decompress_bz2) [â†’](/reference/operators/decompress_bz2) Decompresses a stream of bytes in the Bzip2 format. ```tql decompress_bz2 ``` ### [decompress\_gzip](/reference/operators/decompress_gzip) [â†’](/reference/operators/decompress_gzip) Decompresses a stream of bytes in the Gzip format. ```tql decompress_gzip ``` ### [decompress\_lz4](/reference/operators/decompress_lz4) [â†’](/reference/operators/decompress_lz4) Decompresses a stream of bytes in the Lz4 format. ```tql decompress_lz4 ``` ### [decompress\_zstd](/reference/operators/decompress_zstd) [â†’](/reference/operators/decompress_zstd) Decompresses a stream of bytes in the Zstd format. ```tql decompress_zstd ``` ## Escape Hatches [Section titled â€œEscape Hatchesâ€](#escape-hatches) ### [python](/reference/operators/python) [â†’](/reference/operators/python) Executes Python code against each event of the input. ```tql python "self.x = self.y" ``` ### [shell](/reference/operators/shell) [â†’](/reference/operators/shell) Executes a system command and hooks its stdin and stdout into the pipeline. ```tql shell "echo hello" ``` ## Filter [Section titled â€œFilterâ€](#filter) ### [assert](/reference/operators/assert) [â†’](/reference/operators/assert) Drops events and emits a warning if the invariant is violated. ```tql assert name.starts_with("John") ``` ### [assert\_throughput](/reference/operators/assert_throughput) [â†’](/reference/operators/assert_throughput) Emits a warning if the pipeline does not have the expected throughput ```tql assert_throughput 1000, within=1s ``` ### [deduplicate](/reference/operators/deduplicate) [â†’](/reference/operators/deduplicate) Removes duplicate events based on a common key. ```tql deduplicate src_ip ``` ### [head](/reference/operators/head) [â†’](/reference/operators/head) Limits the input to the first `n` events. ```tql head 20 ``` ### [sample](/reference/operators/sample) [â†’](/reference/operators/sample) Dynamically samples events from a event stream. ```tql sample 30s, max_samples=2k ``` ### [slice](/reference/operators/slice) [â†’](/reference/operators/slice) Keeps a range of events within the interval `[begin, end)` stepping by `stride`. ```tql slice begin=10, end=30 ``` ### [tail](/reference/operators/tail) [â†’](/reference/operators/tail) Limits the input to the last `n` events. ```tql tail 20 ``` ### [taste](/reference/operators/taste) [â†’](/reference/operators/taste) Limits the input to `n` events per unique schema. ```tql taste 1 ``` ### [where](/reference/operators/where) [â†’](/reference/operators/where) Keeps only events for which the given predicate is true. ```tql where name.starts_with("John") ``` ## Flow Control [Section titled â€œFlow Controlâ€](#flow-control) ### [cron](/reference/operators/cron) [â†’](/reference/operators/cron) Runs a pipeline periodically according to a cron expression. ```tql cron "* */10 * * * MON-FRI" { from "https://example.org" } ``` ### [delay](/reference/operators/delay) [â†’](/reference/operators/delay) Delays events relative to a given start time, with an optional speedup. ```tql delay ts, speed=2.5 ``` ### [discard](/reference/operators/discard) [â†’](/reference/operators/discard) Discards all incoming events. ```tql discard ``` ### [every](/reference/operators/every) [â†’](/reference/operators/every) Runs a pipeline periodically at a fixed interval. ```tql every 10s { summarize sum(amount) } ``` ### [fork](/reference/operators/fork) [â†’](/reference/operators/fork) Executes a subpipeline with a copy of the input. ```tql fork { to "copy.json" } ``` ### [load\_balance](/reference/operators/load_balance) [â†’](/reference/operators/load_balance) Routes the data to one of multiple subpipelines. ```tql load_balance $over { publish $over } ``` ### [pass](/reference/operators/pass) [â†’](/reference/operators/pass) Does nothing with the input. ```tql pass ``` ### [repeat](/reference/operators/repeat) [â†’](/reference/operators/repeat) Repeats the input a number of times. ```tql repeat 100 ``` ### [throttle](/reference/operators/throttle) [â†’](/reference/operators/throttle) Limits the bandwidth of a pipeline. ```tql throttle 100M, within=1min ``` ## Host Inspection [Section titled â€œHost Inspectionâ€](#host-inspection) ### [files](/reference/operators/files) [â†’](/reference/operators/files) Shows file information for a given directory. ```tql files "/var/log/", recurse=true ``` ### [nics](/reference/operators/nics) [â†’](/reference/operators/nics) Shows a snapshot of available network interfaces. ```tql nics ``` ### [processes](/reference/operators/processes) [â†’](/reference/operators/processes) Shows a snapshot of running processes. ```tql processes ``` ### [sockets](/reference/operators/sockets) [â†’](/reference/operators/sockets) Shows a snapshot of open sockets. ```tql sockets ``` ## Internals [Section titled â€œInternalsâ€](#internals) ### [api](/reference/operators/api) [â†’](/reference/operators/api) Use Tenzir's REST API directly from a pipeline. ```tql api "/pipeline/list" ``` ### [batch](/reference/operators/batch) [â†’](/reference/operators/batch) The `batch` operator controls the batch size of events. ```tql batch timeout=1s ``` ### [buffer](/reference/operators/buffer) [â†’](/reference/operators/buffer) An in-memory buffer to improve handling of data spikes in upstream operators. ```tql buffer 10M, policy="drop" ``` ### [cache](/reference/operators/cache) [â†’](/reference/operators/cache) An in-memory cache shared between pipelines. ```tql cache "w01wyhTZm3", ttl=10min ``` ### [local](/reference/operators/local) [â†’](/reference/operators/local) Forces a pipeline to run locally. ```tql local { sort foo } ``` ### [measure](/reference/operators/measure) [â†’](/reference/operators/measure) Replaces the input with metrics describing the input. ```tql measure ``` ### [remote](/reference/operators/remote) [â†’](/reference/operators/remote) Forces a pipeline to run remotely at a node. ```tql remote { version } ``` ### [serve](/reference/operators/serve) [â†’](/reference/operators/serve) Make events available under the `/serve` REST API endpoint ```tql serve "abcde12345" ``` ### [strict](/reference/operators/strict) [â†’](/reference/operators/strict) Treats all warnings as errors. ```tql strict { assert false } ``` ### [unordered](/reference/operators/unordered) [â†’](/reference/operators/unordered) Removes ordering assumptions from a pipeline. ```tql unordered { read_ndjson } ``` ## Modify [Section titled â€œModifyâ€](#modify) ### [dns\_lookup](/reference/operators/dns_lookup) [â†’](/reference/operators/dns_lookup) Performs DNS lookups to resolve IP addresses to hostnames or hostnames to IP addresses. ```tql dns_lookup ip_address, result=dns_info ``` ### [drop](/reference/operators/drop) [â†’](/reference/operators/drop) Removes fields from the event. ```tql drop name, metadata.id ``` ### [drop\_null\_fields](/reference/operators/drop_null_fields) [â†’](/reference/operators/drop_null_fields) Removes fields containing null values from the event. ```tql drop_null_fields name, metadata.id ``` ### [enumerate](/reference/operators/enumerate) [â†’](/reference/operators/enumerate) Add a field with the number of preceding events. ```tql enumerate num ``` ### [http](/reference/operators/http) [â†’](/reference/operators/http) Sends HTTP/1.1 requests and forwards the response. ```tql http "example.com" ``` ### [move](/reference/operators/move) [â†’](/reference/operators/move) Moves values from one field to another, removing the original field. ```tql move id=parsed_id, ctx.message=incoming.status ``` ### [replace](/reference/operators/replace) [â†’](/reference/operators/replace) Replaces all occurrences of a value with another value. ```tql replace what=42, with=null ``` ### [select](/reference/operators/select) [â†’](/reference/operators/select) Selects some values and discards the rest. ```tql select name, id=metadata.id ``` ### [set](/reference/operators/set) [â†’](/reference/operators/set) Assigns a value to a field, creating it if necessary. ```tql name = "Tenzir" ``` ### [timeshift](/reference/operators/timeshift) [â†’](/reference/operators/timeshift) Adjusts timestamps relative to a given start time, with an optional speedup. ```tql timeshift ts, start=2020-01-01 ``` ### [unroll](/reference/operators/unroll) [â†’](/reference/operators/unroll) Returns a new event for each member of a list or a record in an event, duplicating the surrounding event. ```tql unroll names ``` ## OCSF [Section titled â€œOCSFâ€](#ocsf) ### [ocsf::apply](/reference/operators/ocsf/apply) [â†’](/reference/operators/ocsf/apply) Casts incoming events to their OCSF type. ```tql ocsf::apply ``` ### [ocsf::derive](/reference/operators/ocsf/derive) [â†’](/reference/operators/ocsf/derive) Automatically assigns enum strings from their integer counterparts and vice versa. ```tql ocsf::derive ``` ### [ocsf::trim](/reference/operators/ocsf/trim) [â†’](/reference/operators/ocsf/trim) Drops fields from OCSF events to reduce their size. ```tql ocsf::trim ``` ## Packages [Section titled â€œPackagesâ€](#packages) ### [package::add](/reference/operators/package/add) [â†’](/reference/operators/package/add) Installs a package. ```tql package::add "suricata-ocsf" ``` ### [package::list](/reference/operators/package/list) [â†’](/reference/operators/package/list) Shows installed packages. ```tql package::list ``` ### [package::remove](/reference/operators/package/remove) [â†’](/reference/operators/package/remove) Uninstalls a package. ```tql package::remove "suricata-ocsf" ``` ## Parsing [Section titled â€œParsingâ€](#parsing) ### [read\_all](/reference/operators/read_all) [â†’](/reference/operators/read_all) Parses an incoming bytes stream into a single event. ```tql read_all binary=true ``` ### [read\_bitz](/reference/operators/read_bitz) [â†’](/reference/operators/read_bitz) Parses bytes as *BITZ* format. ```tql read_bitz ``` ### [read\_cef](/reference/operators/read_cef) [â†’](/reference/operators/read_cef) Parses an incoming Common Event Format (CEF) stream into events. ```tql read_cef ``` ### [read\_csv](/reference/operators/read_csv) [â†’](/reference/operators/read_csv) Read CSV (Comma-Separated Values) from a byte stream. ```tql read_csv null_value="-" ``` ### [read\_delimited](/reference/operators/read_delimited) [â†’](/reference/operators/read_delimited) Parses an incoming bytes stream into events using a string as delimiter. ```tql read_delimited "|" ``` ### [read\_delimited\_regex](/reference/operators/read_delimited_regex) [â†’](/reference/operators/read_delimited_regex) Parses an incoming bytes stream into events using a regular expression as delimiter. ```tql read_delimited_regex r"\s+" ``` ### [read\_feather](/reference/operators/read_feather) [â†’](/reference/operators/read_feather) Parses an incoming Feather byte stream into events. ```tql read_feather ``` ### [read\_gelf](/reference/operators/read_gelf) [â†’](/reference/operators/read_gelf) Parses an incoming GELF stream into events. ```tql read_gelf ``` ### [read\_grok](/reference/operators/read_grok) [â†’](/reference/operators/read_grok) Parses lines of input with a grok pattern. ```tql read_grok "%{IP:client} %{WORD:action}" ``` ### [read\_json](/reference/operators/read_json) [â†’](/reference/operators/read_json) Parses an incoming JSON stream into events. ```tql read_json arrays_of_objects=true ``` ### [read\_kv](/reference/operators/read_kv) [â†’](/reference/operators/read_kv) Read Key-Value pairs from a byte stream. ```tql read_kv r"(\s+)[A-Z_]+:", r":\s*" ``` ### [read\_leef](/reference/operators/read_leef) [â†’](/reference/operators/read_leef) Parses an incoming \[LEEF]\[leef] stream into events. ```tql read_leef ``` ### [read\_lines](/reference/operators/read_lines) [â†’](/reference/operators/read_lines) Parses an incoming bytes stream into events. ```tql read_lines ``` ### [read\_ndjson](/reference/operators/read_ndjson) [â†’](/reference/operators/read_ndjson) Parses an incoming NDJSON (newline-delimited JSON) stream into events. ```tql read_ndjson ``` ### [read\_parquet](/reference/operators/read_parquet) [â†’](/reference/operators/read_parquet) Reads events from a Parquet byte stream. ```tql read_parquet ``` ### [read\_pcap](/reference/operators/read_pcap) [â†’](/reference/operators/read_pcap) Reads raw network packets in PCAP file format. ```tql read_pcap ``` ### [read\_ssv](/reference/operators/read_ssv) [â†’](/reference/operators/read_ssv) Read SSV (Space-Separated Values) from a byte stream. ```tql read_ssv header="name count" ``` ### [read\_suricata](/reference/operators/read_suricata) [â†’](/reference/operators/read_suricata) Parse an incoming \[Suricata EVE JSON]\[eve-json] stream into events. ```tql read_suricata ``` ### [read\_syslog](/reference/operators/read_syslog) [â†’](/reference/operators/read_syslog) Parses an incoming Syslog stream into events. ```tql read_syslog ``` ### [read\_tsv](/reference/operators/read_tsv) [â†’](/reference/operators/read_tsv) Read TSV (Tab-Separated Values) from a byte stream. ```tql read_tsv auto_expand=true ``` ### [read\_xsv](/reference/operators/read_xsv) [â†’](/reference/operators/read_xsv) Read XSV from a byte stream. ```tql read_xsv ";", ":", "N/A" ``` ### [read\_yaml](/reference/operators/read_yaml) [â†’](/reference/operators/read_yaml) Parses an incoming YAML stream into events. ```tql read_yaml ``` ### [read\_zeek\_json](/reference/operators/read_zeek_json) [â†’](/reference/operators/read_zeek_json) Parse an incoming Zeek JSON stream into events. ```tql read_zeek_json ``` ### [read\_zeek\_tsv](/reference/operators/read_zeek_tsv) [â†’](/reference/operators/read_zeek_tsv) Parses an incoming `Zeek TSV` stream into events. ```tql read_zeek_tsv ``` ## Pipelines [Section titled â€œPipelinesâ€](#pipelines) ### [pipeline::activity](/reference/operators/pipeline/activity) [â†’](/reference/operators/pipeline/activity) Summarizes the activity of pipelines. ```tql pipeline::activity range=1d, interval=1h ``` ### [pipeline::detach](/reference/operators/pipeline/detach) [â†’](/reference/operators/pipeline/detach) Starts a pipeline in the node. ```tql pipeline::detach { â€¦ } ``` ### [pipeline::list](/reference/operators/pipeline/list) [â†’](/reference/operators/pipeline/list) Shows managed pipelines. ```tql pipeline::list ``` ### [pipeline::run](/reference/operators/pipeline/run) [â†’](/reference/operators/pipeline/run) Starts a pipeline in the node and waits for it to complete. ```tql pipeline::run { â€¦ } ``` ## Printing [Section titled â€œPrintingâ€](#printing) ### [write\_bitz](/reference/operators/write_bitz) [â†’](/reference/operators/write_bitz) Writes events in *BITZ* format. ```tql write_bitz ``` ### [write\_csv](/reference/operators/write_csv) [â†’](/reference/operators/write_csv) Transforms event stream to CSV (Comma-Separated Values) byte stream. ```tql write_csv ``` ### [write\_feather](/reference/operators/write_feather) [â†’](/reference/operators/write_feather) Transforms the input event stream to Feather byte stream. ```tql write_feather ``` ### [write\_json](/reference/operators/write_json) [â†’](/reference/operators/write_json) Transforms the input event stream to a JSON byte stream. ```tql write_json ``` ### [write\_kv](/reference/operators/write_kv) [â†’](/reference/operators/write_kv) Writes events in a Key-Value format. ```tql write_kv ``` ### [write\_lines](/reference/operators/write_lines) [â†’](/reference/operators/write_lines) Writes events as key-value pairsthe *values* of an event. ```tql write_lines ``` ### [write\_ndjson](/reference/operators/write_ndjson) [â†’](/reference/operators/write_ndjson) Transforms the input event stream to a Newline-Delimited JSON byte stream. ```tql write_ndjson ``` ### [write\_parquet](/reference/operators/write_parquet) [â†’](/reference/operators/write_parquet) Transforms event stream to a Parquet byte stream. ```tql write_parquet ``` ### [write\_pcap](/reference/operators/write_pcap) [â†’](/reference/operators/write_pcap) Transforms event stream to PCAP byte stream. ```tql write_pcap ``` ### [write\_ssv](/reference/operators/write_ssv) [â†’](/reference/operators/write_ssv) Transforms event stream to SSV (Space-Separated Values) byte stream. ```tql write_ssv ``` ### [write\_syslog](/reference/operators/write_syslog) [â†’](/reference/operators/write_syslog) Writes events as syslog. ```tql write_syslog ``` ### [write\_tql](/reference/operators/write_tql) [â†’](/reference/operators/write_tql) Transforms the input event stream to a TQL notation byte stream. ```tql write_tql ``` ### [write\_tsv](/reference/operators/write_tsv) [â†’](/reference/operators/write_tsv) Transforms event stream to TSV (Tab-Separated Values) byte stream. ```tql write_tsv ``` ### [write\_xsv](/reference/operators/write_xsv) [â†’](/reference/operators/write_xsv) Transforms event stream to XSV byte stream. ```tql write_xsv ``` ### [write\_yaml](/reference/operators/write_yaml) [â†’](/reference/operators/write_yaml) Transforms the input event stream to YAML byte stream. ```tql write_yaml ``` ### [write\_zeek\_tsv](/reference/operators/write_zeek_tsv) [â†’](/reference/operators/write_zeek_tsv) Transforms event stream into Zeek Tab-Separated Value byte stream. ```tql write_zeek_tsv ``` ## Inputs [Section titled â€œInputsâ€](#inputs) ### Bytes [Section titled â€œBytesâ€](#bytes) ### [load\_amqp](/reference/operators/load_amqp) [â†’](/reference/operators/load_amqp) Loads a byte stream via AMQP messages. ```tql load_amqp ``` ### [load\_azure\_blob\_storage](/reference/operators/load_azure_blob_storage) [â†’](/reference/operators/load_azure_blob_storage) Loads bytes from Azure Blob Storage. ```tql load_azure_blob_storage "abfs://container/file" ``` ### [load\_file](/reference/operators/load_file) [â†’](/reference/operators/load_file) Loads the contents of the file at `path` as a byte stream. ```tql load_file "/tmp/data.json" ``` ### [load\_ftp](/reference/operators/load_ftp) [â†’](/reference/operators/load_ftp) Loads a byte stream via FTP. ```tql load_ftp "ftp.example.org" ``` ### [load\_gcs](/reference/operators/load_gcs) [â†’](/reference/operators/load_gcs) Loads bytes from a Google Cloud Storage object. ```tql load_gcs "gs://bucket/object.json" ``` ### [load\_google\_cloud\_pubsub](/reference/operators/load_google_cloud_pubsub) [â†’](/reference/operators/load_google_cloud_pubsub) Subscribes to a Google Cloud Pub/Sub subscription and obtains bytes. ```tql load_google_cloud_pubsub project_id="my-project" ``` ### [load\_http](/reference/operators/load_http) [â†’](/reference/operators/load_http) Loads a byte stream via HTTP. ```tql load_http "example.org", params={n: 5} ``` ### [load\_kafka](/reference/operators/load_kafka) [â†’](/reference/operators/load_kafka) Loads a byte stream from an Apache Kafka topic. ```tql load_kafka topic="example" ``` ### [load\_nic](/reference/operators/load_nic) [â†’](/reference/operators/load_nic) Loads bytes from a network interface card (NIC). ```tql load_nic "eth0" ``` ### [load\_s3](/reference/operators/load_s3) [â†’](/reference/operators/load_s3) Loads from an Amazon S3 object. ```tql load_s3 "s3://my-bucket/obj.csv" ``` ### [load\_sqs](/reference/operators/load_sqs) [â†’](/reference/operators/load_sqs) Loads bytes from \[Amazon SQS]\[sqs] queues. ```tql load_sqs "sqs://tenzir" ``` ### [load\_stdin](/reference/operators/load_stdin) [â†’](/reference/operators/load_stdin) Accepts bytes from standard input. ```tql load_stdin ``` ### [load\_tcp](/reference/operators/load_tcp) [â†’](/reference/operators/load_tcp) Loads bytes from a TCP or TLS connection. ```tql load_tcp "0.0.0.0:8090" { read_json } ``` ### [load\_udp](/reference/operators/load_udp) [â†’](/reference/operators/load_udp) Loads bytes from a UDP socket. ```tql load_udp "0.0.0.0:8090" ``` ### [load\_zmq](/reference/operators/load_zmq) [â†’](/reference/operators/load_zmq) Receives ZeroMQ messages. ```tql load_zmq ``` ### Events [Section titled â€œEventsâ€](#events) ### [from](/reference/operators/from) [â†’](/reference/operators/from) Obtains events from an URI, inferring the source, compression and format. ```tql from "data.json" ``` ### [from\_azure\_blob\_storage](/reference/operators/from_azure_blob_storage) [â†’](/reference/operators/from_azure_blob_storage) Reads one or multiple files from Azure Blob Storage. ```tql from_azure_blob_storage "abfs://container/data/**.json" ``` ### [from\_file](/reference/operators/from_file) [â†’](/reference/operators/from_file) Reads one or multiple files from a filesystem. ```tql from_file "s3://data/**.json" ``` ### [from\_fluent\_bit](/reference/operators/from_fluent_bit) [â†’](/reference/operators/from_fluent_bit) Receives events via Fluent Bit. ```tql from_fluent_bit "opentelemetry" ``` ### [from\_gcs](/reference/operators/from_gcs) [â†’](/reference/operators/from_gcs) Reads one or multiple files from Google Cloud Storage. ```tql from_gcs "gs://my-bucket/data/**.json" ``` ### [from\_http](/reference/operators/from_http) [â†’](/reference/operators/from_http) Sends and receives HTTP/1.1 requests. ```tql from_http "0.0.0.0:8080" ``` ### [from\_opensearch](/reference/operators/from_opensearch) [â†’](/reference/operators/from_opensearch) Receives events via Opensearch Bulk API. ```tql from_opensearch ``` ### [from\_s3](/reference/operators/from_s3) [â†’](/reference/operators/from_s3) Reads one or multiple files from Amazon S3. ```tql from_s3 "s3://my-bucket/data/**.json" ``` ### [from\_udp](/reference/operators/from_udp) [â†’](/reference/operators/from_udp) Receives UDP datagrams and outputs structured events. ```tql from_udp "0.0.0.0:8090" ``` ### [from\_velociraptor](/reference/operators/from_velociraptor) [â†’](/reference/operators/from_velociraptor) Submits VQL to a Velociraptor server and returns the response as events. ```tql from_velociraptor subscribe="Windows" ``` ## Node [Section titled â€œNodeâ€](#node) ### Inspection [Section titled â€œInspectionâ€](#inspection) ### [diagnostics](/reference/operators/diagnostics) [â†’](/reference/operators/diagnostics) Retrieves diagnostic events from a Tenzir node. ```tql diagnostics ``` ### [metrics](/reference/operators/metrics) [â†’](/reference/operators/metrics) Retrieves metrics events from a Tenzir node. ```tql metrics "cpu" ``` ### [openapi](/reference/operators/openapi) [â†’](/reference/operators/openapi) Shows the node's OpenAPI specification. ```tql openapi ``` ### [plugins](/reference/operators/plugins) [â†’](/reference/operators/plugins) Shows all available plugins and built-ins. ```tql plugins ``` ### [version](/reference/operators/version) [â†’](/reference/operators/version) Shows the current version. ```tql version ``` ### Storage Engine [Section titled â€œStorage Engineâ€](#storage-engine) ### [export](/reference/operators/export) [â†’](/reference/operators/export) Retrieves events from a Tenzir node. ```tql export ``` ### [fields](/reference/operators/fields) [â†’](/reference/operators/fields) Retrieves all fields stored at a node. ```tql fields ``` ### [import](/reference/operators/import) [â†’](/reference/operators/import) Imports events into a Tenzir node. ```tql import ``` ### [partitions](/reference/operators/partitions) [â†’](/reference/operators/partitions) Retrieves metadata about events stored at a node. ```tql partitions src_ip == 1.2.3.4 ``` ### [schemas](/reference/operators/schemas) [â†’](/reference/operators/schemas) Retrieves all schemas for events stored at a node. ```tql schemas ``` ## Outputs [Section titled â€œOutputsâ€](#outputs) ### Bytes [Section titled â€œBytesâ€](#bytes-1) ### [save\_amqp](/reference/operators/save_amqp) [â†’](/reference/operators/save_amqp) Saves a byte stream via AMQP messages. ```tql save_amqp ``` ### [save\_azure\_blob\_storage](/reference/operators/save_azure_blob_storage) [â†’](/reference/operators/save_azure_blob_storage) Saves bytes to Azure Blob Storage. ```tql save_azure_blob_storage "abfs://container/file" ``` ### [save\_email](/reference/operators/save_email) [â†’](/reference/operators/save_email) Saves bytes through an SMTP server. ```tql save_email "user@example.org" ``` ### [save\_file](/reference/operators/save_file) [â†’](/reference/operators/save_file) Writes a byte stream to a file. ```tql save_file "/tmp/out.json" ``` ### [save\_ftp](/reference/operators/save_ftp) [â†’](/reference/operators/save_ftp) Saves a byte stream via FTP. ```tql save_ftp "ftp.example.org" ``` ### [save\_gcs](/reference/operators/save_gcs) [â†’](/reference/operators/save_gcs) Saves bytes to a Google Cloud Storage object. ```tql save_gcs "gs://bucket/object.json" ``` ### [save\_google\_cloud\_pubsub](/reference/operators/save_google_cloud_pubsub) [â†’](/reference/operators/save_google_cloud_pubsub) Publishes to a Google Cloud Pub/Sub topic. ```tql save_google_cloud_pubsub project_id="my-project" ``` ### [save\_http](/reference/operators/save_http) [â†’](/reference/operators/save_http) Sends a byte stream via HTTP. ```tql save_http "example.org/api" ``` ### [save\_kafka](/reference/operators/save_kafka) [â†’](/reference/operators/save_kafka) Saves a byte stream to a Apache Kafka topic. ```tql save_kafka topic="example" ``` ### [save\_s3](/reference/operators/save_s3) [â†’](/reference/operators/save_s3) Saves bytes to an Amazon S3 object. ```tql save_s3 "s3://my-bucket/obj.csv" ``` ### [save\_sqs](/reference/operators/save_sqs) [â†’](/reference/operators/save_sqs) Saves bytes to \[Amazon SQS]\[sqs] queues. ```tql save_sqs "sqs://tenzir" ``` ### [save\_stdout](/reference/operators/save_stdout) [â†’](/reference/operators/save_stdout) Writes a byte stream to standard output. ```tql save_stdout ``` ### [save\_tcp](/reference/operators/save_tcp) [â†’](/reference/operators/save_tcp) Saves bytes to a TCP or TLS connection. ```tql save_tcp "0.0.0.0:8090", tls=true ``` ### [save\_udp](/reference/operators/save_udp) [â†’](/reference/operators/save_udp) Saves bytes to a UDP socket. ```tql save_udp "0.0.0.0:8090" ``` ### [save\_zmq](/reference/operators/save_zmq) [â†’](/reference/operators/save_zmq) Sends bytes as ZeroMQ messages. ```tql save_zmq ``` ### Events [Section titled â€œEventsâ€](#events-1) ### [to](/reference/operators/to) [â†’](/reference/operators/to) Saves to an URI, inferring the destination, compression and format. ```tql to "output.json" ``` ### [to\_amazon\_security\_lake](/reference/operators/to_amazon_security_lake) [â†’](/reference/operators/to_amazon_security_lake) Sends OCSF events to Amazon Security Lake. ```tql to_amazon_security_lake "s3://â€¦" ``` ### [to\_azure\_log\_analytics](/reference/operators/to_azure_log_analytics) [â†’](/reference/operators/to_azure_log_analytics) Sends events to the Microsoft Azure Logs Ingestion API. ```tql to_azure_log_analytics tenant_id="...", workspace_id="..." ``` ### [to\_clickhouse](/reference/operators/to_clickhouse) [â†’](/reference/operators/to_clickhouse) Sends events to a ClickHouse table. ```tql to_clickhouse table="my_table" ``` ### [to\_fluent\_bit](/reference/operators/to_fluent_bit) [â†’](/reference/operators/to_fluent_bit) Sends events via Fluent Bit. ```tql to_fluent_bit "elasticsearch" â€¦ ``` ### [to\_google\_cloud\_logging](/reference/operators/to_google_cloud_logging) [â†’](/reference/operators/to_google_cloud_logging) Sends events to Google Cloud Logging. ```tql to_google_cloud_logging â€¦ ``` ### [to\_google\_secops](/reference/operators/to_google_secops) [â†’](/reference/operators/to_google_secops) Sends unstructured events to a Google SecOps Chronicle instance. ```tql to_google_secops â€¦ ``` ### [to\_hive](/reference/operators/to_hive) [â†’](/reference/operators/to_hive) Writes events to a URI using hive partitioning. ```tql to_hive "s3://â€¦", partition_by=[x] ``` ### [to\_kafka](/reference/operators/to_kafka) [â†’](/reference/operators/to_kafka) Sends messages to an Apache Kafka topic. ```tql to_kafka "topic", message=this.print_json() ``` ### [to\_opensearch](/reference/operators/to_opensearch) [â†’](/reference/operators/to_opensearch) Sends events to an OpenSearch-compatible Bulk API. ```tql to_opensearch "localhost:9200", â€¦ ``` ### [to\_sentinelone\_data\_lake](/reference/operators/to_sentinelone_data_lake) [â†’](/reference/operators/to_sentinelone_data_lake) Sends security events to SentinelOne Singularity Data Lake via REST API. ```tql to_sentinelone_data_lake "https://â€¦", â€¦ ``` ### [to\_snowflake](/reference/operators/to_snowflake) [â†’](/reference/operators/to_snowflake) Sends events to a Snowflake database. ```tql to_snowflake account_identifier="â€¦ ``` ### [to\_splunk](/reference/operators/to_splunk) [â†’](/reference/operators/to_splunk) Sends events to a Splunk \[HTTP Event Collector (HEC)]\[hec]. ```tql to_splunk "localhost:8088", â€¦ ```

# api

Use Tenzirâ€™s REST API directly from a pipeline. ```tql api endpoint:string, [request_body:string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `api` operator interacts with Tenzirâ€™s REST API without needing to spin up a web server, making all APIs accessible from within pipelines. ### `endpoint: string` [Section titled â€œendpoint: stringâ€](#endpoint-string) The endpoint to request, e.g., `/pipeline/list` to list all managed pipelines. Tenzirâ€™s [REST API specification](/reference/node/api) lists all available endpoints. ### `request_body: string (optional)` [Section titled â€œrequest\_body: string (optional)â€](#request_body-string-optional) A single string containing the JSON request body to send with the request. ## Examples [Section titled â€œExamplesâ€](#examples) ### List all running pipelines [Section titled â€œList all running pipelinesâ€](#list-all-running-pipelines) ```tql api "/pipeline/list" ``` ### Create a new pipeline and start it immediately [Section titled â€œCreate a new pipeline and start it immediatelyâ€](#create-a-new-pipeline-and-start-it-immediately) ```tql api "/pipeline/create", { name: "Suricata Import", definition: "from file /tmp/eve.sock read suricata", autostart: { created: true }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`openapi`](/reference/operators/openapi), [`serve`](/reference/operators/serve)

# assert

Drops events and emits a warning if the invariant is violated. ```tql assert invariant:bool, [message=any] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `assert` operator asserts that `invariant` is `true` for events. In case an event does not satisfy the invariant, it is dropped and a warning is emitted. ### `invariant: bool` [Section titled â€œinvariant: boolâ€](#invariant-bool) Condition to assert being `true`. ### `message = any (optional)` [Section titled â€œmessage = any (optional)â€](#message--any-optional) Context to associate with the assertion failure. ## Examples [Section titled â€œExamplesâ€](#examples) ### Make sure that `x != 2` [Section titled â€œMake sure that x != 2â€](#make-sure-that-x--2) ```tql from {x: 1}, {x: 2}, {x: 3} assert x != 2 ``` ```tql {x: 1} // warning: assertion failure {x: 3} ``` ### Check that a topic only contains certain events [Section titled â€œCheck that a topic only contains certain eventsâ€](#check-that-a-topic-only-contains-certain-events) ```tql subscribe "network" assert @name == "ocsf.network_activity" // continue processing ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`assert_throughput`](/reference/operators/assert_throughput), [`where`](/reference/operators/where)

# assert_throughput

Emits a warning if the pipeline does not have the expected throughput ```tql assert_throughput min_events:int, within=duration, [retries=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `assert_throughput` operator checks a pipelineâ€™s throughput, emitting a warning if the minimum specified throughput is unmet, and optionally an error if the number of retries is exceeded. ## Examples [Section titled â€œExamplesâ€](#examples) ### Require 1,000 events per second, failing if the issue persists for 30s [Section titled â€œRequire 1,000 events per second, failing if the issue persists for 30sâ€](#require-1000-events-per-second-failing-if-the-issue-persists-for-30s) ```tql from "udp://0.0.0.0:514" { read_syslog } assert_throughput 1k, within=1s, retries=30 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`assert`](/reference/operators/assert)

# batch

The `batch` operator controls the batch size of events. ```tql batch [limit:int, timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `batch` operator takes its input and rewrites it into batches of up to the desired size. Expert Operator The `batch` operator is a lower-level building block that lets users explicitly control batching, which otherwise is controlled automatically by Tenzirâ€™s underlying pipeline execution engine. Use with caution! ### `limit: int (optional)` [Section titled â€œlimit: int (optional)â€](#limit-int-optional) How many events to put into one batch at most. Defaults to `65536`. ### `timeout = duration (optional)` [Section titled â€œtimeout = duration (optional)â€](#timeout--duration-optional) Specifies a maximum latency for events passing through the batch operator. When unspecified, an infinite duration is used.

# buffer

An in-memory buffer to improve handling of data spikes in upstream operators. ```tql buffer [capacity:int, policy=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `buffer` operator buffers up to the specified number of events or bytes in memory. By default, operators in a pipeline run only when their downstream operators want to receive input. This mechanism is called back pressure. The `buffer` operator effectively breaks back pressure by storing up to the specified number of events in memory, always requesting more input, which allows upstream operators to run uninterruptedly even in case the downstream operators of the buffer are unable to keep up. This allows pipelines to handle data spikes more easily. ### `capacity: int (optional)` [Section titled â€œcapacity: int (optional)â€](#capacity-int-optional) The number of events or bytes that may be kept at most in the buffer. Note that every operator already buffers up to `254Ki` events before it starts applying back pressure. Smaller buffers may decrease performance. ### `policy = string (optional)` [Section titled â€œpolicy = string (optional)â€](#policy--string-optional) Specifies what the operator does when the buffer runs full. * `"drop"`: Drop events that do not fit into the buffer. This policy is not supported for bytes inputs. * `"block"`: Use back pressure to slow down upstream operators. When buffering events, this option defaults to `"block"` for pipelines visible on the overview page on [app.tenzir.com](https://app.tenzir.com), and to `"drop"` otherwise. When buffering bytes, this option always defaults to `"block"`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Buffer up to 10 million events or bytes [Section titled â€œBuffer up to 10 million events or bytesâ€](#buffer-up-to-10-million-events-or-bytes) Buffer and drop events if downstream cannot keep up: ```tql buffer 10M, policy="drop" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`cache`](/reference/operators/cache)

# cache

An in-memory cache shared between pipelines. ```tql cache id:string, [mode=string, capacity=int, read_timeout=duration, write_timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `cache` operator caches events in an in-memory buffer at a node. Caches must have a user-provided unique ID. The first pipeline to use a cache writes into the cache. All further pipelines using the same cache will read from the cache instead of executing the operators before the `cache` operator in the same pipeline. ### `id: string` [Section titled â€œid: stringâ€](#id-string) An arbitrary string that uniquely identifies the cache. ### `mode = string (optional)` [Section titled â€œmode = string (optional)â€](#mode--string-optional) Configures whether the operator is used an input, an output, or a transformation. The following modes are available currently: * `"read"`: The operators acts as an input operator reading from a cache that is requires to already exist. * `"write"`: The operator acts as an output operator writing into a cache that must not already exist. * `"readwrite"`: The operator acts as a transformation passing through events, lazily creating a cache if it does not already exist. If a cache exists, upstream operators will not be run and instead the cache is read. Defaults to `"readwrite"`. ### `capacity = int (optional)` [Section titled â€œcapacity = int (optional)â€](#capacity--int-optional) Stores how many events the cache can hold. Caches stop accepting events if the capacity is reached and emit a warning. Defaults to unlimited. ### `read_timeout = duration (optional)` [Section titled â€œread\_timeout = duration (optional)â€](#read_timeout--duration-optional) Defines the maximum inactivity time until the cache is evicted from memory. The timer starts when writing the cache completes (or runs into the capacity limit), and resets whenever the cache is read from. Defaults to `10min`, or the value specified in the `tenzir.cache.lifetime` option. ### `write_timeout = duration (optional)` [Section titled â€œwrite\_timeout = duration (optional)â€](#write_timeout--duration-optional) If set, defines an upper bound for the lifetime of the cache. Unlike the `read_timeout` option, this does not refresh when the cache is accessed. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cache the results of an expensive query [Section titled â€œCache the results of an expensive queryâ€](#cache-the-results-of-an-expensive-query) ```tql export where @name == "suricata.flow" summarize total=sum(bytes_toserver), src_ip, dest_ip cache "some-unique-identifier" ``` ### Get high-level statistics about a query [Section titled â€œGet high-level statistics about a queryâ€](#get-high-level-statistics-about-a-query) This calculates the cache again only if the query does not exist anymore, and delete the cache if itâ€™s unused for more than a minute. ```tql export where @name == "suricata.flow" summarize src_ip, total=sum(bytes_toserver), dest_ip cache "some-unique-identifier", read_timeout=1min summarize src_ip, total=sum(total), destinations=count(dest_ip) ``` Get the same statistics, assuming the cache still exists: ```tql cache "some-unique-identifier", mode="read" summarize src_ip, total=sum(total), destinations=count(dest_ip) ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`buffer`](/reference/operators/buffer)

# chart_area

Plots events on an area chart. ```tql chart_area x=field, y=any, [x_min=any, x_max=any, y_min=any, y_max=any, resolution=duration, fill=any, x_log=bool, y_log=bool, group=any, position=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) Visualizes events with an area chart on the [Tenzir Platform](https://app.tenzir.com). ### `x = field` [Section titled â€œx = fieldâ€](#x--field) Positions on the x-axis for each data point. ### `y = any` [Section titled â€œy = anyâ€](#y--any) Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation). Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`. For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`. ### `x_min = any (optional)` [Section titled â€œx\_min = any (optional)â€](#x_min--any-optional) If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket. ### `x_max = any (optional)` [Section titled â€œx\_max = any (optional)â€](#x_max--any-optional) If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket. ### `y_min = any (optional)` [Section titled â€œy\_min = any (optional)â€](#y_min--any-optional) If specified, any `y` values less than `y_min` will appear clipped out of the chart. ### `y_max = any (optional)` [Section titled â€œy\_max = any (optional)â€](#y_max--any-optional) If specified, any `y` values greater than `y_max` will appear clipped out of the chart. ### `resolution = duration (optional)` [Section titled â€œresolution = duration (optional)â€](#resolution--duration-optional) This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified. For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket. ### `fill = any (optional)` [Section titled â€œfill = any (optional)â€](#fill--any-optional) Optional value to fill gaps and replace `null`s with. ### `x_log = bool (optional)` [Section titled â€œx\_log = bool (optional)â€](#x_log--bool-optional) If `true`, use a logarithmic scale for the x-axis. Defaults to `false`. ### `y_log = bool (optional)` [Section titled â€œy\_log = bool (optional)â€](#y_log--bool-optional) If `true`, use a logarithmic scale for the y-axis. Defaults to `false`. ### `group = any (optional)` [Section titled â€œgroup = any (optional)â€](#group--any-optional) Optional expression to group the aggregations with. ### `position = string (optional)` [Section titled â€œposition = string (optional)â€](#position--string-optional) Determines how the `y` values are displayed. Possible values: * `grouped` * `stacked` Defaults to `grouped`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Chart TCP metrics [Section titled â€œChart TCP metricsâ€](#chart-tcp-metrics) This pipeline charts MBs read and written by different pipelines over TCP in hourly intervals for the past 24 hours. ```tql metrics "tcp" chart_area x=timestamp, y={tx: sum(bytes_written/1M), rx: sum(bytes_read/1M)}, x_min=now()-1d, resolution=1h, group=pipeline_id ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`chart_bar`](/reference/operators/chart_bar), [`chart_line`](/reference/operators/chart_line), [`chart_pie`](/reference/operators/chart_pie)

# chart_bar

Plots events on an bar chart. ```tql chart_bar x|label=field, y|value=any, [x_min=any, x_max=any, y_min=any, y_max=any, resolution=duration, fill=any, x_log=bool, y_log=bool, group=any, position=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) Visualizes events with an bar chart on the [Tenzir Platform](https://app.tenzir.com). ### `x|label = field` [Section titled â€œx|label = fieldâ€](#xlabel--field) Label for each bar. ### `y|value = any` [Section titled â€œy|value = anyâ€](#yvalue--any) Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation). Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`. For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`. ### `x_min = any (optional)` [Section titled â€œx\_min = any (optional)â€](#x_min--any-optional) If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket. ### `x_max = any (optional)` [Section titled â€œx\_max = any (optional)â€](#x_max--any-optional) If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket. ### `y_min = any (optional)` [Section titled â€œy\_min = any (optional)â€](#y_min--any-optional) If specified, any `y` values less than `y_min` will appear clipped out of the chart. ### `y_max = any (optional)` [Section titled â€œy\_max = any (optional)â€](#y_max--any-optional) If specified, any `y` values greater than `y_max` will appear clipped out of the chart. ### `resolution = duration (optional)` [Section titled â€œresolution = duration (optional)â€](#resolution--duration-optional) This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified. For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket. ### `fill = any (optional)` [Section titled â€œfill = any (optional)â€](#fill--any-optional) Optional value to fill gaps and replace `null`s with. ### `x_log = bool (optional)` [Section titled â€œx\_log = bool (optional)â€](#x_log--bool-optional) If `true`, use a logarithmic scale for the x-axis. Defaults to `false`. ### `y_log = bool (optional)` [Section titled â€œy\_log = bool (optional)â€](#y_log--bool-optional) If `true`, use a logarithmic scale for the y-axis. Defaults to `false`. ### `group = any (optional)` [Section titled â€œgroup = any (optional)â€](#group--any-optional) Optional expression to group the aggregations with. ### `position = string (optional)` [Section titled â€œposition = string (optional)â€](#position--string-optional) Determines how the `y` values are displayed. Possible values: * `grouped` * `stacked` Defaults to `grouped`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Chart count of events imported for every unique schema [Section titled â€œChart count of events imported for every unique schemaâ€](#chart-count-of-events-imported-for-every-unique-schema) ```tql metrics "import" chart_bar x=schema, y=sum(events), x_min=now()-1d ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`chart_area`](/reference/operators/chart_area), [`chart_line`](/reference/operators/chart_line), [`chart_pie`](/reference/operators/chart_pie)

# chart_line

Plots events on an line chart. ```tql chart_line x=field, y=any, [x_min=any, x_max=any, y_min=any, y_max=any, resolution=duration, fill=any, x_log=bool, y_log=bool, group=any] ``` ## Description [Section titled â€œDescriptionâ€](#description) Visualizes events with an line chart on the [Tenzir Platform](https://app.tenzir.com). ### `x = field` [Section titled â€œx = fieldâ€](#x--field) Positions on the x-axis for each data point. ### `y = any` [Section titled â€œy = anyâ€](#y--any) Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation). Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`. For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`. ### `x_min = any (optional)` [Section titled â€œx\_min = any (optional)â€](#x_min--any-optional) If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket. ### `x_max = any (optional)` [Section titled â€œx\_max = any (optional)â€](#x_max--any-optional) If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket. ### `y_min = any (optional)` [Section titled â€œy\_min = any (optional)â€](#y_min--any-optional) If specified, any `y` values less than `y_min` will appear clipped out of the chart. ### `y_max = any (optional)` [Section titled â€œy\_max = any (optional)â€](#y_max--any-optional) If specified, any `y` values greater than `y_max` will appear clipped out of the chart. ### `resolution = duration (optional)` [Section titled â€œresolution = duration (optional)â€](#resolution--duration-optional) This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified. For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket. ### `fill = any (optional)` [Section titled â€œfill = any (optional)â€](#fill--any-optional) Optional value to fill gaps and replace `null`s with. ### `x_log = bool (optional)` [Section titled â€œx\_log = bool (optional)â€](#x_log--bool-optional) If `true`, use a logarithmic scale for the x-axis. Defaults to `false`. ### `y_log = bool (optional)` [Section titled â€œy\_log = bool (optional)â€](#y_log--bool-optional) If `true`, use a logarithmic scale for the y-axis. Defaults to `false`. ### `group = any (optional)` [Section titled â€œgroup = any (optional)â€](#group--any-optional) Optional expression to group the aggregations with. ## Examples [Section titled â€œExamplesâ€](#examples) ### Chart published events [Section titled â€œChart published eventsâ€](#chart-published-events) This pipeline charts number of events published by each pipeline over 30 minute intervals for the past 24 hours. ```tql metrics "publish" chart_line x=timestamp, y=sum(events), x_min=now()-1d, group=pipeline_id, resolution=30min ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`chart_area`](/reference/operators/chart_area), [`chart_bar`](/reference/operators/chart_bar), [`chart_pie`](/reference/operators/chart_pie)

# chart_pie

Plots events on an pie chart. ```tql chart_pie x|label=field, y|value=any, [group=any] ``` ## Description [Section titled â€œDescriptionâ€](#description) Visualizes events with an pie chart on the [Tenzir Platform](https://app.tenzir.com). ### `x|label = field` [Section titled â€œx|label = fieldâ€](#xlabel--field) Name of each slice on the chart. ### `y|value = any` [Section titled â€œy|value = anyâ€](#yvalue--any) Value of each slice on the chart. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation). Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`. For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`. ### `group = any (optional)` [Section titled â€œgroup = any (optional)â€](#group--any-optional) Optional expression to group the aggregations with. ## Examples [Section titled â€œExamplesâ€](#examples) ### Chart count of events imported for every unique schema [Section titled â€œChart count of events imported for every unique schemaâ€](#chart-count-of-events-imported-for-every-unique-schema) ```tql metrics "import" where timestamp > now() - 1d chart_pie label=schema, value=sum(events) ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`chart_area`](/reference/operators/chart_area), [`chart_bar`](/reference/operators/chart_bar), [`chart_line`](/reference/operators/chart_line)

# compress

Compresses a stream of bytes. ```tql compress codec:string, [level=int] ``` Deprecated The `compress` operator is deprecated. You should use the [bespoke operators](/reference/operators#encode--decode) instead. These operators offer more options for some of the formats. ## Description [Section titled â€œDescriptionâ€](#description) The `compress` operator compresses bytes in a pipeline incrementally with a known codec. ### `codec: string` [Section titled â€œcodec: stringâ€](#codec-string) An identifier of the codec to use. Currently supported are `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`. ### `level = int (optional)` [Section titled â€œlevel = int (optional)â€](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ## Examples [Section titled â€œExamplesâ€](#examples) ### Export all events in a Gzip-compressed NDJSON file [Section titled â€œExport all events in a Gzip-compressed NDJSON fileâ€](#export-all-events-in-a-gzip-compressed-ndjson-file) ```tql export write_ndjson compress "gzip" save_file "/tmp/backup.json.gz" ``` ### Recompress a Zstd-compressed file at a higher compression level [Section titled â€œRecompress a Zstd-compressed file at a higher compression levelâ€](#recompress-a-zstd-compressed-file-at-a-higher-compression-level) ```tql load_file "in.zst" decompress "zstd" compress "zstd", level=18 save_file "out.zst" ```

# compress_brotli

Compresses a stream of bytes using Brotli compression. ```tql compress_brotli [level=int, window_bits=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `compress_brotli` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled â€œlevel = int (optional)â€](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ### `window_bits = int (optional)` [Section titled â€œwindow\_bits = int (optional)â€](#window_bits--int-optional) A number representing the encoder window bits. ## Examples [Section titled â€œExamplesâ€](#examples) ### Export all events in a Brotli-compressed NDJSON file [Section titled â€œExport all events in a Brotli-compressed NDJSON fileâ€](#export-all-events-in-a-brotli-compressed-ndjson-file) ```tql export write_ndjson compress_brotli save_file "/tmp/backup.json.bt" ``` ### Recompress a Brotli-compressed file at a different compression level [Section titled â€œRecompress a Brotli-compressed file at a different compression levelâ€](#recompress-a-brotli-compressed-file-at-a-different-compression-level) ```tql load_file "in.brotli" decompress_brotli compress_brotli level=18 save_file "out.brotli" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_brotli`](/reference/operators/decompress_brotli)

# compress_bz2

Compresses a stream of bytes using bz2 compression. ```tql compress_bz2 [level=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `compress_bz2` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled â€œlevel = int (optional)â€](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ## Examples [Section titled â€œExamplesâ€](#examples) ### Export all events in a Bzip2-compressed NDJSON file [Section titled â€œExport all events in a Bzip2-compressed NDJSON fileâ€](#export-all-events-in-a-bzip2-compressed-ndjson-file) ```tql export write_ndjson compress_bz2 save_file "/tmp/backup.json.bz2" ``` ### Recompress a Bzip2-compressed file at a different compression level [Section titled â€œRecompress a Bzip2-compressed file at a different compression levelâ€](#recompress-a-bzip2-compressed-file-at-a-different-compression-level) ```tql load_file "in.bz2" decompress_bz2 compress_bz2 level=18 save_file "out.bz2" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_bz2`](/reference/operators/decompress_bz2)

# compress_gzip

Compresses a stream of bytes using gzip compression. ```tql compress_gzip [level=int, window_bits=int, format=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `compress_gzip` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled â€œlevel = int (optional)â€](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ### `window_bits = int (optional)` [Section titled â€œwindow\_bits = int (optional)â€](#window_bits--int-optional) A number representing the encoder window bits. ### `format = string (optional)` [Section titled â€œformat = string (optional)â€](#format--string-optional) A string representing the used format. Possible values are `zlib`, `deflate` and `gzip`. Defaults to `gzip`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Export all events in a Gzip-compressed NDJSON file [Section titled â€œExport all events in a Gzip-compressed NDJSON fileâ€](#export-all-events-in-a-gzip-compressed-ndjson-file) ```tql export write_ndjson compress_gzip save_file "/tmp/backup.json.gz" ``` ### Compress using Gzip deflate [Section titled â€œCompress using Gzip deflateâ€](#compress-using-gzip-deflate) ```tql export write_ndjson compress_gzip format="deflate" ``` ### Recompress a Gzip-compressed file at a different compression level [Section titled â€œRecompress a Gzip-compressed file at a different compression levelâ€](#recompress-a-gzip-compressed-file-at-a-different-compression-level) ```tql load_file "in.gzip" decompress_gzip compress_gzip level=18 save_file "out.gzip" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_gzip`](/reference/operators/decompress_gzip)

# compress_lz4

Compresses a stream of bytes using lz4 compression. ```tql compress_lz4 [level=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `compress_lz4` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled â€œlevel = int (optional)â€](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ## Examples [Section titled â€œExamplesâ€](#examples) ### Export all events in a Lz4-compressed NDJSON file [Section titled â€œExport all events in a Lz4-compressed NDJSON fileâ€](#export-all-events-in-a-lz4-compressed-ndjson-file) ```tql export write_ndjson compress_lz4 save_file "/tmp/backup.json.lz4" ``` ### Recompress a Lz4-compressed file at a different compression level [Section titled â€œRecompress a Lz4-compressed file at a different compression levelâ€](#recompress-a-lz4-compressed-file-at-a-different-compression-level) ```tql load_file "in.lz4" decompress_lz4 compress_lz4 level=18 save_file "out.lz4" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_lz4`](/reference/operators/decompress_lz4)

# compress_zstd

Compresses a stream of bytes using zstd compression. ```tql compress_zstd [level=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `compress_zstd` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled â€œlevel = int (optional)â€](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ## Examples [Section titled â€œExamplesâ€](#examples) ### Export all events in a Zstd-compressed NDJSON file [Section titled â€œExport all events in a Zstd-compressed NDJSON fileâ€](#export-all-events-in-a-zstd-compressed-ndjson-file) ```tql export write_ndjson compress_zstd save_file "/tmp/backup.json.zstd" ``` ### Recompress a Zstd-compressed file at a different compression level [Section titled â€œRecompress a Zstd-compressed file at a different compression levelâ€](#recompress-a-zstd-compressed-file-at-a-different-compression-level) ```tql load_file "in.zstd" decompress_zstd compress_zstd level=18 save_file "out.zstd" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# context::create_bloom_filter

Creates a Bloom filter context. ```tql context::create_bloom_filter name:string, capacity=int, fp_probability=float ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::create_bloom_filter` operator constructs a new context of type [Bloom filter](/explanations/enrichment#bloom-filter). To find suitable values for the capacity and false-positive probability, consult Thomas Hurstâ€™s [Bloom Filter Calculator](https://hur.st/bloomfilter/). The parameter `n` corresponds to `capacity` and `p` to `fp_probability`. You can also create a Bloom filter context as code by adding it to `tenzir.contexts` in your `tenzir.yaml`: \<prefix>/etc/tenzir/tenzir.yaml ```yaml tenzir: contexts: my-iocs: type: bloom-filter arguments: capacity: 1B fp-probability: 0.001 ``` Making changes to `arguments` of an already created context has no effect. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the new Bloom filter. ### `capacity = uint` [Section titled â€œcapacity = uintâ€](#capacity--uint) The maximum number of items in the filter that maintain the false positive probability. Adding more elements does not yield an error, but lookups will more likely return false positives. ### `fp_probability = float` [Section titled â€œfp\_probability = floatâ€](#fp_probability--float) The false-positive probability of the Bloom filter. ## Examples [Section titled â€œExamplesâ€](#examples) ### Create a new Bloom filter context [Section titled â€œCreate a new Bloom filter contextâ€](#create-a-new-bloom-filter-context) ```tql context::create_bloom_filter "ctx", capacity=1B, fp_probability=0.001 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::create_geoip

Creates a GeoIP context. ```tql context::create_geoip name:string, [db_path=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::create_geoip` operator constructs a new context of type [GeoIP](/explanations/enrichment#geoip-database). You must either provide a database with the `db_path` argument or use [`context::load`](/reference/operators/context/load) to populate the context after creation. You can also create a GeoIP context as code by adding it to `tenzir.contexts` in your `tenzir.yaml`: \<prefix>/etc/tenzir/tenzir.yaml ```yaml tenzir: contexts: my-geoips: type: geoip arguments: db-path: /usr/local/share/stuff/high-res-geoips.mmdb ``` Making changes to `arguments` of an already created context has no effect. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the new GeoIP context. ### `db_path = string (optional)` [Section titled â€œdb\_path = string (optional)â€](#db_path--string-optional) The path to the [MMDB](https://maxmind.github.io/MaxMind-DB/) database, relative to the nodeâ€™s working directory. ## Examples [Section titled â€œExamplesâ€](#examples) ### Create a new GeoIP context [Section titled â€œCreate a new GeoIP contextâ€](#create-a-new-geoip-context) ```tql context::create_geoip "ctx", db_path="GeoLite2-City.mmdb" ``` ### Populate a GeoIP context from a remote location [Section titled â€œPopulate a GeoIP context from a remote locationâ€](#populate-a-geoip-context-from-a-remote-location) Load [CIRCLâ€™s Geo Open](https://data.public.lu/en/datasets/geo-open-ip-address-geolocation-per-country-in-mmdb-format/) dataset from November 12, 2024: ```tql load_http "https://data.public.lu/fr/datasets/r/69064b5d-bf46-4244-b752-2096b16917a4" context::load "ctx" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::create_lookup_table

Creates a lookup table context. ```tql context::create_lookup_table name:string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::create_lookup_table` operator constructs a new context of type [lookup table](/explanations/enrichment#lookup-table). You can also create a lookup table as code by adding it to `tenzir.contexts` in your `tenzir.yaml`: \<prefix>/etc/tenzir/tenzir.yaml ```yaml tenzir: contexts: my-table: type: lookup-table ``` ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the new lookup table. ## Examples [Section titled â€œExamplesâ€](#examples) ### Create a new lookup table context [Section titled â€œCreate a new lookup table contextâ€](#create-a-new-lookup-table-context) ```tql context::create_lookup_table "ctx" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::enrich

Enriches events with data from a context. ```tql context::enrich name:string, key=any, [into=field, mode=string, format=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::enrich` operator enriches events with data from a context. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the context to enrich with. ### `key = any` [Section titled â€œkey = anyâ€](#key--any) The field to use for the context lookup. ### `into = field (optional)` [Section titled â€œinto = field (optional)â€](#into--field-optional) The field into which to write the enrichment. Defaults to the context name (`name`). ### `mode = string (optional)` [Section titled â€œmode = string (optional)â€](#mode--string-optional) The mode of the enrichment operation: * `set`: overwrites the field specified by `into`. * `append`: appends into the list specified by `into`. If `into` is `null` or an `empty` list, a new list is created. If `into` is not a list, the enrichment will fail with a warning. Defaults to `set`. ### `format = string (optional)` [Section titled â€œformat = string (optional)â€](#format--string-optional) The style of the enriched value: * `plain`: formats the enrichment as retrieved from the context. * `ocsf`: formats the enrichment as an [OCSF Enrichment](https://schema.ocsf.io/1.4.0-dev/objects/enrichment?extensions=) object with fields `data`, `provider`, `type`, and `value`. Defaults to `plain`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Enrich with a lookup table [Section titled â€œEnrich with a lookup tableâ€](#enrich-with-a-lookup-table) Create a lookup table: ```tql context::create_lookup_table "ctx" ``` Add data to the lookup table: ```tql from {x:1, y:"a"}, {x:2, y:"b"} context::update "ctx", key=x, value=y ``` Enrich with the table: ```tql from {x:1} context::enrich "ctx", key=x ``` ```tql { x: 1, ctx: "a", } ``` ### Enrich as OCSF Enrichment [Section titled â€œEnrich as OCSF Enrichmentâ€](#enrich-as-ocsf-enrichment) Assume the same table preparation as above, but followed by a different call to `context::enrich` using the `format` option: ```tql from {x:1} context::enrich "ctx", key=x, format="ocsf" ``` ```tql { x: 1, ctx: { created_time: 2024-11-18T16:35:48.069981, name: "x", value: 1, data: "a", } } ``` ### Enrich by appending to an array [Section titled â€œEnrich by appending to an arrayâ€](#enrich-by-appending-to-an-array) Enrich twice with the same context and accumulate enrichments into an array: ```tql from {x:1} context::enrich "ctx", key=x, into=enrichments, mode="append" context::enrich "ctx", key=x, into=enrichments, mode="append" ``` ```tql { x: 1, enrichments: [ "a", "a", ] } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::erase

Removes entries from a context. ```tql context::erase name:string, key=any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::erase` operator removes data from a context. Use the `key` argument to specify the field in the input that should be deleted from the context. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the context to remove entries from. ### `key = any` [Section titled â€œkey = anyâ€](#key--any) The field that represents the enrichment key in the data. ## Examples [Section titled â€œExamplesâ€](#examples) ### Delete entries from a context [Section titled â€œDelete entries from a contextâ€](#delete-entries-from-a-context) ```plaintext from {network: 10.0.0.1/16} context::erase "network-classification", key=network ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::inspect

Resets a context. ```tql context::inspect name:string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::inspect` operator shows details about a specified context. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the context to inspect. ## Examples [Section titled â€œExamplesâ€](#examples) ### Inspect a context [Section titled â€œInspect a contextâ€](#inspect-a-context) Add data to the lookup table: ```tql from {x:1, y:"a"}, {x:2, y:"b"} context::update "ctx", key=x, value=y ``` Retrieve the lookup table contents: ```tql context::inspect "ctx" ``` ```tql {key: 2, value: "b"} {key: 1, value: "a"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::erase`](/reference/operators/context/enrich), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::list

Lists all contexts ```tql context::list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::list` operator retrieves all contexts. ## Examples [Section titled â€œExamplesâ€](#examples) ### Show all contexts [Section titled â€œShow all contextsâ€](#show-all-contexts) ```tql context::list ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::load

Loads context state. ```tql context::load name:string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::load` operator replaces the state of the specified context with its (binary) input. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the context whose state to update. ## Examples [Section titled â€œExamplesâ€](#examples) ### Replace the database of a GeoIP context [Section titled â€œReplace the database of a GeoIP contextâ€](#replace-the-database-of-a-geoip-context) ```tql load_file "ultra-high-res.mmdb", mmap=true context::load "ctx" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::remove

Deletes a context. ```tql context::remove name:string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::remove` operator deletes the specified context. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the context to delete. ## Examples [Section titled â€œExamplesâ€](#examples) ### Delete a context [Section titled â€œDelete a contextâ€](#delete-a-context) ```tql context::delete "ctx" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::reset

Resets a context. ```tql context::reset name:string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::reset` operator erases all data that has been added with `context::update`. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the context to reset. ## Examples [Section titled â€œExamplesâ€](#examples) ### Reset a context [Section titled â€œReset a contextâ€](#reset-a-context) ```tql context::reset "ctx" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_geoip`](/reference/operators/context/create_geoip), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::save

Saves context state. ```tql context::save name:string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::save` operator dumps the state of the specified context into its (binary) output. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the context whose state to save. ## Examples [Section titled â€œExamplesâ€](#examples) ### Store the database of a GeoIP context [Section titled â€œStore the database of a GeoIP contextâ€](#store-the-database-of-a-geoip-context) ```tql context::save "ctx" save_file "snapshot.mmdb" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::update

Updates a context with new data. ```tql context::update name:string, key=any, [value=any, create_timeout=duration, write_timeout=duration, read_timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `context::update` operator adds new data to a specified context. Use the `key` argument to specify the field in the input that should be associated with the context. The [`context::enrich`](/reference/operators/context/enrich) operator uses this key to access the context. For contexts that support assigning a value with a given key, you can provide an expression to customize whatâ€™s being associated with the given key. The three arguments `create_timeout`, `write_timeout`, and `read_timeout` only work with lookup tables and set the respective timeouts per table entry. ### `name: string` [Section titled â€œname: stringâ€](#name-string) The name of the context to update. ### `key = any` [Section titled â€œkey = anyâ€](#key--any) The field that represents the enrichment key in the data. ### `value = any (optional)` [Section titled â€œvalue = any (optional)â€](#value--any-optional) The field that represents the enrichment value to associate with `key`. Defaults to `this`. ### `create_timeout = duration (optional)` [Section titled â€œcreate\_timeout = duration (optional)â€](#create_timeout--duration-optional) Expires a context entry after a given duration since entry creation. ### `write_timeout = duration (optional)` [Section titled â€œwrite\_timeout = duration (optional)â€](#write_timeout--duration-optional) Expires a context entry after a given duration since the last update time. Every Every call to `context::update` resets the timeout for the respective key. ### `read_timeout = duration (optional)` [Section titled â€œread\_timeout = duration (optional)â€](#read_timeout--duration-optional) Expires a context entry after a given duration since the last access time. Every call to `context::enrich` resets the timeout for the respective key. ## Examples [Section titled â€œExamplesâ€](#examples) ### Populate a lookup table with data [Section titled â€œPopulate a lookup table with dataâ€](#populate-a-lookup-table-with-data) Create a lookup table: ```tql context::create_lookup_table "ctx" ``` Add data to the lookup table via `context::update`: ```tql from {x:1, y:"a"}, {x:2, y:"b"} context::update "ctx", key=x, value=y ``` Retrieve the lookup table contents: ```tql context::inspect "ctx" ``` ```tql {key: 2, value: "b"} {key: 1, value: "a"} ``` ### Use a custom value as lookup table [Section titled â€œUse a custom value as lookup tableâ€](#use-a-custom-value-as-lookup-table) ```tql from {x:1}, {x:2} context::update "ctx", key=x, value=x*x ``` ```tql {key: 2, value: 4} {key: 1, value: 1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list)

# cron

Runs a pipeline periodically according to a cron expression. ```tql cron schedule:string { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `cron` operator performs scheduled execution of a pipeline indefinitely according to a [cron expression](https://en.wikipedia.org/wiki/Cron). The executor spawns a new pipeline according to the cadence given by `schedule`. If the pipeline runs longer than the interval to the next scheduled time point, the next run immediately starts. ### `schedule: string` [Section titled â€œschedule: stringâ€](#schedule-string) The cron expression with the following syntax: ```plaintext <seconds> <minutes> <hours> <days of month> <months> <days of week> ``` The 6 fields are separated by a space. Allowed values for each field are: | Field | Value range\* | Special characters | Alternative Literals | | ------------ | ------------- | ----------------------- | -------------------- | | seconds | 0-59 | `*` `,` `-` | | | minutes | 0-59 | `*` `,` `-` | | | hours | 0-23 | `*` `,` `-` | | | days of | 1-31 | `*` `,` `-` `?` `L` `W` | | | months | 1-12 | `*` `,` `-` | `JAN` â€¦ `DEC` | | days of week | 0-6 | `*` `,` `-` `?` `L` `#` | `SUN` â€¦ `SAT` | The special characters have the following meaning: | Special character | Meaning | Description | | ----------------- | ----------------- | ------------------------------------------------- | | `*` | all values | selects all values within a field | | `?` | no specific value | specify one field and leave the other unspecified | | `-` | range | specify ranges | | `,` | comma | specify additional values | | `/` | slash | specify increments | | `L` | last | last day of the month or last day of the week | | `W` | weekday | the weekday nearest to the given day | | `#` | nth | specify the Nth day of the month | ## Examples [Section titled â€œExamplesâ€](#examples) ### Fetch the results from an API every 10 minutes [Section titled â€œFetch the results from an API every 10 minutesâ€](#fetch-the-results-from-an-api-every-10-minutes) Pull an endpoint on every 10th minute, Monday through Friday: ```tql cron "* */10 * * * MON-FRI" { from "https://example.org/api" } publish "api" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`every`](/reference/operators/every)

# decompress

Decompresses a stream of bytes. ```tql decompress codec:string ``` Deprecated The `decompress` operator is deprecated. You should use the [bespoke operators](/reference/operators#encode--decode) instead. ## Description [Section titled â€œDescriptionâ€](#description) The `decompress` operator decompresses bytes in a pipeline incrementally with a known codec. The operator supports decompressing multiple concatenated streams of the same codec transparently. ### `codec: string` [Section titled â€œcodec: stringâ€](#codec-string) An identifier of the codec to use. Currently supported are `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Import Suricata events from a Zstd-compressed file [Section titled â€œImport Suricata events from a Zstd-compressed fileâ€](#import-suricata-events-from-a-zstd-compressed-file) ```tql load_file "eve.json.zst" decompress "zstd" read_suricata import ``` ### Convert a Zstd-compressed file into an LZ4-compressed file [Section titled â€œConvert a Zstd-compressed file into an LZ4-compressed fileâ€](#convert-a-zstd-compressed-file-into-an-lz4-compressed-file) ```tql load_file "in.zst" decompress "zstd" compress "lz4" save_file "out.lz4" ```

# decompress_brotli

Decompresses a stream of bytes in the Brotli format. ```tql decompress_brotli ``` ## Description [Section titled â€œDescriptionâ€](#description) The `decompress_brotli` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled â€œExamplesâ€](#examples) ### Import Suricata events from a Brotli-compressed file [Section titled â€œImport Suricata events from a Brotli-compressed fileâ€](#import-suricata-events-from-a-brotli-compressed-file) ```tql load_file "eve.json.brotli" decompress_brotli read_suricata import ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_bz2

Decompresses a stream of bytes in the Bzip2 format. ```tql decompress_bz2 ``` ## Description [Section titled â€œDescriptionâ€](#description) The `decompress_bz2` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled â€œExamplesâ€](#examples) ### Import Suricata events from a Bzip2-compressed file [Section titled â€œImport Suricata events from a Bzip2-compressed fileâ€](#import-suricata-events-from-a-bzip2-compressed-file) ```tql load_file "eve.json.bz" decompress_bz2 read_suricata import ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_bz2`](/reference/operators/compress_bz2), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_gzip

Decompresses a stream of bytes in the Gzip format. ```tql decompress_gzip ``` ## Description [Section titled â€œDescriptionâ€](#description) The `decompress_gzip` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled â€œExamplesâ€](#examples) ### Import Suricata events from a Gzip-compressed file [Section titled â€œImport Suricata events from a Gzip-compressed fileâ€](#import-suricata-events-from-a-gzip-compressed-file) ```tql load_file "eve.json.gz" decompress_brotli decompress_gzip import ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_gzip`](/reference/operators/compress_gzip), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_lz4

Decompresses a stream of bytes in the Lz4 format. ```tql decompress_lz4 ``` ## Description [Section titled â€œDescriptionâ€](#description) The `decompress_lz4` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled â€œExamplesâ€](#examples) ### Import Suricata events from a LZ4-compressed file [Section titled â€œImport Suricata events from a LZ4-compressed fileâ€](#import-suricata-events-from-a-lz4-compressed-file) ```tql load_file "eve.json.lz4" decompress_lz4 read_suricata import ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_lz4`](/reference/operators/compress_lz4), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_zstd

Decompresses a stream of bytes in the Zstd format. ```tql decompress_zstd ``` ## Description [Section titled â€œDescriptionâ€](#description) The `decompress_zstd` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled â€œExamplesâ€](#examples) ### Import Suricata events from a Zstd-compressed file [Section titled â€œImport Suricata events from a Zstd-compressed fileâ€](#import-suricata-events-from-a-zstd-compressed-file) ```tql load_file "eve.json.zstd" decompress_zstd read_suricata import ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# deduplicate

Removes duplicate events based on a common key. ```tql deduplicate [key:any, limit=int, distance=int, create_timeout=duration, write_timeout=duration, read_timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `deduplicate` operator removes duplicates from a stream of events, based on the value of one or more fields. ### `key: any (optional)` [Section titled â€œkey: any (optional)â€](#key-any-optional) The key to deduplicate. To deduplicate multiple fields, use a record expression like `{foo: bar, baz: qux}`. Defaults to `this`, i.e., deduplicating entire events. ### `limit = int (optional)` [Section titled â€œlimit = int (optional)â€](#limit--int-optional) The number of duplicate keys allowed before an event is suppressed. Defaults to `1`, which is equivalent to removing all duplicates. ### `distance = int (optional)` [Section titled â€œdistance = int (optional)â€](#distance--int-optional) Distance between two events that can be considered duplicates. A value of `1` means that only adjacent events can be considered duplicate. When unspecified, the distance is infinite. ### `create_timeout = duration (optional)` [Section titled â€œcreate\_timeout = duration (optional)â€](#create_timeout--duration-optional) The time that needs to pass until a surpressed event is no longer considered a duplicate. The timeout resets when the first event for a given key is let through. ### `write_timeout = duration (optional)` [Section titled â€œwrite\_timeout = duration (optional)â€](#write_timeout--duration-optional) The time that needs to pass until a suppressed event is no longer considered a duplicate. The timeout resets when any event for a given key is let through. For a limit of `1`, the write timeout is equivalent to the create timeout. The write timeout must be smaller than the create timeout. ### `read_timeout = duration (optional)` [Section titled â€œread\_timeout = duration (optional)â€](#read_timeout--duration-optional) The time that needs to pass until a suppressed event is no longer considered a duplicate. The timeout resets when a key is seen, even if the event is suppressed. The read timeout must be smaller than the write and create timeouts. ## Examples [Section titled â€œExamplesâ€](#examples) ### Simple deduplication [Section titled â€œSimple deduplicationâ€](#simple-deduplication) Consider the following data: ```tql {foo: 1, bar: "a"} {foo: 1, bar: "a"} {foo: 1, bar: "a"} {foo: 1, bar: "b"} {foo: null, bar: "b"} {bar: "b"} {foo: null, bar: "b"} {foo: null, bar: "b"} ``` For `deduplicate`, all duplicate events are removed: ```tql {foo: 1, bar: "a"} {foo: 1, bar: "b"} {foo: null, bar: "b"} {bar: "b"} ``` If `deduplicate bar` is used, only the field `bar` is considered when determining whether an event is a duplicate: ```tql {foo: 1, bar: "a"} {foo: 1, bar: "b"} ``` And for `deduplicate foo`, only the field `foo` is considered. Note, how the missing `foo` field is treated as if it had the value `null`, i.e., itâ€™s not included in the output. ```tql {foo: 1, bar: "a"} {foo: null, bar: "b"} ``` ### Get up to 10 warnings per hour for each run of a pipeline [Section titled â€œGet up to 10 warnings per hour for each run of a pipelineâ€](#get-up-to-10-warnings-per-hour-for-each-run-of-a-pipeline) ```tql diagnostics live=true deduplicate {id: pipeline_id, run: run}, limit=10, create_timeout=1h ``` ### Get an event whenever the node disconnected from the Tenzir Platform [Section titled â€œGet an event whenever the node disconnected from the Tenzir Platformâ€](#get-an-event-whenever-the-node-disconnected-from-the-tenzir-platform) ```tql metrics "platform", live=true deduplicate connected, distance=1 where not connected ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`sample`](/reference/operators/sample)

# delay

Delays events relative to a given start time, with an optional speedup. ```tql delay field:time, [start=time, speed=double] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `delay` operator replays a dataflow according to a time field by introducing sleeping periods proportional to the inter-arrival times of the events. With the `speed` option, you can adjust the sleep time of the time series induced by `field` with a multiplicative factor. This has the effect of making the time series â€œfasterâ€ for values great than 1 and â€œslowerâ€ for values less than 1. Unless you provide a start time with `start`, the operator will anchor the timestamps in `field` to begin with the current wall clock time, as if you provided `start=now()`. The diagram below illustrates the effect of applying `delay` to dataflow. If an event in the stream has a timestamp the precedes the previous event, `delay` emits it instantly. Otherwise `delay` sleeps the amount of time to reach the next timestamp. As shown in the last illustration, the `speed` factor has a scaling effect on the inter-arrival times. ![Delay](/_astro/delay.excalidraw.BSSAawE0_19DKCs.svg) The options `start` and `speed` work independently, i.e., you can use them separately or both together. ### `field: time` [Section titled â€œfield: timeâ€](#field-time) The field in the event containing the timestamp values. ### `start = time (optional)` [Section titled â€œstart = time (optional)â€](#start--time-optional) The timestamp to anchor the time values around. Defaults to the first non-null timestamp in `field`. ### `speed = double (optional)` [Section titled â€œspeed = double (optional)â€](#speed--double-optional) A constant factor to be divided by the inter-arrival time. For example, 2.0 decreases the event gaps by a factor of two, resulting a twice as fast dataflow. A value of 0.1 creates dataflow that spans ten times the original time frame. Defaults to 1.0. ## Examples [Section titled â€œExamplesâ€](#examples) ### Replay logs in real time [Section titled â€œReplay logs in real timeâ€](#replay-logs-in-real-time) Replay the M57 Zeek logs with real-world inter-arrival times from the `ts` field. For example, if an event arrives at time *t* and the next event at time *u*, then the `delay` operator will wait time *u - t* between emitting the two events. If *t > u* then the operator immediately emits next event. ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" read_zeek_tsv delay ts ``` ### Replay logs at 10.5 times the original speed [Section titled â€œReplay logs at 10.5 times the original speedâ€](#replay-logs-at-105-times-the-original-speed) ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" read_zeek_tsv delay ts, speed=10.5 ``` ### Replay and delay after a given timestamp [Section titled â€œReplay and delay after a given timestampâ€](#replay-and-delay-after-a-given-timestamp) Replay and start delaying only after `ts` exceeds `2021-11-17T16:35` and emit all events prior to that timestamp immediately. ```tql load_file "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" read_zeek_tsv delay ts, start=2021-11-17T16:35, speed=10.0 ``` Adjust the timestamp to the present, and then start replaying in 2 hours from now: ```tql load_file "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" decompress "zstd" read_zeek_tsv timeshift ts delay ts, start=now()+2h ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`timeshift`](/reference/operators/timeshift)

# diagnostics

Retrieves diagnostic events from a Tenzir node. ```tql diagnostics [live=bool, retro=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `diagnostics` operator retrieves diagnostic events from a Tenzir node. ### `live = bool (optional)` [Section titled â€œlive = bool (optional)â€](#live--bool-optional) If `true`, emits diagnostic events as they are generated in real-time. Unless `retro=true` is also given, this makes it so that previous diagnostics events are not returned. ### `retro = bool (optional)` [Section titled â€œretro = bool (optional)â€](#retro--bool-optional) Return diagnostic events that were generated in the past. Unless `live=true` is given, this is the default. If both are set to `true`, all previous events are returned before beginning with the live events. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir emits diagnostic information with the following schema: ### `tenzir.diagnostic` [Section titled â€œtenzir.diagnosticâ€](#tenzirdiagnostic) Contains detailed information about the diagnostic. | Field | Type | Description | | :------------ | :------------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline that created the diagnostic. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `timestamp` | `time` | The exact timestamp of the diagnostic creation. | | `message` | `string` | The diagnostic message. | | `severity` | `string` | The diagnostic severity. | | `notes` | `list<record>` | The diagnostic notes. Can be empty. | | `annotations` | `list<record>` | The diagnostic annotations. Can be empty. | | `rendered` | `string` | The rendered diagnostic, as printed on the command-line. | The records in `notes` have the following schema: | Field | Type | Description | | :-------- | :------- | :------------------------------------------------------------ | | `kind` | `string` | The kind of note, which is `note`, `usage`, `hint` or `docs`. | | `message` | `string` | The message of this note. | The records in `annotations` have the following schema: | Field | Type | Description | | :-------- | :------- | :----------------------------------------------------------------------------------------------------------- | | `primary` | `bool` | True if the `source` represents the underlying reason for the diagnostic, false if it is only related to it. | | `text` | `string` | A message for explanations. Can be empty. | | `source` | `string` | The character range in the pipeline string that this annotation is associated to. | ## Examples [Section titled â€œExamplesâ€](#examples) ### View all diagnostics generated in the past 5 minutes [Section titled â€œView all diagnostics generated in the past 5 minutesâ€](#view-all-diagnostics-generated-in-the-past-5-minutes) ```tql diagnostics where timestamp > now() - 5min ``` ### Get a live feed of error diagnostics [Section titled â€œGet a live feed of error diagnosticsâ€](#get-a-live-feed-of-error-diagnostics) ```tql diagnostics live=true where severity == "error" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`metrics`](/reference/operators/metrics)

# discard

Discards all incoming events. ```tql discard ``` ## Description [Section titled â€œDescriptionâ€](#description) The `discard` operator discards all the incoming events immediately, without rendering them or doing any additional processing. This operator is mainly used to test or benchmark pipelines. ## Examples [Section titled â€œExamplesâ€](#examples) ### Benchmark to see how long it takes to export everything [Section titled â€œBenchmark to see how long it takes to export everythingâ€](#benchmark-to-see-how-long-it-takes-to-export-everything) ```tql export discard ```

# dns_lookup

Performs DNS lookups to resolve IP addresses to hostnames or hostnames to IP addresses. ```tql dns_lookup field, [result=field] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `dns_lookup` operator performs DNS resolution on the specified field. It automatically detects whether to perform a forward lookup (hostname to IP) or reverse lookup (IP to hostname) based on the fieldâ€™s content. * **Reverse lookup**: When the field contains an IP address, the operator performs a PTR query to find the associated hostname. * **Forward lookup**: When the field contains a string, the operator performs A and AAAA queries to find associated IP addresses. The result is stored as a record in the specified result field. ### `field: ip|string` [Section titled â€œfield: ip|stringâ€](#field-ipstring) The field containing either an IP address or hostname to look up. ### `result = field (optional)` [Section titled â€œresult = field (optional)â€](#result--field-optional) The field where the DNS lookup result will be stored. Defaults to `dns_lookup`. The result is a record with the following structure: For reverse lookups (IP to hostname): ```tql { hostname: string } ``` For forward lookups (hostname to IP): ```tql list<record> ``` Where each record has the structure: ```tql { address: ip, type: string, ttl: duration } ``` If the lookup fails or times out, the result field will be `null`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Reverse DNS lookup [Section titled â€œReverse DNS lookupâ€](#reverse-dns-lookup) Resolve an IP address to its hostname: ```tql from {src_ip: 8.8.8.8, dst_ip: 192.168.1.1} dns_lookup src_ip, result=src_dns ``` ```tql { src_ip: 8.8.8.8, dst_ip: 192.168.1.1, src_dns: { hostname: "dns.google" } } ``` ### Forward DNS lookup [Section titled â€œForward DNS lookupâ€](#forward-dns-lookup) Resolve a hostname to its IP addresses: ```tql from {domain: "example.com", timestamp: 2024-01-15T10:30:00} dns_lookup domain, result=ip_info ``` ```tql { domain: "example.com", timestamp: 2024-01-15T10:30:00, ip_info: [ {address: 93.184.215.14, type: "A", ttl: 5m}, {address: 2606:2800:21f:cb07:6820:80da:af6b:8b2c, type: "AAAA", ttl: 5m} ] } ``` ### Handling lookup failures [Section titled â€œHandling lookup failuresâ€](#handling-lookup-failures) When a DNS lookup fails, the result field is set to `null`: ```tql from {ip: 192.168.1.123} dns_lookup ip, result=hostname_info ``` ```tql { ip: 192.168.1.123, hostname_info: null } ``` ### Multiple lookups in a pipeline [Section titled â€œMultiple lookups in a pipelineâ€](#multiple-lookups-in-a-pipeline) ```tql from { source: 1.1.1.1, destination: "tenzir.com" } dns_lookup source, result=source_dns dns_lookup destination, result=dest_ips ``` ```tql { source: 1.1.1.1, destination: "tenzir.com", source_dns: { hostname: "one.one.one.one" }, dest_ips: [ {address: 185.199.108.153, type: "A", ttl: 1h}, {address: 185.199.109.153, type: "A", ttl: 1h}, {address: 185.199.110.153, type: "A", ttl: 1h}, {address: 185.199.111.153, type: "A", ttl: 1h} ] } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`set`](/reference/operators/set)

# drop

Removes fields from the event. ```tql drop field... ``` ## Description [Section titled â€œDescriptionâ€](#description) Removes the given fields from the events. Issues a warning if a field is not present. ## Examples [Section titled â€œExamplesâ€](#examples) ### Drop fields from the input [Section titled â€œDrop fields from the inputâ€](#drop-fields-from-the-input) ```tql from { src: 192.168.0.4, dst: 192.168.0.31, role: "admin", info: { id: "cR32kdMD9", msg: 8411, }, } drop role, info.id ``` ```tql { src: 192.168.0.4, dst: 192.168.0.31, info: { msg: 8411, }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`select`](/reference/operators/select), [`where`](/reference/operators/where)

# drop_null_fields

Removes fields containing null values from the event. ```tql drop_null_fields [field...] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `drop_null_fields` operator removes fields that have `null` values from events. Without arguments, it removes all fields with `null` values from the entire event. When provided with specific field paths, it removes those fields if they contain null values, and for record fields, it also recursively removes any null fields within them. ### `field... (optional)` [Section titled â€œfield... (optional)â€](#field-optional) A comma-separated list of field paths to process. When specified: * If a field contains `null`, it will be removed * If a field is a record, all null fields within it will be removed recursively * Other null fields outside the specified paths will be preserved ## Examples [Section titled â€œExamplesâ€](#examples) ### Drop all null fields from the input [Section titled â€œDrop all null fields from the inputâ€](#drop-all-null-fields-from-the-input) ```tql from { src: 192.168.0.4, dst: null, role: "admin", info: { id: null, msg: 8411, }, } drop_null_fields ``` ```tql { src: 192.168.0.4, role: "admin", info: { msg: 8411, }, } ``` ### Drop specific null fields [Section titled â€œDrop specific null fieldsâ€](#drop-specific-null-fields) ```tql from { src: 192.168.0.4, dst: null, role: null, info: { id: null, msg: 8411, }, } drop_null_fields dst, info.id ``` ```tql { src: 192.168.0.4, role: null, info: { msg: 8411, }, } ``` ### Drop null fields within a record field [Section titled â€œDrop null fields within a record fieldâ€](#drop-null-fields-within-a-record-field) When specifying a record field, all null fields within it are removed recursively: ```tql from { metadata: { created: "2024-01-01", updated: null, tags: null, author: "admin" }, data: { value: 42, comment: null } } drop_null_fields metadata ``` ```tql { metadata: { created: "2024-01-01", author: "admin", }, data: { value: 42, comment: null, }, } ``` Note that `data.comment` remains null because only `metadata` was specified. ### Behavior with records inside lists [Section titled â€œBehavior with records inside listsâ€](#behavior-with-records-inside-lists) The `drop_null_fields` operator does not remove fields from records that are inside lists: ```tql from { id: 1, items: [{name: "a", value: 1}, {name: "b", value: null}], metadata: null, tags: ["x", null, "y"] } drop_null_fields ``` ```tql { id: 1, items: [ { name: "a", value: 1, }, { name: "b", value: null, }, ], tags: [ "x", null, "y", ], } ``` In this example: * The `metadata` field is removed because it contains `null` * The `items` field is kept with all its internal structure intact * The `tags` field is kept even though it contains `null` elements * Fields within records inside lists (like `value`) are not dropped even if they contain `null` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`drop`](/reference/operators/drop), [`select`](/reference/operators/select), [`where`](/reference/operators/where)

# enumerate

Add a field with the number of preceding events. ```tql enumerate [out:field, group=any] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `enumerate` operator adds a new field with the number of preceding events to the beginning of the input record. ### `out: field (optional)` [Section titled â€œout: field (optional)â€](#out-field-optional) Sets the name of the output field. Defaults to `"#"`. ### `group: any (optional)` [Section titled â€œgroup: any (optional)â€](#group-any-optional) Groups events by the specified expression and enumerates within each group. When provided, an independent enumeration counter is used for each unique value of the grouping expression. ## Examples [Section titled â€œExamplesâ€](#examples) ### Enumerate the input by prepending row numbers [Section titled â€œEnumerate the input by prepending row numbersâ€](#enumerate-the-input-by-prepending-row-numbers) ```tql from {x: "a"}, {x: "b"}, {x: "c"} enumerate ``` ```tql {"#": 0, x: "a"} {"#": 1, x: "b"} {"#": 2, x: "c"} ``` ### Use a custom field for the row numbers [Section titled â€œUse a custom field for the row numbersâ€](#use-a-custom-field-for-the-row-numbers) ```tql from {x: true}, {x: false} enumerate index ``` ```tql {index: 0, x: true} {index: 1, x: false} ``` ### Count within groups [Section titled â€œCount within groupsâ€](#count-within-groups) ```tql from {x: 1}, {x: 2}, {x: 1}, {x: 2} enumerate count, group=x count = count + 1 ``` ```tql {count: 1, x: 1} {count: 1, x: 2} {count: 2, x: 1} {count: 2, x: 2} ```

# every

Runs a pipeline periodically at a fixed interval. ```tql every interval:duration { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `every` operator repeats running a pipeline indefinitely at a fixed interval. The first run is starts directly when the outer pipeline itself starts. Every `interval`, the executor spawns a new pipeline that runs to completion. If the pipeline runs longer than `interval`, the next run immediately starts. ## Examples [Section titled â€œExamplesâ€](#examples) ### Produce one event per second and enumerate the result [Section titled â€œProduce one event per second and enumerate the resultâ€](#produce-one-event-per-second-and-enumerate-the-result) ```tql every 1s { from {} } enumerate ``` ```tql {"#": 0} // immediately {"#": 1} // after 1s {"#": 2} // after 2s {"#": 3} // after 3s // â€¦ continues like this ``` ### Fetch the results from an API every 10 minutes [Section titled â€œFetch the results from an API every 10 minutesâ€](#fetch-the-results-from-an-api-every-10-minutes) ```tql every 10min { load_http "example.org/api/threats" read_json } publish "threat-feed" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`cron`](/reference/operators/cron)

# export

Retrieves events from a Tenzir node. ```tql export [live=bool, retro=bool, internal=bool, parallel=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `export` operator retrieves events from a Tenzir node. This operator is the dual to [`import`](/reference/operators/import). ### `live = bool (optional)` [Section titled â€œlive = bool (optional)â€](#live--bool-optional) Work on all events that are imported with `import` operators in real-time instead of on events persisted at a Tenzir node. Note that live exports may drop events if the following pipeline fails to keep up. To connect pipelines with back pressure, use the [`publish`](/reference/operators/publish) and [`subscribe`](/reference/operators/subscribe) operators. ### `retro = bool (optional)` [Section titled â€œretro = bool (optional)â€](#retro--bool-optional) Export persistent events at a Tenzir node. Unless `live=true` is given, this is implied. Use `retro=true, live=true` to export past events, and live events afterwards. ### `internal = bool (optional)` [Section titled â€œinternal = bool (optional)â€](#internal--bool-optional) Export internal events, such as metrics or diagnostics, instead. By default, `export` only returns events that were previously imported with `import`. In contrast, `export internal=true` exports internal events such as operator metrics. ### `parallel = int (optional)` [Section titled â€œparallel = int (optional)â€](#parallel--int-optional) The parallel level controls how many worker threads the operator uses at most for querying historical events. Defaults to 3. ## Examples [Section titled â€œExamplesâ€](#examples) ### Export all stored events as JSON [Section titled â€œExport all stored events as JSONâ€](#export-all-stored-events-as-json) ```tql export write_json ``` ### Get a subset of matching events [Section titled â€œGet a subset of matching eventsâ€](#get-a-subset-of-matching-events) ```tql export where src_ip == 1.2.3.4 head 20 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`import`](/reference/operators/import), [`subscribe`](/reference/operators/subscribe)

# fields

Retrieves all fields stored at a node. ```tql fields ``` ## Description [Section titled â€œDescriptionâ€](#description) The `fields` operator shows a list of all fields stored at a node across all available schemas. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the top-5 most frequently used fields across schemas [Section titled â€œGet the top-5 most frequently used fields across schemasâ€](#get-the-top-5-most-frequently-used-fields-across-schemas) ```tql fields summarize field, count=count_distinct(schema), schemas=distinct(schema) sort -count head 5 ```

# files

Shows file information for a given directory. ```tql files [dir:string, recurse=bool, follow_symlinks=bool, skip_permission_denied=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `files` operator shows file information for all files in the given directory. ### `dir: string (optional)` [Section titled â€œdir: string (optional)â€](#dir-string-optional) The directory to list files in. Defaults to the current working directory. ### `recurse = bool (optional)` [Section titled â€œrecurse = bool (optional)â€](#recurse--bool-optional) Recursively list files in subdirectories. ### `follow_symlinks = bool (optional)` [Section titled â€œfollow\_symlinks = bool (optional)â€](#follow_symlinks--bool-optional) Follow directory symlinks. ### `skip_permission_denied = bool (optional)` [Section titled â€œskip\_permission\_denied = bool (optional)â€](#skip_permission_denied--bool-optional) Skip directories that would otherwise result in permission denied errors. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir emits file information with the following schema. ### `tenzir.file` [Section titled â€œtenzir.fileâ€](#tenzirfile) Contains detailed information about the file. | Field | Type | Description | | :---------------- | :------- | :--------------------------------------- | | `path` | `string` | The file path. | | `type` | `string` | The type of the file (see below). | | `permissions` | `record` | The permissions of the file (see below). | | `owner` | `string` | The fileâ€™s owner. | | `group` | `string` | The fileâ€™s group. | | `file_size` | `uint64` | The file size in bytes. | | `hard_link_count` | `uint64` | The number of hard links to the file. | | `last_write_time` | `time` | The time of the last write to the file. | The `type` field can have one of the following values: | Value | Description | | :---------- | :------------------------------ | | `regular` | The file is a regular file. | | `directory` | The file is a directory. | | `symlink` | The file is a symbolic link. | | `block` | The file is a block device. | | `character` | The file is a character device. | | `fifo` | The file is a named IPC pipe. | | `socket` | The file is a named IPC socket. | | `not_found` | The file does not exist. | | `unknown` | The file has an unknown type. | The `permissions` record contains the following fields: | Field | Type | Description | | :------- | :------- | :---------------------------------- | | `owner` | `record` | The file permissions for the owner. | | `group` | `record` | The file permissions for the group. | | `others` | `record` | The file permissions for others. | The `owner`, `group`, and `others` records contain the following fields: | Field | Type | Description | | :-------- | :----- | :------------------------------ | | `read` | `bool` | Whether the file is readable. | | `write` | `bool` | Whether the file is writeable. | | `execute` | `bool` | Whether the file is executable. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the total file size of the current directory [Section titled â€œCompute the total file size of the current directoryâ€](#compute-the-total-file-size-of-the-current-directory) ```tql files recurse=true summarize total_size=sum(file_size) ``` ### Find all named pipes in `/tmp` [Section titled â€œFind all named pipes in /tmpâ€](#find-all-named-pipes-in-tmp) ```tql files "/tmp", recurse=true, skip_permission_denied=true where type == "fifo" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_file`](/reference/operators/load_file), [`processes`](/reference/operators/processes), [`save_file`](/reference/operators/save_file), [`sockets`](/reference/operators/sockets)

# fork

Executes a subpipeline with a copy of the input. ```tql fork { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `fork` operator executes a subpipeline with a copy of its input, that is: whenever an event arrives, it is sent both to the given pipeline and forwarded at the same time to the next operator. ### `{ â€¦ }` [Section titled â€œ{ â€¦ }â€](#-) The pipeline to execute. Must have a sink. ## Examples [Section titled â€œExamplesâ€](#examples) ### Publish incoming events while importing them simultaneously [Section titled â€œPublish incoming events while importing them simultaneouslyâ€](#publish-incoming-events-while-importing-them-simultaneously) ```tql fork { publish "imported-events" } import ```

# from

Obtains events from an URI, inferring the source, compression and format. ```tql from uri:string, [loader_argsâ€¦ { â€¦ }] from eventsâ€¦ ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from` operator is an easy way to get data into Tenzir. It will try to infer the connector, compression and format based on the given URI. Alternatively, it can be used to create events from records. ### `uri: string` [Section titled â€œuri: stringâ€](#uri-string) The URI to load from. ### `loader_argsâ€¦ (optional)` [Section titled â€œloader\_argsâ€¦ (optional)â€](#loader_args-optional) An optional set of arguments passed to the loader. This can be used to e.g. pass credentials to a connector: ```tql from "https://example.org/file.json", headers={Token: "XYZ"} ``` ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) The optional pipeline argument allows for explicitly specifying how `from` decompresses and parses data. By default, the pipeline is inferred based on a set of [rules](#explanation). If inference is not possible, or not sufficient, this argument can be used to control the decompression and parsing. Providing this pipeline disables the inference. [Examples](#load-a-file-with-parser-arguments) ### `eventsâ€¦` [Section titled â€œeventsâ€¦â€](#events) Instead of a URI, you can also provide one or more records, which will be the operators output. This is mostly useful for testing pipelines without loading actual data. ## Explanation [Section titled â€œExplanationâ€](#explanation) Loading a resource into tenzir consists of three steps: * [**Loading**](#loading) the raw bytes * [**Decompressing**](#decompressing) (optional) * [**Reading**](#reading) the bytes as structured data The `from` operator tries to infer all three steps from the given URI. ### Loading [Section titled â€œLoadingâ€](#loading) The connector is inferred based on the URI `scheme://`. See the [URI schemes section](#uri-schemes) for supported schemes. If no scheme is present, the connector attempts to load from the filesystem. ### Decompressing [Section titled â€œDecompressingâ€](#decompressing) The compression is inferred from the â€œfile-endingâ€ in the URI. Under the hood, this uses the [`decompress_*` operators](/reference/operators#encode--decode). Supported compressions can be found in the [list of compression extensions](#compression). The decompression step is optional and will only happen if a compression could be inferred. If you know that the source is compressed and the compression cannot be inferred, you can use the [pipeline argument](#---optional) to specify the decompression manually. ### Reading [Section titled â€œReadingâ€](#reading) The format to read is, just as the compression, inferred from the file-ending. Supported file formats are the common file endings for our [`read_*` operators](/reference/operators#parsing). If you want to provide additional arguments to the parser, you can use the [pipeline argument](#---optional) to specify the parsing manually. This can be useful, if you e.g. know that the input is `suricata` or `ndjson` instead of just plain `json`. ### The pipeline argument & its relation to the loader [Section titled â€œThe pipeline argument & its relation to the loaderâ€](#the-pipeline-argument--its-relation-to-the-loader) Some loaders, such as the [`load_tcp`](/reference/operators/load_tcp) operator, accept a sub-pipeline directly. If the selected loader accepts a sub-pipeline, the `from` operator will dispatch decompression and parsing into that sub-pipeline. If a an explicit pipeline argument is provided it is forwarded as-is. If the loader does not accept a sub-pipeline, the decompression and parsing steps are simply performed as part of the regular pipeline. #### Example transformation: [Section titled â€œExample transformation:â€](#example-transformation) from operator ```tql from "myfile.json.gz" ``` Effective pipeline ```tql load_file "myfile.json.gz" decompress_gzip read_json ``` #### Example with pipeline argument: [Section titled â€œExample with pipeline argument:â€](#example-with-pipeline-argument) from operator ```tql from "tcp://0.0.0.0:12345", parallel=10 { read_gelf } ``` Effective pipeline ```tql load_tcp "tcp://0.0.0.0:12345", parallel=10 { read_gelf } ``` ## Supported Deductions [Section titled â€œSupported Deductionsâ€](#supported-deductions) ### URI schemes [Section titled â€œURI schemesâ€](#uri-schemes) | Scheme | Operator | Example | | :-------------- | :-------------------------------------------------------------------------- | :----------------------------------------------- | | `abfs`,`abfss` | [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage) | `from "abfs://path/to/file.json"` | | `amqp` | [`load_amqp`](/reference/operators/load_amqp) | `from "amqp://â€¦` | | `elasticsearch` | [`from_opensearch`](/reference/operators/from_opensearch) | `from "elasticsearch://1.2.3.4:9200` | | `file` | [`load_file`](/reference/operators/load_file) | `from "file://path/to/file.json"` | | `fluent-bit` | [`from_fluent_bit`](/reference/operators/from_fluent_bit) | `from "fluent-bit://elasticsearch"` | | `ftp`, `ftps` | [`load_ftp`](/reference/operators/load_ftp) | `from "ftp://example.com/file.json"` | | `gcps` | [`load_google_cloud_pubsub`](/reference/operators/load_google_cloud_pubsub) | `from "gcps://project_id/subscription_id" { â€¦ }` | | `gs` | [`load_gcs`](/reference/operators/load_gcs) | `from "gs://bucket/object.json"` | | `http`, `https` | [`load_http`](/reference/operators/load_http) | `from "http://example.com/file.json"` | | `inproc` | [`load_zmq`](/reference/operators/load_zmq) | `from "inproc://127.0.0.1:56789" { read_json }` | | `kafka` | [`load_kafka`](/reference/operators/load_kafka) | `from "kafka://topic" { read_json }` | | `opensearch` | [`from_opensearch`](/reference/operators/from_opensearch) | `from "opensearch://1.2.3.4:9200` | | `s3` | [`load_s3`](/reference/operators/load_s3) | `from "s3://bucket/file.json"` | | `sqs` | [`load_sqs`](/reference/operators/load_sqs) | `from "sqs://my-queue" { read_json }` | | `tcp` | [`load_tcp`](/reference/operators/load_tcp) | `from "tcp://127.0.0.1:13245" { read_json }` | | `udp` | [`load_udp`](/reference/operators/load_udp) | `from "udp://127.0.0.1:56789" { read_json }` | | `zmq` | [`load_zmq`](/reference/operators/load_zmq) | `from "zmq://127.0.0.1:56789" { read_json }` | Please see the respective operator pages for details on the URIâ€™s locator format. ### File extensions [Section titled â€œFile extensionsâ€](#file-extensions) #### Format [Section titled â€œFormatâ€](#format) The `from` operator can deduce the file format based on these file-endings: | Format | File Endings | Operator | | :------ | :------------------- | :-------------------------------------------------- | | CSV | `.csv` | [`read_csv`](/reference/operators/read_csv) | | Feather | `.feather`, `.arrow` | [`read_feather`](/reference/operators/read_feather) | | JSON | `.json` | [`read_json`](/reference/operators/read_json) | | NDJSON | `.ndjson`, `.jsonl` | [`read_ndjson`](/reference/operators/read_ndjson) | | Parquet | `.parquet` | [`read_parquet`](/reference/operators/read_parquet) | | Pcap | `.pcap` | [`read_pcap`](/reference/operators/read_pcap) | | SSV | `.ssv` | [`read_ssv`](/reference/operators/read_ssv) | | TSV | `.tsv` | [`read_tsv`](/reference/operators/read_tsv) | | YAML | `.yaml` | [`read_yaml`](/reference/operators/read_yaml) | #### Compression [Section titled â€œCompressionâ€](#compression) The `from` operator can deduce the following compressions based on these file-endings: | Compression | File Endings | | :---------- | :--------------- | | Brotli | `.br`, `.brotli` | | Bzip2 | `.bz2` | | Gzip | `.gz`, `.gzip` | | LZ4 | `.lz4` | | Zstd | `.zst`, `.zstd` | ## Examples [Section titled â€œExamplesâ€](#examples) ### Load a local file [Section titled â€œLoad a local fileâ€](#load-a-local-file) ```tql from "path/to/my/load/file.csv" ``` ### Load a compressed file [Section titled â€œLoad a compressed fileâ€](#load-a-compressed-file) ```tql from "path/to/my/load/file.json.bz2" ``` ### Load a file with parser arguments [Section titled â€œLoad a file with parser argumentsâ€](#load-a-file-with-parser-arguments) Provide an explicit header to the CSV parser: ```tql from "path/to/my/load/file.csv.bz2" { decompress_brotli // this is now necessary due to the pipeline argument read_csv header="col1,col2,col3" } ``` ### Pick a more suitable parser [Section titled â€œPick a more suitable parserâ€](#pick-a-more-suitable-parser) The file `eve.json` contains Suricata logs, but the `from` operator does not know this. We provide an explicit `read_suricata` instead: ```tql from "path/to/my/load/eve.json" { read_suricata } ``` ### Load from HTTP with a header [Section titled â€œLoad from HTTP with a headerâ€](#load-from-http-with-a-header) ```tql from "https://example.org/file.json", headers={Token: "1234"} ``` ### Create events from records [Section titled â€œCreate events from recordsâ€](#create-events-from-records) ```tql from {message: "Value", endpoint: {ip: 127.0.0.1, port: 42}}, {message: "Value", endpoint: {ip: 127.0.0.1, port: 42}, raw: "text"}, {message: "Value", endpoint: null} ``` ```tql { message: "Value", endpoint: { ip: 127.0.0.1, port: 42 } } { message: "Value", endpoint: { ip: 127.0.0.1, port: 42 }, raw: "text" } { message: "Value", endpoint: null } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`from_file`](/reference/operators/from_file), [`to`](/reference/operators/to)

# from_azure_blob_storage

Reads one or multiple files from Azure Blob Storage. ```tql from_azure_blob_storage url:string, [account_key=string, watch=bool, remove=bool, rename=string->string, path_field=field] { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_azure_blob_storage` operator reads files from Azure Blob Storage, with support for glob patterns, automatic format detection, and file monitoring. By default, authentication is handled by the Azure SDKâ€™s credential chain which may read from multiple environment variables, such as: * `AZURE_TENANT_ID` * `AZURE_CLIENT_ID` * `AZURE_CLIENT_SECRET` * `AZURE_AUTHORITY_HOST` * `AZURE_CLIENT_CERTIFICATE_PATH` * `AZURE_FEDERATED_TOKEN_FILE` ### `url: string` [Section titled â€œurl: stringâ€](#url-string) URL identifying the Azure Blob Storage location where data should be read from. The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `container/**/data` matches `container/data`. Supported URI formats: 1. `abfs[s]://<account>.blob.core.windows.net[/<container>[/<path>]]` 2. `abfs[s]://<container>@<account>.dfs.core.windows.net[/<path>]` 3. `abfs[s]://[<account>@]<host>[.<domain>][:<port>][/<container>[/<path>]]` 4. `abfs[s]://[<account>@]<container>[/<path>]` (1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2). ### `account_key = string (optional)` [Section titled â€œaccount\_key = string (optional)â€](#account_key--string-optional) Account key for authenticating with Azure Blob Storage. ### `watch = bool (optional)` [Section titled â€œwatch = bool (optional)â€](#watch--bool-optional) In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s. Defaults to `false`. ### `remove = bool (optional)` [Section titled â€œremove = bool (optional)â€](#remove--bool-optional) Deletes files after they have been read completely. Defaults to `false`. ### `rename = string -> string (optional)` [Section titled â€œrename = string -> string (optional)â€](#rename--string---string-optional) Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path. If the target path already exists, the operator will overwrite the file. The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path. ### `path_field = field (optional)` [Section titled â€œpath\_field = field (optional)â€](#path_field--field-optional) This makes the operator insert the path to the file where an event originated from before emitting it. By default, paths will not be inserted into the outgoing events. ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read every JSON file from a container [Section titled â€œRead every JSON file from a containerâ€](#read-every-json-file-from-a-container) ```tql from_azure_blob_storage "abfs://my-container/data/**.json" ``` ### Read CSV files using account key authentication [Section titled â€œRead CSV files using account key authenticationâ€](#read-csv-files-using-account-key-authentication) ```tql from_azure_blob_storage "abfs://container/data.csv", account_key="your-account-key" ``` ### Read Suricata EVE JSON logs continuously [Section titled â€œRead Suricata EVE JSON logs continuouslyâ€](#read-suricata-eve-json-logs-continuously) ```tql from_azure_blob_storage "abfs://logs/suricata/**.json", watch=true { read_suricata } ``` ### Process files and move them to an archive container [Section titled â€œProcess files and move them to an archive containerâ€](#process-files-and-move-them-to-an-archive-container) ```tql from_azure_blob_storage "abfs://input/**.json", rename=(path => "/archive/" + path) ``` ### Add source path to events [Section titled â€œAdd source path to eventsâ€](#add-source-path-to-events) ```tql from_azure_blob_storage "abfs://data/**.json", path_field=source_file ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`from_file`](/reference/operators/from_file), [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage), [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage)

# from_file

Reads one or multiple files from a filesystem. ```tql from_file url:string, [watch=bool, remove=bool, rename=string->string, path_field=field] { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_file` operator reads files from local filesystems or cloud storage, with support for glob patterns, automatic format detection, and file monitoring. ### `url: string` [Section titled â€œurl: stringâ€](#url-string) URL or local filesystem path where data should be read from. The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `foo/**/bar` matches `foo/bar`. The URL can include additional options. For `s3://`, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`. For `gs://`, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`. ### `watch = bool (optional)` [Section titled â€œwatch = bool (optional)â€](#watch--bool-optional) In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s. Defaults to `false`. ### `remove = bool (optional)` [Section titled â€œremove = bool (optional)â€](#remove--bool-optional) Deletes files after they have been read completely. Defaults to `false`. ### `rename = string -> string (optional)` [Section titled â€œrename = string -> string (optional)â€](#rename--string---string-optional) Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path. If the target path already exists, the operator will overwrite the file. The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path. ### `path_field = field (optional)` [Section titled â€œpath\_field = field (optional)â€](#path_field--field-optional) This makes the operator insert the path to the file where an event originated from before emitting it. By default, paths will not be inserted into the outgoing events. ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. This is using the same logic as [`from`](/reference/operators/from). ## Examples [Section titled â€œExamplesâ€](#examples) ### Read every `.csv` file from S3 [Section titled â€œRead every .csv file from S3â€](#read-every-csv-file-from-s3) ```tql from_file "s3://my-bucket/**.csv" ``` ### Read every `.json` file in `/data` as Suricata EVE JSON [Section titled â€œRead every .json file in /data as Suricata EVE JSONâ€](#read-every-json-file-in-data-as-suricata-eve-json) ```tql from_file "/data/**.json" { read_suricata } ``` ### Read all files from S3 continuously and delete them afterwards [Section titled â€œRead all files from S3 continuously and delete them afterwardsâ€](#read-all-files-from-s3-continuously-and-delete-them-afterwards) ```tql from_file "s3://my-bucket/**", watch=true, remove=true ``` ### Move files to a directory, preserving filenames [Section titled â€œMove files to a directory, preserving filenamesâ€](#move-files-to-a-directory-preserving-filenames) ```tql // The trailing slash automatically appends the original filename from_file "/input/*.json", rename=path => "/output/" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`from`](/reference/operators/from), [`load_file`](/reference/operators/load_file)

# from_fluent_bit

Receives events via Fluent Bit. ```tql from_fluent_bit plugin:string, [options=record, fluent_bit_options=record, schema=string, selector=string, schema_only=bool, merge=bool, raw=bool, unflatten=string, tls=bool, cacert=string, certfile=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_fluent_bit` operator acts as a bridge into the [Fluent Bit](https://docs.fluentbit.io) ecosystem, making it possible to acquire events from a Fluent Bit [input plugin](https://docs.fluentbit.io/manual/pipeline/inputs). An invocation of the `fluent-bit` commandline utility ```bash fluent-bit -o plugin -p key1=value1 -p key2=value2 -pâ€¦ ``` translates to our `from_fluent_bit` operator as follows: ```tql from_fluent_bit "plugin", options={key1: value1, key2: value2, â€¦} ``` ### `plugin: string` [Section titled â€œplugin: stringâ€](#plugin-string) The name of the Fluent Bit plugin. Run `fluent-bit -h` and look under the **Inputs** section of the help text for available plugin names. The web documentation often comes with an example invocation near the bottom of the page, which also provides a good idea how you could use the operator. ### `options = record (optional)` [Section titled â€œoptions = record (optional)â€](#options--record-optional) Sets plugin configuration properties. The key-value pairs in this record are equivalent to `-p key=value` for the `fluent-bit` executable. ### `fluent_bit_options = record (optional)` [Section titled â€œfluent\_bit\_options = record (optional)â€](#fluent_bit_options--record-optional) Sets global properties of the Fluent Bit service., e.g., `fluent_bit_options={flush:1, grace:3}`. Consult the list of available [key-value pairs](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file#config_section) to configure Fluent Bit according to your needs. We recommend factoring these options into the plugin-specific `fluent-bit.yaml` so that they are independent of the `fluent-bit` operator arguments. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## URI support & integration with `from` [Section titled â€œURI support & integration with fromâ€](#uri-support--integration-with-from) The `from_fluent_bit` operator can also be used from the [`from`](/reference/operators/from) operator. For this, the `fluentbit://` scheme can be used. The URI is then translated: ```tql from "fluentbit://plugin" ``` ```tql from_fluent_bit "plugin" ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### OpenTelemetry [Section titled â€œOpenTelemetryâ€](#opentelemetry) Ingest [OpenTelemetry](https://docs.fluentbit.io/manual/pipeline/inputs/slack) logs, metrics, and traces: ```tql from_fluent_bit "opentelemetry" ``` You can then send JSON-encoded log data to a freshly created API endpoint: ```bash curl \ --header "Content-Type: application/json" \ --request POST \ --data '{"resourceLogs":[{"resource":{},"scopeLogs":[{"scope":{},"logRecords":[{"timeUnixNano":"1660296023390371588","body":{"stringValue":"{\"message\":\"dummy\"}"},"traceId":"","spanId":""}]}]}]}' \ http://0.0.0.0:4318/v1/logs ``` ### Splunk [Section titled â€œSplunkâ€](#splunk) Handle [Splunk](https://docs.fluentbit.io/manual/pipeline/inputs/splunk) HEC requests: ```tql from_fluent_bit "splunk", options={port: 8088} ```

# from_gcs

Reads one or multiple files from Google Cloud Storage. ```tql from_gcs url:string, [anonymous=bool, watch=bool, remove=bool, rename=string->string, path_field=field] { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_gcs` operator reads files from Google Cloud Storage, with support for glob patterns, automatic format detection, and file monitoring. By default, authentication is handled by Googleâ€™s Application Default Credentials (ADC) chain, which may read from multiple sources: * `GOOGLE_APPLICATION_CREDENTIALS` environment variable pointing to a service account key file * User credentials from `gcloud auth application-default login` * Service account attached to the compute instance (Compute Engine, GKE) * Google Cloud SDK credentials ### `url: string` [Section titled â€œurl: stringâ€](#url-string) URL identifying the Google Cloud Storage location where data should be read from. The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `bucket/**/data` matches `bucket/data`. The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist: > For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`. ### `anonymous = bool (optional)` [Section titled â€œanonymous = bool (optional)â€](#anonymous--bool-optional) Use anonymous credentials instead of any configured authentication. This only works for publicly readable buckets and objects. Defaults to `false`. ### `watch = bool (optional)` [Section titled â€œwatch = bool (optional)â€](#watch--bool-optional) In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s. Defaults to `false`. ### `remove = bool (optional)` [Section titled â€œremove = bool (optional)â€](#remove--bool-optional) Deletes files after they have been read completely. Defaults to `false`. ### `rename = string -> string (optional)` [Section titled â€œrename = string -> string (optional)â€](#rename--string---string-optional) Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path. If the target path already exists, the operator will overwrite the file. The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path. ### `path_field = field (optional)` [Section titled â€œpath\_field = field (optional)â€](#path_field--field-optional) This makes the operator insert the path to the file where an event originated from before emitting it. By default, paths will not be inserted into the outgoing events. ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read every JSON file from a bucket [Section titled â€œRead every JSON file from a bucketâ€](#read-every-json-file-from-a-bucket) ```tql from_gcs "gs://my-bucket/data/**.json" ``` ### Read CSV files from a public bucket [Section titled â€œRead CSV files from a public bucketâ€](#read-csv-files-from-a-public-bucket) ```tql from_gcs "gs://public-dataset/data.csv", anonymous=true ``` ### Read Zeek logs continuously [Section titled â€œRead Zeek logs continuouslyâ€](#read-zeek-logs-continuously) ```tql from_gcs "gs://logs/zeek/**.log", watch=true { read_zeek_tsv } ``` ### Add source path to events [Section titled â€œAdd source path to eventsâ€](#add-source-path-to-events) ```tql from_gcs "gs://data-bucket/**.json", path_field=source_file ``` ### Read Suricata EVE JSON logs with custom parsing [Section titled â€œRead Suricata EVE JSON logs with custom parsingâ€](#read-suricata-eve-json-logs-with-custom-parsing) ```tql from_gcs "gs://security-logs/suricata/**.json" { read_suricata } ```

# from_http

Sends and receives HTTP/1.1 requests. ```tql from_http url:string, [method=string, body=record|string|blob, encode=string, headers=record, metadata_field=field, error_field=field, paginate=record->string, paginate_delay=duration, connection_timeout=duration, max_retry_count=int, retry_delay=duration, tls=bool, certfile=string, keyfile=string, password=string { â€¦ }] from_http url:string, server=true, [metadata_field=field, responses=record, max_request_size=int, tls=bool, certfile=string, keyfile=string, password=string { â€¦ }] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_http` operator issues HTTP requests or spins up an HTTP/1.1 server on a given address and forwards received requests as events. ### `url: string` [Section titled â€œurl: stringâ€](#url-string) URL to listen on or to connect to. Must have the form `<host>:<port>` when `server=true`. ### `method = string (optional)` [Section titled â€œmethod = string (optional)â€](#method--string-optional) One of the following HTTP methods to use when using the client: * `get` * `head` * `post` * `put` * `del` * `connect` * `options` * `trace` Defaults to `get`, or `post` if `body` is specified. ### `body = blob|record|string (optional)` [Section titled â€œbody = blob|record|string (optional)â€](#body--blobrecordstring-optional) Body to send with the HTTP request. If the value is a `record`, then the body is encoded according to the `encode` option and an appropriate `Content-Type` is set for the request. ### `encode = string (optional)` [Section titled â€œencode = string (optional)â€](#encode--string-optional) Specifies how to encode `record` bodies. Supported values: * `json` * `form` Defaults to `json`. ### `headers = record (optional)` [Section titled â€œheaders = record (optional)â€](#headers--record-optional) Record of headers to send with the request. ### `metadata_field = field (optional)` [Section titled â€œmetadata\_field = field (optional)â€](#metadata_field--field-optional) Field to insert metadata into when using the parsing pipeline. The response metadata (when using the client mode) has the following schema: | Field | Type | Description | | :-------- | :------- | :------------------------------------ | | `code` | `uint64` | The HTTP status code of the response. | | `headers` | `record` | The response headers. | The request metadata (when using the server mode) has the following schema: | Field | Type | Description | | :--------- | :------- | :----------------------------------- | | `headers` | `record` | The request headers. | | `query` | `record` | The query parameters of the request. | | `path` | `string` | The path requested. | | `fragment` | `string` | The URI fragment of the request. | | `method` | `string` | The HTTP method of the request. | | `version` | `string` | The HTTP version of the request. | ### `error_field = field (optional)` [Section titled â€œerror\_field = field (optional)â€](#error_field--field-optional) Field to insert the response body for HTTP error responses (status codes not in the 2xx or 3xx range). When set, any HTTP response with a status code outside the 200â€“399 range will have its body stored in this field as a `blob`. Otherwise, error responses, alongside the original event, are skipped and an error is emitted. ### `paginate = record -> string (optional)` [Section titled â€œpaginate = record -> string (optional)â€](#paginate--record---string-optional) A lambda expression to evaluate against the result of the request (optionally parsed by the given pipeline). If the expression evaluation is successful and non-null, the resulting string is used as the URL for a new GET request with the same headers. ### `paginate_delay = duration (optional)` [Section titled â€œpaginate\_delay = duration (optional)â€](#paginate_delay--duration-optional) The duration to wait between consecutive pagination requests. Defaults to `0s`. ### `connection_timeout = duration (optional)` [Section titled â€œconnection\_timeout = duration (optional)â€](#connection_timeout--duration-optional) Timeout for the connection. Defaults to `5s`. ### `max_retry_count = int (optional)` [Section titled â€œmax\_retry\_count = int (optional)â€](#max_retry_count--int-optional) The maximum times to retry a failed request. Every request has its own retry count. Defaults to `0`. ### `retry_delay = duration (optional)` [Section titled â€œretry\_delay = duration (optional)â€](#retry_delay--duration-optional) The duration to wait between each retry. Defaults to `1s`. ### `server = bool (optional)` [Section titled â€œserver = bool (optional)â€](#server--bool-optional) Whether to spin up an HTTP server or act as an HTTP client. Defaults to `false`, i.e., the HTTP client. ### `responses = record (optional)` [Section titled â€œresponses = record (optional)â€](#responses--record-optional) Specify custom responses for endpoints on the server. For example, ```tql responses = { "/resource/create": { code: 200, content_type: "text/html", body: "Created!" }, "/resource/delete": { code: 401, content_type: "text/html", body: "Unauthorized!" } } ``` creates two special routes on the server with different responses. Requests to an unspecified endpoint are responded with HTTP Status `200 OK`. ### `max_request_size = int (optional)` [Section titled â€œmax\_request\_size = int (optional)â€](#max_request_size--int-optional) The maximum size of an incoming request to accept. Defaults to `10MiB`. ### `tls = bool (optional)` [Section titled â€œtls = bool (optional)â€](#tls--bool-optional) Enables TLS. Defaults to `false`. ### `certfile = string (optional)` [Section titled â€œcertfile = string (optional)â€](#certfile--string-optional) Path to the client certificate. Required for server if `tls` is `true`. ### `keyfile = string (optional)` [Section titled â€œkeyfile = string (optional)â€](#keyfile--string-optional) Path to the key for the client certificate. Required for server if `tls` is `true`. ### `password = string (optional)` [Section titled â€œpassword = string (optional)â€](#password--string-optional) Password for keyfile. ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) A pipeline that receives the response body as bytes, allowing parsing per request. This is especially useful in scenarios where the response body can be parsed into multiple events. If not provided, the operator will attempt to infer the parsing operator from the `Content-Type` header. Should this inference fail (e.g., unsupported or missing `Content-Type`), the operator raises an error. ## Examples [Section titled â€œExamplesâ€](#examples) ### Make a GET request [Section titled â€œMake a GET requestâ€](#make-a-get-request) Make a request to [urlscan.io](https://urlscan.io/docs/api#search) to search for scans for `tenzir.com` and get the first result. ```tql from_http "https://urlscan.io/api/v1/search?q=tenzir.com" unroll results head 1 ``` ```tql { results: { submitter: { ... }, task: { ... }, stats: { ... }, page: { ... }, _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30", _score: null, sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ], result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/", screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png", }, total: 9, took: 296, has_more: false, } ``` ### Paginated API Requests [Section titled â€œPaginated API Requestsâ€](#paginated-api-requests) Use the `paginate` parameter to handle paginated APIs: ```tql from_http "https://api.example.com/data", paginate=(x => x.next_url?) ``` This sends a GET request to the initial URL and evaluates the `x.next_url` field in the response to determine the next URL for subsequent requests. ### Retry Failed Requests [Section titled â€œRetry Failed Requestsâ€](#retry-failed-requests) Configure retries for failed requests: ```tql from_http "https://api.example.com/data", max_retry_count=3, retry_delay=2s ``` This tries up to 3 times, waiting 2 seconds between each retry. ### Listen on port 8080 [Section titled â€œListen on port 8080â€](#listen-on-port-8080) Spin up a server with: ```tql from_http "0.0.0.0:8080", server=true, metadata_field=metadata ``` Send a request to the HTTP endpoint via `curl`: ```sh echo '{"key": "value"}' | gzip | curl localhost:8080 --data-binary @- -H 'Content-Encoding: gzip' -H 'Content-Type: application/json' ``` Observe the request in the Tenzir pipeline, parsed and decompressed: ```tql { key: "value", metadata: { headers: { Host: "localhost:8080", "User-Agent": "curl/8.13.0", Accept: "*/*", "Content-Encoding": "gzip", "Content-Length": "37", "Content-Type": "application/json", }, path: "/", method: "post", version: "HTTP/1.1", }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`http`](/reference/operators/http), [`serve`](/reference/operators/serve)

# from_opensearch

Receives events via Opensearch Bulk API. ```tql from_opensearch [url:string, keep_actions=bool, max_request_size=int, tls=bool, certfile=string, keyfile=string, password=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_opensearch` operator emulates simple situations for the [Opensearch Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/). ### `url: string (optional)` [Section titled â€œurl: string (optional)â€](#url-string-optional) URL to listen on. Must have the form `host[:port]`. Defaults to `"0.0.0.0:9200"`. ### `keep_actions = bool (optional)` [Section titled â€œkeep\_actions = bool (optional)â€](#keep_actions--bool-optional) Whether to keep the command objects such as `{"create": ...}`. Defaults to `false`. ### `max_request_size = int (optional)` [Section titled â€œmax\_request\_size = int (optional)â€](#max_request_size--int-optional) The maximum size of an incoming request to accept. Defaults to `10Mib`. ### `tls = bool (optional)` [Section titled â€œtls = bool (optional)â€](#tls--bool-optional) Enables TLS. Defaults to `false`. ### `certfile = string (optional)` [Section titled â€œcertfile = string (optional)â€](#certfile--string-optional) Path to the client certificate. Required if `tls` is `true`. ### `keyfile = string (optional)` [Section titled â€œkeyfile = string (optional)â€](#keyfile--string-optional) Path to the key for the client certificate. Required if `tls` is `true`. ### `password = string (optional)` [Section titled â€œpassword = string (optional)â€](#password--string-optional) Password for keyfile. ## Examples [Section titled â€œExamplesâ€](#examples) ### Listen on port 8080 on an interface with IP 1.2.3.4 [Section titled â€œListen on port 8080 on an interface with IP 1.2.3.4â€](#listen-on-port-8080-on-an-interface-with-ip-1234) ```tql from_opensearch "1.2.3.4:8080" ``` ### Listen with TLS [Section titled â€œListen with TLSâ€](#listen-with-tls) ```tql from_opensearch tls=true, certfile="server.crt", keyfile="private.key" ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`to_opensearch`](/reference/operators/to_opensearch)

# from_s3

Reads one or multiple files from Amazon S3. ```tql from_s3 url:string, [anonymous=bool, access_key=string, secret_key=string, session_token=string, role=string, external_id=string, watch=bool, remove=bool, rename=string->string, path_field=field] { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_s3` operator reads files from Amazon S3, with support for glob patterns, automatic format detection, and file monitoring. By default, authentication is handled by AWSâ€™s default credentials provider chain, which may read from multiple environment variables and credential files: * `~/.aws/credentials` and `~/.aws/config` * `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` * `AWS_SESSION_TOKEN` * EC2 instance metadata service * ECS container credentials ### `url: string` [Section titled â€œurl: stringâ€](#url-string) URL identifying the S3 location where data should be read from. The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `bucket/**/data` matches `bucket/data`. Supported URI format: `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)` Options can be appended to the path as query parameters: * `region`: AWS region (e.g., `us-east-1`) * `scheme`: Connection scheme (`http` or `https`) * `endpoint_override`: Custom S3-compatible endpoint * `allow_bucket_creation`: Allow creating buckets if they donâ€™t exist * `allow_bucket_deletion`: Allow deleting buckets ### `anonymous = bool (optional)` [Section titled â€œanonymous = bool (optional)â€](#anonymous--bool-optional) Use anonymous credentials instead of any configured authentication. Defaults to `false`. ### `access_key = string (optional)` [Section titled â€œaccess\_key = string (optional)â€](#access_key--string-optional) AWS access key ID for authentication. ### `secret_key = string (optional)` [Section titled â€œsecret\_key = string (optional)â€](#secret_key--string-optional) AWS secret access key for authentication. Required if `access_key` is provided. ### `session_token = string (optional)` [Section titled â€œsession\_token = string (optional)â€](#session_token--string-optional) AWS session token for temporary credentials. ### `role = string (optional)` [Section titled â€œrole = string (optional)â€](#role--string-optional) IAM role to assume when accessing S3. ### `external_id = string (optional)` [Section titled â€œexternal\_id = string (optional)â€](#external_id--string-optional) External ID to use when assuming the specified `role`. ### `watch = bool (optional)` [Section titled â€œwatch = bool (optional)â€](#watch--bool-optional) In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s. Defaults to `false`. ### `remove = bool (optional)` [Section titled â€œremove = bool (optional)â€](#remove--bool-optional) Deletes files after they have been read completely. Defaults to `false`. ### `rename = string -> string (optional)` [Section titled â€œrename = string -> string (optional)â€](#rename--string---string-optional) Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path. If the target path already exists, the operator will overwrite the file. The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path. ### `path_field = field (optional)` [Section titled â€œpath\_field = field (optional)â€](#path_field--field-optional) This makes the operator insert the path to the file where an event originated from before emitting it. By default, paths will not be inserted into the outgoing events. ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read every JSON file from a bucket [Section titled â€œRead every JSON file from a bucketâ€](#read-every-json-file-from-a-bucket) ```tql from_s3 "s3://my-bucket/data/**.json" ``` ### Read CSV files using explicit credentials [Section titled â€œRead CSV files using explicit credentialsâ€](#read-csv-files-using-explicit-credentials) ```tql from_s3 "s3://my-bucket/data.csv", access_key=secret("AWS_ACCESS_KEY"), secret_key=secret("AWS_SECRET_KEY") ``` ### Read from S3-compatible service with custom endpoint [Section titled â€œRead from S3-compatible service with custom endpointâ€](#read-from-s3-compatible-service-with-custom-endpoint) ```tql from_s3 "s3://my-bucket/data/**.json?endpoint_override=minio.example.com:9000&scheme=http" ``` ### Read files continuously and assume IAM role [Section titled â€œRead files continuously and assume IAM roleâ€](#read-files-continuously-and-assume-iam-role) ```tql from_s3 "s3://logs/application/**.json", watch=true, role="arn:aws:iam::123456789012:role/LogReaderRole" ``` ### Process files and move them to an archive bucket [Section titled â€œProcess files and move them to an archive bucketâ€](#process-files-and-move-them-to-an-archive-bucket) ```tql from_s3 "s3://input-bucket/**.json", rename=(path => "archive/" + path) ``` ### Add source path to events [Section titled â€œAdd source path to eventsâ€](#add-source-path-to-events) ```tql from_s3 "s3://data-bucket/**.json", path_field=source_file ``` ### Read Zeek logs with anonymous access [Section titled â€œRead Zeek logs with anonymous accessâ€](#read-zeek-logs-with-anonymous-access) ```tql from_s3 "s3://public-bucket/zeek/**.log", anonymous=true { read_zeek_tsv } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`from_file`](/reference/operators/from_file), [`load_s3`](/reference/operators/load_s3), [`save_s3`](/reference/operators/save_s3)

# from_udp

Receives UDP datagrams and outputs structured events. ```tql from_udp endpoint:string, [resolve_hostnames=bool], [binary=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Listens for UDP datagrams on the specified endpoint and outputs each datagram as a structured event containing the data and peer information. Unlike [`load_udp`](/reference/operators/load_udp), which outputs raw bytes, `from_udp` produces structured events with metadata about the sender. ### `endpoint: string` [Section titled â€œendpoint: stringâ€](#endpoint-string) The address to listen on. Must be of the format: `[udp://]host:port`. Use `0.0.0.0` as the host to accept datagrams on all interfaces. The [`nics`](/reference/operators/nics) operator lists all available interfaces. ### `resolve_hostnames = bool (optional)` [Section titled â€œresolve\_hostnames = bool (optional)â€](#resolve_hostnames--bool-optional) Perform DNS lookups to resolve hostnames for sender IP addresses. Defaults to `false` since DNS lookups can be slow and may impact performance when receiving many datagrams. ### `binary = bool (optional)` [Section titled â€œbinary = bool (optional)â€](#binary--bool-optional) Output datagram data as binary (`blob`) instead of text (`string`). Defaults to `false`. When `false`, the data field contains a UTF-8 string. When `true`, the data field contains raw bytes as a blob. ## Output Schema [Section titled â€œOutput Schemaâ€](#output-schema) Each UDP datagram produces one event with the following structure: ```json { "data": <string|blob>, // string by default, blob when binary=true "peer": { "ip": <ip>, "port": <uint64>, "hostname": <string> // Does not exist when `resolve_hostnames=false` } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Receive UDP datagrams with sender information [Section titled â€œReceive UDP datagrams with sender informationâ€](#receive-udp-datagrams-with-sender-information) ```tql from_udp "0.0.0.0:1234" ``` This might output events like: ```json { "data": "Hello World", "peer": { "ip": "192.168.1.10", "port": 5678 } } ``` ### Parse JSON data from UDP datagrams [Section titled â€œParse JSON data from UDP datagramsâ€](#parse-json-data-from-udp-datagrams) ```tql from_udp "127.0.0.1:8080" select data = data.parse_json() ``` ### Filter by sender and decode data [Section titled â€œFilter by sender and decode dataâ€](#filter-by-sender-and-decode-data) ```tql from_udp "0.0.0.0:9999" where peer.ip == 192.168.1.100 select data ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_udp`](/reference/operators/load_udp), [`save_udp`](/reference/operators/save_udp)

# from_velociraptor

Submits VQL to a Velociraptor server and returns the response as events. ```tql from_velociraptor [request_name=string, org_id=string, max_rows=int, subscribe=string, query=string, max_wait=duration, profile=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `from_velociraptor` input operator provides a request-response interface to a [Velociraptor](https://docs.velociraptor.app) server: ![Velociraptor](/_astro/velociraptor.excalidraw.DqyMhvp8_19DKCs.svg) The pipeline operator is the client and it establishes a connection to a Velociraptor server. The client request contains a query written in the [Velociraptor Query Language (VQL)](https://docs.velociraptor.app/docs/vql), a SQL-inspired language with a `SELECT .. FROM .. WHERE` structure. You can either send a raw VQL query via `from_velociraptor query "<vql>"` to a server and processs the response, or hook into a continuous feed of artifacts via `from_velociraptor subscribe <artifact>`. Whenever a hunt runs that contains this artifact, the server will forward it to the pipeline and emit the artifact payload in the response field `HuntResults`. All Velociraptor client-to-server communication is mutually authenticated and encrypted via TLS certificates. This means you must provide client-side certificate, which you can generate as follows. (Velociraptor ships as a static binary that we refer to as `velociraptor-binary` here.) 1. Create a server configuration `server.yaml`: ```bash velociraptor-binary config generate > server.yaml ``` 2. Create an API client: ```bash velociraptor-binary -c server.yaml config api_client name tenzir client.yaml ``` Copy the generated `client.yaml` to your Tenzir plugin configuration directory as `velociraptor.yaml` so that the operator can find it: ```bash cp client.yaml /etc/tenzir/plugin/velociraptor.yaml ``` 3. Run the frontend with the server configuration: ```bash velociraptor-binary -c server.yaml frontend ``` Now you are ready to run VQL queries! ### `request_name = string (optional)` [Section titled â€œrequest\_name = string (optional)â€](#request_name--string-optional) An identifier for the request to the Velociraptor server. Defaults to a randoum UUID. ### `org_id = string (optional)` [Section titled â€œorg\_id = string (optional)â€](#org_id--string-optional) The ID of the Velociraptor organization. Defaults to `"root"`. ### `query = string (optional)` [Section titled â€œquery = string (optional)â€](#query--string-optional) The [VQL](https://docs.velociraptor.app/docs/vql) query string. ### `max_rows = int (optional)` [Section titled â€œmax\_rows = int (optional)â€](#max_rows--int-optional) The maxium number of rows to return in a the stream gRPC messages returned by the server. Defaults to `1000`. ### `subscribe = string (optional)` [Section titled â€œsubscribe = string (optional)â€](#subscribe--string-optional) Subscribes to a flow artifact. This option generates a larger VQL expression under the hood that creates one event per flow and artifact. The response contains a field `HuntResult` that contains the result of the hunt. ### `max_wait = duration (optional)` [Section titled â€œmax\_wait = duration (optional)â€](#max_wait--duration-optional) Controls how long to wait before releasing a partial result set. Defaults to `1s`. ### `profile = string (optional)` [Section titled â€œprofile = string (optional)â€](#profile--string-optional) Specifies the configuration profile for the Velociraptor instance. This enables connecting to multiple Velociraptor instances from the same Tenzir node. To use profiles, edit your `velociraptor.yaml` configuration like this, where `<config>` refers to the contents of the configuration file created by Velociraptor, and `<profile>` to the desired profile name. ```yaml --- title: before --- <config> --- title: after --- profiles: <profile>: <config> ``` If profiles are defined, the operator defaults to the first profile. ## Examples [Section titled â€œExamplesâ€](#examples) ### Show all processes [Section titled â€œShow all processesâ€](#show-all-processes) ```tql from_velociraptor query="select * from pslist()" ``` ### Subscribe to a hunt flow containing the `Windows` artifact [Section titled â€œSubscribe to a hunt flow containing the Windows artifactâ€](#subscribe-to-a-hunt-flow-containing-the-windows-artifact) ```tql from_velociraptor subscribe="Windows" ```

# head

Limits the input to the first `n` events. ```tql head [n:int] ``` ## Description [Section titled â€œDescriptionâ€](#description) Forwards the first `n` events and discards the rest. `head n` is a shorthand notation for [`slice end=n`](/reference/operators/slice). ### `n: int (optional)` [Section titled â€œn: int (optional)â€](#n-int-optional) The number of events to keep. Defaults to `10`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the first 10 events [Section titled â€œGet the first 10 eventsâ€](#get-the-first-10-events) ```tql head ``` ### Get the first 5 events [Section titled â€œGet the first 5 eventsâ€](#get-the-first-5-events) ```tql head 5 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`slice`](/reference/operators/slice), [`tail`](/reference/operators/tail)

# http

Sends HTTP/1.1 requests and forwards the response. ```tql http url:string, [method=string, body=record|string|blob, encode=string, headers=record, response_field=field, metadata_field=field, error_field=field, paginate=record->string, paginate_delay=duration, parallel=int, tls=bool, certfile=string, keyfile=string, password=string, connection_timeout=duration, max_retry_count=int, retry_delay=duration { â€¦ }] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `http` operator issues HTTP/1.1 requests and forwards received responses as events. ### `url: string` [Section titled â€œurl: stringâ€](#url-string) URL to connect to. ### `method = string (optional)` [Section titled â€œmethod = string (optional)â€](#method--string-optional) One of the following HTTP methods to use when using the client: * `get` * `head` * `post` * `put` * `del` * `connect` * `options` * `trace` Defaults to `get`, or `post` if `body` is specified. ### `body = blob|record|string (optional)` [Section titled â€œbody = blob|record|string (optional)â€](#body--blobrecordstring-optional) Body to send with the HTTP request. If the value is a `record`, then the body is encoded according to the `encode` option and an appropriate `Content-Type` is set for the request. ### `encode = string (optional)` [Section titled â€œencode = string (optional)â€](#encode--string-optional) Specifies how to encode `record` bodies. Supported values: * `json` * `form` Defaults to `json`. ### `headers = record (optional)` [Section titled â€œheaders = record (optional)â€](#headers--record-optional) Record of headers to send with the request. ### `response_field = field (optional)` [Section titled â€œresponse\_field = field (optional)â€](#response_field--field-optional) Field to insert the response into. Defaults to `this`. ### `metadata_field = field (optional)` [Section titled â€œmetadata\_field = field (optional)â€](#metadata_field--field-optional) Field to insert metadata into when using the parsing pipeline. The metadata has the following schema: | Field | Type | Description | | :-------- | :------- | :------------------------------------ | | `code` | `uint64` | The HTTP status code of the response. | | `headers` | `record` | The response headers. | ### `error_field = field (optional)` [Section titled â€œerror\_field = field (optional)â€](#error_field--field-optional) Field to insert the response body for HTTP error responses (status codes not in the 2xx or 3xx range). When set, any HTTP response with a status code outside the 200â€“399 range will have its body stored in this field as a `blob`. Otherwise, error responses, alongside the original event, are skipped and a warning is emitted. ### `paginate = record -> string (optional)` [Section titled â€œpaginate = record -> string (optional)â€](#paginate--record---string-optional) A lambda expression to evaluate against the result of the request (optionally parsed by the given pipeline). If the expression evaluation is successful and non-null, the resulting string is used as the URL for a new GET request with the same headers. ### `paginate_delay = duration (optional)` [Section titled â€œpaginate\_delay = duration (optional)â€](#paginate_delay--duration-optional) The duration to wait between consecutive pagination requests. Defaults to `0s`. ### `parallel = int (optional)` [Section titled â€œparallel = int (optional)â€](#parallel--int-optional) Maximum amount of requests that can be in progress at any time. Defaults to `1`. ### `tls = bool (optional)` [Section titled â€œtls = bool (optional)â€](#tls--bool-optional) Enables TLS. ### `certfile = string (optional)` [Section titled â€œcertfile = string (optional)â€](#certfile--string-optional) Path to the client certificate. ### `keyfile = string (optional)` [Section titled â€œkeyfile = string (optional)â€](#keyfile--string-optional) Path to the key for the client certificate. ### `password = string (optional)` [Section titled â€œpassword = string (optional)â€](#password--string-optional) Password file for keyfile. ### `connection_timeout = duration (optional)` [Section titled â€œconnection\_timeout = duration (optional)â€](#connection_timeout--duration-optional) Timeout for the connection. Defaults to `5s`. ### `max_retry_count = int (optional)` [Section titled â€œmax\_retry\_count = int (optional)â€](#max_retry_count--int-optional) The maximum times to retry a failed request. Every request has its own retry count. Defaults to `0`. ### `retry_delay = duration (optional)` [Section titled â€œretry\_delay = duration (optional)â€](#retry_delay--duration-optional) The duration to wait between each retry. Defaults to `1s`. ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) A pipeline that receives the response body as bytes, allowing parsing per request. This is especially useful in scenarios where the response body can be parsed into multiple events. If not provided, the operator will attempt to infer the parsing operator from the `Content-Type` header. Should this inference fail (e.g., unsupported or missing `Content-Type`), the operator raises a warning and skips the request. ## Examples [Section titled â€œExamplesâ€](#examples) ### Make a GET request [Section titled â€œMake a GET requestâ€](#make-a-get-request) Here we make a request to [urlscan.io](https://urlscan.io/docs/api#search) to search for scans for `tenzir.com` and get the first result. ```tql from {} http "https://urlscan.io/api/v1/search?q=tenzir.com" unroll results head 1 ``` ```tql { results: { submitter: { ... }, task: { ... }, stats: { ... }, page: { ... }, _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30", _score: null, sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ], result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/", screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png", }, total: 9, took: 296, has_more: false, } ``` ### Keeping input context [Section titled â€œKeeping input contextâ€](#keeping-input-context) Frequently, the purpose of making real-time requests in a pipeline is to enrich the incoming data with additional context. In these cases, we want to keep the original event around. This can be done simply by specifying the `response_field` and `metadata_field` options as appropriate. E.g. in the above example, letâ€™s assume we had some initial context that we want to keep around: ```tql from { ctx: {severity: "HIGH"}, domain: "tenzir.com", ip: 0.0.0.0 } http "https://urlscan.io/api/v1/search?q=" + domain, response_field=scan scan.results = scan.results[0] ``` ```tql { ctx: { severity: "HIGH", }, domain: "tenzir.com", ip: 0.0.0.0, scan: { results: { submitter: { ... }, task: { ... }, stats: { ... }, page: { ... }, _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30", _score: null, sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ], result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/", screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png", }, total: 9, took: 88, has_more: false, }, } ``` ### Paginate an API [Section titled â€œPaginate an APIâ€](#paginate-an-api) We can utilize the `sort` and `has_more` fields to get more pages from the API. ```tql let $URL = "https://urlscan.io/api/v1/search?q=example.com" from {} http $URL, paginate=(x => $URL + "&search_after=" + results.last().sort.first() + "," + results.last().sort.last().slice(begin=1, end=-1) if has_more?) head 10 ``` Here we construct the next url for pagination by extracting values from the responses. The query parameter `search_after` expects the two values from the `sort` key in the response to be joined with a `,`. Thus forming a URL like `https://urlscan.io/api/v1/search?q=example.com&search_after=1747796723608,0196f0cd-6fda-761a-81a6-ae1b18914e61`. The `if has_more?` ensures pagination only continues till we have a `has_more` field that is `true`. Additionally, we limit the maximum pages by a simple `head 10`. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`from_http`](/reference/operators/from_http)

# import

Imports events into a Tenzir node. ```tql import ``` ## Description [Section titled â€œDescriptionâ€](#description) The `import` operator persists events in a Tenzir node. This operator is the dual to [`export`](/reference/operators/export). ## Examples [Section titled â€œExamplesâ€](#examples) ### Import Zeek connection logs in TSV format [Section titled â€œImport Zeek connection logs in TSV formatâ€](#import-zeek-connection-logs-in-tsv-format) ```tql load_file "conn.log" read_zeek_tsv import ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`export`](/reference/operators/export), [`publish`](/reference/operators/publish)

# load_amqp

Loads a byte stream via AMQP messages. ```tql load_amqp [url:str, channel=int, exchange=str, routing_key=str, queue=str, options=record, passive=bool, durable=bool, exclusive=bool, no_auto_delete=bool, no_local=bool, ack=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_amqp` operator is an [AMQP](https://www.amqp.org/) 0-9-1 client to receive messages from a queue. ### `url: str (optional)` [Section titled â€œurl: str (optional)â€](#url-str-optional) A URL that specifies the AMQP server. The URL must have the following format: ```plaintext amqp://[USERNAME[:PASSWORD]@]HOSTNAME[:PORT]/[VHOST] ``` When the URL is present, it will overwrite the corresponding values of the configuration options. ### `channel = int (optional)` [Section titled â€œchannel = int (optional)â€](#channel--int-optional) The channel number to use. Defaults to `1`. ### `exchange = str (optional)` [Section titled â€œexchange = str (optional)â€](#exchange--str-optional) The exchange to interact with. Defaults to `"amq.direct"`. ### `routing_key = str (optional)` [Section titled â€œrouting\_key = str (optional)â€](#routing_key--str-optional) The name of the routing key to bind a queue to an exchange. Defaults to the empty string. ### `options = record (optional)` [Section titled â€œoptions = record (optional)â€](#options--record-optional) An option record for RabbitMQ , e.g., `{max_channels: 42, frame_size: 1024, sasl_method: "external"}`. Available options are: ```yaml hostname: 127.0.0.1 port: 5672 ssl: false vhost: / max_channels: 2047 frame_size: 131072 heartbeat: 0 sasl_method: plain username: guest password: guest ``` ### `queue = str (optional)` [Section titled â€œqueue = str (optional)â€](#queue--str-optional) The name of the queue to declare and then bind to. Defaults to the empty string, resulting in auto-generated queue names, such as `"amq.gen-XNTLF0FwabIn9FFKKtQHzg"`. ### `passive = bool (optional)` [Section titled â€œpassive = bool (optional)â€](#passive--bool-optional) If `true`, the server will reply with OK if an exchange already exists with the same name, and raise an error otherwise. Defaults to `false`. ### `durable = bool (optional)` [Section titled â€œdurable = bool (optional)â€](#durable--bool-optional) If `true` when creating a new exchange, the exchange will be marked as durable. Durable exchanges remain active when a server restarts. Non-durable exchanges (transient exchanges) are purged if/when a server restarts. Defaults to `false`. ### `exclusive = bool (optional)` [Section titled â€œexclusive = bool (optional)â€](#exclusive--bool-optional) If `true`, marks the queue as exclusive. Exclusive queues may only be accessed by the current connection, and are deleted when that connection closes. Passive declaration of an exclusive queue by other connections are not allowed. Defaults to `false`. ### `no_auto_delete = bool (optional)` [Section titled â€œno\_auto\_delete = bool (optional)â€](#no_auto_delete--bool-optional) If `true`, the exchange will *not* be deleted when all queues have finished using it. Defaults to `false`. ### `no_local = bool (optional)` [Section titled â€œno\_local = bool (optional)â€](#no_local--bool-optional) If `true`, the server will not send messages to the connection that published them. Defaults to `false`. ### `ack = bool (optional)` [Section titled â€œack = bool (optional)â€](#ack--bool-optional) If `true`, the server expects acknowledgements for messages. Otherwise, when a message is delivered to the client the server assumes the delivery will succeed and immediately dequeues it. This functionality may decrease performance, while improving reliability. Without this flag, messages can get lost if a client dies before they are delivered to the application. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Consume a message from a specified AMQP queue [Section titled â€œConsume a message from a specified AMQP queueâ€](#consume-a-message-from-a-specified-amqp-queue) ```tql load_amqp "amqp://admin:pass@0.0.0.1:5672/vhost", queue="foo" read_json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_amqp`](/reference/operators/save_amqp)

# load_azure_blob_storage

Loads bytes from Azure Blob Storage. ```tql load_azure_blob_storage uri:string, [account_key=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_azure_blob_storage` operator loads bytes from an Azure Blob Storage. By default, authentication is handled by the Azure SDKâ€™s credential chain which may read from multiple environment variables, such as: * `AZURE_TENANT_ID` * `AZURE_CLIENT_ID` * `AZURE_CLIENT_SECRET` * `AZURE_AUTHORITY_HOST` * `AZURE_CLIENT_CERTIFICATE_PATH` * `AZURE_FEDERATED_TOKEN_FILE` ### `uri: string` [Section titled â€œuri: stringâ€](#uri-string) A URI identifying the blob to load from. Supported URI formats: 1. `abfs[s]://[:<password>@]<account>.blob.core.windows.net[/<container>[/<path>]]` 2. `abfs[s]://<container>[:<password>]@<account>.dfs.core.windows.net[/path]` 3. `abfs[s]://[<account[:<password>]@]<host[.domain]>[<:port>][/<container>[/path]]` 4. `abfs[s]://[<account[:<password>]@]<container>[/path]` (1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs 1, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2). ### `account_key = string (optional)` [Section titled â€œaccount\_key = string (optional)â€](#account_key--string-optional) Account key to authenticate with. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write JSON [Section titled â€œWrite JSONâ€](#write-json) Read JSON from a blob `obj.json` in the blob container `container`, using the `tenzirdev` user: ```tql load_azure_blob_storage "abfss://tenzirdev@container/obj.json" read_json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage)

# load_balance

Routes the data to one of multiple subpipelines. ```tql load_balance over:list { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_balance` operator spawns a nested pipeline for each element in the given list. Incoming events are distributed to exactly one of the nested pipelines. This operator may reorder the event stream. ### `over: list` [Section titled â€œover: listâ€](#over-list) This must be a `$`-variable, previously declared with `let`. For example, to load balance over a list of ports, use `let $cfg = [8080, 8081, 8082]` followed by `load_balance $cfg { â€¦ }`. ### `{ â€¦ }` [Section titled â€œ{ â€¦ }â€](#-) The nested pipeline to spawn. This pipeline can use the same variable as passed to `over`, which will be resolved to one of the list items. The following example spawns three nested pipelines, where `$port` is bound to `8080`, `8081` and `8082`, respectively. ```tql let $cfg = [8080, 8081, 8082] load_balance $cfg { let $port = $cfg // â€¦ continue here } ``` The given subpipeline must end with a sink. This limitation might be removed in future versions. ## Examples [Section titled â€œExamplesâ€](#examples) ### Route data to multiple TCP ports [Section titled â€œRoute data to multiple TCP portsâ€](#route-data-to-multiple-tcp-ports) ```tql let $cfg = ["192.168.0.30:8080", "192.168.0.30:8081"] subscribe "input" load_balance $cfg { write_json save_tcp $cfg } ``` ### Route data to multiple Splunk endpoints [Section titled â€œRoute data to multiple Splunk endpointsâ€](#route-data-to-multiple-splunk-endpoints) ```tql let $cfg = [{ ip: 192.168.0.30, token: "example-token-1234", }, { ip: 192.168.0.31, token: "example-token-5678", }] subscribe "input" load_balance $cfg { let $endpoint = string($cfg.ip) + ":8080" to_splunk $endpoint, hec_token=$cfg.token } ```

# load_file

Loads the contents of the file at `path` as a byte stream. ```tql load_file path:string, [follow=bool, mmap=bool, timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `file` loader acquires raw bytes from a file. ### `path: string` [Section titled â€œpath: stringâ€](#path-string) The file path to load from. When `~` is the first character, it will be substituted with the value of the `$HOME` environment variable. ### `follow = bool (optional)` [Section titled â€œfollow = bool (optional)â€](#follow--bool-optional) Do not stop when the end of file is reached, but rather to wait for additional data to be appended to the input. ### `mmap = bool (optional)` [Section titled â€œmmap = bool (optional)â€](#mmap--bool-optional) Use the `mmap(2)` system call to map the file and produce only one single chunk of bytes, instead of producing data piecemeal via `read(2)`. This option effectively gives the downstream parser full control over reads. <!-- TODO: Add this back once they are ported. For the [`feather`](TODO) and [`parquet`](TODO) parsers, this significantly reduces memory usage and improves performance. --> ### `timeout = duration (optional)` [Section titled â€œtimeout = duration (optional)â€](#timeout--duration-optional) Wait at most for the provided duration when performing a blocking system call. This flags comes in handy in combination with `follow=true` to produce a steady pulse of input in the pipeline execution, as input (even if empty) drives the processing forward. ## Examples [Section titled â€œExamplesâ€](#examples) ### Load the raw contents of a file [Section titled â€œLoad the raw contents of a fileâ€](#load-the-raw-contents-of-a-file) ```tql load_file "example.txt" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`files`](/reference/operators/files), [`from_file`](/reference/operators/from_file), [`load_stdin`](/reference/operators/load_stdin), [`save_file`](/reference/operators/save_file)

# load_ftp

Loads a byte stream via FTP. ```tql load_ftp url:str [tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Loads a byte stream via FTP. ### `url: str` [Section titled â€œurl: strâ€](#url-str) The URL to request from. The `ftp://` scheme can be omitted. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql load_ftp "ftp.example.org" ```

# load_gcs

Loads bytes from a Google Cloud Storage object. ```tql load_gcs uri:string, [anonymous=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_gcs` operator connects to a GCS bucket to acquire raw bytes from a GCS object. The connector tries to retrieve the appropriate credentials using Googleâ€™s [Application Default Credentials](https://google.aip.dev/auth/4110). ### `uri: string` [Section titled â€œuri: stringâ€](#uri-string) The path to the GCS object. The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist: > For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`. ### `anonymous = bool (optional)` [Section titled â€œanonymous = bool (optional)â€](#anonymous--bool-optional) Ignore any predefined credentials and try to use anonymous credentials. ## Examples [Section titled â€œExamplesâ€](#examples) Read JSON from an object `log.json` in the folder `logs` in `bucket`. ```tql load_gcs "gs://bucket/logs/log.json" read_json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_gcs`](/reference/operators/save_gcs)

# load_google_cloud_pubsub

Subscribes to a Google Cloud Pub/Sub subscription and obtains bytes. ```tql load_google_cloud_pubsub project_id=string, subscription_id=string, [timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The operator acquires raw bytes from a Google Cloud Pub/Sub subscription. ### `project_id = string` [Section titled â€œproject\_id = stringâ€](#project_id--string) The project to connect to. Note that this is the project id, not the display name. ### `subscription_id = string` [Section titled â€œsubscription\_id = stringâ€](#subscription_id--string) The subscription to subscribe to. ### `timeout = duration (optional)` [Section titled â€œtimeout = duration (optional)â€](#timeout--duration-optional) How long to wait for messages before ending the connection. A duration of zero means the operator will run forever. The default value is `0s`. ## URI support & integration with `from` [Section titled â€œURI support & integration with fromâ€](#uri-support--integration-with-from) The `load_google_cloud_pubsub` operator can also be used from the [`from`](/reference/operators/from) operator. For this, the `gcps://` scheme can be used. The URI is then translated: ```tql from "gcps://my_project/my_subscription" ``` ```tql load_google_cloud_pubsub project_id="my_project", subscription_id="my_subscription" ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Read JSON messages from a subscription [Section titled â€œRead JSON messages from a subscriptionâ€](#read-json-messages-from-a-subscription) Subscribe to `my-subscription` in the project `amazing-project-123456` and parse the messages as JSON: ```tql load_google_cloud_pubsub project_id="amazing-project-123456", subscription_id="my-subscription" read_json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_google_cloud_pubsub`](/reference/operators/save_google_cloud_pubsub)

# load_http

Loads a byte stream via HTTP. ```tql load_http url:string, [data=record, params=record, headers=record, method=string, form=bool, chunked=bool, multipart=bool, parallel=int, tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_http` operator performs a HTTP request and returns the response. ### `url: string` [Section titled â€œurl: stringâ€](#url-string) The URL to request from. The `http://` scheme can be omitted. ### `method = string (optional)` [Section titled â€œmethod = string (optional)â€](#method--string-optional) The HTTP method, such as `POST` or `GET`. The default is `"GET"`. ### `params = record (optional)` [Section titled â€œparams = record (optional)â€](#params--record-optional) The query parameters for the request. ### `headers = record (optional)` [Section titled â€œheaders = record (optional)â€](#headers--record-optional) The headers for the request. ### `data = record (optional)` [Section titled â€œdata = record (optional)â€](#data--record-optional) The request body as a record of key-value pairs. The body is encoded as JSON unless `form=true` has been set. ### `form = bool (optional)` [Section titled â€œform = bool (optional)â€](#form--bool-optional) Submits the HTTP request body as form-encoded data. This automatically sets the `Content-Type` header to `application/x-www-form-urlencoded`. Defaults to `false`. ### `chunked = bool (optional)` [Section titled â€œchunked = bool (optional)â€](#chunked--bool-optional) Whether to enable [chunked transfer encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding). This is equivalent to manually setting the header `Transfer-Encoding: chunked`. Defaults to `false`. ### `multipart = bool (optional)` [Section titled â€œmultipart = bool (optional)â€](#multipart--bool-optional) Whether to encode the HTTP request body as [multipart message](https://en.wikipedia.org/wiki/MIME#Multipart_messages). This automatically sets the `Content-Type` header to `application/form-multipart; X` where `X` contains the MIME part boundary. Defaults to `false`. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Perform an API call and get the response [Section titled â€œPerform an API call and get the responseâ€](#perform-an-api-call-and-get-the-response) ```tql load_http "example.org/api", headers={"X-API-Token": "0000-0000-0000"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_http`](/reference/operators/save_http)

# load_kafka

Loads a byte stream from an Apache Kafka topic. ```tql load_kafka topic:string, [count=int, exit=bool, offset=int|string, options=record, aws_iam=record, commit_batch_size=int, commit_timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_kafka` operator reads bytes from a Kafka topic. The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`. The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them: * `bootstrap.servers`: `localhost` * `client.id`: `tenzir` * `group.id`: `tenzir` * `enable.auto.commit`: `false` (This option cannot be changed) ### `topic: string` [Section titled â€œtopic: stringâ€](#topic-string) The Kafka topic to use. ### `count = int (optional)` [Section titled â€œcount = int (optional)â€](#count--int-optional) Exit successfully after having consumed `count` messages. ### `exit = bool (optional)` [Section titled â€œexit = bool (optional)â€](#exit--bool-optional) Exit successfully after having received the last message. Without this option, the operator waits for new messages after consuming the last one. ### `offset = int|string (optional)` [Section titled â€œoffset = int|string (optional)â€](#offset--intstring-optional) The offset to start consuming from. Possible values are: * `"beginning"`: first offset * `"end"`: last offset * `"stored"`: stored offset * `<value>`: absolute offset * `-<value>`: relative offset from end The default is `"stored"`. <!-- - `s@<value>`: timestamp in ms to start at - `e@<value>`: timestamp in ms to stop at (not included) --> ### `options = record (optional)` [Section titled â€œoptions = record (optional)â€](#options--record-optional) A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"auto.offset.reset" : "earliest", "enable.partition.eof": true}`. The `load_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs. We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `load_kafka` arguments. ### `commit_batch_size = int (optional)` [Section titled â€œcommit\_batch\_size = int (optional)â€](#commit_batch_size--int-optional) The operator commits offsets after receiving `commit_batch_size` messages to improve throughput. If you need to ensure exactly-once semantics for your pipeline, set this option to `1` to commit every message individually. Defaults to `1000`. ### `commit_timeout = duration (optional)` [Section titled â€œcommit\_timeout = duration (optional)â€](#commit_timeout--duration-optional) A timeout after which the operator commits messages, even if it accepted fewer than `commit_batch_size`. This helps with long-running, low-volume pipelines. Defaults to `10s`. ### `aws_iam = record (optional)` [Section titled â€œaws\_iam = record (optional)â€](#aws_iam--record-optional) If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified. Available keys: * `region`: Region of the MSK Clusters. Must be specified when using IAM. * `assume_role`: Optional role ARN to assume. * `session_name`: Optional session name to use when assuming a role. * `external_id`: Optional external id to use when assuming a role. The operator tries to get credentials in the following order: 1. Checks your environment variables for AWS Credentials. 2. Checks your `$HOME/.aws/credentials` file for a profile and credentials 3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`. 4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that isnâ€™t directly supported by AWS. 5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set. 6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read 100 JSON messages from the topic `tenzir` [Section titled â€œRead 100 JSON messages from the topic tenzirâ€](#read-100-json-messages-from-the-topic-tenzir) ```tql load_kafka "tenzir", count=100 read_json ``` ### Read Zeek Streaming JSON logs starting at the beginning [Section titled â€œRead Zeek Streaming JSON logs starting at the beginningâ€](#read-zeek-streaming-json-logs-starting-at-the-beginning) ```tql load_kafka "zeek", offset="beginning" read_zeek_json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_kafka`](/reference/operators/save_kafka)

# load_nic

Loads bytes from a network interface card (NIC). ```tql load_nic iface:str, [snaplen=int, emit_file_headers=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_nic` operator uses libpcap to acquire packets from a network interface and packs them into blocks of bytes that represent PCAP packet records. The received first packet triggers also emission of PCAP file header such that downstream operators can treat the packet stream as valid PCAP capture file. ### `iface: str` [Section titled â€œiface: strâ€](#iface-str) The interface to load bytes from. ### `snaplen = int (optional)` [Section titled â€œsnaplen = int (optional)â€](#snaplen--int-optional) Sets the snapshot length of the captured packets. This value is an upper bound on the packet size. Packets larger than this size get truncated to `snaplen` bytes. Defaults to `262144`. ### `emit_file_headers = bool (optional)` [Section titled â€œemit\_file\_headers = bool (optional)â€](#emit_file_headers--bool-optional) Creates PCAP file headers for every flushed batch. The operator emits chunk of bytes that represent a stream of packets. When setting `emit_file_headers` every chunk gets its own PCAP file header, as opposed to just the very first. This yields a continuous stream of concatenated PCAP files. Our [`read_pcap`](/reference/operators/read_pcap) operator can handle such concatenated traces, and optionally re-emit thes file headers as separate events. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read PCAP packets from `eth0` [Section titled â€œRead PCAP packets from eth0â€](#read-pcap-packets-from-eth0) ```tql load_nic "eth0" read_pcap ``` ### Perform the equivalent of `tcpdump -i en0 -w trace.pcap` [Section titled â€œPerform the equivalent of tcpdump -i en0 -w trace.pcapâ€](#perform-the-equivalent-of-tcpdump--i-en0--w-tracepcap) ```tql load_nic "en0" read_pcap write_pcap save_file "trace.pcap" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_pcap`](/reference/operators/read_pcap), [`nics`](/reference/operators/nics), [`write_pcap`](/reference/operators/write_pcap)

# load_s3

Loads from an Amazon S3 object. ```tql load_s3 uri:str, [anonymous=bool, role=string, external_id=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_s3` operator connects to an S3 bucket to acquire raw bytes from an S3 object. The connector tries to retrieve the appropriate credentials using AWSâ€™s [default credentials provider chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). If a config file `<prefix>/etc/tenzir/plugin/s3.yaml` or `~/.config/tenzir/plugin/s3.yaml` exists, it is always preferred over the default AWS credentials. The configuration file must have the following format: ```yaml access-key: your-access-key secret-key: your-secret-key session-token: your-session-token (optional) ``` ### `uri: str` [Section titled â€œuri: strâ€](#uri-str) The path to the S3 object. The syntax is `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)`. Options can be appended to the path as query parameters, as per [Arrow](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri): > For S3, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`. ### `anonymous = bool (optional)` [Section titled â€œanonymous = bool (optional)â€](#anonymous--bool-optional) Whether to ignore any predefined credentials and try to load with anonymous credentials. ### `role = string (optional)` [Section titled â€œrole = string (optional)â€](#role--string-optional) A role to assume when writing to S3. ### `external_id = string (optional)` [Section titled â€œexternal\_id = string (optional)â€](#external_id--string-optional) The external ID to use when assuming the `role`. Defaults to no ID. ## Examples [Section titled â€œExamplesâ€](#examples) Read CSV from an object `obj.csv` in the bucket `examplebucket`: ```tql load_s3 "s3://examplebucket/obj.csv" read_csv ``` Read JSON from an object `test.json` in the bucket `examplebucket`, but using a different, S3-compatible endpoint: ```tql load_s3 "s3://examplebucket/test.json?endpoint_override=s3.us-west.mycloudservice.com" read_json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_s3`](/reference/operators/save_s3)

# load_sqs

Loads bytes from [Amazon SQS](https://docs.aws.amazon.com/sqs/) queues. ```tql load_sqs queue:str, [poll_time=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) [Amazon Simple Queue Service (Amazon SQS)](https://docs.aws.amazon.com/sqs/) is a fully managed message queuing service to decouple and scale microservices, distributed systems, and serverless applications. The `load_sqs` operator reads bytes from messages of an SQS queue. The `load_sqs` operator uses long polling, which helps reduce your cost of using SQS by reducing the number of empty responses when there are no messages available to return in reply to a message request. Use the `poll_time` option to adjust the timeout. The operator requires the following AWS permissions: * `sqs:GetQueueUrl` * `sqs:ReceiveMessage` * `sqs:DeleteMessage` ### `queue: str` [Section titled â€œqueue: strâ€](#queue-str) The name of the queue to use. ### `poll_time = duration (optional)` [Section titled â€œpoll\_time = duration (optional)â€](#poll_time--duration-optional) The long polling timeout per request. The value must be between 1 and 20 seconds. Defaults to `3s`. ## Examples [Section titled â€œExamplesâ€](#examples) Read JSON messages from the SQS queue `tenzir`: ```tql load_sqs "tenzir" ``` Read JSON messages with a 20-second long poll timeout: ```tql load_sqs "tenzir", poll_time=20s ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_sqs`](/reference/operators/save_sqs)

# load_stdin

Accepts bytes from standard input. ```tql load_stdin ``` ## Description [Section titled â€œDescriptionâ€](#description) Accepts bytes from standard input. This is mostly useful when using the `tenzir` executable as part of a shell script. ## Examples [Section titled â€œExamplesâ€](#examples) ### Pipe text into `tenzir` [Section titled â€œPipe text into tenzirâ€](#pipe-text-into-tenzir) ```sh echo "Hello World" | tenzir ``` ```tql load_stdin read_lines ``` ```tql { line: "Hello World", } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_stdout`](/reference/operators/save_stdout), [`load_file`](/reference/operators/load_file)

# load_tcp

Loads bytes from a TCP or TLS connection. ```tql load_tcp endpoint:string, [parallel=int, peer_field=field, tls=bool, cacert=string, certifle=string, max_buffered_chunks=int { â€¦ }] ``` ## Description [Section titled â€œDescriptionâ€](#description) Reads bytes from the given endpoint via TCP or TLS. ### `endpoint: string` [Section titled â€œendpoint: stringâ€](#endpoint-string) The endpoint at which the server will listen. Must be of the form `[tcp://]<hostname>:<port>`. Use the hostname `0.0.0.0` to accept connections on all interfaces. ### `parallel = int (optional)` [Section titled â€œparallel = int (optional)â€](#parallel--int-optional) Number of threads to use for reading from connections. Defaults to 1. ### `peer_field = field (optional)` [Section titled â€œpeer\_field = field (optional)â€](#peer_field--field-optional) Write a record with the fields `ip`, `port`, and `hostname` resembling the peer endpoint of the respective TCP connection into the specified field at the end of the nested pipeline. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ### `max_buffered_chunks = int (optional)` [Section titled â€œmax\_buffered\_chunks = int (optional)â€](#max_buffered_chunks--int-optional) Maximum number of buffered chunks per connection. Defaults to 10. ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) The pipeline to run for each individual TCP connection. If none is specified, no transformations are applied to the output streams. Unless you are sure that there is at most one active connection at a time, it is recommended to specify a pipeline that parses the individual connection streams into events, for instance `{ read_json }`. Otherwise, the output can be interleaved. ## Examples [Section titled â€œExamplesâ€](#examples) ### Listen for incoming Syslog over TCP [Section titled â€œListen for incoming Syslog over TCPâ€](#listen-for-incoming-syslog-over-tcp) Listen on all network interfaces, parsing each individual connection as syslog: ```tql load_tcp "0.0.0.0:8090" { read_syslog } ``` ### Connect to a remote endpoint and read JSON [Section titled â€œConnect to a remote endpoint and read JSONâ€](#connect-to-a-remote-endpoint-and-read-json) ```tql // We know that there is only one connection, so we do not specify a pipeline. load_tcp "example.org:8090", connect=true read_json ``` ### Listen on localhost with TLS enabled [Section titled â€œListen on localhost with TLS enabledâ€](#listen-on-localhost-with-tls-enabled) Wait for connections on localhost with TLS enabled, parsing incoming JSON streams according to the schema `"my_schema"`, forwarding no more than 20 events per individual connection: ```tql load_tcp "127.0.0.1:4000", tls=true, certfile="key_and_cert.pem", keyfile="key_and_cert.pem" { read_json schema="my_schema" head 20 } ``` This example may use a self-signed certificate that can be generated like this: ```bash openssl req -x509 -newkey rsa:2048 -keyout key_and_cert.pem -out key_and_cert.pem -days 365 -nodes ``` You can test the endpoint locally by issuing a TLS connection: ```bash openssl s_client 127.0.0.1:4000 ```

# load_udp

Loads bytes from a UDP socket. ```tql load_udp endpoint:str, [connect=bool, insert_newlines=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Loads bytes from a UDP socket. The operator defaults to creating a socket in listening mode. Use `connect=true` if the operator should initiate the connection instead. When you have a socket in listening mode, use `0.0.0.0` to accept connections on all interfaces. The [`nics`](/reference/operators/nics) operator lists all all available interfaces. ### `endpoint: str` [Section titled â€œendpoint: strâ€](#endpoint-str) The address of the remote endpoint to load bytes from. Must be of the format: `[udp://]host:port`. ### `connect = bool (optional)` [Section titled â€œconnect = bool (optional)â€](#connect--bool-optional) Connect to `endpoint` instead of listening at it. Defaults to `false`. ### `insert_newlines = bool (optional)` [Section titled â€œinsert\_newlines = bool (optional)â€](#insert_newlines--bool-optional) Append a newline character (`\n`) at the end of every datagram. This option comes in handy in combination with line-based parsers downstream, such as NDJSON. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Import JSON via UDP by listenting on localhost [Section titled â€œImport JSON via UDP by listenting on localhostâ€](#import-json-via-udp-by-listenting-on-localhost) ```tql load_udp "127.0.0.1:56789" import ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_udp`](/reference/operators/save_udp)

# load_zmq

Receives ZeroMQ messages. ```tql load_zmq [endpoint:str, filter=str, listen=bool, connect=bool, insert_separator=string, monitor=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `load_zmq` operator processes the bytes in a ZeroMQ message received by a `SUB` socket. Indpendent of the socket type, the `load_zmq` operator supports specfiying the direction of connection establishment with `listen` and `connect`. This can be helpful to work around firewall restrictions and fit into broader set of existing ZeroMQ applications. With the `monitor` option, you can activate message buffering for TCP sockets that hold off sending messages until *at least one* remote peer has connected. This can be helpful when you want to delay publishing until you have one connected subscriber, e.g., when the publisher spawns before any subscriber exists. ### `endpoint: str (optional)` [Section titled â€œendpoint: str (optional)â€](#endpoint-str-optional) The endpoint for connecting to or listening on a ZeroMQ socket. Defaults to `tcp://127.0.0.1:5555`. ### `filter = str (optional)` [Section titled â€œfilter = str (optional)â€](#filter--str-optional) Installs a filter for the ZeroMQ `SUB` socket at the source. Filting in ZeroMQ means performing a prefix-match on the raw bytes of the entire message. Defaults to the empty string, which is equivalent to no filtering. ### `listen = bool (optional)` [Section titled â€œlisten = bool (optional)â€](#listen--bool-optional) Bind to the ZeroMQ socket. Defaults to `false`. ### `connect = bool (optional)` [Section titled â€œconnect = bool (optional)â€](#connect--bool-optional) Connect to the ZeroMQ socket. Defaults to `true`. ### `insert_separator = string (optional)` [Section titled â€œinsert\_separator = string (optional)â€](#insert_separator--string-optional) A separator string to append to each received ZeroMQ message. Defaults to no separator. ### `monitor = bool (optional)` [Section titled â€œmonitor = bool (optional)â€](#monitor--bool-optional) Monitors a 0mq socket over TCP until the remote side establishes a connection. ## Examples [Section titled â€œExamplesâ€](#examples) ### Interpret ZeroMQ messages as JSON [Section titled â€œInterpret ZeroMQ messages as JSONâ€](#interpret-zeromq-messages-as-json) ```tql load_zmq "1.2.3.4:56789" read_json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`save_zmq`](/reference/operators/save_zmq)

# local

Forces a pipeline to run locally. ```tql local { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `local` operator takes a pipeline as an argument and forces it to run at a client process. This operator has no effect when running a pipeline through the API or Tenzir Platform. ## Examples [Section titled â€œExamplesâ€](#examples) ### Do an expensive sort locally [Section titled â€œDo an expensive sort locallyâ€](#do-an-expensive-sort-locally) ```tql export where @name.starts_with("suricata") local { sort timestamp } write_ndjson save_file "eve.json" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`remote`](/reference/operators/remote)

# measure

Replaces the input with metrics describing the input. ```tql measure [real_time=bool, cumulative=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `measure` operator yields metrics for each received batch of events or bytes using the following schema, respectively: Events Metrics ```text type tenzir.measure.events = record{ timestamp: time, events: uint64, schema_id: string, schema: string, } ``` Bytes Metrics ```text type tenzir.measure.bytes = record{ timestamp: time, bytes: uint64, } ``` ### `real_time = bool (optional)` [Section titled â€œreal\_time = bool (optional)â€](#real_time--bool-optional) Whether to emit metrics immediately with every batch, rather than buffering until the upstream operator stalls, i.e., is idle or waiting for further input. The is especially useful when `measure` should emit data without latency. ### `cumulative = bool (optional)` [Section titled â€œcumulative = bool (optional)â€](#cumulative--bool-optional) Whether to emit running totals for the `events` and `bytes` fields rather than per-batch statistics. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the number of bytes read incrementally for a file [Section titled â€œGet the number of bytes read incrementally for a fileâ€](#get-the-number-of-bytes-read-incrementally-for-a-file) ```tql load_file "input.json" measure ``` ```tql {timestamp: 2023-04-28T10:22:10.192322, bytes: 16384} {timestamp: 2023-04-28T10:22:10.223612, bytes: 16384} {timestamp: 2023-04-28T10:22:10.297169, bytes: 16384} {timestamp: 2023-04-28T10:22:10.387172, bytes: 16384} {timestamp: 2023-04-28T10:22:10.408171, bytes: 8232} ``` ### Get the number of events read incrementally from a file [Section titled â€œGet the number of events read incrementally from a fileâ€](#get-the-number-of-events-read-incrementally-from-a-file) ```tql load_file "eve.json" read_suricata measure ``` ```tql { timestamp: 2023-04-28T10:26:45.159885, events: 65536, schema_id: "d49102998baae44a", schema: "suricata.dns" } { timestamp: 2023-04-28T10:26:45.812321, events: 412, schema_id: "d49102998baae44a", schema: "suricata.dns" } ``` ### Get the total number of events in a file, grouped by schema [Section titled â€œGet the total number of events in a file, grouped by schemaâ€](#get-the-total-number-of-events-in-a-file-grouped-by-schema) ```tql load_file "eve.json" read_suricata measure summarize schema, events=sum(events) ``` ```tql {schema: "suricata.dns", events: 65948} ```

# metrics

Retrieves metrics events from a Tenzir node. ```tql metrics [name:string, live=bool, retro=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `metrics` operator retrieves metrics events from a Tenzir node. Metrics events are collected every second. ### `name: string (optional)` [Section titled â€œname: string (optional)â€](#name-string-optional) Show only metrics with the specified name. For example, `metrics "cpu"` only shows CPU metrics. ### `live = bool (optional)` [Section titled â€œlive = bool (optional)â€](#live--bool-optional) Work on all metrics events as they are generated in real-time instead of on metrics events persisted at a Tenzir node. ### `retro = bool (optional)` [Section titled â€œretro = bool (optional)â€](#retro--bool-optional) Work on persisted diagnostic events (first), even when `live` is given. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir collects metrics with the following schemas. ### `tenzir.metrics.api` [Section titled â€œtenzir.metrics.apiâ€](#tenzirmetricsapi) Contains information about all accessed API endpoints, emitted once per second. | Field | Type | Description | | :-------------- | :--------- | :----------------------------------------------------- | | `timestamp` | `time` | The time at which the API request was received. | | `request_id` | `string` | The unique request ID assigned by the Tenzir Platform. | | `method` | `double` | The HTTP method used to access the API. | | `path` | `double` | The path of the accessed API endpoint. | | `response_time` | `duration` | The time the API endpoint took to respond. | | `status_code` | `uint64` | The HTTP status code of the API response. | | `params` | `record` | The API endpoints parameters passed inused. | The schema of the record `params` depends on the API endpoint used. Refer to the [API documentation](/reference/node/api) to see the available parameters per endpoint. ### `tenzir.metrics.caf` [Section titled â€œtenzir.metrics.cafâ€](#tenzirmetricscaf) Contains metrics about the CAF (C++ Actor Framework) runtime system. Aimed at Developers CAF metrics primarily exist for debugging purposes. Actor names and other details contained in these metrics are documented only in source code, and we may change them without notice. Do not rely on specific actor names or metrics in production systems. | Field | Type | Description | | :---------- | :------------- | :---------------------------------------- | | `system` | `record` | Metrics about the CAF actor system. | | `middleman` | `record` | Metrics about CAFâ€™s network layer. | | `actors` | `list<record>` | Per-actor metrics for all running actors. | The record `system` has the following schema: | Field | Type | Description | | :----------------------- | :------- | :--------------------------------------------------------------------- | | `running_actors` | `int64` | Number of currently running actors. | | `running_actors_by_name` | `list` | Number of running actors, grouped by actor name. | | `all_messages` | `record` | Information about the total message metrics. | | `messages_by_actor` | `list` | Information about the message metrics, grouped by receiving actor name | The `running_actors_by_name` field is a `list` of `record`s with the following schema: | Field | Type | Description | | :------ | :------- | :------------------------------------------------- | | `name` | `string` | Actor name. | | `count` | `int64` | Number of actors with this name currently running. | The `all_messages` field has the following schema: | Field | Type | Description | | :---------- | :------ | :---------------------------- | | `processed` | `int64` | Number of processed messages. | | `rejected` | `int64` | Number of rejected messages. | The `messages_by_actor` field is a `list` of `record`s with the following schema: | Field | Type | Description | | :---------- | :------- | :-------------------------------------------------------------------------------------- | | `name` | `string` | Name of the receiving actor. This may be null for messages without an associated actor. | | `processed` | `int64` | Number of processed messages. | | `rejected` | `int64` | Number of rejected messages. | The record `middleman` has the following schema: | Field | Type | Description | | :----------------------- | :--------- | :---------------------------------------------------- | | `inbound_messages_size` | `int64` | Size of received messages in bytes since last metric. | | `outbound_messages_size` | `int64` | Size of sent messages in bytes since last metric. | | `serialization_time` | `duration` | Time spent serializing messages since last metric. | | `deserialization_time` | `duration` | Time spent deserializing messages since last metric. | Each record in the `actors` list has the following schema: | Field | Type | Description | | :---------------- | :--------- | :------------------------------------------------ | | `name` | `string` | Name of the actor. | | `processing_time` | `duration` | Time spent processing messages since last metric. | | `mailbox_time` | `duration` | Time messages spent in mailbox since last metric. | | `mailbox_size` | `int64` | Current number of messages in actorâ€™s mailbox. | ### `tenzir.metrics.buffer` [Section titled â€œtenzir.metrics.bufferâ€](#tenzirmetricsbuffer) Contains information about the `buffer` operatorâ€™s internal buffer. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `buffer` operator in the pipeline. | | `used` | `uint64` | The number of events stored in the buffer. | | `free` | `uint64` | The remaining capacity of the buffer. | | `dropped` | `uint64` | The number of events dropped by the buffer. | ### `tenzir.metrics.cpu` [Section titled â€œtenzir.metrics.cpuâ€](#tenzirmetricscpu) Contains a measurement of CPU utilization. | Field | Type | Description | | :------------ | :------- | :------------------------------------------ | | `timestamp` | `time` | The time at which this metric was recorded. | | `loadavg_1m` | `double` | The load average over the last minute. | | `loadavg_5m` | `double` | The load average over the last 5 minutes. | | `loadavg_15m` | `double` | The load average over the last 15 minutes. | ### `tenzir.metrics.disk` [Section titled â€œtenzir.metrics.diskâ€](#tenzirmetricsdisk) Contains a measurement of disk space usage. | Field | Type | Description | | :------------ | :------- | :--------------------------------------------------------------------------------- | | `timestamp` | `time` | The time at which this metric was recorded. | | `path` | `string` | The byte measurements below refer to the filesystem on which this path is located. | | `total_bytes` | `uint64` | The total size of the volume, in bytes. | | `used_bytes` | `uint64` | The number of bytes occupied on the volume. | | `free_bytes` | `uint64` | The number of bytes still free on the volume. | ### `tenzir.metrics.enrich` [Section titled â€œtenzir.metrics.enrichâ€](#tenzirmetricsenrich) Contains a measurement of the `enrich` operator, emitted once every second. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `enrich` operator in the pipeline. | | `context` | `string` | The name of the context the associated operator is using. | | `events` | `uint64` | The amount of input events that entered the `enrich` operator since the last metric. | | `hits` | `uint64` | The amount of successfully enriched events since the last metric. | ### `tenzir.metrics.export` [Section titled â€œtenzir.metrics.exportâ€](#tenzirmetricsexport) Contains a measurement of the `export` operator, emitted once every second per schema. Note that internal events like metrics or diagnostics do not emit metrics themselves. | Field | Type | Description | | :-------------- | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `export` operator in the pipeline. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were imported. | | `queued_events` | `uint64` | The total amount of events that are enqueued in the export. | ### `tenzir.metrics.import` [Section titled â€œtenzir.metrics.importâ€](#tenzirmetricsimport) Contains a measurement the `import` operator, emitted once every second per schema. Note that internal events like metrics or diagnostics do not emit metrics themselves. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `import` operator in the pipeline. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were imported. | ### `tenzir.metrics.ingest` [Section titled â€œtenzir.metrics.ingestâ€](#tenzirmetricsingest) Contains a measurement of all data ingested into the database, emitted once per second and schema. | Field | Type | Description | | :---------- | :------- | :------------------------------------------ | | `timestamp` | `time` | The time at which this metric was recorded. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were ingested. | ### `tenzir.metrics.lookup` [Section titled â€œtenzir.metrics.lookupâ€](#tenzirmetricslookup) Contains a measurement of the `lookup` operator, emitted once every second. | Field | Type | Description | | :---------------- | :------- | :------------------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `lookup` operator in the pipeline. | | `context` | `string` | The name of the context the associated operator is using. | | `live` | `record` | Information about the live lookup. | | `retro` | `record` | Information about the retroactive lookup. | | `context_updates` | `uint64` | The amount of times the underlying context has been updated while the associated lookup is active. | The record `live` has the following schema: | Field | Type | Description | | :------- | :------- | :------------------------------------------------------------------------- | | `events` | `uint64` | The amount of input events used for the live lookup since the last metric. | | `hits` | `uint64` | The amount of live lookup matches since the last metric. | The record `retro` has the following schema: | Field | Type | Description | | :-------------- | :------- | :-------------------------------------------------------------------- | | `events` | `uint64` | The amount of input events used for the lookup since the last metric. | | `hits` | `uint64` | The amount of lookup matches since the last metric. | | `queued_events` | `uint64` | The total amount of events that were in the queue for the lookup. | ### `tenzir.metrics.memory` [Section titled â€œtenzir.metrics.memoryâ€](#tenzirmetricsmemory) Contains a measurement of the available memory on the host. | Field | Type | Description | | :------------ | :------- | :------------------------------------------ | | `timestamp` | `time` | The time at which this metric was recorded. | | `total_bytes` | `uint64` | The total available memory, in bytes. | | `used_bytes` | `uint64` | The amount of memory used, in bytes. | | `free_bytes` | `uint64` | The amount of free memory, in bytes. | ### `tenzir.metrics.operator` [Section titled â€œtenzir.metrics.operatorâ€](#tenzirmetricsoperator) Contains input and output measurements over some amount of time for a single operator instantiation. Deprecation Notice Operator metrics are deprecated and will be removed in a future release. Use [pipeline metrics](#tenzirmetricspipeline) instead. While they offered great insight into the performance of operators, they were not as useful as pipeline metrics for understanding the overall performance of a pipeline, and were too expensive to collect and store. | Field | Type | Description | | :-------------------- | :--------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time when this event was emitted (immediately after the collection period). | | `operator_id` | `uint64` | The ID of the operator inside the pipeline referenced above. | | `source` | `bool` | True if this is the first operator in the pipeline. | | `transformation` | `bool` | True if this is neither the first nor the last operator. | | `sink` | `bool` | True if this is the last operator in the pipeline. | | `internal` | `bool` | True if the data flow is considered to internal to Tenzir. | | `duration` | `duration` | The timespan over which this data was collected. | | `starting_duration` | `duration` | The time spent to start the operator. | | `processing_duration` | `duration` | The time spent processing the data. | | `scheduled_duration` | `duration` | The time that the operator was scheduled. | | `running_duration` | `duration` | The time that the operator was running. | | `paused_duration` | `duration` | The time that the operator was paused. | | `input` | `record` | Measurement of the incoming data stream. | | `output` | `record` | Measurement of the outgoing data stream. | The records `input` and `output` have the following schema: | Field | Type | Description | | :------------- | :------- | :-------------------------------------------------------------- | | `unit` | `string` | The type of the elements, which is `void`, `bytes` or `events`. | | `elements` | `uint64` | Number of elements that were seen during the collection period. | | `approx_bytes` | `uint64` | An approximation for the number of bytes transmitted. | | `batches` | `uint64` | The number of batches included in this metric. | ### `tenzir.metrics.pipeline` [Section titled â€œtenzir.metrics.pipelineâ€](#tenzirmetricspipeline) Contains measurements of data flowing through pipelines, emitted once every 10 seconds. | Field | Type | Description | | :------------ | :------- | :---------------------------------------------- | | `timestamp` | `time` | The time at which this metric was recorded. | | `pipeline_id` | `string` | The ID of the pipeline these metrics represent. | | `ingress` | `record` | Measurement of data entering the pipeline. | | `egress` | `record` | Measurement of data exiting the pipeline. | The records `ingress` and `egress` have the following schema: | Field | Type | Description | | :--------- | :--------- | :------------------------------------------------------- | | `duration` | `duration` | The timespan over which this data was collected. | | `events` | `uint64` | Number of events that passed through during this period. | | `bytes` | `uint64` | Approximate number of bytes that passed through. | | `batches` | `uint64` | Number of batches that passed through. | | `internal` | `bool` | True if the data flow is considered internal to Tenzir. | ### `tenzir.metrics.platform` [Section titled â€œtenzir.metrics.platformâ€](#tenzirmetricsplatform) Signals whether the connection to the Tenzir Platform is working from the nodeâ€™s perspective. Emitted once per second. | Field | Type | Description | | :---------- | :----- | :------------------------------------------ | | `timestamp` | `time` | The time at which this metric was recorded. | | `connected` | `bool` | The connection status. | ### `tenzir.metrics.process` [Section titled â€œtenzir.metrics.processâ€](#tenzirmetricsprocess) Contains a measurement of the amount of memory used by the `tenzir-node` process. | Field | Type | Description | | :--------------------- | :------- | :-------------------------------------------------------------------------------- | | `timestamp` | `time` | The time at which this metric was recorded. | | `current_memory_usage` | `uint64` | The memory currently used by this process. | | `peak_memory_usage` | `uint64` | The peak amount of memory, in bytes. | | `swap_space_usage` | `uint64` | The amount of swap space, in bytes. Only available on Linux systems. | | `open_fds` | `uint64` | The amount of open file descriptors by the node. Only available on Linux systems. | ### `tenzir.metrics.publish` [Section titled â€œtenzir.metrics.publishâ€](#tenzirmetricspublish) Contains a measurement of the `publish` operator, emitted once every second per schema. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `publish` operator in the pipeline. | | `topic` | `string` | The topic name. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were published to the `topic`. | ### `tenzir.metrics.rebuild` [Section titled â€œtenzir.metrics.rebuildâ€](#tenzirmetricsrebuild) Contains a measurement of the partition rebuild process, emitted once every second. | Field | Type | Description | | :------------------ | :------- | :-------------------------------------------------------- | | `timestamp` | `time` | The time at which this metric was recorded. | | `partitions` | `uint64` | The number of partitions currently being rebuilt. | | `queued_partitions` | `uint64` | The number of partitions currently queued for rebuilding. | ### `tenzir.metrics.subscribe` [Section titled â€œtenzir.metrics.subscribeâ€](#tenzirmetricssubscribe) Contains a measurement of the `subscribe` operator, emitted once every second per schema. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `subscribe` operator in the pipeline. | | `topic` | `string` | The topic name. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were retrieved from the `topic`. | ### `tenzir.metrics.tcp` [Section titled â€œtenzir.metrics.tcpâ€](#tenzirmetricstcp) Contains measurements about the number of read calls and the received bytes per TCP connection. | Field | Type | Description | | :-------------- | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `publish` operator in the pipeline. | | `native` | `string` | The native handle of the connection (unix: file descriptor). | | `reads` | `uint64` | The number of attempted reads since the last metric. | | `writes` | `uint64` | The number of attempted writes since the last metric. | | `bytes_read` | `uint64` | The number of bytes received since the last metrics. | | `bytes_written` | `uint64` | The number of bytes written since the last metrics. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Sort pipelines by total ingress in bytes [Section titled â€œSort pipelines by total ingress in bytesâ€](#sort-pipelines-by-total-ingress-in-bytes) ```tql metrics "pipeline" summarize pipeline_id, ingress=sum(ingress.bytes if not ingress.internal) sort -ingress ``` ```tql {pipeline_id: "demo-node/m57-suricata", ingress: 59327586} {pipeline_id: "demo-node/m57-zeek", ingress: 43291764} ``` ### Show the CPU usage over the last hour [Section titled â€œShow the CPU usage over the last hourâ€](#show-the-cpu-usage-over-the-last-hour) ```tql metrics "cpu" where timestamp > now() - 1h select timestamp, percent=loadavg_1m ``` ```tql {timestamp: 2023-12-21T12:00:32.631102, percent: 0.40478515625} {timestamp: 2023-12-21T11:59:32.626043, percent: 0.357421875} {timestamp: 2023-12-21T11:58:32.620327, percent: 0.42578125} {timestamp: 2023-12-21T11:57:32.614810, percent: 0.50390625} {timestamp: 2023-12-21T11:56:32.609896, percent: 0.32080078125} {timestamp: 2023-12-21T11:55:32.605871, percent: 0.5458984375} ``` ### Get the current memory usage [Section titled â€œGet the current memory usageâ€](#get-the-current-memory-usage) ```tql metrics "memory" sort -timestamp tail 1 select current_memory_usage ``` ```tql {current_memory_usage: 1083031552} ``` ### Show the total pipeline ingress in bytes [Section titled â€œShow the total pipeline ingress in bytesâ€](#show-the-total-pipeline-ingress-in-bytes) Show the inggress for every day over the last week, excluding pipelines that run in the Explorer: ```tql metrics "operator" where timestamp > now() - 1week where source and not hidden timestamp = floor(timestamp, 1day) summarize timestamp, bytes=sum(output.approx_bytes) ``` ```tql {timestamp: 2023-11-08T00:00:00.000000, bytes: 79927223} {timestamp: 2023-11-09T00:00:00.000000, bytes: 51788928} {timestamp: 2023-11-10T00:00:00.000000, bytes: 80740352} {timestamp: 2023-11-11T00:00:00.000000, bytes: 75497472} {timestamp: 2023-11-12T00:00:00.000000, bytes: 55497472} {timestamp: 2023-11-13T00:00:00.000000, bytes: 76546048} {timestamp: 2023-11-14T00:00:00.000000, bytes: 68643200} ``` ### Show the operators that produced the most events [Section titled â€œShow the operators that produced the most eventsâ€](#show-the-operators-that-produced-the-most-events) Show the three operator instantiations that produced the most events in total and their pipeline IDs: ```tql metrics "operator" where output.unit == "events" summarize pipeline_id, operator_id, events=max(output.elements) sort -events head 3 ``` ```tql {pipeline_id: "70a25089-b16c-448d-9492-af5566789b99", operator_id: 0, events: 391008694 } {pipeline_id: "7842733c-06d6-4713-9b80-e20944927207", operator_id: 0, events: 246914949 } {pipeline_id: "6df003be-0841-45ad-8be0-56ff4b7c19ef", operator_id: 1, events: 83013294 } ``` ### Get the disk usage over time [Section titled â€œGet the disk usage over timeâ€](#get-the-disk-usage-over-time) ```tql metrics "disk" sort timestamp select timestamp, used_bytes ``` ```tql {timestamp: 2023-12-21T12:52:32.900086, used_bytes: 461834444800} {timestamp: 2023-12-21T12:53:32.905548, used_bytes: 461834584064} {timestamp: 2023-12-21T12:54:32.910918, used_bytes: 461840302080} {timestamp: 2023-12-21T12:55:32.916200, used_bytes: 461842751488} ``` ### Get the memory usage over time [Section titled â€œGet the memory usage over timeâ€](#get-the-memory-usage-over-time) ```tql metrics "memory" sort timestamp select timestamp, used_bytes ``` ```tql {timestamp: 2023-12-21T13:08:32.982083, used_bytes: 48572645376} {timestamp: 2023-12-21T13:09:32.986962, used_bytes: 48380682240} {timestamp: 2023-12-21T13:10:32.992494, used_bytes: 48438878208} {timestamp: 2023-12-21T13:11:32.997889, used_bytes: 48491839488} {timestamp: 2023-12-21T13:12:33.003323, used_bytes: 48529952768} ``` ### Get inbound TCP traffic over time [Section titled â€œGet inbound TCP traffic over timeâ€](#get-inbound-tcp-traffic-over-time) ```tql metrics "tcp" sort timestamp select timestamp, port, handle, reads, bytes ``` ```tql { timestamp: 2024-09-04T15:43:38.011350, port: 10000, handle: "12", reads: 884, writes: 0, bytes_read: 10608, bytes_written: 0 } { timestamp: 2024-09-04T15:43:39.013575, port: 10000, handle: "12", reads: 428, writes: 0, bytes_read: 5136, bytes_written: 0 } { timestamp: 2024-09-04T15:43:40.015376, port: 10000, handle: "12", reads: 429, writes: 0, bytes_read: 5148, bytes_written: 0 } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`diagnostics`](/reference/operators/diagnostics)

# move

Moves values from one field to another, removing the original field. ```tql move to=from, â€¦ ``` ## Description [Section titled â€œDescriptionâ€](#description) Moves from the field `from` to the field `to`. ### `to: field` [Section titled â€œto: fieldâ€](#to-field) The field to move into. ### `from: field` [Section titled â€œfrom: fieldâ€](#from-field) The field to move from. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from {x: 1, y: 2} move z=y, w.x=x ``` ```tql { z: 2, w: { x: 1, }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`set`](/reference/operators/set)

# nics

Shows a snapshot of available network interfaces. ```tql nics ``` ## Description [Section titled â€œDescriptionâ€](#description) The `nics` operator shows a snapshot of all available network interfaces. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir emits network interface card information with the following schema. ### `tenzir.nic` [Section titled â€œtenzir.nicâ€](#tenzirnic) Contains detailed information about the network interface. | Field | Type | Description | | :------------ | :------- | :--------------------------------------------------------------------------- | | `name` | `string` | The name of the network interface. | | `description` | `string` | A brief note or explanation about the network interface. | | `addresses` | `list` | A list of IP addresses assigned to the network interface. | | `loopback` | `bool` | Indicates if the network interface is a loopback interface. | | `up` | `bool` | Indicates if the network interface is up and can transmit data. | | `running` | `bool` | Indicates if the network interface is running and operational. | | `wireless` | `bool` | Indicates if the network interface is a wireless interface. | | `status` | `record` | A record containing detailed status information about the network interface. | The record `status` has the following schema: | Field | Type | Description | | :--------------- | :----- | :---------------------------------------------------- | | `unknown` | `bool` | Indicates if the network interface status is unknown. | | `connected` | `bool` | Indicates if the network interface is connected. | | `disconnected` | `bool` | Indicates if the network interface is disconnected. | | `not_applicable` | `bool` | Indicates if the network interface is not applicable. | ## Examples [Section titled â€œExamplesâ€](#examples) ### List all connected network interfaces [Section titled â€œList all connected network interfacesâ€](#list-all-connected-network-interfaces) ```tql nics where status.connected ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_nic`](/reference/operators/load_nic)

# ocsf::apply

Casts incoming events to their OCSF type. ```tql ocsf::apply [preserve_variants=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `ocsf::apply` operator casts incoming events to their corresponding OCSF event class type. The resulting type is determined by four fields: `metadata.version`, `metadata.profiles`, `metadata.extensions` and `class_uid`. Events sharing the same values for these fields are cast to the same type. Tenzir supports all OCSF versions (including `-dev` versions), all profiles, and all event classes. Extensions are currently limited to those versioned with OCSF, including the `win` and `linux` extensions. To this end, the operator performs the following steps: * Add optional fields that are not present in the original event with a `null` value * Emit a warning for extra fields that should not be there and drop them * Encode free-form objects (such as `unmapped`) using their JSON representation * Assign `@name` depending on the class name, for example: `ocsf.dns_activity` The types used for OCSF events are slightly adjusted. For example, timestamps use the native `time` type instead of an integer representing the number of milliseconds since the Unix epoch. Furthermore, some fields that would lead to infinite recursion are currently left out. We plan to support recursion up to a certain depth in the future. Furthermore, this operator will likely be extended with additional features, such as the ability to drop all optional fields, or to automatically assign OCSF enumerations based on their sibling ID. ### `preserve_variants = bool` [Section titled â€œpreserve\_variants = boolâ€](#preserve_variants--bool) Setting this option to `true` preserves free-form objects such as `unmapped` as-is, instead of being JSON-encoded. Note that this means the resulting event schema is no longer consistent across events of the same class, as changes to these free-form objects lead to different schemas. For schema-consistency and performance reasons, we recommend keeping this option `false` and instead using `unmapped.parse_json()` to extract fields on-demand. ## Examples [Section titled â€œExamplesâ€](#examples) ### Cast a single pre-defined event [Section titled â€œCast a single pre-defined eventâ€](#cast-a-single-pre-defined-event) ```tql from { class_uid: 4001, class_name: "Network Activity", metadata: { version: "1.5.0", }, unmapped: { foo: 1, bar: 2, }, // â€¦ some more fields } ocsf::apply ``` ```tql { class_uid: 4001, class_name: "Network Activity", metadata: { version: "1.5.0", // â€¦ all other metadata fields set to `null` }, unmapped: "{\"foo\": 1, \"bar\": 2}", // â€¦ other fields (with `null` if they didn't exist before) } ``` ### Preserve `unmapped` as a record [Section titled â€œPreserve unmapped as a recordâ€](#preserve-unmapped-as-a-record) ```tql from { class_uid: 4001, class_name: "Network Activity", metadata: { version: "1.5.0", }, unmapped: { foo: 1, bar: 2, }, } ocsf::apply preserve_variants=true select unmapped ``` ```tql { unmapped: { foo: 1, bar: 2, }, } ``` ### Filter, transform and send events to ClickHouse [Section titled â€œFilter, transform and send events to ClickHouseâ€](#filter-transform-and-send-events-to-clickhouse) ```tql subscribe "ocsf" where class_name == "Network Activity" and metadata.version == "1.5.0" ocsf::apply to_clickhouse table="network_activity" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::derive`](/reference/operators/ocsf/derive), [`ocsf::trim`](/reference/operators/ocsf/trim)

# ocsf::derive

Automatically assigns enum strings from their integer counterparts and vice versa. ```tql ocsf::derive ``` ## Description [Section titled â€œDescriptionâ€](#description) The `ocsf::derive` operator performs bidirectional enum derivation for OCSF events by automatically assigning enum string values based on their integer counterparts and vice versa. In the future, this operator will also assign automatically assign computable values such as `type_uid` based on what is available in the event. ## Examples [Section titled â€œExamplesâ€](#examples) ### Integer to string [Section titled â€œInteger to stringâ€](#integer-to-string) ```tql from { activity_id: 1, class_uid: 1001, metadata: { version: "1.5.0", }, } ocsf::derive ``` ```tql { activity_id: 1, activity_name: "Create", class_name: "File System Activity", class_uid: 1001, metadata: { version: "1.5.0", }, } ``` ### String to Integer [Section titled â€œString to Integerâ€](#string-to-integer) ```tql from { activity_name: "Read", class_uid: 1001, metadata: { version: "1.5.0", }, } ocsf::derive ``` ```tql { activity_id: 2, activity_name: "Read", class_name: "File System Activity", class_uid: 1001, metadata: { version: "1.5.0", }, } ``` ### Bidirectional enum validation [Section titled â€œBidirectional enum validationâ€](#bidirectional-enum-validation) ```tql from { activity_id: 1, activity_name: "Delete", // Inconsistent with activity_id=1 class_uid: 1001, metadata: { version: "1.5.0", }, } ocsf::derive ``` ```tql { activity_id: 1, activity_name: "Delete", class_name: "File System Activity", class_uid: 1001, metadata: { version: "1.5.0", }, } ``` This will emit a warning about inconsistent values and preserve both original values without modification, allowing the user to decide how to handle the conflict. ```plaintext warning: found inconsistency between `activity_id` and `activity_name` --> <input>:9:1 | 9 | ocsf::derive | ~~~~~~~~~~~~ | = note: got 1 ("Create") and "Delete" (4) ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::apply`](/reference/operators/ocsf/apply), [`ocsf::trim`](/reference/operators/ocsf/trim)

# ocsf::trim

Drops fields from OCSF events to reduce their size. ```tql ocsf::trim [drop_optional=bool, drop_recommended=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `ocsf::trim` operator uses intelligent analysis to determine which fields to remove from OCSF events, optimizing data size while preserving essential information. ### `drop_optional = bool` [Section titled â€œdrop\_optional = boolâ€](#drop_optional--bool) If specified, explicitly controls whether to remove fields marked as optional in the OCSF schema. Otherwise, this decision is left to the operator itself. ### `drop_recommended = bool` [Section titled â€œdrop\_recommended = boolâ€](#drop_recommended--bool) If specified, explicitly controls whether to remove fields marked as recommended in the OCSF schema. Otherwise, this decision is left to the operator itself. ## Examples [Section titled â€œExamplesâ€](#examples) ### Use intelligent field selection (default behavior) [Section titled â€œUse intelligent field selection (default behavior)â€](#use-intelligent-field-selection-default-behavior) ```tql from { class_uid: 3002, class_name: "Authentication", // will be removed metadata: { version: "1.5.0", }, user: { name: "alice", uid: "1000", display_name: "Alice", // will be removed }, auth_protocol: "Kerberos", status: "Success", status_id: 1, } ocsf::trim ``` ```tql { class_uid: 3002, metadata: { version: "1.5.0", }, user: { name: "alice", uid: "1000", }, auth_protocol: "Kerberos", status: "Success", status_id: 1, } ``` ### Explicitly remove optional fields only [Section titled â€œExplicitly remove optional fields onlyâ€](#explicitly-remove-optional-fields-only) ```tql from { class_uid: 1001, class_name: "File System Activity", metadata: {version: "1.5.0"}, file: { name: "document.txt", path: "/home/user/document.txt", size: 1024, // optional: will be removed type: "Regular File", // optional: also removed }, activity_id: 1, } ocsf::trim drop_optional=true, drop_recommended=false ``` ```tql { class_uid: 1001, metadata: { version: "1.5.0", }, file: { name: "document.txt", path: "/home/user/document.txt", }, activity_id: 1, } ``` ### Only keep required fields to minimize event size [Section titled â€œOnly keep required fields to minimize event sizeâ€](#only-keep-required-fields-to-minimize-event-size) ```tql from { class_uid: 4001, class_name: "Network Activity", metadata: {version: "1.5.0"}, src_endpoint: { ip: "192.168.1.100", port: 443, hostname: "client.local", }, severity: "Critical", severity_id: 5, } ocsf::trim drop_optional=true, drop_recommended=true ``` ```tql { class_uid: 4001, metadata: { version: "1.5.0", }, severity_id: 5, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::apply`](/reference/operators/ocsf/apply), [`ocsf::derive`](/reference/operators/ocsf/derive)

# openapi

Shows the nodeâ€™s OpenAPI specification. ```tql openapi ``` ## Description [Section titled â€œDescriptionâ€](#description) The `openapi` operator shows the current Tenzir nodeâ€™s [OpenAPI specification](/reference/node/api) for all available REST endpoint plugins. ## Examples [Section titled â€œExamplesâ€](#examples) ### Render the OpenAPI specification as YAML [Section titled â€œRender the OpenAPI specification as YAMLâ€](#render-the-openapi-specification-as-yaml) ```tql openapi write_yaml ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`api`](/reference/operators/api), [`serve`](/reference/operators/serve)

# package::add

Installs a package. ```tql package::add [package_path:string, inputs=record] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `package::add` operator installs all operators, pipelines, and contexts from a package. ### `package_path : string (optional)` [Section titled â€œpackage\_path : string (optional)â€](#package_path--string-optional) The path to a package located on the file system. ### `inputs = record (optional)` [Section titled â€œinputs = record (optional)â€](#inputs--record-optional) A record of optional package inputs that configure the package. ## Examples [Section titled â€œExamplesâ€](#examples) ### Add a package from the Community Library [Section titled â€œAdd a package from the Community Libraryâ€](#add-a-package-from-the-community-library) ```tql package::add "suricata-ocsf" ``` ### Add a local package with inputs [Section titled â€œAdd a local package with inputsâ€](#add-a-local-package-with-inputs) ```tql package::add "/mnt/config/tenzir/library/zeek", inputs={format: "tsv", "log-directory": "/opt/tenzir/logs"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`list`](/reference/operators/package/list), [`remove`](/reference/operators/package/remove)

# package::list

Shows installed packages. ```tql package::list [format=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `package::list` operator returns the list of all installed packages. ### `format = string (optional)` [Section titled â€œformat = string (optional)â€](#format--string-optional) Controls the output format. Valid options are `compact` and `extended`. Defaults to `compact`. ## Schemas [Section titled â€œSchemasâ€](#schemas) The `package::list` operator produces two output formats, controlled by the `format` option: * `compact`: succinct output in a human-readable format * `extended`: verbose output in a machine-readable format The formats generate the following schemas below. ### `tenzir.package.compact` [Section titled â€œtenzir.package.compactâ€](#tenzirpackagecompact) The compact format prints the package information according to the following schema: | Field | Type | Description | | :------------ | :------- | :-------------------------------------- | | `id` | `string` | The unique package id. | | `name` | `string` | The name of this package. | | `author` | `string` | The package author. | | `description` | `string` | The description of this package. | | `config` | `record` | The user-provided package configuration | ### `tenzir.package.extended` [Section titled â€œtenzir.package.extendedâ€](#tenzirpackageextended) The `extended` format is mainly intended for use by non-human consumers, like shell scripts or frontend code. It contains all available information about a package. | Field | Type | Description | | :------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------ | | `package_definition` | `record` | The original package definition object asa found in the library. | | `resolved_package` | `record` | The effective package definition that was produced by applying all inputs and overrides from the `config` section and removing all disabled pipelines and contexts. | | `config` | `record` | The user-provided package configuration. | | `package_status` | `record` | Run-time information about the package provided the package manager. | The `config` object has the following schema, where all fields are optional: | Field | Type | Description | | :---------- | :------- | :------------------------------------------------------------ | | `version` | `string` | The package version. | | `source` | `record` | The upstream location of the package definition. | | `inputs` | `record` | User-provided values for the package inputs. | | `overrides` | `record` | User-provided overrides for fields in the package definition. | | `metadata` | `record` | An opaque record that can be set during installation. | The `package_status` object has the following schema: | Field | Type | Description | | :------------------- | :------- | :---------------------------------------------------------------------------------------------- | | `install_state` | `string` | The install state of this package. One of `installing`, `installed`, `removing` or `zombie`. | | `from_configuration` | `bool` | Whether the package was installed from the `package add` operator or from a configuration file. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Show all installed packages [Section titled â€œShow all installed packagesâ€](#show-all-installed-packages) ```tql package::list ``` ```tql { "id": "suricata-ocsf", "name": "Suricata OCSF Mappings", "author": "Tenzir", "description": "[Suricata](https://suricata.io) is an open-source network monitor and\nthreat detection tool.\n\nThis package converts all Suricata events published on the topic `suricata` to\nOCSF and publishes the converted events on the topic `ocsf`.\n", "config": { "inputs": {}, "overrides": {} } } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`list`](/reference/operators/pipeline/list), [`package::add`](/reference/operators/package/add), [`package::remove`](/reference/operators/package/remove)

# package::remove

Uninstalls a package. ```tql package::remove package_id:string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `package::remove` operator uninstalls a previously installed package. ### `package_id : string` [Section titled â€œpackage\_id : stringâ€](#package_id--string) The unique ID of the package as in the package definition. ## Examples [Section titled â€œExamplesâ€](#examples) ### Remove an installed package [Section titled â€œRemove an installed packageâ€](#remove-an-installed-package) ```tql package::remove "suricata-ocsf" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`package::add`](/reference/operators/package/add)

# partitions

Retrieves metadata about events stored at a node. ```tql partitions [predicate:expr] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `partitions` operator shows a summary of candidate partitions at a node. ### `predicate: expr (optional)` [Section titled â€œpredicate: expr (optional)â€](#predicate-expr-optional) Show only partitions which would be considered for pipelines of the form `export | where <expr>` instead of returning all data. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir emits partition information with the following schema: ### `tenzir.partition` [Section titled â€œtenzir.partitionâ€](#tenzirpartition) Contains detailed information about a partition. | Field | Type | Description | | :---------------- | :------- | :----------------------------------------------------------------------------------- | | `uuid` | `string` | The unique ID of the partition in the UUIDv4 format. | | `memusage` | `uint64` | The memory usage of the partition in bytes. | | `diskusage` | `uint64` | The disk usage of the partition in bytes. | | `events` | `uint64` | The number of events contained in the partition. | | `min_import_time` | `time` | The time at which the first event of the partition arrived at the `import` operator. | | `max_import_time` | `time` | The time at which the last event of the partition arrived at the `import` operator. | | `version` | `uint64` | The version number of the internal partition storage format. | | `schema` | `string` | The schema name of the events contained in the partition. | | `schema_id` | `string` | A unique identifier for the physical layout of the partition. | | `store` | `record` | Resource information about the partitionâ€™s store. | | `indexes` | `record` | Resource information about the partitionâ€™s indexes. | | `sketches` | `record` | Resource information about the partitionâ€™s sketches. | The records `store`, `indexes`, and `sketches` have the following schema: | Field | Type | Description | | :----- | :------- | :------------------------ | | `url` | `string` | The URL of the resource. | | `size` | `uint64` | The size of the resource. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Get memory and disk requirements of all stored data [Section titled â€œGet memory and disk requirements of all stored dataâ€](#get-memory-and-disk-requirements-of-all-stored-data) ```tql partitions summarize schema, events=sum(events), diskusage=sum(diskusage), memusage=sum(memusage) sort schema ``` ### Get an upper bound of events that have a field `src_ip` with 127.0.0.1 [Section titled â€œGet an upper bound of events that have a field src\_ip with 127.0.0.1â€](#get-an-upper-bound-of-events-that-have-a-field-src_ip-with-127001) ```tql partitions src_ip == 127.0.0.1 summarize candidates=sum(events) ``` ### See how many partitions contain a non-null value for the field `hostname` [Section titled â€œSee how many partitions contain a non-null value for the field hostnameâ€](#see-how-many-partitions-contain-a-non-null-value-for-the-field-hostname) ```tql partitions hostname != null ```

# pass

Does nothing with the input. ```tql pass ``` ## Description [Section titled â€œDescriptionâ€](#description) The `pass` operator relays the input without any modification. Outside of testing and debugging, it is only used when an empty pipeline needs to created, as `{}` is a record, while `{ pass }` is a pipeline. ## Examples [Section titled â€œExamplesâ€](#examples) ### Forward the input without any changes [Section titled â€œForward the input without any changesâ€](#forward-the-input-without-any-changes) ```tql pass ``` ### Do nothing every 10s [Section titled â€œDo nothing every 10sâ€](#do-nothing-every-10s) ```tql every 10s { pass } ```

# pipeline::activity

Summarizes the activity of pipelines. ```tql pipeline::activity range=duration, interval=duration ``` ## Description [Section titled â€œDescriptionâ€](#description) Internal Operator This operator is only for internal usage by the Tenzir Platform. It can change without notice. ### `range = duration` [Section titled â€œrange = durationâ€](#range--duration) The range for which the activity should be fetched. Note that the individual rates returned by this operator typically represent a larger range because they are aligned with the interval. ### `interval = duration` [Section titled â€œinterval = durationâ€](#interval--duration) The interval used to summarize the individual throughout rates. Needs to be a multiple of the built-in storage interval, which is typically `10min`. Also needs to cleanly divide `range`. ## Schemas [Section titled â€œSchemasâ€](#schemas) ### `tenzir.activity` [Section titled â€œtenzir.activityâ€](#tenziractivity) | Field | Type | Description | | :---------- | :------------- | :-------------------------------------------------------- | | `first` | `time` | The time of the first throughput rate in the lists below. | | `last` | `time` | The time of the last throughput rate in the lists below. | | `pipelines` | `list<record>` | The activity for individual pipelines. | The records in `pipelines` have the following schema: | Field | Type | Description | | :-------- | :------- | :----------------------------------------------------------------- | | `id` | `string` | The ID uniquely identifying the pipeline this activity belongs to. | | `ingress` | `record` | The activity at the source of the pipeline. | | `egress` | `record` | The activity at the destination of the pipeline. | The records `ingress` and `egress` have the following schema: | Field | Type | Description | | :--------- | :------------- | :------------------------------------------------------- | | `internal` | `bool` | Whether this end of the pipeline is considered internal. | | `bytes` | `uint64` | The total number of bytes over the range. | | `rates` | `list<uint64>` | The throughput in bytes/second over time. | You can derive the time associated with a given throughput rate with the formula `first + index*interval`, except the last value, which is associated with `last`. The recommended way to chart these values is to show a sliding window over `[last - range, last]`. The value in `bytes` is an approximation for the total number of bytes inside that window. ## Examples [Section titled â€œExamplesâ€](#examples) ### Show the activity over the last 20s [Section titled â€œShow the activity over the last 20sâ€](#show-the-activity-over-the-last-20s) ```tql pipeline::activity range=20s, interval=20s ``` ```tql { first: 2025-05-07T08:33:40.000Z, last: 2025-05-07T08:34:10.000Z, pipelines: [ { id: "3b43d497-5f4d-47f4-b191-5f432644d5ba", ingress: { internal: true, bytes: 289800, rates: [ 14490, 14490, 14490, ], }, egress: { internal: true, bytes: 292360, rates: [ 14721.75, 14514.25, 14488.8, ], }, }, ], } ```

# pipeline::detach

Starts a pipeline in the node. ```tql pipeline::detach { â€¦ }, [id=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `pipeline::detach` operator starts a hidden managed pipeline in the node, and returns as soon as the pipeline has started. Subject to Change This operator primarily exists for testing purposes, where it is often required to run pipelines in the background, but to be able to wait until the pipeline has started. The operator may change without further notice. ### `id = string (optional)` [Section titled â€œid = string (optional)â€](#id--string-optional) Sets the pipelineâ€™s ID explicitly, instead of assigning a random ID. This corresponds to the `id` field in the output of `pipeline::list`, and the `pipeline_id` field in the output of `metrics` and `diagnostics`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Run a pipeline in the background [Section titled â€œRun a pipeline in the backgroundâ€](#run-a-pipeline-in-the-background) ```tql pipeline::detach { every 1min { version } select version write_lines save_stdout } ``` ## See also [Section titled â€œSee alsoâ€](#see-also) [`run`](/reference/operators/pipeline/run)

# pipeline::list

Shows managed pipelines. ```tql pipeline::list ``` ## Description [Section titled â€œDescriptionâ€](#description) The `pipeline::list` operator returns the list of all managed pipelines. Managed pipelines are pipelines created through the [`/pipeline` API](/reference/node/api), which includes all pipelines run through the Tenzir Platform. ## Examples [Section titled â€œExamplesâ€](#examples) ### Count pipelines per state [Section titled â€œCount pipelines per stateâ€](#count-pipelines-per-state) ```tql pipeline::list top state ``` ```tql { "state": "running", "count": 31 } { "state": "failed", "count": 4 } { "state": "stopped", "count": 2 } ``` ### Show pipelines per package [Section titled â€œShow pipelines per packageâ€](#show-pipelines-per-package) ```tql pipeline::list summarize package, names=collect(name) ``` ```tql { "package": "suricata-ocsf", "names": [ "Suricata Flow to OCSF Network Activity", "Suricata DNS to OCSF DNS Activity", "Suricata SMB to OCSF SMB Activity", // â€¦ ] } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`list`](/reference/operators/package/list), [`run`](/reference/operators/pipeline/run)

# pipeline::run

Starts a pipeline in the node and waits for it to complete. ```tql pipeline::run { â€¦ }, [id=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `pipeline::run` operator starts a hidden managed pipeline in the node, and returns when the pipeline has finished. Note that pipelines may emit diagnostics after they have finished. Subject to Change This operator primarily exists for testing purposes, where it is often required to run pipelines with an explicitly specified pipeline id. ### `{ â€¦ }` [Section titled â€œ{ â€¦ }â€](#-) The pipeline to execute. This pipeline runs as a separate managed pipeline within the node. ### `id = string (optional)` [Section titled â€œid = string (optional)â€](#id--string-optional) Sets the pipelineâ€™s ID explicitly, instead of assigning a random ID. This corresponds to the `id` field in the output of `pipeline::list`, and the `pipeline_id` field in the output of `metrics` and `diagnostics`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Run a pipeline in the background and wait for it to complete [Section titled â€œRun a pipeline in the background and wait for it to completeâ€](#run-a-pipeline-in-the-background-and-wait-for-it-to-complete) ```tql pipeline::run { every 1min { version } select version write_lines save_stdout } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`pipeline::detach`](/reference/operators/pipeline/detach), [`pipeline::list`](/reference/operators/pipeline/list)

# plugins

Shows all available plugins and built-ins. ```tql plugins ``` ## Description [Section titled â€œDescriptionâ€](#description) The `plugins` operator shows all available plugins and built-ins. Tenzir is built on a modular monolith architecture. Most features are available as plugins and extensible by developers. Tenzir comes with a set of built-ins and bundled plugins. The former use the plugin API but are available as part of the core library, and the latter are plugins shipped with Tenzir. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir emits plugin information with the following schema. ### `tenzir.plugin` [Section titled â€œtenzir.pluginâ€](#tenzirplugin) Contains detailed information about the available plugins. | Field | Type | Description | | :------------- | :------------- | :------------------------------------------------------------------------------------------ | | `name` | `string` | The unique, case-insensitive name of the plugin. | | `version` | `string` | The version identifier of the plugin, or `bundled` if the plugin has no version of its own. | | `kind` | `string` | The kind of plugin. One of `builtin`, `static`, or `dynamic`. | | `types` | `list<string>` | The interfaces implemented by the plugin, e.g., `operator` or `function`. | | `dependencies` | `list<string>` | Plugins that must be loaded for this plugin to function. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Show all currently available functions [Section titled â€œShow all currently available functionsâ€](#show-all-currently-available-functions) ```tql plugins where "function" in types summarize functions=collect(name) ```

# processes

Shows a snapshot of running processes. ```tql processes ``` ## Description [Section titled â€œDescriptionâ€](#description) The `processes` operator shows a snapshot of all currently running processes. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir emits process information with the following schema. ### `tenzir.process` [Section titled â€œtenzir.processâ€](#tenzirprocess) Contains detailed information about the process. | Field | Type | Description | | :------------- | :------------- | :----------------------------------------------------------- | | `name` | `string` | The process name. | | `command_line` | `list<string>` | The command line of the process. | | `pid` | `uint64` | The process identifier. | | `ppid` | `uint64` | The parent process identifier. | | `uid` | `uint64` | The user identifier of the process owner. | | `gid` | `uint64` | The group identifier of the process owner. | | `ruid` | `uint64` | The real user identifier of the process owner. | | `rgid` | `uint64` | The real group identifier of the process owner. | | `priority` | `string` | The priority level of the process. | | `startup` | `time` | The time when the process was started. | | `vsize` | `uint64` | The virtual memory size of the process. | | `rsize` | `uint64` | The resident set size (physical memory used) of the process. | | `swap` | `uint64` | The amount of swap memory used by the process. | | `peak_mem` | `uint64` | Peak memory usage of the process. | | `open_fds` | `uint64` | The number of open file descriptors by the process. | | `utime` | `duration` | The user CPU time consumed by the process. | | `stime` | `duration` | The system CPU time consumed by the process. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Show running processes by runtime [Section titled â€œShow running processes by runtimeâ€](#show-running-processes-by-runtime) ```tql processes sort -startup ``` ### Show the top five running processes by name [Section titled â€œShow the top five running processes by nameâ€](#show-the-top-five-running-processes-by-name) ```tql processes top name head 5 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`files`](/reference/operators/files), [`sockets`](/reference/operators/sockets)

# publish

Publishes events to a channel with a topic. ```tql publish [topic:string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `publish` operator publishes events at a node in a channel with the specified topic. All [`subscribers`](/reference/operators/subscribe) of the channel operator receive the events immediately. ### `topic: string (optional)` [Section titled â€œtopic: string (optional)â€](#topic-string-optional) An optional topic for publishing events under. If unspecified, the operator publishes events to the topic `main`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Publish Zeek connection logs under the fixed topic `zeek` [Section titled â€œPublish Zeek connection logs under the fixed topic zeekâ€](#publish-zeek-connection-logs-under-the-fixed-topic-zeek) ```tql from "conn.log.gz" { decompress_gzip read_zeek_tsv } publish "zeek" ``` ### Publish Suricata events under a dynamic topic depending on their event type [Section titled â€œPublish Suricata events under a dynamic topic depending on their event typeâ€](#publish-suricata-events-under-a-dynamic-topic-depending-on-their-event-type) ```tql from "eve.json" { read_suricata } publish f"suricata.{event_type}" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`import`](/reference/operators/import), [`subscribe`](/reference/operators/subscribe)

# python

Executes Python code against each event of the input. ```tql python code:string, [requirements=string] python file=string, [requirements=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `python` operator executes user-provided Python code against each event of the input. By default, the Tenzir node executing the pipeline creates a virtual environment into which the `tenzir` Python package is installed. This behavior can be turned off in the node configuration using the `plugin.python.create-venvs` boolean option. ### `code: string` [Section titled â€œcode: stringâ€](#code-string) The provided Python code describes an event-to-event transformation, i.e., it is executed once for each input event and produces exactly output event. An implicitly defined `self` variable represents the event. Modify it to alter the output of the operator. Fields of the event can be accessed with the dot notation. For example, if the input event contains fields `a` and `b` then the Python code can access and modify them using `self.a` and `self.b`. Similarly, new fields are added by assigning to `self.fieldname` and existing fields can be removed by deleting them from `self`. When new fields are added, it is required that the new field has the same type for every row of the event. ### `file: string` [Section titled â€œfile: stringâ€](#file-string) Instead of providing the code inline, the `file` option allows for passing a path to a file containing the code the operator executes per event. ### `requirements = string (optional)` [Section titled â€œrequirements = string (optional)â€](#requirements--string-optional) The `requirements` flag can be used to pass additional package dependencies in the pip format. When it is used, the argument is passed on to `pip install` in a dedicated virtual environment. The string is passed verbatim to `pip install`. To add multiple dependencies, separate them with a space: `requirements="foo bar"`. ## Secrets [Section titled â€œSecretsâ€](#secrets) By default, the `python` operator does not accept secrets. If you want to allow usage of secrets in the `code` argument, you can enable the configuration option `tenzir.allow-secrets-in-escape-hatches`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Insert or modify a field [Section titled â€œInsert or modify a fieldâ€](#insert-or-modify-a-field) Set field `x` to `"hello, world"` ```tql python "self.x = 'hello, world'" ``` ### Remove all fields from an event [Section titled â€œRemove all fields from an eventâ€](#remove-all-fields-from-an-event) Clear the contents of `self` to remove the implicit input values from the output: ```tql python " self.clear() " ``` ### Insert a new field [Section titled â€œInsert a new fieldâ€](#insert-a-new-field) Define a new field `x` as the square root of the field `y`, and remove `y` from the output: ```tql python " import math self.x = math.sqrt(self.y) del self.y " ``` ### Make use of third party packages [Section titled â€œMake use of third party packagesâ€](#make-use-of-third-party-packages) ```tql python r#" import requests requests.post("http://imaginary.api/receive", data=self) "#, requirements="requests=^2.30" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`shell`](/reference/operators/shell)

# rare

Shows the least common values. ```tql rare x:field ``` ## Description [Section titled â€œDescriptionâ€](#description) Shows the least common values for a given field. For each unique value, a new event containing its count will be produced. In general, `rare x` is equivalent to: This operator is the dual to [`top`](/reference/operators/top). ```tql summarize x, count=count() sort count ``` ### `x: field` [Section titled â€œx: fieldâ€](#x-field) The name of the field to find the least common values for. ## Examples [Section titled â€œExamplesâ€](#examples) ### Find the least common values [Section titled â€œFind the least common valuesâ€](#find-the-least-common-values) ```tql from {x: "B"}, {x: "A"}, {x: "A"}, {x: "B"}, {x: "A"}, {x: "D"}, {x: "C"}, {x: "C"} rare x ``` ```tql {x: "D", count: 1} {x: "C", count: 2} {x: "B", count: 2} {x: "A", count: 3} ``` ### Show the five least common values for `id.orig_h` [Section titled â€œShow the five least common values for id.orig\_hâ€](#show-the-five-least-common-values-for-idorig_h) ```tql rare id.orig_h head 5 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`summarize`](/reference/operators/summarize), [`sort`](/reference/operators/sort), [`top`](/reference/operators/top)

# read_all

Parses an incoming bytes stream into a single event. ```tql read_all [binary=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_all` operator takes its input bytes and produces a single event that contains everything. This is useful if the entire stream is needed for further processing at once. The resulting events have a single field called `data`. ### `binary = bool (optional)` [Section titled â€œbinary = bool (optional)â€](#binary--bool-optional) Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read an entire text file into a single event [Section titled â€œRead an entire text file into a single eventâ€](#read-an-entire-text-file-into-a-single-event) ```tql load_file "data.txt" read_all ``` ```tql {data: "<file contents>"} ``` ### Read an entire binary file into a single event [Section titled â€œRead an entire binary file into a single eventâ€](#read-an-entire-binary-file-into-a-single-event) ```tql load_file "data.bin" read_all binary=true ``` ```tql {data: b"<file contents>"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_delimited`](/reference/operators/read_delimited), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_lines`](/reference/operators/read_lines),

# read_bitz

Parses bytes as *BITZ* format. ```tql read_bitz ``` ## Description [Section titled â€œDescriptionâ€](#description) BITZ is short for **Bi**nary **T**en**z**ir and is our internal wire format. Use BITZ when you need high-throughput structured data exchange with minimal overhead. BITZ is a thin wrapper around Arrowâ€™s record batches. That is, BITZ lays out data in a (compressed) columnar fashion that makes it conducive for analytical workloads. Since itâ€™s padded and byte-aligned, it is portable and doesnâ€™t induce any deserialization cost, making it suitable for write-once-read-many use cases. Internally, BITZ uses Arrowâ€™s IPC format for serialization and deserialization, but prefixes each message with a 64 bit size prefix to support changing schemas between batchesâ€”something that Arrowâ€™s IPC format does not support on its own. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_bitz`](/reference/operators/read_bitz), [`read_feather`](/reference/operators/write_feather), [`read_parquet`](/reference/operators/write_parquet), [`write_bitz`](/reference/operators/write_bitz)

# read_cef

Parses an incoming Common Event Format (CEF) stream into events. ```tql read_cef [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The [Common Event Format (CEF)](https://community.microfocus.com/cfs-file/__key/communityserver-wikis-components-files/00-00-00-00-23/3731.CommonEventFormatV25.pdf) is a text-based event format that originally stems from ArcSight. It is line-based and human readable. The first 7 fields of a CEF event are always the same, and the 8th *extension* field is an optional list of key-value pairs: ```plaintext CEF:Version|Device Vendor|Device Product|Device Version|Device Event Class ID|Name|Severity|[Extension] ``` Here is a real-world example: ```plaintext CEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8| externalId=6 clientId=2251997 scanGroupId=3 scanGroupName=Manually Installed Agents sev=High duser=tikasrv01\\administrator cat=END-POINT Alert dhost=TikaSrv01 src=172.31.5.93 filePath=c:\\windows\\temp\\javac.exe fname=javac.exe rt=3/30/2022 10:55:34 AM fileHash=2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD rtUtc=Mar 30 2022 10:55:34.688 dtUtc=Mar 30 2022 10:55:32.458 hostLS=2022-03-30 10:55:34 GMT+00:00 osVer=Windows Server 2016 Datacenter x64 1607 epsVer=4.5.5.6845 confVer=637842168250000000 prUser=tikasrv01\\administrator pParams="C:\\Windows\\Temp\\javac.exe" sign=Not signed pct=2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609 pFileHash=1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7 pprUser=tikasrv01\\administrator ppParams=C:\\Windows\\Explorer.EXE pssdeep=49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3 pSign=Signed and has certificate info gpFileHash=CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F gpprUser=tikasrv01\\administrator gpParams=C:\\Windows\\system32\\userinit.exe gpssdeep=384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu gpSign=Signed actRem=Kill, Rename ``` The [CEF specification](https://community.microfocus.com/cfs-file/__key/communityserver-wikis-components-files/00-00-00-00-23/3731.CommonEventFormatV25.pdf) pre-defines several extension field key names and data types for the corresponding values. Tenzirâ€™s parser does not enforce the strict definitions and instead tries to infer the type from the provided values. Tenzir translates the `extension` field to a nested record, where the key-value pairs of the extensions map to record fields. Here is an example of the above event: Output (shortened) ```json { "cef_version": 0, "device_vendor": "Cynet", "device_product": "Cynet 360", "device_version": "4.5.4.22139", "signature_id": "0", "name": "Memory Pattern - Cobalt Strike Beacon ReflectiveLoader", "severity": "8", "extension": { "externalId": 6, "clientId": 2251997, "scanGroupId": 3, ... "gpssdeep": "384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu", "gpSign": "Signed", "actRem": "Kill, Rename" } } ``` ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_cef`](/reference/functions/parse_cef), [`print_cef`](/reference/functions/print_cef), [`read_leef`](/reference/operators/read_leef)

# read_csv

Read CSV (Comma-Separated Values) from a byte stream. ```tql read_csv [list_separator=string, null_value=string, comments=bool, header=string, quotes=string, auto_expand=bool, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_csv` operator transforms a byte stream into a event stream by parsing the bytes as [CSV](https://en.wikipedia.org/wiki/Comma-separated_values). ### `auto_expand = bool (optional)` [Section titled â€œauto\_expand = bool (optional)â€](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `comments = bool (optional)` [Section titled â€œcomments = bool (optional)â€](#comments--bool-optional) Treat lines beginning with â€#â€ as comments. ### `header = list<string>|string (optional)` [Section titled â€œheader = list\<string>|string (optional)â€](#header--liststringstring-optional) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header. ### `list_separator = string (optional)` [Section titled â€œlist\_separator = string (optional)â€](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `;`. ### `null_value = string (optional)` [Section titled â€œnull\_value = string (optional)â€](#null_value--string-optional) The `string` denoting an absent value. Defaults to empty string (`""`). ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Read a CSV file with header [Section titled â€œRead a CSV file with headerâ€](#read-a-csv-file-with-header) input.csv ```txt message,count,ip some text,42,"1.1.1.1" more text,100,"1.1.1.2" ``` ```tql load "input.csv" read_csv ``` ```tql {message: "some text", count: 42, ip: 1.1.1.1} {message: "more text", count: 100, ip: 1.1.1.2} ``` ### Manually specify a header [Section titled â€œManually specify a headerâ€](#manually-specify-a-header) input\_no\_header.csv ```txt some text,42,"1.1.1.1" more text,100,"1.1.1.2" ``` ```tql load "input_no_header.csv" read_csv header="message,count,ip" ``` ```tql {message: "some text", count: 42, ip: 1.1.1.1} {message: "more text", count: 100, ip: 1.1.1.2} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_csv`](/reference/functions/parse_csv), [`print_csv`](/reference/functions/print_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv), [`write_csv`](/reference/operators/write_csv)

# read_delimited

Parses an incoming bytes stream into events using a string as delimiter. ```tql read_delimited separator:string|blob, [binary=bool, include_separator=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_delimited` operator takes its input bytes and splits it using the provided string as a delimiter. This is useful for parsing data that uses simple string delimiters instead of regular expressions or standard newlines. The resulting events have a single field called `data`. ### `separator: string|blob (required)` [Section titled â€œseparator: string|blob (required)â€](#separator-stringblob-required) The string or blob to use as delimiter. The operator will split the input whenever this exact sequence is matched. When a blob literal is provided (e.g., `b"\x00\x01"`), the `binary` option defaults to `true`. ### `binary = bool (optional)` [Section titled â€œbinary = bool (optional)â€](#binary--bool-optional) Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`. ### `include_separator = bool (optional)` [Section titled â€œinclude\_separator = bool (optional)â€](#include_separator--bool-optional) When enabled, includes the matched separator string in the output events. By default, the separator is excluded from the results. ## Examples [Section titled â€œExamplesâ€](#examples) ### Split on a simple delimiter [Section titled â€œSplit on a simple delimiterâ€](#split-on-a-simple-delimiter) ```tql load_file "data.txt" read_delimited "||" ``` ### Parse CSV-like data with custom delimiter [Section titled â€œParse CSV-like data with custom delimiterâ€](#parse-csv-like-data-with-custom-delimiter) ```tql load_file "custom.csv" read_delimited ";;;" ``` ### Include the separator in the output [Section titled â€œInclude the separator in the outputâ€](#include-the-separator-in-the-output) ```tql load_file "data.txt" read_delimited "||", include_separator=true ``` ### Parse binary data with blob delimiters [Section titled â€œParse binary data with blob delimitersâ€](#parse-binary-data-with-blob-delimiters) ```tql load_file "binary.dat" read_delimited b"\x00\x01" ``` ### Use blob separator with include\_separator [Section titled â€œUse blob separator with include\_separatorâ€](#use-blob-separator-with-include_separator) ```tql load_file "data.txt" read_delimited b"||", include_separator=true ``` ### Parse binary data with string delimiters [Section titled â€œParse binary data with string delimitersâ€](#parse-binary-data-with-string-delimiters) ```tql load_file "binary.dat" read_delimited "\x00\x01", binary=true ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_all`](/reference/operators/read_all), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_lines`](/reference/operators/read_lines), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_delimited_regex

Parses an incoming bytes stream into events using a regular expression as delimiter. ```tql read_delimited_regex regex:string|blob, [binary=bool, include_separator=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_delimited_regex` operator takes its input bytes and splits it using the provided regular expression as a delimiter. This is useful for parsing data that uses custom delimiters or patterns instead of standard newlines. The regular expression flavor is Perl compatible and documented [here](https://www.boost.org/doc/libs/1_88_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html). The resulting events have a single field called `data`. ### `regex: string|blob (required)` [Section titled â€œregex: string|blob (required)â€](#regex-stringblob-required) The regular expression pattern to use as delimiter. This can be provided as a string or blob literal. The operator will split the input whenever this pattern is matched. When a blob literal is provided (e.g., `b"\\x00\\x01"`), the `binary` option defaults to `true`. ### `binary = bool (optional)` [Section titled â€œbinary = bool (optional)â€](#binary--bool-optional) Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`. ### `include_separator = bool (optional)` [Section titled â€œinclude\_separator = bool (optional)â€](#include_separator--bool-optional) When enabled, includes the matched separator pattern in the output events. By default, the separator is excluded from the results. ## Examples [Section titled â€œExamplesâ€](#examples) ### Split Syslog-like events without newline terminators from a TCP input [Section titled â€œSplit Syslog-like events without newline terminators from a TCP inputâ€](#split-syslog-like-events-without-newline-terminators-from-a-tcp-input) ```tql load_tcp "0.0.0.0:514" read_delimited_regex "(?=<[0-9]+>)" this = data.parse_syslog() ``` ### Parse log entries separated by timestamps [Section titled â€œParse log entries separated by timestampsâ€](#parse-log-entries-separated-by-timestamps) ```tql load_file "application.log" read_delimited_regex "(?=\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})" ``` ### Split on multiple possible delimiters [Section titled â€œSplit on multiple possible delimitersâ€](#split-on-multiple-possible-delimiters) ```tql load_file "mixed_delimiters.txt" read_delimited_regex "[;|]" ``` ### Include the separator in the output [Section titled â€œInclude the separator in the outputâ€](#include-the-separator-in-the-output) ```tql load_file "data.txt" read_delimited_regex "\\|\\|", include_separator=true ``` ### Parse binary data with blob patterns [Section titled â€œParse binary data with blob patternsâ€](#parse-binary-data-with-blob-patterns) ```tql load_file "binary.dat" read_delimited_regex b"\\x00\\x01" ``` ### Use blob pattern with include\_separator for binary delimiters [Section titled â€œUse blob pattern with include\_separator for binary delimitersâ€](#use-blob-pattern-with-include_separator-for-binary-delimiters) ```tql load_file "protocol.dat" read_delimited_regex b"\\xFF\\xFE", include_separator=true ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_all`](/reference/operators/read_all), [`read_delimited`](/reference/operators/read_delimited), [`read_lines`](/reference/operators/read_lines), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_feather

Parses an incoming Feather byte stream into events. ```tql read_feather ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms the input [Feather](https://arrow.apache.org/docs/python/feather.html) (a thin wrapper around [Apache Arrowâ€™s IPC](https://arrow.apache.org/docs/python/ipc.html) wire format) byte stream to event stream. ## Examples [Section titled â€œExamplesâ€](#examples) ### Publish a feather logs file [Section titled â€œPublish a feather logs fileâ€](#publish-a-feather-logs-file) ```tql load_file "logs.feather" read_feather pulish "log" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_parquet`](/reference/operators/read_parquet), [`write_feather`](/reference/operators/write_feather)

# read_gelf

Parses an incoming GELF stream into events. ```tql read_gelf [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) Parses an incoming [GELF](https://go2docs.graylog.org/current/getting_in_log_data/gelf.html) stream into events. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Read a GELF stream from a TCP socket [Section titled â€œRead a GELF stream from a TCP socketâ€](#read-a-gelf-stream-from-a-tcp-socket) ```tql load_tcp "0.0.0.0:54321" read_gelf ```

# read_grok

Parses lines of input with a grok pattern. ```tql read_grok pattern:string, [pattern_definitions=record|string, indexed_captures=bool, include_unnamed=bool, schema=string, selector=string, schema_only=bool, merge=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) `read_grok` uses a regular expression based parser similar to the [Logstash `grok` plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) in Elasticsearch. Tenzir ships with the same built-in patterns as Elasticsearch, found [here](https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns/ecs-v1). In short, `pattern` consists of replacement fields, that look like `%{SYNTAX[:SEMANTIC[:CONVERSION]]}`, where: * `SYNTAX` is a reference to a pattern, either built-in or user-defined through the `pattern_defintions` option. * `SEMANTIC` is an identifier that names the field in the parsed record. * `CONVERSION` is either `infer` (default), `string` (default with `raw=true`), `int`, or `float`. The supported regular expression syntax is the one supported by [Boost.Regex](https://www.boost.org/doc/libs/1_81_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html), which is effectively Perl-compatible. ### `pattern: string` [Section titled â€œpattern: stringâ€](#pattern-string) The `grok` pattern used for matching. Must match the input in its entirety. ### `pattern_definitions = record|string (optional)` [Section titled â€œpattern\_definitions = record|string (optional)â€](#pattern_definitions--recordstring-optional) New pattern definitions to use. This may be a record of the form ```tql { pattern_name: "pattern" } ``` For example, the built-in pattern `INT` would be defined as ```tql { INT: "(?:[+-]?(?:[0-9]+))" } ``` Alternatively, this may be a user-defined newline-delimited list of patterns, where a line starts with the pattern name, followed by a space, and the `grok`-pattern for that pattern. For example, the built-in pattern `INT` is defined as follows: ```plaintext INT (?:[+-]?(?:[0-9]+)) ``` ### `indexed_captures = bool (optional)` [Section titled â€œindexed\_captures = bool (optional)â€](#indexed_captures--bool-optional) All subexpression captures are included in the output, with the `SEMANTIC` used as the field name if possible, and the capture index otherwise. ### `include_unnamed = bool (optional)` [Section titled â€œinclude\_unnamed = bool (optional)â€](#include_unnamed--bool-optional) By default, only fields that were given a name with `SEMANTIC`, or with the regular expression named capture syntax `(?<name>...)` are included in the resulting record. With `include_unnamed=true`, replacement fields without a `SEMANTIC` are included in the output, using their `SYNTAX` value as the record field name. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse a fictional HTTP request log [Section titled â€œParse a fictional HTTP request logâ€](#parse-a-fictional-http-request-log) ```tql // Input: 55.3.244.1 GET /index.html 15824 0.043 let $pattern = "%{IP:client} %{WORD} %{URIPATHPARAM:req} %{NUMBER:bytes} %{NUMBER:dur}" read_grok $pattern ``` ```tql { client: 55.3.244.1, req: "/index.html", bytes: 15824, dur: 0.043 } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_grok`](/reference/functions/parse_grok)

# read_json

Parses an incoming JSON stream into events. ```tql read_json [schema=string, selector=string, schema_only=bool, merge=bool, raw=bool, unflatten_separator=string, arrays_of_objects=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Parses an incoming JSON byte stream into events. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ### `arrays_of_objects = bool (optional)` [Section titled â€œarrays\_of\_objects = bool (optional)â€](#arrays_of_objects--bool-optional) Default: `false`. Parse arrays of objects, with every object in the outermost arrays resulting in one event each. This is particularly useful when interfacing with REST APIs, which often yield large arrays of objects instead of newline-delimited JSON objects. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read a JSON file [Section titled â€œRead a JSON fileâ€](#read-a-json-file) input.json ```json { "product": "Tenzir", "version.major": 4, "version.minor": 22 } { "product": "Tenzir", "version.major": 4, "version.minor": 21, "version.dirty": true } ``` Pipeline ```tql load_file "events.json" read_json unflatten="." ``` Output ```json { "product": "Tenzir", "version": { "major": 4, "minor": 22 } } { "product": "Tenzir", "version": { "major": 4, "minor": 21, "dirty": true } } ``` ### Read a JSON array [Section titled â€œRead a JSON arrayâ€](#read-a-json-array) [JA4+](https://ja4db.com/) provides fingerprints via a REST API, which returns a single JSON array. You can easily ingest this into Tenzir using Pipeline ```tql load "https://ja4db.com/api/read/" read_json arrays_of_objects=true ``` Example Output ```json { "application": "SemrushBot", "library": null, "device": null, "os": "Other", "user_agent_string": null, "certificate_authority": null, "observation_count": 449, "verified": false, "notes": null, "ja4_fingerprint": "t13d301000_01455d0db58d_5ac7197df9d2", "ja4_fingerprint_string": null, "ja4s_fingerprint": null, "ja4h_fingerprint": "ge11nn100000_c910c42e1704_e3b0c44298fc_e3b0c44298fc", "ja4x_fingerprint": null, "ja4t_fingerprint": null, "ja4ts_fingerprint": null, "ja4tscan_fingerprint": null }, { "application": null, "library": null, "device": "Epson Printer", "os": null, "user_agent_string": null, "certificate_authority": null, "observation_count": 1, "verified": true, "notes": null, "ja4_fingerprint": null, "ja4s_fingerprint": null, "ja4h_fingerprint": null, "ja4x_fingerprint": null, "ja4t_fingerprint": null, "ja4ts_fingerprint": null, "ja4tscan_fingerprint": "28960_2-4-8-1-3_1460_3_1-4-8-16" } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_json`](/reference/functions/parse_json), [`read_ndjson`](/reference/operators/read_ndjson)

# read_kv

Read Key-Value pairs from a byte stream. ```tql read_kv [field_split=string, value_split=string, merge=bool, raw=bool, quotes=string, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_kv` operator transforms a byte stream into a event stream by parsing the bytes as Key-Value pairs. Incoming strings are first split into fields according to `field_split`. This can be a regular expression. For example, the input `foo: bar, baz: 42` can be split into `foo: bar` and `baz: 42` with the regular expression `r",\s*"` (a comma, followed by any amount of whitespace) as the field splitter. Note that the matched separators are removed when splitting a string. Afterwards, the extracted fields are split into their key and value by `value_split`, which can again be a regular expression. In our example, `r":\s*"` could be used to split `foo: bar` into the key `foo` and its value `bar`, and similarly `baz: 42` into `baz` and `42`. The result would thus be `{"foo": "bar", "baz": 42}`. If the regex matches multiple substrings, only the first match is used. If no match is found, the â€œfieldâ€ is considered an extension of the previous fields value. The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `read_kv` at the moment. However, if the regular expression has a capture group, it is assumed that only the content of the capture group shall be used as the separator. This means that unsupported regular expressions such as `(?=foo)bar(?<=baz)` can be effectively expressed as `foo(bar)baz` instead. ### Quoted Values [Section titled â€œQuoted Valuesâ€](#quoted-values) The parser is aware of double-quotes (`"`). If the `field_split` or `value_split` are found within enclosing quotes, they are not considered matches. This means that both the key and the value may be enclosed in double-quotes. For example, given `field_split` `\s*,\s*` and `value_split` `=`, the input ```plaintext "key"="nested = value",key2="value, and more" ``` will parse as ```tql { key: "nested = value", key2: "value, and more", } ``` ### `field_split = string (optional)` [Section titled â€œfield\_split = string (optional)â€](#field_split--string-optional) The regular expression used to separate individual fields. Defaults to `r"\s"`. ### `value_split = string (optional)` [Section titled â€œvalue\_split = string (optional)â€](#value_split--string-optional) The regular expression used to separate a key from its value. Defaults to `"="`. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Read comma-separated key-value pairs [Section titled â€œRead comma-separated key-value pairsâ€](#read-comma-separated-key-value-pairs) Input ```txt surname:"John Norman", family_name:Smith, date_of_birth: 1995-05-26 ``` ```tql read_kv field_split=r"\s*,\s*", value_split=r"\s*:\s*" ``` ```tql { surname: "John Norman", family_name: "Smith", date_of_birth: 1995-05-26, } ``` ### Extract key-value pairs with more complex rules [Section titled â€œExtract key-value pairs with more complex rulesâ€](#extract-key-value-pairs-with-more-complex-rules) Input ```txt PATH: C:\foo INPUT_MESSAGE: hello world PATH: D:\bar VALUE: 42 INFO: Great ``` ```tql read_kv field_split=r"(\s+)[A-Z][A-Z_]+:", value_split=r":\s*" ``` ```tql { PATH: "C:\\foo", INPUT_MESSAGE: "hello world", } { PATH: "D:\\bar", VALUE: 42, INFO: "Great", } ``` This requires lookahead because not every whitespace acts as a field separator. Instead, we only want to split if the whitespace is followed by `[A-Z][A-Z_]+:`, i.e., at least two uppercase characters followed by a colon. We can express this as `"(\s+)[A-Z][A-Z_]+:"`, which yields `PATH: C:\foo` and `INPUT_MESSAGE: hello world`. We then split the key from its value with `":\s*"`. Since only the first match is used to split key and value, this leaves the path intact. ### Fields without a `value_split` [Section titled â€œFields without a value\_splitâ€](#fields-without-a-value_split) Input ```txt x=1 y=2 z=3 4 5 a=6 ``` ```tql read_kv ``` ```tql { x: 1, y: 2, z: "3 4 5", a: 6, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_kv`](/reference/functions/parse_kv), [`write_kv`](/reference/operators/write_kv)

# read_leef

Parses an incoming [LEEF](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) stream into events. ```tql read_leef [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The [Log Event Extended Format (LEEF)](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) is an event representation popularized by IBM QRadar. Many tools send LEEF over [Syslog](/reference/operators/read_syslog). LEEF is a line-based format and every line begins with a *header* that is followed by *attributes* in the form of key-value pairs. LEEF v1.0 defines 5 header fields and LEEF v2.0 has an additional field to customize the key-value pair separator, which can be a single character or the hex value prefixed by `0x` or `x`: ```plaintext LEEF:1.0|Vendor|Product|Version|EventID| LEEF:2.0|Vendor|Product|Version|EventID|DelimiterCharacter| ``` For LEEF v1.0, the tab (`\t`) character is hard-coded as attribute separator. Here are some real-world LEEF events: ```plaintext LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1 dst=2.10.20.20 spt=1200 LEEF:2.0|Lancope|StealthWatch|1.0|41|^|src=10.0.1.8^dst=10.0.0.5^sev=5^srcPort=81^dstPort=21 ``` Tenzir translates the event attributes into a nested record, where the key-value pairs map to record fields. Here is an example of the parsed events from above: ```tql { leef_version: "1.0", vendor: "Microsoft", product_name: "MSExchange", product_version: "2016", event_class_id: "15345", attributes: { src: 10.50.1.1, dst: 2.10.20.20, spt: 1200, } } { leef_version: "2.0", vendor: "Lancope", product_name: "StealthWatch", product_version: "1.0", event_class_id: "41", attributes: { src: 10.0.1.8, dst: 10.0.0.5, sev: 5, srcPort: 81, dstPort: 21 } } ``` ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_leef`](/reference/functions/parse_leef), [`print_leef`](/reference/functions/print_leef), [`read_cef`](/reference/operators/read_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# read_lines

Parses an incoming bytes stream into events. ```tql read_lines [skip_empty=bool, split_at_null=bool, split_at_regex=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_lines` operator takes its input bytes and splits it at a newline character. Newline characters include: * `\n` * `\r\n` The resulting events have a single field called `line`. ### `skip_empty = bool (optional)` [Section titled â€œskip\_empty = bool (optional)â€](#skip_empty--bool-optional) Ignores empty lines in the input. ### `split_at_null = bool (optional)` [Section titled â€œsplit\_at\_null = bool (optional)â€](#split_at_null--bool-optional) Deprecated This option is deprecated. Use [`read_delimited`](/reference/operators/read_delimited) instead. Use null byte (`\0`) as the delimiter instead of newline characters. ### `split_at_regex = string (optional)` [Section titled â€œsplit\_at\_regex = string (optional)â€](#split_at_regex--string-optional) Deprecated This option is deprecated. Use [`read_delimited_regex`](/reference/operators/read_delimited_regex) instead. Use the specified regex as the delimiter instead of newline characters. The regex flavor is Perl compatible and documented [here](https://www.boost.org/doc/libs/1_88_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html). ## Examples [Section titled â€œExamplesâ€](#examples) ### Reads lines from a file [Section titled â€œReads lines from a fileâ€](#reads-lines-from-a-file) ```tql load_file "events.log" read_lines is_error = line.starts_with("error:") ``` ### Split Syslog-like events without newline terminators from a TCP input [Section titled â€œSplit Syslog-like events without newline terminators from a TCP inputâ€](#split-syslog-like-events-without-newline-terminators-from-a-tcp-input) Consider using [`read_delimited_regex`](/reference/operators/read_delimited_regex) for regex-based splitting: ```tql load_tcp "0.0.0.0:514" read_delimited_regex "(?=<[0-9]+>)" this = line.parse_syslog() ``` ```tql load_tcp "0.0.0.0:514" read_lines split_at_regex="(?=<[0-9]+>)" this = line.parse_syslog() ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_all`](/reference/operators/read_all), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_xsv`](/reference/operators/read_xsv), [`write_lines`](/reference/operators/write_lines)

# read_ndjson

Parses an incoming NDJSON (newline-delimited JSON) stream into events. ```tql read_ndjson [schema=string, selector=string, schema_only=bool, merge=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) Parses an incoming NDJSON byte stream into events. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Read a newline-delimited JSON file [Section titled â€œRead a newline-delimited JSON fileâ€](#read-a-newline-delimited-json-file) versions.json ```json {"product": "Tenzir", "version.major": 4, "version.minor": 22} {"product": "Tenzir", "version.major": 4, "version.minor": 21} ``` ```tql load_file "versions.json" read_ndjson unflatten="." ``` ```tql { product: "Tenzir", version: { major: 4, minor: 22, } } { product: "Tenzir", version: { major: 4, minor: 21, } } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_json`](/reference/operators/read_json)

# read_parquet

Reads events from a Parquet byte stream. ```tql read_parquet ``` ## Description [Section titled â€œDescriptionâ€](#description) Reads events from a [Parquet](https://parquet.apache.org/) byte stream. [Apache Parquet](https://parquet.apache.org/) is a columnar storage format that a variety of data tools support. Limitation Tenzir currently assumes that all Parquet files use metadata recognized by Tenzir. We plan to lift this restriction in the future. ## Examples [Section titled â€œExamplesâ€](#examples) Read a Parquet file: ```tql load_file "/tmp/data.prq", mmap=true read_parquet ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_feather`](/reference/operators/read_feather), [`to_hive`](/reference/operators/to_hive), [`write_parquet`](/reference/operators/write_parquet)

# read_pcap

Reads raw network packets in PCAP file format. ```tql read_pcap [emit_file_headers=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_pcap` operator converts raw bytes representing a [PCAP](https://datatracker.ietf.org/doc/id/draft-gharris-opsawg-pcap-00.html) file into events. ### `emit_file_headers = bool (optional)` [Section titled â€œemit\_file\_headers = bool (optional)â€](#emit_file_headers--bool-optional) Emit a `pcap.file_header` event that represents the PCAP file header. If present, the parser injects this additional event before the subsequent stream of packets. Emitting this extra event makes it possible to seed the [`write_pcap`](/reference/operators/write_pcap) operator with a file header from the input. This allows for controlling the timestamp formatting (microseconds vs. nanosecond granularity) and byte order in the packet headers. When the PCAP parser processes a concatenated stream of PCAP files, specifying `emit_file_headers` will also re-emit every intermediate file header as separate event. Use this option when you would like to reproduce the identical trace file layout of the PCAP input. ## Schemas [Section titled â€œSchemasâ€](#schemas) The operator emits events with the following schema. ### `pcap.packet` [Section titled â€œpcap.packetâ€](#pcappacket) Contains information about all accessed API endpoints, emitted once per second. | Field | Type | Description | | :----------------------- | :------- | :------------------------------------ | | `timestamp` | `time` | The time of capturing the packet. | | `linktype` | `uint64` | The linktype of the captured packet. | | `original_packet_length` | `uint64` | The length of the original packet. | | `captured_packet_length` | `uint64` | The length of the captured packet. | | `data` | `blob` | The captured packetâ€™s data as a blob. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Read packets from a PCAP file [Section titled â€œRead packets from a PCAP fileâ€](#read-packets-from-a-pcap-file) ```tql load_file "/tmp/trace.pcap" read_pcap ``` ### Read packets from the [network interface](/reference/operators/load_nic) `eth0` [Section titled â€œRead packets from the network interface eth0â€](#read-packets-from-the-network-interface-eth0) ```tql load_nic "eth0" read_pcap ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_nic`](/reference/operators/load_nic), [`write_pcap`](/reference/operators/write_pcap)

# read_ssv

Read SSV (Space-Separated Values) from a byte stream. ```tql read_ssv [list_separator=string, null_value=string, comments=bool, header=string, quotes=string, auto_expand=bool, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_ssv` operator transforms a byte stream into a event stream by parsing the bytes as SSV. ### `auto_expand = bool (optional)` [Section titled â€œauto\_expand = bool (optional)â€](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `comments = bool (optional)` [Section titled â€œcomments = bool (optional)â€](#comments--bool-optional) Treat lines beginning with â€#â€ as comments. ### `header = list<string>|string (optional)` [Section titled â€œheader = list\<string>|string (optional)â€](#header--liststringstring-optional) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header. ### `list_separator = string (optional)` [Section titled â€œlist\_separator = string (optional)â€](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `,`. ### `null_value = string (optional)` [Section titled â€œnull\_value = string (optional)â€](#null_value--string-optional) The `string` denoting an absent value. Defaults to `-`. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse an SSV file [Section titled â€œParse an SSV fileâ€](#parse-an-ssv-file) input.ssv ```txt message count ip text 42 "1.1.1.1" "longer string" 100 1.1.1.2 ``` ```tql load "input.ssv" read_ssv ``` ```tql {message: "text", count: 42, ip: 1.1.1.1} {message: "longer string", count: 100, ip: 1.1.1.2} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_ssv`](/reference/functions/parse_ssv), [`read_csv`](/reference/operators/read_csv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_suricata

Parse an incoming [Suricata EVE JSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html) stream into events. ```tql read_suricata [schema_only=bool, raw=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The [Suricata](https://suricata.io) network security monitor converts network traffic into a stream of metadata events and provides a rule matching engine to generate alerts. Suricata emits events in the [EVE JSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html) format. The output is a single stream of events where the `event_type` field disambiguates the event type. Tenzirâ€™s [`JSON`](/reference/operators/read_json) can handle EVE JSON correctly, but for the schema names to match the value from the `event_type` field, you need to pass the option `selector=event_type:suricata`. The `suricata` parser does this by default. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. This means that JSON numbers will be parsed as numbers, but every JSON string remains a string, unless the field is in the `schema`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse a Suricata EVE JSON log file [Section titled â€œParse a Suricata EVE JSON log fileâ€](#parse-a-suricata-eve-json-log-file) Hereâ€™s an `eve.log` sample: ```json {"timestamp":"2011-08-12T14:52:57.716360+0200","flow_id":1031464864740687,"pcap_cnt":83,"event_type":"alert","src_ip":"147.32.84.165","src_port":1181,"dest_ip":"78.40.125.4","dest_port":6667,"proto":"TCP","alert":{"action":"allowed","gid":1,"signature_id":2017318,"rev":4,"signature":"ET CURRENT_EVENTS SUSPICIOUS IRC - PRIVMSG *.(exe|tar|tgz|zip) download command","category":"Potentially Bad Traffic","severity":2},"flow":{"pkts_toserver":27,"pkts_toclient":35,"bytes_toserver":2302,"bytes_toclient":4520,"start":"2011-08-12T14:47:24.357711+0200"},"payload":"UFJJVk1TRyAjemFyYXNhNDggOiBzbXNzLmV4ZSAoMzY4KQ0K","payload_printable":"PRIVMSG #zarasa48 : smss.exe (368)\r\n","stream":0,"packet":"AB5J2xnDCAAntbcZCABFAABMGV5AAIAGLlyTIFSlTih9BASdGgvw0QvAxUWHdVAY+rCL4gAAUFJJVk1TRyAjemFyYXNhNDggOiBzbXNzLmV4ZSAoMzY4KQ0K","packet_info":{"linktype":1}} {"timestamp":"2011-08-12T14:55:22.154618+0200","flow_id":2247896271051770,"pcap_cnt":775,"event_type":"dns","src_ip":"147.32.84.165","src_port":1141,"dest_ip":"147.32.80.9","dest_port":53,"proto":"UDP","dns":{"type":"query","id":553,"rrname":"irc.freenode.net","rrtype":"A","tx_id":0}} {"timestamp":"2011-08-12T16:59:22.181050+0200","flow_id":472067367468746,"pcap_cnt":25767,"event_type":"fileinfo","src_ip":"74.207.254.18","src_port":80,"dest_ip":"147.32.84.165","dest_port":1046,"proto":"TCP","http":{"hostname":"www.nmap.org","url":"/","http_user_agent":"Mozilla/4.0 (compatible)","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":301,"redirect":"http://nmap.org/","length":301},"app_proto":"http","fileinfo":{"filename":"/","magic":"HTML document, ASCII text","gaps":false,"state":"CLOSED","md5":"70041821acf87389e40ddcb092004184","sha1":"10395ab3566395ca050232d2c1a0dbad69eb5fd2","sha256":"2e4c462b3424afcc04f43429d5f001e4ef9a28143bfeefb9af2254b4df3a7c1a","stored":true,"file_id":1,"size":301,"tx_id":0}} ``` Import it as follows: ```tql read_file "eve.log" read_suricata import ``` ### Read Suricata EVE JSON from a Unix domain socket [Section titled â€œRead Suricata EVE JSON from a Unix domain socketâ€](#read-suricata-eve-json-from-a-unix-domain-socket) Instead of writing to a file, Suricata can also log to a Unix domain socket that Tenzir can then read from. This saves a filesystem round-trip. This requires the following settings in your `suricata.yaml`: ```yaml outputs: - eve-log: enabled: yes filetype: unix_stream filename: eve.sock ``` Suricata creates `eve.sock` upon startup. Thereafter, you can read from the socket: ```tql load_file "eve.sock" read_suricata ```

# read_syslog

Parses an incoming Syslog stream into events. ```tql read_syslog [octet_counting=bool, merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) [Syslog](https://en.wikipedia.org/wiki/Syslog) is a standard format for message logging. Tenzir supports reading syslog messages in both the standardized â€œSyslog Protocolâ€ format ([RFC 5424](https://tools.ietf.org/html/rfc5424)), and the older â€œBSD syslog Protocolâ€ format ([RFC 3164](https://tools.ietf.org/html/rfc3164)). Depending on the syslog format, the result can be different. Hereâ€™s an example of a syslog message in RFC 5424 format: ```plaintext <165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource="Application" eventID="1011"] Event log entry ``` With this input, the parser will produce the following output, with the schema name `syslog.rfc5424`: ```tql { input: "<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource=\"Application\" eventID=\"1011\"] Event log entry", output: { facility: 20, severity: 5, version: 8, timestamp: 2023-10-11T22:14:15.003Z, hostname: "mymachineexamplecom", app_name: "evntslog", process_id: "1370", message_id: "ID47", structured_data: { "exampleSDID@32473": { eventSource: "Application", eventID: 1011, }, }, message: "Event log entry", }, } ``` Hereâ€™s an example of a syslog message in RFC 3164 format: ```plaintext <34>Nov 16 14:55:56 mymachine PROGRAM: Freeform message ``` With this input, the parser will produce the following output, with the schema name `syslog.rfc3164`: ```json { "facility": 4, "severity": 2, "timestamp": "Nov 16 14:55:56", "hostname": "mymachine", "app_name": "PROGRAM", "process_id": null, "content": "Freeform message" } ``` ### `octet_counting = bool (optional)` [Section titled â€œoctet\_counting = bool (optional)â€](#octet_counting--bool-optional) Employs â€œoctet countingâ€ according to [RFC6587](https://datatracker.ietf.org/doc/html/rfc6587#section-3.4.1) to determine message boundaries instead of the parsing heuristic for multi-line messages. Defaults to `false`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Read in the `auth.log` [Section titled â€œRead in the auth.logâ€](#read-in-the-authlog) Pipeline ```tql load_file "/var/log/auth.log" read_syslog ``` ```tql { facility: null, severity: null, timestamp: 2024-10-14T07:15:01.348027, hostname: "tenzirs-magic-machine", app_name: "CRON", process_id: "895756", content: "pam_unix(cron:session): session opened for user root(uid=0) by root(uid=0)", } { facility: null, severity: null, timestamp: 2024-10-14T07:15:01.349838, hostname: "tenzirs-magic-machine", app_name: "CRON", process_id: "895756", content: "pam_unix(cron:session): session closed for user root" } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_syslog`](/reference/functions/parse_syslog), [`write_syslog`](/reference/operators/write_syslog)

# read_tsv

Read TSV (Tab-Separated Values) from a byte stream. ```tql read_tsv [list_separator=string, null_value=string, comments=bool, header=string, quotes=string, auto_expand=bool, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_tsv` operator transforms a byte stream into a event stream by parsing the bytes as [TSV](https://en.wikipedia.org/wiki/Tab-separated_values). ### `auto_expand = bool (optional)` [Section titled â€œauto\_expand = bool (optional)â€](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `comments = bool (optional)` [Section titled â€œcomments = bool (optional)â€](#comments--bool-optional) Treat lines beginning with â€#â€ as comments. ### `header = list<string>|string (optional)` [Section titled â€œheader = list\<string>|string (optional)â€](#header--liststringstring-optional) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header. ### `list_separator = string (optional)` [Section titled â€œlist\_separator = string (optional)â€](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `,`. ### `null_value = string (optional)` [Section titled â€œnull\_value = string (optional)â€](#null_value--string-optional) The `string` denoting an absent value. Defaults to `-`. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse a TSV file [Section titled â€œParse a TSV fileâ€](#parse-a-tsv-file) input.tsv ```txt message count ip text 42 "1.1.1.1" "longer string" 100 "1.1.1.2" ``` ```tql load "input.tsv" read_tsv ``` ```tql {message: "text", count: 42, ip: 1.1.1.1} {message: "longer string", count: 100, ip: 1.1.1.2} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_tsv`](/reference/functions/parse_tsv), [`read_csv`](/reference/operators/read_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_xsv`](/reference/operators/read_xsv)

# read_xsv

Read XSV from a byte stream. ```tql read_xsv field_separator=string, list_separator=string, null_value=string, [comments=bool, header=string, auto_expand=bool, quotes=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `read_xsv` operator transforms a byte stream into a event stream by parsing the bytes as [XSV](https://en.wikipedia.org/wiki/Delimiter-separated_values), a generalization of CSV with a more flexible separator specification. The following table lists existing XSV configurations: | Format | Field Separator | List Separator | Null Value | | -------------------------------------- | :-------------: | :------------: | :--------: | | [`csv`](/reference/operators/read_csv) | `,` | `;` | empty | | [`ssv`](/reference/operators/read_ssv) | `<space>` | `,` | `-` | | [`tsv`](/reference/operators/read_tsv) | `\t` | `,` | `-` | ### `field_separator = string` [Section titled â€œfield\_separator = stringâ€](#field_separator--string) The string separating different fields. ### `list_separator = string` [Section titled â€œlist\_separator = stringâ€](#list_separator--string) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. ### `null_value = string` [Section titled â€œnull\_value = stringâ€](#null_value--string) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled â€œauto\_expand = bool (optional)â€](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `comments = bool (optional)` [Section titled â€œcomments = bool (optional)â€](#comments--bool-optional) Treat lines beginning with `#` as comments. ### `header = list<string>|string (optional)` [Section titled â€œheader = list\<string>|string (optional)â€](#header--liststringstring-optional) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header. ### `quotes = string (optional)` [Section titled â€œquotes = string (optional)â€](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_xsv`](/reference/functions/parse_xsv), [`read_csv`](/reference/operators/read_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv)

# read_yaml

Parses an incoming YAML stream into events. ```tql read_yaml [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) Parses an incoming [YAML](https://en.wikipedia.org/wiki/YAML) stream into events. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse a YAML file [Section titled â€œParse a YAML fileâ€](#parse-a-yaml-file) input.yaml ```yaml --- name: yaml version: bundled kind: builtin types: - parser - printer dependencies: [] ... ``` ```tql load_file "input.yaml" read_yaml ``` ```tql { name: "yaml", version: "bundled", kind: "builtin", types: [ "parser", "printer", ], dependencies: [], } ``` *** ## title: See Also [Section titled â€œtitle: See Alsoâ€](#title-see-also) [`parse_yaml`](/reference/functions/parse_yaml), [`print_yaml`](/reference/functions/print_yaml), [`write_yaml`](/reference/operators/write_yaml)

# read_zeek_json

Parse an incoming Zeek JSON stream into events. ```tql read_zeek_json [schema_only=bool, raw=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. This means that JSON numbers will be parsed as numbers, but every JSON string remains a string, unless the field is in the `schema`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Load a Zeek connection log [Section titled â€œLoad a Zeek connection logâ€](#load-a-zeek-connection-log) zeek.json ```json {"__name":"sensor_10_0_0_2","_write_ts":"2020-02-26T04:00:03.734769Z","ts":"2020-02-26T03:40:03.724911Z","uid":"Cx3bf12iVwo5m7Gkd1","id.orig_h":"193.10.255.99","id.orig_p":6667,"id.resp_h":"141.9.40.50","id.resp_p":21,"proto":"tcp","duration":1196.975041,"orig_bytes":0,"resp_bytes":0,"conn_state":"S1","local_orig":false,"local_resp":true,"missed_bytes":0,"history":"Sh","orig_pkts":194,"orig_ip_bytes":7760,"resp_pkts":191,"resp_ip_bytes":8404} {"_path":"_0_0_2","_write_ts":"2020-02-11T03:48:57.477193Z","ts":"2020-02-11T03:48:57.477193Z","uid":"Cpk0Nl33Zb5ZWLP1tc","id.orig_h":"185.100.59.59","id.orig_p":6667,"id.resp_h":"141.9.255.157","id.resp_p":8080,"proto":"tcp","note":"LongConnection::found","msg":"185.100.59.59 -> 141.9.255.157:8080/tcp remained alive for longer than 19m55s","sub":"1194.62","src":"185.100.59.59","dst":"141.9.255.157","p":8080,"peer_descr":"worker-02","actions":["Notice::ACTION_LOG"],"suppress_for":3600} ``` ```tql load "zeek.json" read_zeek_json ``` ```tql { __name: "sensor_10_0_0_2", _write_ts: 2020-02-26T04:00:03.734769, ts: 2020-02-26T03:40:03.724911, uid: "Cx3bf12iVwo5m7Gkd1", id: { orig_h: 193.10.255.99, orig_p: 6667, resp_h: 141.9.40.50, resp_p: 21, }, proto: "tcp", duration: 1196.975041, orig_bytes: 0, resp_bytes: 0, conn_state: "S1", local_orig: false, local_resp: true, missed_bytes: 0, history: "Sh", orig_pkts: 194, orig_ip_bytes: 7760, resp_pkts: 191, resp_ip_bytes: 8404, } { _write_ts: 2020-02-11T03:48:57.477193, ts: 2020-02-11T03:48:57.477193, uid: "Cpk0Nl33Zb5ZWLP1tc", id: { orig_h: 185.100.59.59, orig_p: 6667, resp_h: 141.9.255.157, resp_p: 8080, }, proto: "tcp", _path: "_0_0_2", note: "LongConnection::found", msg: "185.100.59.59 -> 141.9.255.157:8080/tcp remained alive for longer than 19m55s", sub: "1194.62", src: 185.100.59.59, dst: 141.9.255.157, p: 8080, peer_descr: "worker-02", actions: [ Notice::ACTION_LOG ], suppress_for: 3600, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_zeek_tsv`](/reference/operators/read_zeek_tsv), [`write_zeek_tsv`](/reference/operators/write_zeek_tsv)

# read_zeek_tsv

Parses an incoming `Zeek TSV` stream into events. ```tql read_zeek_tsv ``` ## Description [Section titled â€œDescriptionâ€](#description) The [Zeek](https://zeek.org) network security monitor comes with its own tab-separated value (TSV) format for representing logs. This format includes additional header fields with field names, type annotations, and additional metadata. The `read_zeek_tsv` operator processes this metadata to extract a schema for the subsequent log entries. The Zeek types `count`, `real`, and `addr` map to the respective Tenzir types `uint64`, `double`, and `ip`. Hereâ€™s an example of a typical Zeek `conn.log` in TSV form: ```txt #separator \x09 #set_separator , #empty_field (empty) #unset_field - #path conn #open 2014-05-23-18-02-04 #fields ts uid id.orig_h id.orig_p id.resp_h id.resp_p proto service duration â€¦orig_bytes resp_bytes conn_state local_orig missed_bytes history orig_pkts â€¦orig_ip_bytes resp_pkts resp_ip_bytes tunnel_parents #types time string addr port addr port enum string interval count counâ€¦t string bool count string count count count count table[string] 1258531221.486539 Pii6cUUq1v4 192.168.1.102 68 192.168.1.1 67 udp - 0.163820 â€¦301 300 SF - 0 Dd 1 329 1 328 (empty) 1258531680.237254 nkCxlvNN8pi 192.168.1.103 137 192.168.1.255 137 udp dns 3.7801â€¦25 350 0 S0 - 0 D 7 546 0 0 (empty) 1258531693.816224 9VdICMMnxQ7 192.168.1.102 137 192.168.1.255 137 udp dns 3.7486â€¦47 350 0 S0 - 0 D 7 546 0 0 (empty) 1258531635.800933 bEgBnkI31Vf 192.168.1.103 138 192.168.1.255 138 udp - 46.72538â€¦0 560 0 S0 - 0 D 3 644 0 0 (empty) 1258531693.825212 Ol4qkvXOksc 192.168.1.102 138 192.168.1.255 138 udp - 2.248589â€¦ 348 0 S0 - 0 D 2 404 0 0 (empty) 1258531803.872834 kmnBNBtl96d 192.168.1.104 137 192.168.1.255 137 udp dns 3.7488â€¦93 350 0 S0 - 0 D 7 546 0 0 (empty) 1258531747.077012 CFIX6YVTFp2 192.168.1.104 138 192.168.1.255 138 udp - 59.05289â€¦8 549 0 S0 - 0 D 3 633 0 0 (empty) 1258531924.321413 KlF6tbPUSQ1 192.168.1.103 68 192.168.1.1 67 udp - 0.044779 â€¦303 300 SF - 0 Dd 1 331 1 328 (empty) 1258531939.613071 tP3DM6npTdj 192.168.1.102 138 192.168.1.255 138 udp - - - - S0â€¦ - 0 D 1 229 0 0 (empty) 1258532046.693816 Jb4jIDToo77 192.168.1.104 68 192.168.1.1 67 udp - 0.002103 â€¦311 300 SF - 0 Dd 1 339 1 328 (empty) 1258532143.457078 xvWLhxgUmj5 192.168.1.102 1170 192.168.1.1 53 udp dns 0.0685â€¦11 36 215 SF - 0 Dd 1 64 1 243 (empty) 1258532203.657268 feNcvrZfDbf 192.168.1.104 1174 192.168.1.1 53 udp dns 0.1709â€¦62 36 215 SF - 0 Dd 1 64 1 243 (empty) 1258532331.365294 aLsTcZJHAwa 192.168.1.1 5353 224.0.0.251 5353 udp dns 0.1003â€¦81 273 0 S0 - 0 D 2 329 0 0 (empty) ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Read a Zeek connection log from a file [Section titled â€œRead a Zeek connection log from a fileâ€](#read-a-zeek-connection-log-from-a-file) ```tql load_file "/tmp/conn.log" read_zeek_tsv ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_zeek_json`](/reference/operators/read_zeek_json), [`write_zeek_tsv`](/reference/operators/write_zeek_tsv)

# remote

Forces a pipeline to run remotely at a node. ```tql remote { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `remote` operator takes a pipeline as an argument and forces it to run at a Tenzir Node. This operator has no effect when running a pipeline through the API or Tenzir Platform. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the version of a node [Section titled â€œGet the version of a nodeâ€](#get-the-version-of-a-node) ```tql remote { version } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`local`](/reference/operators/local)

# repeat

Repeats the input a number of times. ```tql repeat [count:int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `repeat` operator relays the input without any modification, and repeats its inputs a specified number of times. It is primarily used for testing and when working with generated data. ### `count: int (optional)` [Section titled â€œcount: int (optional)â€](#count-int-optional) The number of times to repeat the input data. If not specified, the operator repeats its input indefinitely. ## Examples [Section titled â€œExamplesâ€](#examples) ### Repeat input indefinitely [Section titled â€œRepeat input indefinitelyâ€](#repeat-input-indefinitely) Given the following events: ```tql {number: 1, "text": "one"} {number: 2, "text": "two"} ``` The `repeat` operator will repeat them indefinitely, in order: ```tql repeat ``` ```tql {number: 1, "text": "one"} {number: 2, "text": "two"} {number: 1, "text": "one"} {number: 2, "text": "two"} {number: 1, "text": "one"} {number: 2, "text": "two"} // â€¦ ``` ### Repeat the first event 5 times [Section titled â€œRepeat the first event 5 timesâ€](#repeat-the-first-event-5-times) ```tql head 1 repeat 5 ``` ```tql {number: 1, "text": "one"} {number: 1, "text": "one"} {number: 1, "text": "one"} {number: 1, "text": "one"} {number: 1, "text": "one"} ```

# replace

Replaces all occurrences of a value with another value. ```tql replace [path:field...], what=any, with=any ``` ## Description [Section titled â€œDescriptionâ€](#description) The `replace` operator scans all fields of each input event and replaces every occurrence of a value equal to `what` with the value specified by `with`. ### `path: field... (optional)` [Section titled â€œpath: field... (optional)â€](#path-field-optional) An optional set of paths to restrict replacements to. ### `what: any` [Section titled â€œwhat: anyâ€](#what-any) The value to search for and replace. ### `with: any` [Section titled â€œwith: anyâ€](#with-any) The value to replace in place of `what`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Replace all occurrences of 42 with null [Section titled â€œReplace all occurrences of 42 with nullâ€](#replace-all-occurrences-of-42-with-null) ```tql from { count: 42, data: {value: 42, other: 100}, list: [42, 24, 42] } replace what=42, with=null ``` ```tql { count: null, data: {value: null, other: 100}, list: [42, 24, 42] } ``` ### Replace only within specific fields [Section titled â€œReplace only within specific fieldsâ€](#replace-only-within-specific-fields) ```tql from { count: 42, data: {value: 42, other: 100}, } replace data, what=42, with=null ``` ```tql { count: 42, data: {value: null, other: 100}, } ``` ### Replace a specific IP address with a redacted value [Section titled â€œReplace a specific IP address with a redacted valueâ€](#replace-a-specific-ip-address-with-a-redacted-value) ```tql from { src_ip: 192.168.1.1, dst_ip: 10.0.0.1, metadata: {source: 192.168.1.1} } replace what=192.168.1.1, with="REDACTED" ``` ```tql { src_ip: "REDACTED", dst_ip: 10.0.0.1, metadata: { source: "REDACTED", }, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`replace`](/reference/functions/replace)

# reverse

Reverses the event order. ```tql reverse ``` ## Description [Section titled â€œDescriptionâ€](#description) `reverse` is a shorthand notation for [`slice stride=-1`](/reference/operators/slice). ## Examples [Section titled â€œExamplesâ€](#examples) ### Reverse a stream of events [Section titled â€œReverse a stream of eventsâ€](#reverse-a-stream-of-events) ```tql from {x: 1}, {x: 2}, {x: 3} reverse ``` ```tql {x: 3} {x: 2} {x: 1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`sort`](/reference/operators/sort)

# sample

Dynamically samples events from a event stream. ```tql sample [period:duration, mode=string, min_events=int, max_rate=int, max_samples=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) Dynamically samples input data from a stream based on the frequency of receiving events for streams with varying load. The operator counts the number of events received in the `period` and applies the specified function on the count to calculate the sampling rate for the next period. ### `period: duration (optional)` [Section titled â€œperiod: duration (optional)â€](#period-duration-optional) The duration to count events in, i.e., how often the sample rate is computed. The sampling rate for the first window is `1:1`. Defaults to `30s`. ### `mode = string (optional)` [Section titled â€œmode = string (optional)â€](#mode--string-optional) The function used to compute the sampling rate: * `"ln"` (default) * `"log2"` * `"log10"` * `"sqrt"` ### `min_events = int (optional)` [Section titled â€œmin\_events = int (optional)â€](#min_events--int-optional) The minimum number of events that must be received during the previous sampling period for the sampling mode to be applied in the current period. If the number of events in a sample group falls below this threshold, a `1:1` sample rate is used instead. Defaults to `30`. ### `max_rate = int (optional)` [Section titled â€œmax\_rate = int (optional)â€](#max_rate--int-optional) The sampling rate is capped to this value if the computed rate is higher than this. ### `max_samples = int (optional)` [Section titled â€œmax\_samples = int (optional)â€](#max_samples--int-optional) The maximum number of events to emit per `period`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Sample the input every 30s dynamically [Section titled â€œSample the input every 30s dynamicallyâ€](#sample-the-input-every-30s-dynamically) Sample a feed `log-stream` every 30s dynamically, only changing rate when more than 50 events (`min_events`) are received. Additionally, cap the max sampling rate to `1:500`, i.e., 1 sample for every 500 events or more (`max_rate`). ```tql subscribe "log-stream" sample 30s, min_events=50, max_rate=500 ``` ### Sample metrics every hour [Section titled â€œSample metrics every hourâ€](#sample-metrics-every-hour) Sample some `metrics` every hour, limiting the max samples per period to 5,000 samples (`max_samples`) and limiting the overall sample count to 100,000 samples ([`head`](/reference/operators/head)). ```tql subscribe "metrics" sample 1h, max_samples=5k head 100k ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`deduplicate`](/reference/operators/deduplicate)

# save_amqp

Saves a byte stream via AMQP messages. ```tql save_amqp [url:str, channel=int, exchange=str, routing_key=str, options=record, mandatory=bool, immediate=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `save_amqp` operator is an [AMQP](https://www.amqp.org/) 0-9-1 client to send messages to an exchange. ### `url: str (optional)` [Section titled â€œurl: str (optional)â€](#url-str-optional) A URL that specifies the AMQP server. The URL must have the following format: ```plaintext amqp://[USERNAME[:PASSWORD]@]HOSTNAME[:PORT]/[VHOST] ``` When the URL is present, it will overwrite the corresponding values of the configuration options. ### `channel = int (optional)` [Section titled â€œchannel = int (optional)â€](#channel--int-optional) The channel number to use. Defaults to `1`. ### `exchange = str (optional)` [Section titled â€œexchange = str (optional)â€](#exchange--str-optional) The exchange to interact with. Defaults to `"amq.direct"`. ### `routing_key = str (optional)` [Section titled â€œrouting\_key = str (optional)â€](#routing_key--str-optional) The routing key to publish messages with. Defaults to the empty string. ### `options = record (optional)` [Section titled â€œoptions = record (optional)â€](#options--record-optional) An option record for RabbitMQ , e.g., `{max_channels: 42, frame_size: 1024, sasl_method: "external"}`. Available options are: ```yaml hostname: 127.0.0.1 port: 5672 ssl: false vhost: / max_channels: 2047 frame_size: 131072 heartbeat: 0 sasl_method: plain username: guest password: guest ``` ### `mandatory = bool (optional)` [Section titled â€œmandatory = bool (optional)â€](#mandatory--bool-optional) This flag tells the server how to react if the message cannot be routed to a queue. If `true`, the server will return an unroutable message with a Return method. Otherwise the server silently drops the message. Defaults to `false`. ### `immediate = bool (optional)` [Section titled â€œimmediate = bool (optional)â€](#immediate--bool-optional) This flag tells the server how to react if the message cannot be routed to a queue consumer immediately. If `true`, the server will return an undeliverable message with a Return method. If `false`, the server will queue the message, but with no guarantee that it will ever be consumed. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Send the list of plugins as [JSON](/reference/operators/write_json) [Section titled â€œSend the list of plugins as JSONâ€](#send-the-list-of-plugins-as-json) ```tql plugins write_json save_amqp ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_amqp`](/reference/operators/load_amqp)

# save_azure_blob_storage

Saves bytes to Azure Blob Storage. ```tql save_azure_blob_storage uri:string, [account_key=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `save_azure_blob_storage` operator writes bytes to a blob in an Azure Blob Store. By default, authentication is handled by the Azure SDKâ€™s credential chain which may read from multiple environment variables, such as: * `AZURE_TENANT_ID` * `AZURE_CLIENT_ID` * `AZURE_CLIENT_SECRET` * `AZURE_AUTHORITY_HOST` * `AZURE_CLIENT_CERTIFICATE_PATH` * `AZURE_FEDERATED_TOKEN_FILE` ### `uri: string` [Section titled â€œuri: stringâ€](#uri-string) An URI identifying the blob to save to. If the blob and/or do not exist, they will be created. Supported URI formats: 1. `abfs[s]://[:<password>@]<account>.blob.core.windows.net[/<container>[/<path>]]` 2. `abfs[s]://<container>[:<password>]@<account>.dfs.core.windows.net[/path]` 3. `abfs[s]://[<account[:<password>]@]<host[.domain]>[<:port>][/<container>[/path]]` 4. `abfs[s]://[<account[:<password>]@]<container>[/path]` (1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs 1, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2). ### `account_key = string (optional)` [Section titled â€œaccount\_key = string (optional)â€](#account_key--string-optional) Account key to authenticate with. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write JSON [Section titled â€œWrite JSONâ€](#write-json) Write to blob `obj.json` in the blob container `container`, using the `tenzirdev` user: ```tql write_json save_azure_blob_storage "abfss://tenzirdev@container/obj.json" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage)

# save_email

Saves bytes through an SMTP server. ```tql save_email recipient:str, [endpoint=str, from=str, subject=str, username=str, password=str, authzid=str, authorization=str, tls=bool, skip_peer_verification=bool, cacert=string, certfile=string, keyfile=string, mime=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `save_email` operator establishes a SMTP(S) connection to a mail server and sends bytes as email body. ### `recipient: str` [Section titled â€œrecipient: strâ€](#recipient-str) The recipient of the mail. The expected format is either `Name <user@example.org>` with the email in angle brackets, or a plain email adress, such as `user@example.org`. ### `endpoint = str (optional)` [Section titled â€œendpoint = str (optional)â€](#endpoint--str-optional) The endpoint of the mail server. To choose between SMTP and SMTPS, provide a URL with with the corresponding scheme. For example, `smtp://127.0.0.1:25` will establish an unencrypted connection, whereas `smtps://127.0.0.1:25` an encrypted one. If you specify a server without a schema, the protocol defaults to SMTPS. Defaults to `smtp://localhost:25`. ### `from = str (optional)` [Section titled â€œfrom = str (optional)â€](#from--str-optional) The `From` header. If you do not specify this parameter, an empty address is sent to the SMTP server which might cause the email to be rejected. ### `subject = str (optional)` [Section titled â€œsubject = str (optional)â€](#subject--str-optional) The `Subject` header. ### `username = str (optional)` [Section titled â€œusername = str (optional)â€](#username--str-optional) The username in an authenticated SMTP connection. ### `password = str (optional)` [Section titled â€œpassword = str (optional)â€](#password--str-optional) The password in an authenticated SMTP connection. ### `authzid = str (optional)` [Section titled â€œauthzid = str (optional)â€](#authzid--str-optional) The authorization identity in an authenticated SMTP connection. This option is only applicable to the PLAIN SASL authentication mechanism where it is optional. When not specified only the authentication identity (`authcid`) as specified by the username is sent to the server, along with the password. The server derives an `authzid` from the `authcid` when not provided, which it then uses internally. When the `authzid` is specified it can be used to access another userâ€™s inbox, that the user has been granted access to, or a shared mailbox. ### `authorization = str (optional)` [Section titled â€œauthorization = str (optional)â€](#authorization--str-optional) The authorization options for an authenticated SMTP connection. This login option defines the preferred authentication mechanism, e.g., `AUTH=PLAIN`, `AUTH=LOGIN`, or `AUTH=*`. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ### `mime = bool (optional)` [Section titled â€œmime = bool (optional)â€](#mime--bool-optional) Whether to wrap the chunk into a MIME part. The operator uses the metadata of the byte chunk for the `Content-Type` MIME header. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) Send the Tenzir version string as CSV to `user@example.org`: ```tql version write_csv save_email "user@example.org" ``` Send the email body as MIME part: ```tql version write_json save_email "user@example.org", mime=true ``` This may result in the following email body: ```plaintext --------------------------s89ecto6c12ILX7893YOEf Content-Type: application/json Content-Transfer-Encoding: quoted-printable { "version": "4.10.4+ge0a060567b-dirty", "build": "ge0a060567b-dirty", "major": 4, "minor": 10, "patch": 4 } --------------------------s89ecto6c12ILX7893YOEf-- ```

# save_file

Writes a byte stream to a file. ```tql save_file path:string, [append=bool, real_time=bool, uds=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Writes a byte stream to a file. ### `path: string` [Section titled â€œpath: stringâ€](#path-string) The file path to write to. If intermediate directories do not exist, they will be created. When `~` is the first character, it will be substituted with the value of the `$HOME` environment variable. ### `append = bool (optional)` [Section titled â€œappend = bool (optional)â€](#append--bool-optional) If `true`, appends to the file instead of overwriting it. ### `real_time = bool (optional)` [Section titled â€œreal\_time = bool (optional)â€](#real_time--bool-optional) If `true`, immediately synchronizes the file with every chunk of bytes instead of buffering bytes to batch filesystem write operations. ### `uds = bool (optional)` [Section titled â€œuds = bool (optional)â€](#uds--bool-optional) If `true`, creates a Unix Domain Socket instead of a normal file. Cannot be combined with `append=true`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Save bytes to a file [Section titled â€œSave bytes to a fileâ€](#save-bytes-to-a-file) ```tql save_file "/tmp/out.txt" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`files`](/reference/operators/files), [`load_file`](/reference/operators/load_file), [`save_stdout`](/reference/operators/save_stdout)

# save_ftp

Saves a byte stream via FTP. ```tql save_ftp url:str [tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Saves a byte stream via FTP. ### `url: str` [Section titled â€œurl: strâ€](#url-str) The URL to request from. The `ftp://` scheme can be omitted. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql save_ftp "ftp.example.org" ```

# save_gcs

Saves bytes to a Google Cloud Storage object. ```tql save_gcs uri:string, [anonymous=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `save_gcs` operator connects to a GCS bucket to save raw bytes to a GCS object. The connector tries to retrieve the appropriate credentials using Googleâ€™s [Application Default Credentials](https://google.aip.dev/auth/4110). ### `uri: string` [Section titled â€œuri: stringâ€](#uri-string) The path to the GCS object. The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist: > For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`. ### `anonymous = bool (optional)` [Section titled â€œanonymous = bool (optional)â€](#anonymous--bool-optional) Ignore any predefined credentials and try to use anonymous credentials. ## Examples [Section titled â€œExamplesâ€](#examples) Write JSON to an object `test.json` in `bucket`, but using a different GCS-compatible endpoint: ```tql write_json save_gcs "gs://bucket/test.json?endpoint_override=gcs.mycloudservice.com" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_gcs`](/reference/operators/load_gcs)

# save_google_cloud_pubsub

Publishes to a Google Cloud Pub/Sub topic. ```tql save_google_cloud_pubsub project_id=string, topic_id=string ``` ## Description [Section titled â€œDescriptionâ€](#description) The operator publishes bytes to a Google Cloud Pub/Sub topic. ### `project_id = string` [Section titled â€œproject\_id = stringâ€](#project_id--string) The project to connect to. Note that this is the project\_id, not the display name. ### `topic_id = string` [Section titled â€œtopic\_id = stringâ€](#topic_id--string) The topic to publish to. ## URI support & integration with `from` [Section titled â€œURI support & integration with fromâ€](#uri-support--integration-with-from) The `save_google_cloud_pubsub` operator can also be used from the [`to`](/reference/operators/to) operator. For this, the `gcps://` scheme can be used. The URI is then translated: ```tql to "gcps://my_project/my_topic" ``` ```tql save_google_cloud_pubsub project_id="my_project", topic_id="my_topic" ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Publish alerts to a given topic [Section titled â€œPublish alerts to a given topicâ€](#publish-alerts-to-a-given-topic) Publish `suricata.alert` events as JSON to `alerts-topic`: ```tql export where @name = "suricata.alert" write_json save_google_cloud_pubsub project_id="amazing-project-123456", topic_id="alerts-topic" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_google_cloud_pubsub`](/reference/operators/load_google_cloud_pubsub)

# save_http

Sends a byte stream via HTTP. ```tql save_http url:string, [params=record, headers=record, method=string, parallel=int, tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `save_http` operator performs a HTTP request with the request body being the bytes provided by the previous operator. ### `url: string` [Section titled â€œurl: stringâ€](#url-string) The URL to write to. The `http://` scheme can be omitted. ### `method = string (optional)` [Section titled â€œmethod = string (optional)â€](#method--string-optional) The HTTP method, such as `POST` or `GET`. The default is `"POST"`. ### `params = record (optional)` [Section titled â€œparams = record (optional)â€](#params--record-optional) The query parameters for the request. ### `headers = record (optional)` [Section titled â€œheaders = record (optional)â€](#headers--record-optional) The headers for the request. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Call a webhook with pipeline data [Section titled â€œCall a webhook with pipeline dataâ€](#call-a-webhook-with-pipeline-data) ```tql save_http "example.org/api", headers={"X-API-Token": "0000-0000-0000"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_http`](/reference/operators/load_http)

# save_kafka

Saves a byte stream to a Apache Kafka topic. ```tql save_kafka topic:string, [key=string, timestamp=time, options=record, aws_iam=record] ``` ## Description [Section titled â€œDescriptionâ€](#description) Deprecated The `save_kafka` operator does not respect event boundaries and can combine multiple events into a single message, causing issues for consumers. Consider using `to_kafka` instead. The `save_kafka` operator saves bytes to a Kafka topic. The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`. The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them: * `bootstrap.servers`: `localhost` * `client.id`: `tenzir` * `group.id`: `tenzir` ### `topic: string` [Section titled â€œtopic: stringâ€](#topic-string) The Kafka topic to use. ### `key = string (optional)` [Section titled â€œkey = string (optional)â€](#key--string-optional) Sets a fixed key for all messages. ### `timestamp = time (optional)` [Section titled â€œtimestamp = time (optional)â€](#timestamp--time-optional) Sets a fixed timestamp for all messages. ### `options = record (optional)` [Section titled â€œoptions = record (optional)â€](#options--record-optional) A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"auto.offset.reset" : "earliest", "enable.partition.eof": true}`. The `save_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs. We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `save_kafka` arguments. ### `aws_iam = record (optional)` [Section titled â€œaws\_iam = record (optional)â€](#aws_iam--record-optional) If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified. Available keys: * `region`: Region of the MSK Clusters. Must be specified when using IAM. * `assume_role`: Optional Role ARN to assume. * `session_name`: Optional session name to use when assuming a role. * `external_id`: Optional external id to use when assuming a role. The operator will try to get credentials in the following order: 1. Checks your environment variables for AWS Credentials. 2. Checks your `$HOME/.aws/credentials` file for a profile and credentials 3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`. 4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that are not directly supported by AWS. 5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set. 6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write the Tenzir version to topic `tenzir` with timestamp from the past [Section titled â€œWrite the Tenzir version to topic tenzir with timestamp from the pastâ€](#write-the-tenzir-version-to-topic-tenzir-with-timestamp-from-the-past) ```tql version write_json save_kafka "tenzir", timestamp=1984-01-01 ``` ### Follow a CSV file and publish it to topic `data` [Section titled â€œFollow a CSV file and publish it to topic dataâ€](#follow-a-csv-file-and-publish-it-to-topic-data) ```tql load_file "/tmp/data.csv" read_csv write_json save_kafka "data" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_kafka`](/reference/operators/load_kafka)

# save_s3

Saves bytes to an Amazon S3 object. ```tql save_s3 uri:str, [anonymous=bool, role=string, external_id=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `save_s3` operator writes bytes to an S3 object in an S3 bucket. The connector tries to retrieve the appropriate credentials using AWSâ€™s [default credentials provider chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). If a config file `<prefix>/etc/tenzir/plugin/s3.yaml` or `~/.config/tenzir/plugin/s3.yaml` exists, it is always preferred over the default AWS credentials. The configuration file must have the following format: ```yaml access-key: your-access-key secret-key: your-secret-key session-token: your-session-token (optional) ``` ### `uri: str` [Section titled â€œuri: strâ€](#uri-str) The path to the S3 object. The syntax is `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)`. Options can be appended to the path as query parameters, as per [Arrow](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri): > For S3, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`. ### `anonymous = bool (optional)` [Section titled â€œanonymous = bool (optional)â€](#anonymous--bool-optional) Whether to ignore any predefined credentials and try to save with anonymous credentials. ### `role = string (optional)` [Section titled â€œrole = string (optional)â€](#role--string-optional) A role to assume when writing to S3. ### `external_id = string (optional)` [Section titled â€œexternal\_id = string (optional)â€](#external_id--string-optional) The external ID to use when assuming the `role`. Defaults to no ID. ## Examples [Section titled â€œExamplesâ€](#examples) Read CSV from an object `obj.csv` in the bucket `examplebucket` and save it as YAML to another bucket `examplebucket2`: ```tql load_s3 "s3://examplebucket/obj.csv" read_csv write_yaml save_s3 "s3://examplebucket2/obj.yaml" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_s3`](/reference/operators/load_s3), [`to_amazon_security_lake`](/reference/operators/to_amazon_security_lake)

# save_sqs

Saves bytes to [Amazon SQS](https://docs.aws.amazon.com/sqs/) queues. ```tql save_sqs queue:str, [poll_time=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) [Amazon Simple Queue Service (Amazon SQS)](https://docs.aws.amazon.com/sqs/) is a fully managed message queuing service to decouple and scale microservices, distributed systems, and serverless applications. The `save_sqs` operator writes bytes as messages into an SQS queue. The `save_sqs` operator uses long polling, which helps reduce your cost of using SQS by reducing the number of empty responses when there are no messages available to return in reply to a message request. Use the `poll_time` option to adjust the timeout. The operator requires the following AWS permissions: * `sqs:GetQueueUrl` * `sqs:SendMessage` ### `queue: str` [Section titled â€œqueue: strâ€](#queue-str) The name of the queue to use. ### `poll_time = duration (optional)` [Section titled â€œpoll\_time = duration (optional)â€](#poll_time--duration-optional) The long polling timeout per request. The value must be between 1 and 20 seconds. Defaults to `3s`. ## Examples [Section titled â€œExamplesâ€](#examples) Write JSON messages from a source feed to the SQS queue `tenzir`: ```tql subscribe "to-sqs" write_json save_sqs "tenzir" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_sqs`](/reference/operators/load_sqs)

# save_stdout

Writes a byte stream to standard output. ```tql save_stdout ``` ## Description [Section titled â€œDescriptionâ€](#description) Writes a byte stream to standard output. This is mostly useful when using the `tenzir` executable as part of a shell script. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write colored, compact TQL-style [Section titled â€œWrite colored, compact TQL-styleâ€](#write-colored-compact-tql-style) ```tql from {x: "Hello World"} write_tql compact=true, color=true save_stdout ``` ```tql {x: "Hello World"} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_stdin`](/reference/operators/load_stdin), [`save_file`](/reference/operators/save_file)

# save_tcp

Saves bytes to a TCP or TLS connection. ```tql save_tcp endpoint:string, [retry_delay=duration, max_retry_count=int, tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Saves bytes to the given endpoint via TCP or TLS. Attempts to reconnect automatically for `max_retry_count` in case of recoverable connection errors. ### `endpoint: string` [Section titled â€œendpoint: stringâ€](#endpoint-string) The endpoint to which the server will connect. Must be of the form `[tcp://]<hostname>:<port>`. You can also use an IANA service name instead of a numeric port. ### `retry_delay = duration (optional)` [Section titled â€œretry\_delay = duration (optional)â€](#retry_delay--duration-optional) The amount of time to wait before attempting to reconnect in case a connection attempt fails and the error is deemed recoverable. Defaults to `30s`. ### \`max\_retry\_count = int (optional) [Section titled â€œ\`max\_retry\_count = int (optional)â€](#max_retry_count--int-optional) The number of retries to attempt in case of connection errors before transitioning into the error state. Defaults to `10`. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Transform incoming Syslog to BITZ and save over TCP [Section titled â€œTransform incoming Syslog to BITZ and save over TCPâ€](#transform-incoming-syslog-to-bitz-and-save-over-tcp) ```tql load_tcp "0.0.0.0:8090" { read_syslog } write_bitz save_tcp "127.0.0.1:4000" ``` ### Save to localhost with TLS [Section titled â€œSave to localhost with TLSâ€](#save-to-localhost-with-tls) ```tql subscribe "feed" write_json save_tcp "127.0.0.1:4000", tls=true, skip_peer_verification=true ```

# save_udp

Saves bytes to a UDP socket. ```tql save_udp endpoint:str ``` ## Description [Section titled â€œDescriptionâ€](#description) Saves bytes to a UDP socket. ### `endpoint: str` [Section titled â€œendpoint: strâ€](#endpoint-str) The address of the remote endpoint to load bytes from. Must be of the format: `[udp://]host:port`. ## Examples [Section titled â€œExamplesâ€](#examples) Send the Tenzir version as CSV file to a remote endpoint via UDP: ```tql version write_csv save_udp "127.0.0.1:56789" ``` Use `nc -ul 127.0.0.1 56789` to spin up a UDP server to test the above pipeline. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_udp`](/reference/operators/load_udp)

# save_zmq

Sends bytes as ZeroMQ messages. ```tql save_zmq [endpoint:str, listen=bool, connect=bool, monitor=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `save_zmq` operator sends bytes as a ZeroMQ message via a `PUB` socket. Indpendent of the socket type, the `save_zmq` operator supports specfiying the direction of connection establishment with `listen` and `connect`. This can be helpful to work around firewall restrictions and fit into broader set of existing ZeroMQ applications. With the `monitor` option, you can activate message buffering for TCP sockets that hold off sending messages until *at least one* remote peer has connected. This can be helpful when you want to delay publishing until you have one connected subscriber, e.g., when the publisher spawns before any subscriber exists. ### `endpoint: str (optional)` [Section titled â€œendpoint: str (optional)â€](#endpoint-str-optional) The endpoint for connecting to or listening on a ZeroMQ socket. Defaults to `tcp://127.0.0.1:5555`. ### `listen = bool (optional)` [Section titled â€œlisten = bool (optional)â€](#listen--bool-optional) Bind to the ZeroMQ socket. Defaults to `true`. ### `connect = bool (optional)` [Section titled â€œconnect = bool (optional)â€](#connect--bool-optional) Connect to the ZeroMQ socket. Defaults to `false`. ### `monitor = bool (optional)` [Section titled â€œmonitor = bool (optional)â€](#monitor--bool-optional) Monitors a 0mq socket over TCP until the remote side establishes a connection. ## Examples [Section titled â€œExamplesâ€](#examples) ### Publish events by connecting to a PUB socket [Section titled â€œPublish events by connecting to a PUB socketâ€](#publish-events-by-connecting-to-a-pub-socket) ```tql from {x: 42} write_csv save_zmq connect=true ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_zmq`](/reference/operators/load_zmq)

# schemas

Retrieves all schemas for events stored at a node. ```tql schemas ``` ## Description [Section titled â€œDescriptionâ€](#description) The `schemas` operator shows all schemas of all events stored at a node. Note that there may be multiple schema definitions with the same name, but a different set of fields, e.g., because the imported dataâ€™s schema changed over time. ## Examples [Section titled â€œExamplesâ€](#examples) ### See all available definitions for a given schema [Section titled â€œSee all available definitions for a given schemaâ€](#see-all-available-definitions-for-a-given-schema) ```tql schemas where name == "suricata.alert" ```

# select

Selects some values and discards the rest. ```tql select (field|assignment)... ``` ## Description [Section titled â€œDescriptionâ€](#description) This operator keeps only the provided fields and drops the rest. ### `field` [Section titled â€œfieldâ€](#field) The field to keep. If it does not exist, itâ€™s given the value `null` and a warning is emitted. ### `assignment` [Section titled â€œassignmentâ€](#assignment) An assignment of the form `<field>=<expr>`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Select and create columns [Section titled â€œSelect and create columnsâ€](#select-and-create-columns) Keep `a` and introduce `y` with the value of `b`: ```tql from {a: 1, b: 2, c: 3} select a, y=b ``` ```tql {a: 1, y: 2} ``` A more complex example with expressions and selection through records: ```tql from { name: "foo", pos: { x: 1, y: 2, }, state: "active", } select id=name.to_upper(), pos.x, added=true ``` ```tql { id: "FOO", pos: { x: 1, }, added: true, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`drop`](/reference/operators/drop), [`where`](/reference/operators/where)

# serve

Make events available under the `/serve` REST API endpoint ```tql serve id:string, [buffer_size=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `serve` operator bridges between pipelines and the corresponding `/serve` [REST API endpoint](/reference/node/api). ![Serve Operator](/_astro/serve.excalidraw.CC8LHhCr_19DKCs.svg) Pipelines ending with the `serve` operator exit when all events have been delivered over the corresponding endpoint. ### `id: string` [Section titled â€œid: stringâ€](#id-string) An identifier that uniquely identifies the operator. The `serve` operator errors when receiving a duplicate serve id. ### `buffer_size = int (optional)` [Section titled â€œbuffer\_size = int (optional)â€](#buffer_size--int-optional) The buffer size specifies the maximum number of events to keep in the `serve` operator to make them instantly available in the corresponding endpoint before throttling the pipeline execution. Defaults to `1Ki`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Make the input available as REST API [Section titled â€œMake the input available as REST APIâ€](#make-the-input-available-as-rest-api) Read a Zeek `conn.log` and make it available as `zeek-conn-logs`: ```tql load_file "path/to/conn.log" read_zeek_tsv serve "zeek-conn-logs"' ``` Then fetch the first 100 events from the `/serve` endpoint: ```bash curl \ -X POST \ -H "Content-Type: application/json" \ -d '{"serve_id": "zeek-conn-logs", "continuation_token": null, "timeout": "1s", "max_events": 100}' \ http://localhost:5160/api/v0/serve ``` This will return up to 100 events, or less if the specified timeout of 1 second expired. Subsequent results for further events must specify a continuation token. The token is included in the response under `next_continuation_token` if there are further events to be retrieved from the endpoint. ### Wait for the first event [Section titled â€œWait for the first eventâ€](#wait-for-the-first-event) This pipeline will produce 10 events after 3 seconds of doing nothing. ```tql shell "sleep 3; jq --null-input '{foo: 1}'" read_json repeat 10 serve "slow-events" ``` ```bash curl \ -X POST \ -H "Content-Type: application/json" \ -d '{"serve_id": "slow-events", "continuation_token": null, "timeout": "5s", "min_events": 1}' \ http://localhost:5160/api/v0/serve ``` The call to `/serve` will wait up to 5 seconds for the first event from the pipeline arriving at the serve operator, and return immediately once the first event arrives. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`api`](/reference/operators/api), [`from_http`](/reference/operators/from_http), [`openapi`](/reference/operators/openapi)

# set

Assigns a value to a field, creating it if necessary. ```tql field = expr set field=expr... ``` ## Description [Section titled â€œDescriptionâ€](#description) Assigns a value to a field, creating it if necessary. If the field does not exist, it is appended to the end. If the field name is a path such as `foo.bar.baz`, records for `foo` and `bar` will be created if they do not exist yet. Within assignments, the `move` keyword in front of a field causes a field to be removed from the input after evaluation. ## Examples [Section titled â€œExamplesâ€](#examples) ### Append a new field [Section titled â€œAppend a new fieldâ€](#append-a-new-field) ```tql from {a: 1, b: 2} c = a + b ``` ```tql {a: 1, b: 2, c: 3} ``` ### Update an existing field [Section titled â€œUpdate an existing fieldâ€](#update-an-existing-field) ```tql from {a: 1, b: 2} a = "Hello" ``` ```tql {a: "Hello", b: 2} ``` ### Move a field [Section titled â€œMove a fieldâ€](#move-a-field) ```tql from {a: 1} b = move a ``` ```tql {b: 1} ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`move`](/reference/operators/move)

# shell

Executes a system command and hooks its stdin and stdout into the pipeline. ```tql shell cmd:string ``` ## Description [Section titled â€œDescriptionâ€](#description) The `shell` operator executes the provided command by spawning a new process. The input of the operator is forwarded to the childâ€™s standard input. Similarly, the childâ€™s standard output is forwarded to the output of the operator. ### `cmd: string` [Section titled â€œcmd: stringâ€](#cmd-string) The command to execute and hook into the pipeline processing. It is interpreted by `/bin/sh -c`. ## Secrets [Section titled â€œSecretsâ€](#secrets) By default, the `shell` operator does not accept secrets. If you want to allow usage of secrets in the `cmd` argument, you can enable the configuration option `tenzir.allow-secrets-in-escape-hatches`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Show a live log from the `tenzir-node` service [Section titled â€œShow a live log from the tenzir-node serviceâ€](#show-a-live-log-from-the-tenzir-node-service) ```tql shell "journalctl -u tenzir-node -f" read_json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`python`](/reference/operators/python)

# sigma

Filter the input with Sigma rules and output matching events. ```tql sigma path:string, [refresh_interval=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `sigma` operator executes [Sigma rules](https://github.com/SigmaHQ/sigma) on its input. If a rule matches, the operator emits a `tenzir.sigma` event that wraps the input record into a new record along with the matching rule. The operator discards all events that do not match the provided rules. Sigma uses [value modifiers](https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md#value-modifiers) to select a concrete relational operator for given search predicate. Without a modifier, Sigma uses equality comparison (`==`) of field and value. For example, the `contains` modifier changes the relational operator to substring search, and the `re` modifier switches to a regular expression match. The table below shows what modifiers the `sigma` operator supports, where âœ… means implemented, ğŸš§ not yet implemented but possible, and âŒ not yet supported: | Modifier | Use | sigmac | Tenzir | | ---------------- | -------------------------------------------------------- | :----: | :----: | | `contains` | perform a substring search with the value | âœ… | âœ… | | `startswith` | match the value as a prefix | âœ… | âœ… | | `endswith` | match the value as a suffix | âœ… | âœ… | | `base64` | encode the value with Base64 | âœ… | âœ… | | `base64offset` | encode value as all three possible Base64 variants | âœ… | âœ… | | `utf16le`/`wide` | transform the value to UTF16 little endian | âœ… | ğŸš§ | | `utf16be` | transform the value to UTF16 big endian | âœ… | ğŸš§ | | `utf16` | transform the value to UTF16 | âœ… | ğŸš§ | | `re` | interpret the value as regular expression | âœ… | âœ… | | `cidr` | interpret the value as a IP CIDR | âŒ | âœ… | | `all` | changes the expression logic from OR to AND | âœ… | âœ… | | `lt` | compare less than (`<`) the value | âŒ | âœ… | | `lte` | compare less than or equal to (`<=`) the value | âŒ | âœ… | | `gt` | compare greater than (`>`) the value | âŒ | âœ… | | `gte` | compare greater than or equal to (`>=`) the value | âŒ | âœ… | | `expand` | expand value to placeholder strings, e.g., `%something%` | âŒ | âŒ | ### `path: string` [Section titled â€œpath: stringâ€](#path-string) The rule to match. If `path` points to a rule, the operator transpiles the rule file at the time of pipeline creation. If this points to a directory, the operator watches it and attempts to parse each contained file as a Sigma rule. The `sigma` operator matches if *any* of the contained rules match, effectively creating a disjunction of all rules inside the directory. ### `refresh_interval = duration (optional)` [Section titled â€œrefresh\_interval = duration (optional)â€](#refresh_interval--duration-optional) How often the `sigma` operator looks at the specified rule or directory of rules to update its internal state. Defaults to `5s`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Apply a Sigma rule to an EVTX file [Section titled â€œApply a Sigma rule to an EVTX fileâ€](#apply-a-sigma-rule-to-an-evtx-file) The tool [`evtx_dump`](https://github.com/omerbenamram/evtx) turns an EVTX file into a JSON object. On the command line, use the `tenzir` binary to pipe the `evtx_dump` output to a Tenzir pipeline using the `sigma` operator: ```bash evtx_dump -o jsonl file.evtx | tenzir 'read_json | sigma "rule.yaml"' ``` ### Run a Sigma rule on historical data [Section titled â€œRun a Sigma rule on historical dataâ€](#run-a-sigma-rule-on-historical-data) Apply a Sigma rule over historical data in a node from the last day: ```tql export where ts > now() - 1d sigma "rule.yaml" ``` ### Stream a file and apply a set of Sigma rules to it [Section titled â€œStream a file and apply a set of Sigma rules to itâ€](#stream-a-file-and-apply-a-set-of-sigma-rules-to-it) Watch a directory of Sigma rules and apply all of them on a continuous stream of Suricata events: ```tql load_file "eve.json", follow=true read_suricata sigma "/tmp/rules/" ``` When you add a new file to `/tmp/rules`, the `sigma` operator transpiles it and will match it on all subsequent inputs.

# slice

Keeps a range of events within the interval `[begin, end)` stepping by `stride`. ```tql slice [begin=int, end=int, stride=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `slice` operator selects a range of events from the input. The semantics of the operator match Pythonâ€™s array slicing. ### `begin = int (optional)` [Section titled â€œbegin = int (optional)â€](#begin--int-optional) The beginning (inclusive) of the range to keep. Use a negative number to count from the end. ### `end = int (optional)` [Section titled â€œend = int (optional)â€](#end--int-optional) The end (exclusive) of the range to keep. Use a negative number to count from the end. ### `stride = int (optional)` [Section titled â€œstride = int (optional)â€](#stride--int-optional) The number of elements to advance before the next element. Use a negative number to count from the end, effectively reversing the stream. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the second 100 events [Section titled â€œGet the second 100 eventsâ€](#get-the-second-100-events) ```tql slice begin=100, end=200 ``` ### Get the last 5 events [Section titled â€œGet the last 5 eventsâ€](#get-the-last-5-events) ```tql slice begin=-5 ``` ### Skip the last 10 events [Section titled â€œSkip the last 10 eventsâ€](#skip-the-last-10-events) ```tql slice end=-10 ``` ### Return the last 50 events, except for the last 2 [Section titled â€œReturn the last 50 events, except for the last 2â€](#return-the-last-50-events-except-for-the-last-2) ```tql slice begin=-50, end=-2 ``` ### Skip the first and the last event [Section titled â€œSkip the first and the last eventâ€](#skip-the-first-and-the-last-event) ```tql slice begin=1, end=-1 ``` ### Return every second event starting from the tenth [Section titled â€œReturn every second event starting from the tenthâ€](#return-every-second-event-starting-from-the-tenth) ```tql slice begin=9, stride=2 ``` ### Return all but the last five events in reverse order [Section titled â€œReturn all but the last five events in reverse orderâ€](#return-all-but-the-last-five-events-in-reverse-order) ```tql slice end=-5, stride=-1 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`head`](/reference/operators/head), [`tail`](/reference/operators/tail)

# sockets

Shows a snapshot of open sockets. ```tql sockets ``` ## Description [Section titled â€œDescriptionâ€](#description) The `sockets` operator shows a snapshot of all currently open sockets. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir emits socket information with the following schema. ### `tenzir.socket` [Section titled â€œtenzir.socketâ€](#tenzirsocket) Contains detailed information about the socket. | Field | Type | Description | | :------------ | :------- | :------------------------------------------------- | | `pid` | `uint64` | The process identifier. | | `process` | `string` | The name of the process involved. | | `protocol` | `uint64` | The protocol used for the communication. | | `local_addr` | `ip` | The local IP address involved in the connection. | | `local_port` | `port` | The local port number involved in the connection. | | `remote_addr` | `ip` | The remote IP address involved in the connection. | | `remote_port` | `port` | The remote port number involved in the connection. | | `state` | `string` | The current state of the connection. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Show process ID, local, and remote IP address of all sockets [Section titled â€œShow process ID, local, and remote IP address of all socketsâ€](#show-process-id-local-and-remote-ip-address-of-all-sockets) ```tql sockets select pid, local_addr, remote_addr ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`files`](/reference/operators/files), [`processes`](/reference/operators/processes)

# sort

Sorts events by the given expressions. ```tql sort [-]expr... ``` ## Description [Section titled â€œDescriptionâ€](#description) Sorts events by the given expressions, putting all `null` values at the end. If multiple expressions are specified, the sorting happens lexicographically, that is: Later expressions are only considered if all previous expressions evaluate to equal values. This operator performs a stable sort (preserves relative ordering when all expressions evaluate to the same value). ### `[-]expr` [Section titled â€œ\[-\]exprâ€](#-expr) An expression that is evaluated for each event. Normally, events are sorted in ascending order. If the expression starts with `-`, descending order is used instead. In both cases, `null` is put last. ## Examples [Section titled â€œExamplesâ€](#examples) ### Sort by a field in ascending order [Section titled â€œSort by a field in ascending orderâ€](#sort-by-a-field-in-ascending-order) ```tql sort timestamp ``` ### Sort by a field in descending order [Section titled â€œSort by a field in descending orderâ€](#sort-by-a-field-in-descending-order) ```tql sort -timestamp ``` ### Sort by multiple fields [Section titled â€œSort by multiple fieldsâ€](#sort-by-multiple-fields) Sort by a field `src_ip` and, in case of matching values, sort by `dest_ip`: ```tql sort src_ip, dest_ip ``` Sort by the field `src_ip` in ascending order and by the field `dest_ip` in descending order. ```tql sort src_ip, -dest_ip ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`rare`](/reference/operators/rare), [`reverse`](/reference/operators/reverse), [`top`](/reference/operators/top)

# strict

Treats all warnings as errors. ```tql strict { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `strict` operator takes a pipeline as an argument and treats all warnings emitted by the execution of the pipeline as errors. This is useful when you want to stop a pipeline on warnings or unexpected diagnostics. ## Examples [Section titled â€œExamplesâ€](#examples) ### Stop the pipeline on any warnings when sending logs [Section titled â€œStop the pipeline on any warnings when sending logsâ€](#stop-the-pipeline-on-any-warnings-when-sending-logs) ```tql subscribe "log-feed" strict { to_google_cloud_logging â€¦ } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`assert`](/reference/operators/assert)

# subscribe

Subscribes to events from a channel with a topic. ```tql subscribe [topic:string...] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `subscribe` operator subscribes to events from a channel with the specified topic. Multiple `subscribe` operators with the same topic receive the same events. Subscribers propagate back pressure to publishers. If a subscribing pipeline fails to keep up, all publishers will slow down as well to a matching speed to avoid data loss. This mechanism is disabled for pipelines that are not visible on the overview page on [app.tenzir.com](https://app.tenzir.com), which drop data rather than slow down their publishers. ### `topic: string... (optional)` [Section titled â€œtopic: string... (optional)â€](#topic-string-optional) Optional channel names to subscribe to. If unspecified, the operator subscribes to the topic `main`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Subscribe to the events under a topic [Section titled â€œSubscribe to the events under a topicâ€](#subscribe-to-the-events-under-a-topic) ```tql subscribe "zeek-conn" ``` ### Subscribe to the multiple topics [Section titled â€œSubscribe to the multiple topicsâ€](#subscribe-to-the-multiple-topics) ```tql subscribe "alerts", "notices", "critical" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`export`](/reference/operators/export), [`publish`](/reference/operators/publish)

# summarize

Groups events and applies aggregate functions to each group. ```tql summarize (group|aggregation)... ``` ## Description [Section titled â€œDescriptionâ€](#description) The `summarize` operator groups events according to certain fields and applies [aggregation functions](/reference/functions#aggregation) to each group. The operator consumes the entire input before producing any output, and may reorder the event stream. The order of the output fields follows the sequence of the provided arguments. Unspecified fields are dropped. ### `group` [Section titled â€œgroupâ€](#group) To group by a certain field, use the syntax `<field>` or `<field>=<field>`. For each unique combination of the `group` fields, a single output event will be returned. ### `aggregation` [Section titled â€œaggregationâ€](#aggregation) The [aggregation functions](/reference/functions#aggregation) applied to each group are specified with `f(â€¦)` or `<field>=f(â€¦)`, where `f` is the name of an aggregation function (see below) and `<field>` is an optional name for the result. The aggregation function will produce a single result for each group. If no name is specified, the aggregation function call will automatically generate one. If processing continues after `summarize`, we strongly recommend to specify a custom name. ## Examples [Section titled â€œExamplesâ€](#examples) ### Compute the sum of a field over all events [Section titled â€œCompute the sum of a field over all eventsâ€](#compute-the-sum-of-a-field-over-all-events) ```tql from {x: 1}, {x: 2} summarize x=sum(x) ``` ```tql {x: 3} ``` Group over `y` and compute the sum of `x` for each group: ```tql from {x: 0, y: 0, z: 1}, {x: 1, y: 1, z: 2}, {x: 1, y: 1, z: 3} summarize y, x=sum(x) ``` ```tql {y: 0, x: 0} {y: 1, x: 2} ``` ### Gather unique values in a list [Section titled â€œGather unique values in a listâ€](#gather-unique-values-in-a-list) Group the input by `src_ip` and aggregate all unique `dest_port` values into a list: ```tql summarize src_ip, distinct(dest_port) ``` Same as above, but produce a count of the unique number of values instead of a list: ```tql summarize src_ip, count_distinct(dest_port) ``` ### Compute min and max of a group [Section titled â€œCompute min and max of a groupâ€](#compute-min-and-max-of-a-group) Compute minimum and maximum of the `timestamp` field per `src_ip` group: ```tql summarize min(timestamp), max(timestamp), src_ip ``` Compute minimum and maximum of the `timestamp` field over all events: ```tql summarize min(timestamp), max(timestamp) ``` ### Check if any value of a group is true [Section titled â€œCheck if any value of a group is trueâ€](#check-if-any-value-of-a-group-is-true) Create a boolean flag `originator` that is `true` if any value in the `src_ip` group is `true`: ```tql summarize src_ip, originator=any(is_orig) ``` ### Create 1-hour time buckets [Section titled â€œCreate 1-hour time bucketsâ€](#create-1-hour-time-buckets) Create 1-hour groups and produce a summary of network traffic between host pairs: ```tql ts = round(ts, 1h) summarize ts, src_ip, dest_ip, sum(bytes_in), sum(bytes_out) ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`rare`](/reference/operators/rare), [`top`](/reference/operators/top)

# tail

Limits the input to the last `n` events. ```tql tail [n:int] ``` ## Description [Section titled â€œDescriptionâ€](#description) Forwards the last `n` events and discards the rest. `tail n` is a shorthand notation for [`slice begin=-n`](/reference/operators/slice). ### `n: int (optional)` [Section titled â€œn: int (optional)â€](#n-int-optional) The number of events to keep. Defaults to `10`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Get the last 10 results [Section titled â€œGet the last 10 resultsâ€](#get-the-last-10-results) ```tql export tail ``` ### Get the last 5 results [Section titled â€œGet the last 5 resultsâ€](#get-the-last-5-results) ```tql export tail 5 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`head`](/reference/operators/head), [`slice`](/reference/operators/slice)

# taste

Limits the input to `n` events per unique schema. ```tql taste [n:int] ``` ## Description [Section titled â€œDescriptionâ€](#description) Forwards the first `n` events per unique schema and discards the rest. The `taste` operator provides an exemplary overview of the â€œshapeâ€ of the data described by the pipeline. This helps to understand the diversity of the result, especially when interactively exploring data. ### `n: int (optional)` [Section titled â€œn: int (optional)â€](#n-int-optional) The number of events to keep per schema. Defaults to `10`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Retrieve at most 10 events of each unique schema [Section titled â€œRetrieve at most 10 events of each unique schemaâ€](#retrieve-at-most-10-events-of-each-unique-schema) ```tql export taste ``` ### Get only one sample for every unique event type [Section titled â€œGet only one sample for every unique event typeâ€](#get-only-one-sample-for-every-unique-event-type) ```tql export taste 1 ```

# throttle

Limits the bandwidth of a pipeline. ```tql throttle bandwidth:int, [within=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `throttle` operator limits the amount of data flowing through it to a bandwidth. ### `bandwidth: int` [Section titled â€œbandwidth: intâ€](#bandwidth-int) The maximum bandwidth that is enforced for this pipeline, in bytes per the specified interval. ### `within = duration (optional)` [Section titled â€œwithin = duration (optional)â€](#within--duration-optional) The duration over which to measure the maximum bandwidth. Defaults to `1s`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Read a byte stream at 1 byte per second [Section titled â€œRead a byte stream at 1 byte per secondâ€](#read-a-byte-stream-at-1-byte-per-second) Read a TCP stream at a rate of 1 character per second: ```tql load_tcp "tcp://0.0.0.0:4000" throttle 1 ``` ### Set a throughput limit for a given time window [Section titled â€œSet a throughput limit for a given time windowâ€](#set-a-throughput-limit-for-a-given-time-window) Load a sample input data file at a speed of at most 1MiB every 10s and import it into the node: ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" throttle 1Mi, within=10s decompress "zstd" read_zeek_tsv import ```

# timeshift

Adjusts timestamps relative to a given start time, with an optional speedup. ```tql timeshift field:time, [start=time, speed=double] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `timeshift` operator adjusts a series of time values by anchoring them around a given `start` time. With `speed`, you can adjust the relative speed of the time series induced by `field` with a multiplicative factor. This has the effect of making the time series â€œfasterâ€ for values great than 1 and â€œslowerâ€ for values less than 1. ![Timeshift](/_astro/timeshift.excalidraw.BVaAeL1C_19DKCs.svg) ### `field: time` [Section titled â€œfield: timeâ€](#field-time) The field containing the timestamp values. ### `start = time (optional)` [Section titled â€œstart = time (optional)â€](#start--time-optional) The timestamp to anchor the time values around. Defaults to the first non-null timestamp in `field`. ### `speed = double (optional)` [Section titled â€œspeed = double (optional)â€](#speed--double-optional) A constant factor to be divided by the inter-arrival time. For example, 2.0 decreases the event gaps by a factor of two, resulting a twice as fast dataflow. A value of 0.1 creates dataflow that spans ten times the original time frame. Defaults to `1.0`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Reset events to begin at Jan 1, 1984 [Section titled â€œReset events to begin at Jan 1, 1984â€](#reset-events-to-begin-at-jan-1-1984) ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" decompress "zstd" read_zeek_tsv timeshift ts, start=1984-01-01 ``` ### Scale inter-arrival times by 100x [Section titled â€œScale inter-arrival times by 100xâ€](#scale-inter-arrival-times-by-100x) As above, but also make the time span of the trace 100 times longer: ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" decompress "zstd" read_zeek_tsv timeshift ts, start=1984-01-01, speed=0.01 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`delay`](/reference/operators/delay)

# to

Saves to an URI, inferring the destination, compression and format. ```tql to uri:string, [saver_argsâ€¦ { â€¦ }] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to` operator is an easy way to get data out of Tenzir into It will try to infer the connector, compression and format based on the given URI. ### `uri: string` [Section titled â€œuri: stringâ€](#uri-string) The URI to load from. ### `saver_argsâ€¦ (optional)` [Section titled â€œsaver\_argsâ€¦ (optional)â€](#saver_args-optional) An optional set of arguments passed to the saver. This can be used to e.g. pass credentials to a connector: ```tql to "https://example.org/file.json", headers={Token: "XYZ"} ``` ### `{ â€¦ } (optional)` [Section titled â€œ{ â€¦ } (optional)â€](#---optional) The optional pipeline argument allows for explicitly specifying how `to` compresses and writes data. By default, the pipeline is inferred based on a set of [rules](#explanation). If inference is not possible, or not sufficient, this argument can be used to control compression and writing. Providing this pipeline disables the inference. ## Explanation [Section titled â€œExplanationâ€](#explanation) Saving Tenzir data into some resource consists of three steps: * [**Writing**](#writing) events as bytes according to some format * [**compressing**](#compressing) (optional) * [**Saving**](#saving) saving the bytes to some location The `to` operator tries to infer all three steps from the given URI. ### Writing [Section titled â€œWritingâ€](#writing) The format to write inferred from the file-ending. Supported file formats are the common file endings for our [`read_*` operators](/reference/operators#parsing). If you want to provide additional arguments to the writer, you can use the [pipeline argument](#---optional) to specify the parsing manually. ### Compressing [Section titled â€œCompressingâ€](#compressing) The compression, just as the format, is inferred from the â€œfile-endingâ€ in the URI. Under the hood, this uses the [`decompress_*` operators](/reference/operators#encode--decode). Supported compressions can be found in the [list of compression extensions](#compression). The compression step is optional and will only happen if a compression could be inferred. If you want to write with specific compression settings, you can use the [pipeline argument](#---optional) to specify the decompression manually. ### Saving [Section titled â€œSavingâ€](#saving) The connector is inferred based on the URI `scheme://`. If no scheme is present, the connector attempts to save to the local filesystem. ## Supported Deductions [Section titled â€œSupported Deductionsâ€](#supported-deductions) ### URI schemes [Section titled â€œURI schemesâ€](#uri-schemes) | Scheme | Operator | Example | | :--------------------------------- | :-------------------------------------------------------------------------- | :--------------------------------------------- | | `abfs`,`abfss` | [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage) | `to "abfs://path/to/file.json"` | | `amqp` | [`save_amqp`](/reference/operators/save_amqp) | `to "amqp://â€¦` | | `elasticsearch` | [`to_opensearch`](/reference/operators/to_opensearch) | `to "elasticsearch://â€¦` | | `file` | [`save_file`](/reference/operators/save_file) | `to "file://path/to/file.json"` | | `fluent-bit` | [`to_fluent_bit`](/reference/operators/to_fluent_bit) | `to "fluent-bit://elasticsearch"` | | `ftp`, `ftps` | [`save_ftp`](/reference/operators/save_ftp) | `to "ftp://example.com/file.json"` | | `gcps` | [`save_google_cloud_pubsub`](/reference/operators/save_google_cloud_pubsub) | `to "gcps://project_id/topic_id" { â€¦ }` | | `gs` | [`save_gcs`](/reference/operators/save_gcs) | `to "gs://bucket/object.json"` | | `http`, `https` | [`save_http`](/reference/operators/save_http) | `to "http://example.com/file.json"` | | `inproc` | [`save_zmq`](/reference/operators/save_zmq) | `to "inproc://127.0.0.1:56789" { write_json }` | | `kafka` | [`save_kafka`](/reference/operators/save_kafka) | `to "kafka://topic" { write_json }` | | `opensearch` | [`to_opensearch`](/reference/operators/to_opensearch) | `to "opensearch://â€¦` | | `s3` | [`save_s3`](/reference/operators/save_s3) | `to "s3://bucket/file.json"` | | `sqs` | [`save_sqs`](/reference/operators/save_sqs) | `to "sqs://my-queue" { write_json }` | | `tcp` | [`save_tcp`](/reference/operators/save_tcp) | `to "tcp://127.0.0.1:56789" { write_json }` | | `udp` | [`save_udp`](/reference/operators/save_udp) | `to "udp://127.0.0.1:56789" { write_json }` | | `zmq` | [`save_zmq`](/reference/operators/save_zmq) | `to "zmq://127.0.0.1:56789" { write_json }` | | `smtp`, `smtps`, `mailto`, `email` | [`save_email`](/reference/operators/save_email) | `to "smtp://john@example.com"` | Please see the respective operator pages for details on the URIâ€™s locator format. ### File extensions [Section titled â€œFile extensionsâ€](#file-extensions) #### Format [Section titled â€œFormatâ€](#format) The `to` operator can deduce the file format based on these file-endings: | Format | File Endings | Operator | | :------ | :------------------- | :---------------------------------------------------- | | CSV | `.csv` | [`write_csv`](/reference/operators/write_csv) | | Feather | `.feather`, `.arrow` | [`write_feather`](/reference/operators/write_feather) | | JSON | `.json` | [`write_json`](/reference/operators/write_json) | | NDJSON | `.ndjson`, `.jsonl` | [`write_ndjson`](/reference/operators/write_ndjson) | | Parquet | `.parquet` | [`write_parquet`](/reference/operators/write_parquet) | | Pcap | `.pcap` | [`write_pcap`](/reference/operators/write_pcap) | | SSV | `.ssv` | [`write_ssv`](/reference/operators/write_ssv) | | TSV | `.tsv` | [`write_tsv`](/reference/operators/write_tsv) | | YAML | `.yaml` | [`write_yaml`](/reference/operators/write_yaml) | #### Compression [Section titled â€œCompressionâ€](#compression) The `to` operator can deduce the following compressions based on these file-endings: | Compression | File Endings | | :---------- | :--------------- | | Brotli | `.br`, `.brotli` | | Bzip2 | `.bz2` | | Gzip | `.gz`, `.gzip` | | LZ4 | `.lz4` | | Zstd | `.zst`, `.zstd` | #### Example transformation: [Section titled â€œExample transformation:â€](#example-transformation) to operator ```tql to "myfile.json.gz" ``` Effective pipeline ```tql write_json compress_gzip save_file "myfile.json.gz" ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Save to a local file [Section titled â€œSave to a local fileâ€](#save-to-a-local-file) ```tql to "path/to/my/output.csv" ``` ### Save to a compressed file [Section titled â€œSave to a compressed fileâ€](#save-to-a-compressed-file) ```tql to "path/to/my/output.csv.bz2" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [from](/reference/operators/from)

# to_amazon_security_lake

Sends OCSF events to Amazon Security Lake. ```tql to_amazon_security_lake s3_uri:string, region=string, account_id=string, [timeout=duration, role=string, external_id=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_amazon_security_lake` operator sends OCSF events to [Amazon Security Lake](https://aws.amazon.com/security-lake/), AWSâ€™s centralized security data repository that normalizes and stores security data from multiple sources. The operator automatically handles Amazon Security Lakeâ€™s partitioning requirements and file size constraints, but does not validate the OCSF schema of the events. Consider [`ocsf::apply`](/reference/operators/ocsf/apply) in your pipeline to ensure schema compliance. For a list of OCSF event classes supported by Amazon Security Lake, see the [AWS documentation](https://docs.aws.amazon.com/security-lake/latest/userguide/adding-custom-sources.html#ocsf-eventclass). The operator generates random UUID (v7) file names with a `.parquet` extension. ### `s3_uri: string` [Section titled â€œs3\_uri: stringâ€](#s3_uri-string) The base URI for the S3 storage backing the lake in the form ```plaintext s3://<bucket>/ext/<custom-source-name> ``` Replace the placeholders as follows: * `<bucket>`: the bucket associated with your lake * `<custom-source-name>`: the name of your custom Amazon Security Lake source You can copy this URI directly from the AWS Security Lake custom source interface. ### `region = string` [Section titled â€œregion = stringâ€](#region--string) The region for partitioning. ### `account_id = string` [Section titled â€œaccount\_id = stringâ€](#account_id--string) The AWS account ID or external ID you chose when creating the Amazon Security Lake custom source. ### `timeout = duration (optional)` [Section titled â€œtimeout = duration (optional)â€](#timeout--duration-optional) A duration after which the operator will write to Amazon Security Lake, regardless of file size. Amazon Security Lake requires this to be between `5min` and `1d`. Defaults to `5min`. ### `role = string (optional)` [Section titled â€œrole = string (optional)â€](#role--string-optional) A role to assume when writing to S3. When not specified, the operator automatically uses the standard Amazon Security Lake provider role based on your configuration: `arn:aws:iam::<account_id>:role/AmazonSecurityLake-Provider-<custom-source-name>-<region>` The operator extracts the custom source name from the provided S3 URI. For example, given: * `account_id`: `"123456789012"` * `s3_uri`: `"s3://aws-security-data-lake-â€¦/ext/tnz-ocsf-4001/"` * `region`: `"eu-west-1"` The operator will use: `arn:aws:iam::123456789012:role/AmazonSecurityLake-Provider-tnz-ocsf-4001-eu-west-1` When defaulted, the operator requires an `external_id` to use the role. You can explicitly disable role authorization by setting `role=null`. ### `external_id = string (optional)` [Section titled â€œexternal\_id = string (optional)â€](#external_id--string-optional) The external ID to use when assuming the `role`. This is required when using the default role for the custom source. Defaults to no ID. ## Examples [Section titled â€œExamplesâ€](#examples) ### Send OCSF Network Activity events to Amazon Security Lake [Section titled â€œSend OCSF Network Activity events to Amazon Security Lakeâ€](#send-ocsf-network-activity-events-to-amazon-security-lake) This example shows how to send OCSF Network Activity events to an AWS Security Lake running on `eu-west-2` with a custom source called `tenzir_network_activity` and account ID `123456789012`: ```tql let $s3_uri = "s3://aws-security-data-lake-eu-west-2-lake-abcdefghijklmnopqrstuvwxyz1234/ext/tnz-ocsf-4001/" subscribe "ocsf" where @name == "ocsf.network_activity" ocsf::apply to_amazon_security_lake $s3_uri, region="eu-west-2", account_id="123456789012" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`ocsf::apply`](/reference/operators/ocsf/apply), [`save_s3`](/reference/operators/save_s3)

# to_azure_log_analytics

Sends events to the Microsoft Azure Logs Ingestion API. ```tql to_azure_log_analytics tenant_id=string, client_id=string, client_secret=string, dce=string, dcr=string, stream=string, [batch_timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) Sends events to the Microsoft [Azure Logs Ingestion API](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview). The `to_azure_log_analytics` operator makes it possible to upload events to [supported tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview#supported-tables) or to [custom tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/create-custom-table?tabs=azure-portal-1%2Cazure-portal-2%2Cazure-portal-3#create-a-custom-table) in Microsoft Azure. The operator handles access token retrievals by itself and updates that token automatically, if needed. ### `tenant_id = string` [Section titled â€œtenant\_id = stringâ€](#tenant_id--string) The Microsoft Directory (tenant) ID, written as `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`. ### `client_id = string` [Section titled â€œclient\_id = stringâ€](#client_id--string) The Microsoft Application (client) ID, written as `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`. ### `client_secret = string` [Section titled â€œclient\_secret = stringâ€](#client_secret--string) The client secret. ### `dce = string` [Section titled â€œdce = stringâ€](#dce--string) The data collection endpoint URL. ### `dcr = string` [Section titled â€œdcr = stringâ€](#dcr--string) The data collection rule ID, written as `dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`. ### `stream = string` [Section titled â€œstream = stringâ€](#stream--string) The stream to upload events to. ### `batch_timeout = duration` [Section titled â€œbatch\_timeout = durationâ€](#batch_timeout--duration) Maximum duration to wait for new events before sending a batch. Defaults to `5s`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Upload `custom.mydata` events to the stream `Custom-MyData` [Section titled â€œUpload custom.mydata events to the stream Custom-MyDataâ€](#upload-custommydata-events-to-the-stream-custom-mydata) ```tql export where @name == "custom.mydata" to_azure_log_analytics tenant_id="00a00a00-0a00-0a00-00aa-000aa0a0a000", client_id="000a00a0-0aa0-00a0-0000-00a000a000a0", client_secret="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", dce="https://my-stuff-a0a0.westeurope-1.ingest.monitor.azure.com", dcr="dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", stream="Custom-MyData" ```

# to_clickhouse

Sends events to a ClickHouse table. ```tql to_clickhouse table=string, [host=string, port=int, user=string, password=string, mode=string, primary=field, tls=bool, cacert=string, certfile=string, keyfile=string, skip_peer_verification=bool, skip_host_verification=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) ### `table = string` [Section titled â€œtable = stringâ€](#table--string) The name of the table you want to write to. When giving a plain table name, it will use the `default` database, otherwise `database.table` can be specified. ### `host = string (optional)` [Section titled â€œhost = string (optional)â€](#host--string-optional) The hostname for the ClickHouse server. Defaults to `"localhost"`. ### `port = int (optional)` [Section titled â€œport = int (optional)â€](#port--int-optional) The port for the ClickHouse server. Defaults to `9000` without TLS and `9440` with TLS. ### `user = string (optional)` [Section titled â€œuser = string (optional)â€](#user--string-optional) The user to use for authentication. Defaults to `"default"`. ### `password = string (optional)` [Section titled â€œpassword = string (optional)â€](#password--string-optional) The password for the given user. Defaults to `""`. ### `mode = string (optional)` [Section titled â€œmode = string (optional)â€](#mode--string-optional) * `"create"` if you want to create a table and fail if it already exists * `"append"` to append to an existing table * `"create_append"` to create a table if it does not exist and append to it otherwise. Defaults to `"create_append"`. ### `primary = field (optional)` [Section titled â€œprimary = field (optional)â€](#primary--field-optional) The primary key to use when creating a table. Required for `mode = "create"` as well as for `mode = "create_append"` if the table does not yet exist. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. Path to the key for the client certificate. ## Types [Section titled â€œTypesâ€](#types) Tenzir uses ClickHouseâ€™s [clickhouse-cpp](https://github.com/ClickHouse/clickhouse-cpp) client library to communicate with ClickHouse. The below table explains the translation from Tenzirâ€™s types to ClickHouse: | Tenzir | ClickHouse | Comment | | :--------- | :----------------------------- | :------------------------------------------------------------------------------------------------ | | `bool` | `UInt8` | | | `int64` | `Int64` | | | `uint64` | `UInt64` | | | `double` | `Float64` | | | `ip` | `IPv6` | | | `subnet` | `Tuple(ip IPv6, length UInt8)` | | | `time` | `DateTime64(9)` | | | `duration` | `Int64` | Converted as `nanoseconds(duration)` | | `record` | `Tuple(...)` | Fields in the tuple will be named with the field name. The record must have at least one element. | | `list<T>` | `Array(T)` | | | `blob` | `Array(UInt8)` | Blobs that are `null` will be represented by an empty array | Tenzir also supports `Nullable` versions of the above types (or their nested types). If a `list` itself is `null`, it will be represented by an empty `Array`. If a `record` is `null`, all elements of the `Tuple` will be null, if possible. Otherwise the event will be dropped. ### Table Creation [Section titled â€œTable Creationâ€](#table-creation) When a ClickHouse table is created from Tenzir, all columns except the `primary` will be created as `Nullable`. For example, a column of type `ip` will be created as `Nullable(IPv6)`, while a `list<int64>` will be created as `Array(Nullable(Int64))`. The table will be created from the first event the operator receives. Should this first event contain unsupported types/values, an error is raised. #### Untyped nulls [Section titled â€œUntyped nullsâ€](#untyped-nulls) Tenzir has both typed and untyped nulls. Typed nulls have a type, but no value. They are no problem for `to_clickhouse`. For untyped nulls, the type itself is `null`, which cannot be supported by the `to_clickhouse` operator when creating a table. Typed and Untyped Nulls in Tenzir ```tql from { typed_null: int(null), untyped_null: null, } ``` Untyped nulls are usually directly caused by nulls in the input, such as in a JSON file: ```json { "value": null } ``` If your input format has untyped nulls, but you know the type, you can either define an a schema and use that when parsing the input, or you can explicitly cast the columns to their desired type: ```tql from ( { value: null }, { value: 42 }, ) value = int(value) // explicit cast turns untyped into typed nulls to_clickhouse "example_table", primary=value ``` #### Empty records [Section titled â€œEmpty recordsâ€](#empty-records) Empty records cannot be send to ClickHouse. Should an empty record appear in the first event, an error is raised. ## Examples [Section titled â€œExamplesâ€](#examples) ### Send CSV file to a local ClickHouse instance, without TLS [Section titled â€œSend CSV file to a local ClickHouse instance, without TLSâ€](#send-csv-file-to-a-local-clickhouse-instance-without-tls) ```tql from "my_file.csv" to_clickhouse table="my_table", tls=false ``` ### Create a new table with multiple fields [Section titled â€œCreate a new table with multiple fieldsâ€](#create-a-new-table-with-multiple-fields) ```tql from { i: 42, d: 10.0, b: true, l: [42], r:{ s:"string" } } to_clickhouse table="example", primary=i ``` This creates the following table: ```plaintext â”Œâ”€nameâ”€â”¬â”€typeâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 1. â”‚ i â”‚ Int64 â”‚ 2. â”‚ d â”‚ Nullable(Float64) â”‚ 3. â”‚ b â”‚ Nullable(UInt8) â”‚ 4. â”‚ l â”‚ Array(Nullable(Int64)) â”‚ 5. â”‚ r â”‚ Tuple( â†´â”‚ â”‚ â”‚â†³ s Nullable(String)) â”‚ â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ```

# to_fluent_bit

Sends events via Fluent Bit. ```tql to_fluent_bit plugin:string, [options=record, fluent_bit_options=record, tls=bool, cacert=string, certfile=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_fluent_bit` operator acts as a bridge into the [Fluent Bit](https://docs.fluentbit.io) ecosystem, making it possible to send events to Fluent Bit [output plugin](https://docs.fluentbit.io/manual/pipeline/outputs). An invocation of the `fluent-bit` commandline utility ```bash fluent-bit -o plugin -p key1=value1 -p key2=value2 -pâ€¦ ``` translates to our `to_fluent_bit` operator as follows: ```tql to_fluent_bit "plugin", options={key1: value1, key2:value2, â€¦} ``` ### `plugin: string` [Section titled â€œplugin: stringâ€](#plugin-string) The name of the Fluent Bit plugin. Run `fluent-bit -h` and look under the **Outputs** section of the help text for available plugin names. The web documentation often comes with an example invocation near the bottom of the page, which also provides a good idea how you could use the operator. ### `options = record (optional)` [Section titled â€œoptions = record (optional)â€](#options--record-optional) Sets plugin configuration properties. The key-value pairs in this record are equivalent to `-p key=value` for the `fluent-bit` executable. ### `fluent_bit_options = record (optional)` [Section titled â€œfluent\_bit\_options = record (optional)â€](#fluent_bit_options--record-optional) Sets global properties of the Fluent Bit service. E.g., `fluent_bit_options={flush:1, grace:3}`. Consult the list of available [key-value pairs](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file#config_section) to configure Fluent Bit according to your needs. We recommend factoring these options into the plugin-specific `fluent-bit.yaml` so that they are independent of the `fluent-bit` operator arguments. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## URI support & integration with `from` [Section titled â€œURI support & integration with fromâ€](#uri-support--integration-with-from) The `to_fluent_bit` operator can also be used from the [`to`](/reference/operators/to) operator. For this, the `fluentbit://` scheme can be used. The URI is then translated: ```tql to "fluentbit://plugin" ``` ```tql to_fluent_bit "plugin" ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Slack [Section titled â€œSlackâ€](#slack) Send events to [Slack](https://docs.fluentbit.io/manual/pipeline/outputs/slack): ```tql let $slack_hook = "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX" to_fluent_bit "slack", options={webhook: $slack_hook} ```

# to_google_cloud_logging

Sends events to Google Cloud Logging. ```tql to_google_cloud_logging log_id=string, [project=string, organization=string, billing_account=string, folder=string,] [resource_type=string, resource_labels=record, payload=string, severity=string, timestamp=time, service_credentials=string, batch_timeout=duration, max_batch_size=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) Sends events to [Google Cloud Logging](https://cloud.google.com/logging). ### `log_id = string` [Section titled â€œlog\_id = stringâ€](#log_id--string) ID to associated the ingested logs with. It must be less than 512 characters long and can only include the following characters: upper and lower case alphanumeric characters, forward-slash, underscore, hyphen, and period. ### `project = string (optional)` [Section titled â€œproject = string (optional)â€](#project--string-optional) A project id to associated the ingested logs with. ### `organization = string (optional)` [Section titled â€œorganization = string (optional)â€](#organization--string-optional) An organization id to associated the ingested logs with. ### `billing_account = string (optional)` [Section titled â€œbilling\_account = string (optional)â€](#billing_account--string-optional) A billing account id to associated the ingested logs with. ### `folder = string (optional)` [Section titled â€œfolder = string (optional)â€](#folder--string-optional) A folder id to associated the ingested logs with. ### `resource_type = string (optional)` [Section titled â€œresource\_type = string (optional)â€](#resource_type--string-optional) The type of the [monitored resource](https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource). All available types with their associated labels are listed [here](https://cloud.google.com/logging/docs/api/v2/resource-list). Defaults to `global`. ### `resource_labels = record (optional)` [Section titled â€œresource\_labels = record (optional)â€](#resource_labels--record-optional) Record of associated labels for the resource. Values of the record must be of type `string`. Consult the [official docs](https://cloud.google.com/logging/docs/api/v2/resource-list) for available types with their associated labels. ### `payload = string (optional)` [Section titled â€œpayload = string (optional)â€](#payload--string-optional) The log entry payload. If unspecified, the incoming event is serialised as JSON and sent. ### `service_credentials = string (optional)` [Section titled â€œservice\_credentials = string (optional)â€](#service_credentials--string-optional) JSON credentials to use if using a service account. ### `severity = string (optional)` [Section titled â€œseverity = string (optional)â€](#severity--string-optional) Severity of the event. Consult the [official docs](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogSeverity) for available severity levels. Defaults to `default`. ### `timestamp = time (optional)` [Section titled â€œtimestamp = time (optional)â€](#timestamp--time-optional) Timestamp of the event. ### `batch_timeout = duration (optional)` [Section titled â€œbatch\_timeout = duration (optional)â€](#batch_timeout--duration-optional) Maximum interval between sending the events. Defaults to `5s`. ### `max_batch_size = int (optional)` [Section titled â€œmax\_batch\_size = int (optional)â€](#max_batch_size--int-optional) Maximum events to batch before sending. Defaults to `1k`. ## Example [Section titled â€œExampleâ€](#example) ## Send logs, authenticating automatically via ADC [Section titled â€œSend logs, authenticating automatically via ADCâ€](#send-logs-authenticating-automatically-via-adc) ```tql from { content: "log message", timestamp: now(), } to_google_cloud_logging log_id="LOG_ID", project="PROJECT_ID" ``` ## Send logs using a service account [Section titled â€œSend logs using a service accountâ€](#send-logs-using-a-service-account) ```tql from { content: "totally not a made up log", timestamp: now(), resource: "global", } to_google_cloud_logging log_id="LOG_ID", project="PROJECT_ID" resource_type=resource, service_credentials=file_contents("/path/to/credentials.json") ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`to_google_secops`](/reference/operators/to_google_secops)

# to_google_secops

Sends unstructured events to a Google SecOps Chronicle instance. ```tql to_google_secops customer_id=string, private_key=string, client_email=string, log_type=string, log_text=string, [region=string, timestamp=time, labels=record, namespace=string, max_request_size=int, batch_timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_google_secops` operator makes it possible to ingest events via the [Google SecOps Chronicle unstructured logs ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api#unstructuredlogentries). ### `customer_id = string` [Section titled â€œcustomer\_id = stringâ€](#customer_id--string) The customer UUID to use. ### `private_key = string` [Section titled â€œprivate\_key = stringâ€](#private_key--string) The private key to use for authentication. This corresponds to the `private_key` in the SecOps collector config. ### `client_email = string` [Section titled â€œclient\_email = stringâ€](#client_email--string) The user email to use for authentication. This corresponds to the `client_email` in the SecOps collector config. ### `log_type = string` [Section titled â€œlog\_type = stringâ€](#log_type--string) The log type of the events. ### `log_text = string` [Section titled â€œlog\_text = stringâ€](#log_text--string) The log text to send. ### `region = string (optional)` [Section titled â€œregion = string (optional)â€](#region--string-optional) [Regional prefix](https://cloud.google.com/chronicle/docs/reference/ingestion-api#regional_endpoints) for the Ingestion endpoint (`malachiteingestion-pa.googleapis.com`). ### `timestamp = time (optional)` [Section titled â€œtimestamp = time (optional)â€](#timestamp--time-optional) Optional timestamp field to attach to logs. ### `labels = record (optional)` [Section titled â€œlabels = record (optional)â€](#labels--record-optional) A record of labels to attach to the logs. For example, `{node: "Configured Tenzir Node"}`. ### `namespace = string (optional)` [Section titled â€œnamespace = string (optional)â€](#namespace--string-optional) The namespace to use when ingesting. Defaults to `tenzir`. ### `max_request_size = int (optional)` [Section titled â€œmax\_request\_size = int (optional)â€](#max_request_size--int-optional) The maximum number of bytes in the request payload. Defaults to `1M`. ### `batch_timeout = duration (optional)` [Section titled â€œbatch\_timeout = duration (optional)â€](#batch_timeout--duration-optional) The maximum duration to wait for new events before sending the request. Defaults to `5s`. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from {log: "31-Mar-2025 01:35:02.187 client 0.0.0.0#4238: query: tenzir.com IN A + (255.255.255.255)"} to_google_secops \ customer_id="00000000-0000-0000-00000000000000000", private_key=secret("my_secops_key"), client_email="somebody@example.com", log_text=log, log_type="BIND_DNS", region="europe" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`to_google_cloud_logging`](/reference/operators/to_google_cloud_logging)

# to_hive

Writes events to a URI using hive partitioning. ```tql to_hive uri:string, partition_by=list<field>, format=string, [timeout=duration, max_size=int] ``` ## Description [Section titled â€œDescriptionâ€](#description) Hive partitioning is a partitioning scheme where a set of fields is used to partition events. For each combination of these fields, a directory is derived under which all events with the same field values will be stored. For example, if the events are partitioned by the fields `year` and `month`, then the files in the directory `/year=2024/month=10` will contain all events where `year == 2024` and `month == 10`. Files within each partition directory are named using UUIDv7 for guaranteed uniqueness and natural time-based ordering. This prevents filename conflicts when multiple processes write to the same partition simultaneously. ### `uri: string` [Section titled â€œuri: stringâ€](#uri-string) The base URI for all partitions. ### `partition_by = list<field>` [Section titled â€œpartition\_by = list\<field>â€](#partition_by--listfield) A list of fields that will be used for partitioning. Note that these fields will be elided from the output, as their value is already specified by the path. ### `format = string` [Section titled â€œformat = stringâ€](#format--string) The name of the format that will be used for writing, for example `json` or `parquet`. This will also be used for the file extension. ### `timeout = duration (optional)` [Section titled â€œtimeout = duration (optional)â€](#timeout--duration-optional) The time after which a new file will be opened for the same partition group. Defaults to `5min`. ### `max_size = int (optional)` [Section titled â€œmax\_size = int (optional)â€](#max_size--int-optional) The total file size after which a new file will be opened for the same partition group. Note that files will typically be slightly larger than this limit, because it opens a new file when only after it is exceeded. Defaults to `100M`. ### `compression = string (optional)` [Section titled â€œcompression = string (optional)â€](#compression--string-optional) Compress the output files with the given compression algorithm. See docs for the `compress` operator for supported compression algorithms. ## Examples [Section titled â€œExamplesâ€](#examples) ### Partition by a single field into local JSON files [Section titled â€œPartition by a single field into local JSON filesâ€](#partition-by-a-single-field-into-local-json-files) ```tql from {a: 0, b: 0}, {a: 0, b: 1}, {a: 1, b: 2} to_hive "/tmp/out/", partition_by=[a], format="json" // This pipeline produces two files: // -> /tmp/out/a=0/<uuid>.json: // {"b": 0} // {"b": 1} // -> /tmp/out/a=1/<uuid>.json: // {"b": 2} ``` ### Write a Parquet file into Azure Blob Store [Section titled â€œWrite a Parquet file into Azure Blob Storeâ€](#write-a-parquet-file-into-azure-blob-store) Write as Parquet into the Azure Blob Filesystem, partitioned by year, month and day. ```tql to_hive "abfs://domain/bucket", partition_by=[year, month, day], format="parquet" // -> abfs://domain/bucket/year=<year>/month=<month>/day=<day>/<uuid>.parquet ``` ### Write partitioned JSON into an S3 bucket [Section titled â€œWrite partitioned JSON into an S3 bucketâ€](#write-partitioned-json-into-an-s3-bucket) Write JSON into S3, partitioned by year and month, opening a new file after 1 GB. ```tql year = ts.year() month = ts.month() to_hive "s3://my-bucket/some/subdirectory", partition_by=[year, month], format="json", max_size=1G // -> s3://my-bucket/some/subdirectory/year=<year>/month=<month>/<uuid>.json ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_parquet`](/reference/operators/read_parquet), [`write_bitz`](/reference/operators/write_bitz), [`write_feather`](/reference/operators/write_feather), [`write_parquet`](/reference/operators/write_parquet)

# to_kafka

Sends messages to an Apache Kafka topic. ```tql to_kafka topic:string, [message=blob|string, key=string, timestamp=time, options=record, aws_iam=record] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_kafka` operator sends one message per event to a Kafka topic. The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`. The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them: * `bootstrap.servers`: `localhost` * `client.id`: `tenzir` ### `topic: string` [Section titled â€œtopic: stringâ€](#topic-string) The Kafka topic to send messages to. ### `message = blob|string (optional)` [Section titled â€œmessage = blob|string (optional)â€](#message--blobstring-optional) An expression that evaluates to the message content for each row. Defaults to `this.print_json()` when not specified. ### `key = string (optional)` [Section titled â€œkey = string (optional)â€](#key--string-optional) Sets a fixed key for all messages. ### `timestamp = time (optional)` [Section titled â€œtimestamp = time (optional)â€](#timestamp--time-optional) Sets a fixed timestamp for all messages. ### `options = record (optional)` [Section titled â€œoptions = record (optional)â€](#options--record-optional) A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"acks": "all", "batch.size": 16384}`. The `to_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs. We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `to_kafka` arguments. ### `aws_iam = record (optional)` [Section titled â€œaws\_iam = record (optional)â€](#aws_iam--record-optional) If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified. Available keys: * `region`: Region of the MSK Clusters. Must be specified when using IAM. * `assume_role`: Optional Role ARN to assume. * `session_name`: Optional session name to use when assuming a role. * `external_id`: Optional external id to use when assuming a role. The operator will try to get credentials in the following order: 1. Checks your environment variables for AWS Credentials. 2. Checks your `$HOME/.aws/credentials` file for a profile and credentials 3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`. 4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that are not directly supported by AWS. 5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set. 6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON. ## Examples [Section titled â€œExamplesâ€](#examples) ### Send JSON-formatted events to topic `events` (using default) [Section titled â€œSend JSON-formatted events to topic events (using default)â€](#send-json-formatted-events-to-topic-events-using-default) Stream security events to a Kafka topic with automatic JSON formatting: ```tql subscribe "security-alerts" where severity >= "high" select timestamp, source_ip, alert_type, details to_kafka "events" ``` This pipeline subscribes to security alerts, filters for high-severity events, selects relevant fields, and sends them to Kafka as JSON. Each event is automatically formatted using `this.print_json()`, producing messages like: ```json { "timestamp": "2024-03-15T10:30:00.000000", "source_ip": "192.168.1.100", "alert_type": "brute_force", "details": "Multiple failed login attempts detected" } ``` ### Send JSON-formatted events with explicit message [Section titled â€œSend JSON-formatted events with explicit messageâ€](#send-json-formatted-events-with-explicit-message) ```tql subscribe "logs" to_kafka "events", message=this.print_json() ``` ### Send specific field values with a timestamp [Section titled â€œSend specific field values with a timestampâ€](#send-specific-field-values-with-a-timestamp) ```tql subscribe "logs" to_kafka "alerts", message=alert_msg, timestamp=2024-01-01T00:00:00 ``` ### Send data with a fixed key for partitioning [Section titled â€œSend data with a fixed key for partitioningâ€](#send-data-with-a-fixed-key-for-partitioning) ```tql metrics to_kafka "metrics", message=this.print_json(), key="server-01" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_kafka`](/reference/operators/load_kafka), [`save_kafka`](/reference/operators/save_kafka)

# to_opensearch

Sends events to an OpenSearch-compatible Bulk API. ```tql to_opensearch url:string, action=string, [index=string, id=string, doc=record, user=string, passwd=string, tls=bool, skip_peer_verification=bool, cacert=string, certfile=string, keyfile=string, include_nulls=bool, max_content_length=int, buffer_timeout=duration, compress=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_opensearch` operator sends events to a [OpenSearch-compatible Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/) such as [ElasticSearch](https://www.elastic.co/elasticsearch). The operator accumulates multiple events before sending them as a single request. You can control the maximum request size via the `max_content_length` and the timeout before sending all accumulated events via the `send_timeout` option. ### `url: string` [Section titled â€œurl: stringâ€](#url-string) The URL of the API endpoint. ### `action = string` [Section titled â€œaction = stringâ€](#action--string) An expression for the action that evaluates to a `string`. Supported actions: * `create`: Creates a document if it doesnâ€™t already exist and returns an error otherwise. * `delete`: Deletes a document if it exists. * `index`: Creates a document if it doesnâ€™t yet exist and replace the document if it already exists. * `update`: Updates existing documents and returns an error if the document doesnâ€™t exist. * `upsert`: If a document exists, it is updated; if it does not exist, a new document is indexed. ### `index = string (optional)` [Section titled â€œindex = string (optional)â€](#index--string-optional) An optional expression for the index that evaluates to a `string`. Must be provided if the `url` does not have an index. ### `id = string (optional)` [Section titled â€œid = string (optional)â€](#id--string-optional) The `id` of the document to act on. Must be provided when using the `delete` and `update` actions. ### `doc = record (optional)` [Section titled â€œdoc = record (optional)â€](#doc--record-optional) The document to serialize. Defaults to `this`. ### `user = string (optional)` [Section titled â€œuser = string (optional)â€](#user--string-optional) Optional user for HTTP Basic Authentication. ### `passwd = string (optional)` [Section titled â€œpasswd = string (optional)â€](#passwd--string-optional) Optional password for HTTP Basic Authentication. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ### `include_nulls = bool (optional)` [Section titled â€œinclude\_nulls = bool (optional)â€](#include_nulls--bool-optional) Include fields with null values in the transmitted event data. By default, the operator drops all null values to save space. Defaults to `false`. ### `max_content_length = int (optional)` [Section titled â€œmax\_content\_length = int (optional)â€](#max_content_length--int-optional) The maximum size of the message uncompressed body in bytes. A message may consist of multiple events. If a single event is larger than this limit, it is dropped and a warning is emitted. Defaults to `5Mi`. ### `buffer_timeout = duration (optional)` [Section titled â€œbuffer\_timeout = duration (optional)â€](#buffer_timeout--duration-optional) The maximum amount of time for which the operator accumulates messages before sending them out to the HEC endpoint as a single message. Defaults to `5s`. ### `compress = bool (optional)` [Section titled â€œcompress = bool (optional)â€](#compress--bool-optional) Whether to compress the message body using standard gzip. Defaults to `true`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Send events from a JSON file [Section titled â€œSend events from a JSON fileâ€](#send-events-from-a-json-file) ```tql from "example.json" to_opensearch "localhost:9200", action="create", index="main" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`from_opensearch`](/reference/operators/from_opensearch)

# to_sentinelone_data_lake

Sends security events to SentinelOne Singularity Data Lake via REST API. ```tql to_sentinelone_data_lake url:string, token=string, [session_info=record, timeout=duration] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_sentinelone_data_lake` operator sends incoming events to the [SentinelOne Data Lake REST API](https://support.sentinelone.com/hc/en-us/articles/360004195934-SentinelOne-API-Guide) as structured data, using the `addEvents` endpoint. The operator accumulates multiple events before sending them as a single request, respecting the APIâ€™s limits. If events are OCSF events, the `time` and `severity_id` fields are automatically extracted and added to the events meta information. The OCSF `severity_id` is mapped to the SentinelOne Data Lake `sev` property according to this table: | OCSF `severity_id` | SentinelOne severity | | :----------------: | :------------------: | | 0 (Unknown) | 3 (info) | | 1 (Informational) | 1 (finer) | | 2 (Low) | 2 (fine) | | 3 (Medium) | 3 (info) | | 4 (High) | 4 (warn) | | 5 (Critical) | 5 (error) | | 6 (Fatal) | 6 (fatal) | | 99 (Other) | 3 (info) | ### `url: string` [Section titled â€œurl: stringâ€](#url-string) The ingest URL for the Data Lake. Please note that using the wrong ingestion endpoint, such as an incorrect region, may silently fail, as the SentinelOne API responds with 200 OK, even for some erroneous requests. ### `token = string` [Section titled â€œtoken = stringâ€](#token--string) The token to use for authorization. ### `session_info = record (optional)` [Section titled â€œsession\_info = record (optional)â€](#session_info--record-optional) Some additional sessionInfo to send with each batch of events, as the `sessionInfo` field in the request body. If this option is used, it is recommended that it contains a field `serverHost` to identify the Node. This can also contain a field `parser`, which names a SentinelOne parser that will be applied to the data field `message`. This can be used to ingest unstructured data. ### `timeout = duration (optional)` [Section titled â€œtimeout = duration (optional)â€](#timeout--duration-optional) The delay after which events are sent, even if this results in fewer events sent per message. Defaults to `1min`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Send events to SentinelOne Data Lake [Section titled â€œSend events to SentinelOne Data Lakeâ€](#send-events-to-sentinelone-data-lake) ```tql to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("sentinelone-token") ``` ### Send additional session information [Section titled â€œSend additional session informationâ€](#send-additional-session-information) ```tql to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("sentinelone-token"), session_info={ serverHost: "Node 42", serverType: "Tenzir Node", region: "Planet Earth", } ``` ### Send â€˜unstructuredâ€™ data [Section titled â€œSend â€˜unstructuredâ€™ dataâ€](#send-unstructured-data) The operator can also be used to send unstructured data to be parsed by SentinelOne. For this, the operators input must contain a field `message` and a `parser` must be specified in the `session_info`: ```tql select message = this.print_ndjson(); // Format the entire event as JSON to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("sentinelone-token"), session_info={ serverHost: "Node 42", parser: "json", // Have SentinelOne parse the data } ``` Ingest Costs SentinelOne charges per ingested *value* byte in the events. This means that you get charged for all bytes in `message`, including the keys, structural elements and whitespace. If you already have structured data in Tenzir, prefer sending structured data, as you will only be charged for the values and one byte per key, as opposed to the full keys and structural characters in `message`.

# to_snowflake

Sends events to a Snowflake database. ```tql to_snowflake account_identifier=string, user_name=string, password=string, snowflake_database=string snowflake_schema=string table=string, [ingest_mode=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_snowflake` operator makes it possible to send events to a [Snowflake](https://www.snowflake.com/) database. It uploads the events via bulk-ingestion under the hood and then copies them into the target table. The operator supports nested types as [Snowflake semi-structured types](https://docs.snowflake.com/en/sql-reference/data-types-semistructured). Alternatively, you can use the [`flatten`](/reference/functions/flatten) function operator beforehand. ### `account_identifier = string` [Section titled â€œaccount\_identifier = stringâ€](#account_identifier--string) The [Snowflake account identifier](https://docs.snowflake.com/en/user-guide/admin-account-identifier) to use. ### `user_name = string` [Section titled â€œuser\_name = stringâ€](#user_name--string) The Snowflake user name. The user must have the [`CREATE STAGE`](https://docs.snowflake.com/en/sql-reference/sql/create-stage#access-control-requirements) privilege on the given schema. ### `password = string` [Section titled â€œpassword = stringâ€](#password--string) The password for the user. ### `database = string` [Section titled â€œdatabase = stringâ€](#database--string) The [Snowflake database](https://docs.snowflake.com/en/sql-reference/ddl-database) to write to. The user must be allowed to access it. ### `schema = string` [Section titled â€œschema = stringâ€](#schema--string) The [Snowflake schema](https://docs.snowflake.com/en/sql-reference/ddl-database) to use. The user be allowed to access it. ### `table = string` [Section titled â€œtable = stringâ€](#table--string) The name of the table that should be used/created. The user must have the required permissions to create/write to it. Table columns that are not in the event will be null, while event fields that are not in the table will be dropped. Type mismatches between the table and events are a hard error. ### `ingest_mode = string (optional)` [Section titled â€œingest\_mode = string (optional)â€](#ingest_mode--string-optional) You can set the ingest mode to one of three options: * `"create_append"`: Creates the table if it does not exist, otherwise appends to it. * `"create"`: creates the table, causing an error if it already exists. * `"append"`: appends to the table, causing an error if it does not exist. In case the operator creates the table it will use the the first event to infer the columns. Default to `"create_append"`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Send an event to a Snowflake table [Section titled â€œSend an event to a Snowflake tableâ€](#send-an-event-to-a-snowflake-table) Upload `suricata.alert` events to a table `TENZIR` in `MY_DB@SURICATA_ALERT`: ```tql export where @name == "suricata.alert" to_snowflake \ account_identifier="asldyuf-xgb47555", user_name="tenzir_user", password="password1234", database="MY_DB", schema="SURICATA_ALERT", table="TENZIR" ```

# to_splunk

Sends events to a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector). ```tql to_splunk url:string, hec_token=string, [event=any, host=string, source=string, sourcetype=expr, index=expr, tls=bool, cacert=string, certfile=string, keyfile=string, skip_peer_verification=bool, print_nulls=bool, max_content_length=int, buffer_timeout=duration, compress=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `to_splunk` operator sends events to a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector). The source type defaults to `_json` and the operator renders incoming events as JSON. You can specify a different source type via the `sourcetype` option. The operator accumulates multiple events before sending them as a single message to the HEC endpoint. You can control the maximum message size via the `max_content_length` and the timeout before sending all accumulated events via the `send_timeout` option. ### `url: string` [Section titled â€œurl: stringâ€](#url-string) The address of the Splunk indexer. ### `hec_token = string` [Section titled â€œhec\_token = stringâ€](#hec_token--string) The [HEC token](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector#Create_an_Event_Collector_token_on_Splunk_Cloud_Platform) for authentication. ### `event = any (optional)` [Section titled â€œevent = any (optional)â€](#event--any-optional) The event to send. Defaults to `this`, meaning the entire event is sent. ### `host = string (optional)` [Section titled â€œhost = string (optional)â€](#host--string-optional) An optional value for the [Splunk `host`](https://docs.splunk.com/Splexicon:Host). ### `source = string (optional)` [Section titled â€œsource = string (optional)â€](#source--string-optional) An optional value for the [Splunk `source`](https://docs.splunk.com/Splexicon:Source). ### `sourcetype = expr (optional)` [Section titled â€œsourcetype = expr (optional)â€](#sourcetype--expr-optional) An optional expression for [Splunkâ€™s `sourcetype`](https://docs.splunk.com/Splexicon:Sourcetype) that evaluates to a `string`. You can use this to set the `sourcetype` per event, by providing a field instead of a string. Regardless of the chosen `sourcetype`, the event itself is passed as a json object in `event` key of level object that is sent. Defaults to `_json`. ### `index = expr (optional)` [Section titled â€œindex = expr (optional)â€](#index--expr-optional) An optional expression for the [Splunk `index`](https://docs.splunk.com/Splexicon:Index) that evaluates to a `string`. If you do not provide this option, Splunk will use the default index. **NB**: HEC silently drops events with an invalid `index`. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the serverâ€™s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ### `include_nulls = bool (optional)` [Section titled â€œinclude\_nulls = bool (optional)â€](#include_nulls--bool-optional) Include fields with null values in the transmitted event data. By default, the operator drops all null values to save space. ### `max_content_length = int (optional)` [Section titled â€œmax\_content\_length = int (optional)â€](#max_content_length--int-optional) The maximum size of the message uncompressed body in bytes. A message may consist of multiple events. If a single event is larger than this limit, it is dropped and a warning is emitted. This corresponds with Splunkâ€™s [`max_content_length`](https://docs.splunk.com/Documentation/Splunk/9.3.1/Admin/Limitsconf#.5Bhttp_input.5D) option. Be aware that [Splunk Cloud has a default of `1MB`](https://docs.splunk.com/Documentation/SplunkCloud/9.2.2406/Service/SplunkCloudservice#Using_HTTP_Event_Collector_.28HEC.29) for `max_content_length`. Defaults to `5Mi`. ### `buffer_timeout = duration (optional)` [Section titled â€œbuffer\_timeout = duration (optional)â€](#buffer_timeout--duration-optional) The maximum amount of time for which the operator accumulates messages before sending them out to the HEC endpoint as a single message. Defaults to `5s`. ### `compress = bool (optional)` [Section titled â€œcompress = bool (optional)â€](#compress--bool-optional) Whether to compress the message body using standard gzip. Defaults to `true`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Send a JSON file to a HEC endpoint [Section titled â€œSend a JSON file to a HEC endpointâ€](#send-a-json-file-to-a-hec-endpoint) ```tql load_file "example.json" read_json to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token") ``` ### Data Dependent Splunk framing [Section titled â€œData Dependent Splunk framingâ€](#data-dependent-splunk-framing) By default, the `to_splunk` operator sends the entire event as the `event` field to the HEC, together with any optional Splunk â€œframeâ€ fields such as `host`, `source`, `sourcetype` and `index`. These special properties can be set using the operators respective arguments, with an expression that is evaluated per event. However, this means that these special properties may be transmitted as both part of `event` and as part of the Splunk frame. This can be especially undesirable when the events are supposed to adhere to a specific schema, such as OCSF. In this case, you can specify the additional `event` option to specify which part of the incoming event should be sent as the event. ```tql from { host: "my-host", source: "my-source", a: 42, b: 0, message: "text", nested: { x: 0 }, } // move the entire event into `event` this = { event: this } // hoist the splunk specific fields back out, so they are no longer part of the // sent event move host = event.host, source = event.source to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token"), host=host, source=source, event=event ```

# top

Shows the most common values. ```tql top x:field ``` ## Description [Section titled â€œDescriptionâ€](#description) Shows the most common values for a given field. For each value, a new event containing its count will be produced. In general, `top x` is equivalent to: ```tql summarize x, count=count() sort -count ``` This operator is the dual to [`rare`](/reference/operators/rare). ### `x: field` [Section titled â€œx: fieldâ€](#x-field) The field to find the most common values for. ## Examples [Section titled â€œExamplesâ€](#examples) ### Find the most common values [Section titled â€œFind the most common valuesâ€](#find-the-most-common-values) ```tql from {x: "B"}, {x: "A"}, {x: "A"}, {x: "B"}, {x: "A"}, {x: "D"}, {x: "C"}, {x: "C"} top x ``` ```tql {x: "A", count: 3} {x: "B", count: 2} {x: "C", count: 2} {x: "D", count: 1} ``` ### Show the 5 top-most values [Section titled â€œShow the 5 top-most valuesâ€](#show-the-5-top-most-values) ```tql top id.orig_h head 5 ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`summarize`](/reference/operators/summarize), [`rare`](/reference/operators/rare), [`sort`](/reference/operators/sort)

# unordered

Removes ordering assumptions from a pipeline. ```tql unordered { â€¦ } ``` ## Description [Section titled â€œDescriptionâ€](#description) The `unordered` operator takes a pipeline as an argument and removes ordering assumptions from it. This causes some operators to run faster. Note that some operators implicitly remove ordering assumptions. For example, `sort` tells upstream operators that ordering does not matter. ## Examples [Section titled â€œExamplesâ€](#examples) ### Parse JSON unordered [Section titled â€œParse JSON unorderedâ€](#parse-json-unordered) ```tql unordered { read_json } ```

# unroll

Returns a new event for each member of a list or a record in an event, duplicating the surrounding event. ```tql unroll [field:list|record] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `unroll` returns an event for each member of a specified list or record field, leaving the surrounding event unchanged. Drops events where the specified field is an empty record, an empty list, or null. ### `field: list|record` [Section titled â€œfield: list|recordâ€](#field-listrecord) Sets the name of the list or record field. ## Examples [Section titled â€œExamplesâ€](#examples) ### Unroll a list [Section titled â€œUnroll a listâ€](#unroll-a-list) ```tql from {x: "a", y: [1, 2, 3]}, {x: "b", y: []}, {x: "c", y: [4]} unroll y ``` ```tql {x: "a", y: 1} {x: "a", y: 2} {x: "a", y: 3} {x: "c", y: 4} ``` ### Unroll a record [Section titled â€œUnroll a recordâ€](#unroll-a-record) ```tql from {x: "a", y: {foo: 1, baz: 2}}, {x: "b", y: {foo: null, baz: 3}}, {x: "c", y: null} unroll y ``` ```tql {x: "a", y: {foo: 1}} {x: "a", y: {baz: 2}} {x: "b", y: {foo: null}} {x: "b", y: {baz: 3}} ```

# version

Shows the current version. ```tql version ``` ## Description [Section titled â€œDescriptionâ€](#description) The `version` operator shows the current Tenzir version. ## Schemas [Section titled â€œSchemasâ€](#schemas) Tenzir emits version information with the following schema. ### `tenzir.version` [Section titled â€œtenzir.versionâ€](#tenzirversion) Contains detailed information about the process version. | Field | Type | Description | | :------------- | :------------- | :--------------------------------------------------------------------------------- | | `version` | `string` | The formatted version string. | | `tag` | `string` | An optional identifier of the build. | | `major` | `uint64` | The major release version. | | `minor` | `uint64` | The minor release version. | | `patch` | `uint64` | The patch release version. | | `features` | `list<string>` | A list of feature flags that conditionally enable features in the Tenzir Platform. | | `build` | `record` | Build-time configuration options. | | `dependencies` | `list<record>` | A list of build-time dependencies and their versions. | The `build` record contains the following fields: | Field | Type | Description | | :----------- | :------- | :------------------------------------------------------------------------- | | `type` | `string` | The configured build type. One of `Release`, `Debug`, or `RelWithDebInfo`. | | `tree_hash` | `string` | A hash of all files in the source directory. | | `assertions` | `bool` | Whether potentially expensive run-time checks are enabled. | | `sanitizers` | `record` | Contains information about additional run-time checks from sanitizers. | The `build.sanitzers` record contains the following fields: | Field | Type | Description | | :------------------- | :----- | :--------------------------------------------------- | | `address` | `bool` | Whether the address sanitizer is enabled. | | `undefined_behavior` | `bool` | Whether the undefined behavior sanitizer is enabled. | The `dependencies` record contains the following fields: | Field | Type | Description | | :-------- | :------- | :----------------------------- | | `name` | `string` | The name of the dependency. | | `version` | `string` | THe version of the dependency. | ## Examples [Section titled â€œExamplesâ€](#examples) ### Show the current version [Section titled â€œShow the current versionâ€](#show-the-current-version) ```tql version drop dependencies ``` ```tql { version: "5.0.1+g847fcc6334", tag: "g847fcc6334", major: 5, minor: 0, patch: 1, features: [ "chart_limit", "modules", "tql2_from", "exact_schema", "tql2_only", ], build: { type: "Release", tree_hash: "ef28a81eb124cc46a646250d1fb17390", assertions: false, sanitizers: { address: false, undefined_behavior: false, }, }, } ```

# where

Keeps only events for which the given predicate is true. ```tql where predicate:bool ``` ## Description [Section titled â€œDescriptionâ€](#description) The `where` operator only keeps events that match the provided predicate and discards all other events. Only events for which it evaluates to `true` pass. ## Examples [Section titled â€œExamplesâ€](#examples) ### Keep only events where `src_ip` is `1.2.3.4` [Section titled â€œKeep only events where src\_ip is 1.2.3.4â€](#keep-only-events-where-src_ip-is-1234) ```tql where src_ip == 1.2.3.4 ``` ### Use a nested field name and a temporal constraint on the `ts` field [Section titled â€œUse a nested field name and a temporal constraint on the ts fieldâ€](#use-a-nested-field-name-and-a-temporal-constraint-on-the-ts-field) ```tql where id.orig_h == 1.2.3.4 and ts > now() - 1h ``` ### Combine subnet, size and duration constraints [Section titled â€œCombine subnet, size and duration constraintsâ€](#combine-subnet-size-and-duration-constraints) ```tql where src_ip in 10.10.5.0/25 and (orig_bytes > 1Mi or duration > 30min) ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`assert`](/reference/operators/assert), [`drop`](/reference/operators/drop), [`select`](/reference/operators/select)

# write_bitz

Writes events in *BITZ* format. ```tql write_bitz ``` ## Description [Section titled â€œDescriptionâ€](#description) BITZ is short for **Bi**nary **T**en**z**ir and is our internal wire format. Use BITZ when you need high-throughput structured data exchange with minimal overhead. BITZ is a thin wrapper around Arrowâ€™s record batches. That is, BITZ lays out data in a (compressed) columnar fashion that makes it conducive for analytical workloads. Since itâ€™s padded and byte-aligned, it is portable and doesnâ€™t induce any deserialization cost, making it suitable for write-once-read-many use cases. Internally, BITZ uses Arrowâ€™s IPC format for serialization and deserialization, but prefixes each message with a 64 bit size prefix to support changing schemas between batchesâ€”something that Arrowâ€™s IPC format does not support on its own. ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_bitz`](/reference/operators/read_bitz), [`to_hive`](/reference/operators/to_hive), [`write_feather`](/reference/operators/write_feather), [`write_parquet`](/reference/operators/write_parquet)

# write_csv

Transforms event stream to CSV (Comma-Separated Values) byte stream. ```tql write_csv [list_separator=str, null_value=str, no_header=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `write_csv` operator transforms an event stream into a byte stream by writing the events as CSV. ### `list_separator = str (optional)` [Section titled â€œlist\_separator = str (optional)â€](#list_separator--str-optional) The string separating different elements in a list within a single field. Defaults to `";"`. ### `null_value = str (optional)` [Section titled â€œnull\_value = str (optional)â€](#null_value--str-optional) The string denoting an absent value. Defaults to `" "`. ### `no_header = bool (optional)` [Section titled â€œno\_header = bool (optional)â€](#no_header--bool-optional) Whether to not print a header line containing the field names. ## Examples [Section titled â€œExamplesâ€](#examples) Write an event as CSV. ```tql from {x:1, y:true, z: "String"} write_csv ``` ```plaintext x,y,z 1,true,String ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_csv`](/reference/functions/parse_csv), [`print_csv`](/reference/functions/print_csv), [`read_csv`](/reference/operators/read_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_feather

Transforms the input event stream to Feather byte stream. ```tql write_feather [compression_level=int, compression_type=str, min_space_savings=double] ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms the input event stream to [Feather](https://arrow.apache.org/docs/python/feather.html) (a thin wrapper around [Apache Arrowâ€™s IPC](https://arrow.apache.org/docs/python/ipc.html) wire format) byte stream. ### `compression_level = int (optional)` [Section titled â€œcompression\_level = int (optional)â€](#compression_level--int-optional) An optional compression level for the corresponding compression type. This option is ignored if no compression type is specified. Defaults to the compression typeâ€™s default compression level. ### `compression_type = str (optional)` [Section titled â€œcompression\_type = str (optional)â€](#compression_type--str-optional) Supported options are `zstd` for [Zstandard](http://facebook.github.io/zstd/) compression and `lz4` for [LZ4 Frame](https://android.googlesource.com/platform/external/lz4/+/HEAD/doc/lz4_Frame_format.md) compression. ### `min_space_savings = double (optional)` [Section titled â€œmin\_space\_savings = double (optional)â€](#min_space_savings--double-optional) Minimum space savings percentage required for compression to be applied. This option is ignored if no compression is specified. The provided value must be between 0 and 1 inclusive. Space savings are calculated as `1.0 - compressed_size / uncompressed_size`. For example, with a minimum space savings rate of 0.1, a 100-byte body buffer will not be compressed if its expected compressed size exceeds 90 bytes. Defaults to `0`, i.e., always applying compression. ## Examples [Section titled â€œExamplesâ€](#examples) ### Convert a JSON stream into a Feather file [Section titled â€œConvert a JSON stream into a Feather fileâ€](#convert-a-json-stream-into-a-feather-file) ```tql load_file "input.json" read_json write_feather save_file "output.feather" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_bitz`](/reference/operators/read_bitz), [`read_feather`](/reference/operators/read_feather), [`to_hive`](/reference/operators/to_hive), [`write_bitz`](/reference/operators/write_bitz), [`write_parquet`](/reference/operators/write_parquet)

# write_json

Transforms the input event stream to a JSON byte stream. ```tql write_json [strip=bool, color=bool, arrays_of_objects=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms the input event stream to a JSON byte stream. ### `strip = bool (optional)` [Section titled â€œstrip = bool (optional)â€](#strip--bool-optional) Enables all `strip_*` options. Defaults to `false`. ### `color = bool (optional)` [Section titled â€œcolor = bool (optional)â€](#color--bool-optional) Colorize the output. Defaults to `false`. ### `arrays_of_objects = bool (optional)` [Section titled â€œarrays\_of\_objects = bool (optional)â€](#arrays_of_objects--bool-optional) Prints the input as a single array of objects, instead of as separate objects. Defaults to `false`. ### `strip_null_fields = bool (optional)` [Section titled â€œstrip\_null\_fields = bool (optional)â€](#strip_null_fields--bool-optional) Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` [Section titled â€œstrip\_nulls\_in\_lists = bool (optional)â€](#strip_nulls_in_lists--bool-optional) Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` [Section titled â€œstrip\_empty\_records = bool (optional)â€](#strip_empty_records--bool-optional) Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` [Section titled â€œstrip\_empty\_lists = bool (optional)â€](#strip_empty_lists--bool-optional) Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Convert a YAML stream into a JSON file [Section titled â€œConvert a YAML stream into a JSON fileâ€](#convert-a-yaml-stream-into-a-json-file) ```tql load_file "input.yaml" read_yaml write_json save_file "output.json" ``` ### Strip null fields [Section titled â€œStrip null fieldsâ€](#strip-null-fields) ```tql from { yes: 1, no: null} write_json strip_null_fields=true ``` ```json { "yes": 1 } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_json`](/reference/functions/parse_json), [`print_json`](/reference/functions/print_json), [`read_json`](/reference/operators/read_json), [`write_tql`](/reference/operators/write_tql)

# write_kv

Writes events in a Key-Value format. ```tql write_kv [field_separator=str, value_separator=str, list_separator=str, flatten_separator=str, null_value=str] ``` ## Description [Section titled â€œDescriptionâ€](#description) Writes events in a Key-Value format, with one event per line. Nested data will be flattend, keys or values containing the given separators will be quoted and the special characters `\n`, `\r`, `\` and `"` will be escaped. ### `field_separator = str (optional)` [Section titled â€œfield\_separator = str (optional)â€](#field_separator--str-optional) A string that shall separate the key-value pairs. Must not be an empty string. Defaults to `" "`. ### `value_separator = str (optional)` [Section titled â€œvalue\_separator = str (optional)â€](#value_separator--str-optional) A string that shall separate key and value within key-value pair. Must not be an empty string. Defaults to `"="`. ### `list_separator = str (optional)` [Section titled â€œlist\_separator = str (optional)â€](#list_separator--str-optional) Must not be an empty string. Defaults to `","`. ### `flatten_separator = str (optional)` [Section titled â€œflatten\_separator = str (optional)â€](#flatten_separator--str-optional) A string to join the keys of nested records with. For example, given `flatten="."` Defaults to `"."`. ### `null_value = str (optional)` [Section titled â€œnull\_value = str (optional)â€](#null_value--str-optional) A string to represent null values. Defaults to the empty string. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write key-value pairs with quoted strings [Section titled â€œWrite key-value pairs with quoted stringsâ€](#write-key-value-pairs-with-quoted-strings) ```tql from {x: "hello world", y: "hello=world"} ``` ```txt x="hello world" y:"hello=world" ``` ### Write key-value pairs of nested records [Section titled â€œWrite key-value pairs of nested recordsâ€](#write-key-value-pairs-of-nested-records) ```tql from {x: {y: {z:0}, y2:42}, a: "string" } write_kv ``` ```txt x.y.z=0 y.y2=42 a=string ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_kv`](/reference/functions/parse_kv), [`print_kv`](/reference/functions/print_kv), [`read_kv`](/reference/operators/read_kv), [`write_lines`](/reference/operators/write_lines)

# write_lines

Writes events as key-value pairsthe *values* of an event. ```tql write_lines ``` ## Description [Section titled â€œDescriptionâ€](#description) Each event is printed on a new line, with fields separated by spaces, and nulls skipped. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write the values of an event [Section titled â€œWrite the values of an eventâ€](#write-the-values-of-an-event) ```tql from {x:1, y:true, z: "String"} write_lines ``` ```txt 1 true String ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_lines`](/reference/operators/read_lines), [`write_csv`](/reference/operators/write_csv), [`write_kv`](/reference/operators/write_kv), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_ndjson

Transforms the input event stream to a Newline-Delimited JSON byte stream. ```tql write_ndjson [strip=bool, color=bool, arrays_of_objects=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms the input event stream to a Newline-Delimited JSON byte stream. ### `strip = bool (optional)` [Section titled â€œstrip = bool (optional)â€](#strip--bool-optional) Enables all `strip_*` options. Defaults to `false`. ### `color = bool (optional)` [Section titled â€œcolor = bool (optional)â€](#color--bool-optional) Colorize the output. Defaults to `false`. ### `arrays_of_objects = bool (optional)` [Section titled â€œarrays\_of\_objects = bool (optional)â€](#arrays_of_objects--bool-optional) Prints the input as a single array of objects, instead of as separate objects. Defaults to `false`. ### `strip_null_fields = bool (optional)` [Section titled â€œstrip\_null\_fields = bool (optional)â€](#strip_null_fields--bool-optional) Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` [Section titled â€œstrip\_nulls\_in\_lists = bool (optional)â€](#strip_nulls_in_lists--bool-optional) Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` [Section titled â€œstrip\_empty\_records = bool (optional)â€](#strip_empty_records--bool-optional) Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` [Section titled â€œstrip\_empty\_lists = bool (optional)â€](#strip_empty_lists--bool-optional) Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Convert a YAML stream into a JSON file [Section titled â€œConvert a YAML stream into a JSON fileâ€](#convert-a-yaml-stream-into-a-json-file) ```tql load_file "input.yaml" read_yaml write_ndjson save_file "output.json" ``` ### Strip null fields [Section titled â€œStrip null fieldsâ€](#strip-null-fields) ```tql from { yes: 1, no: null, } write_ndjson strip_null_fields=true ``` ```json { "yes": 1 } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_json`](/reference/functions/parse_json), [`print_ndjson`](/reference/functions/print_ndjson), [`print_json`](/reference/functions/print_json), [`read_json`](/reference/operators/read_json), [`read_ndjson`](/reference/operators/read_ndjson), [`write_json`](/reference/operators/write_json)

# write_parquet

Transforms event stream to a Parquet byte stream. ```tql write_parquet [compression_level=int, compression_type=str] ``` ## Description [Section titled â€œDescriptionâ€](#description) [Apache Parquet](https://parquet.apache.org/) is a columnar storage format that a variety of data tools support. ### `compression_level = int (optional)` [Section titled â€œcompression\_level = int (optional)â€](#compression_level--int-optional) An optional compression level for the corresponding compression type. This option is ignored if no compression type is specified. Defaults to the compression typeâ€™s default compression level. ### `compression_type = str (optional)` [Section titled â€œcompression\_type = str (optional)â€](#compression_type--str-optional) Specifies an optional compression type. Supported options are `zstd` for [Zstandard](http://facebook.github.io/zstd/) compression, `brotli` for [brotli](https://www.brotli.org) compression, `gzip` for [gzip](https://www.gzip.org) compression, and `snappy` for [snappy](https://google.github.io/snappy/) compression. ## Examples [Section titled â€œExamplesâ€](#examples) Write a Parquet file: ```tql load_file "/tmp/data.json" read_json write_parquet ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_bitz`](/reference/operators/read_bitz), [`read_parquet`](/reference/operators/read_parquet), [`to_hive`](/reference/operators/to_hive), [`write_bitz`](/reference/operators/write_bitz), [`write_feather`](/reference/operators/write_feather)

# write_pcap

Transforms event stream to PCAP byte stream. ```tql write_pcap ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms event stream to [PCAP](https://datatracker.ietf.org/doc/id/draft-gharris-opsawg-pcap-00.html) byte stream. The structured representation of packets has the `pcap.packet` schema: ```yaml pcap.packet: record: - linktype: uint64 - time: timestamp: time - captured_packet_length: uint64 - original_packet_length: uint64 - data: string ``` ## Examples [Section titled â€œExamplesâ€](#examples) ### Write packet events as a PCAP file [Section titled â€œWrite packet events as a PCAP fileâ€](#write-packet-events-as-a-pcap-file) ```tql subscribe "packets" write_pcap save_file "/logs/packets.pcap" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`load_nic`](/reference/operators/load_nic), [`read_pcap`](/reference/operators/read_pcap)

# write_ssv

Transforms event stream to SSV (Space-Separated Values) byte stream. ```tql write_ssv [list_separator=str, null_value=str, no_header=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `write_ssv` operator transforms an event stream into a byte stream by writing the events as SSV. ### `list_separator = str (optional)` [Section titled â€œlist\_separator = str (optional)â€](#list_separator--str-optional) The string separating different elements in a list within a single field. Defaults to `","`. ### `null_value = str (optional)` [Section titled â€œnull\_value = str (optional)â€](#null_value--str-optional) The string denoting an absent value. Defaults to `"-"`. ### `no_header = bool (optional)` [Section titled â€œno\_header = bool (optional)â€](#no_header--bool-optional) Whether to not print a header line containing the field names. ## Examples [Section titled â€œExamplesâ€](#examples) Write an event as SSV. ```tql from {x:1, y:true, z: "String"} write_ssv ``` ```plaintext x y z 1 true String ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`print_ssv`](/reference/functions/print_ssv), [`read_ssv`](/reference/operators/read_ssv), [`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_syslog

Writes events as syslog. ```tql write_syslog [facility=int, severity=int, timestamp=time, hostname=string, app_name=string, process_id=string, message_id=string, structured_data=record, message=string] ``` ## Description [Section titled â€œDescriptionâ€](#description) Writes events as [RFC 5424](https://datatracker.ietf.org/doc/html/rfc5424) Syslog messages. All options to the operator try to get values for the respective fields from the same-named fields in the input events if unspecified. ### `facility = int (optional)` [Section titled â€œfacility = int (optional)â€](#facility--int-optional) Set the facility of the syslog. Defaults to `1` if `null`. ### `severity = int (optional)` [Section titled â€œseverity = int (optional)â€](#severity--int-optional) Set the severity of the syslog. Defaults to `6` if `null`. ### `timestamp = time (optional)` [Section titled â€œtimestamp = time (optional)â€](#timestamp--time-optional) Set the timestamp of the syslog. ### `hostname = string (optional)` [Section titled â€œhostname = string (optional)â€](#hostname--string-optional) Set the hostname of the syslog. ### `app_name = string (optional)` [Section titled â€œapp\_name = string (optional)â€](#app_name--string-optional) Set the application name of the syslog. ### `process_id = string (optional)` [Section titled â€œprocess\_id = string (optional)â€](#process_id--string-optional) Set the process id of the syslog. ### `message_id = string (optional)` [Section titled â€œmessage\_id = string (optional)â€](#message_id--string-optional) Set the message id of the syslog. ### `structured_data = record (optional)` [Section titled â€œstructured\_data = record (optional)â€](#structured_data--record-optional) Set the structured data of the syslog. ### `message = string (optional)` [Section titled â€œmessage = string (optional)â€](#message--string-optional) Set the message of the syslog. ## Examples [Section titled â€œExamplesâ€](#examples) ### Create a syslog manually [Section titled â€œCreate a syslog manuallyâ€](#create-a-syslog-manually) ```tql from { facility: 1, severity: 1, timestamp: now(), hostname: "localhost", structured_data: { origin: { key: "value", }, }, message: "Tenzir", } write_syslog ``` ```log <9>1 2025-03-31T13:28:55.971210Z localhost - - - [origin key="value"] Tenzir ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`parse_syslog`](/reference/functions/parse_syslog), [`read_syslog`](/reference/operators/read_syslog)

# write_tql

Transforms the input event stream to a TQL notation byte stream. ```tql write_tql [strip=bool, color=bool, compact=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms the input event stream to a TQL notation byte stream. ### `strip = bool (optional)` [Section titled â€œstrip = bool (optional)â€](#strip--bool-optional) Enables all `strip_*` options. Defaults to `false`. ### `compact = bool (optional)` [Section titled â€œcompact = bool (optional)â€](#compact--bool-optional) Write one event per line, omitting linebreaks and indentation of records. Defaults to `false`. ### `color = bool (optional)` [Section titled â€œcolor = bool (optional)â€](#color--bool-optional) Colorize the output. Defaults to `false`. ### `strip_null_fields = bool (optional)` [Section titled â€œstrip\_null\_fields = bool (optional)â€](#strip_null_fields--bool-optional) Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` [Section titled â€œstrip\_nulls\_in\_lists = bool (optional)â€](#strip_nulls_in_lists--bool-optional) Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` [Section titled â€œstrip\_empty\_records = bool (optional)â€](#strip_empty_records--bool-optional) Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` [Section titled â€œstrip\_empty\_lists = bool (optional)â€](#strip_empty_lists--bool-optional) Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Print an event as TQL [Section titled â€œPrint an event as TQLâ€](#print-an-event-as-tql) ```tql from {activity_id: 16, activity_name: "Query", rdata: 31.3.245.133, dst_endpoint: {ip: 192.168.4.1, port: 53}} write_tql ``` ```tql { activity_id: 16, activity_name: "Query", rdata: 31.3.245.133, dst_endpoint: { ip: 192.168.4.1, port: 53, }, } ``` ### Strip null fields [Section titled â€œStrip null fieldsâ€](#strip-null-fields) ```tql from {yes: 1, no: null} write_tql strip_null_fields=true ``` ```tql { yes: 1, } ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`write_json`](/reference/operators/write_json)

# write_tsv

Transforms event stream to TSV (Tab-Separated Values) byte stream. ```tql write_tsv [list_separator=str, null_value=str, no_header=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `write_tsv` operator transforms an event stream into a byte stream by writing the events as TSV. ### `list_separator = str (optional)` [Section titled â€œlist\_separator = str (optional)â€](#list_separator--str-optional) The string separating different elements in a list within a single field. Defaults to `","`. ### `null_value = str (optional)` [Section titled â€œnull\_value = str (optional)â€](#null_value--str-optional) The string denoting an absent value. Defaults to `"-"`. ### `no_header = bool (optional)` [Section titled â€œno\_header = bool (optional)â€](#no_header--bool-optional) Whether to not print a header line containing the field names. ## Examples [Section titled â€œExamplesâ€](#examples) Write an event as TSV. ```tql from {x:1, y:true, z: "String"} write_tsv ``` ```plaintext x y z 1 true String ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_xsv`](/reference/operators/write_xsv)

# write_xsv

Transforms event stream to XSV byte stream. ```tql write_xsv field_separator=str, list_separator=str, null_value=str, [no_header=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The [`xsv`](https://en.wikipedia.org/wiki/Delimiter-separated_values) format is a generalization of [comma-separated values (CSV)](https://en.wikipedia.org/wiki/Comma-separated_values) data in tabular form with a more flexible separator specification supporting tabs, commas, and spaces. The first line in an XSV file is the header that describes the field names. The remaining lines contain concrete values. One line corresponds to one event, minus the header. The following table lists existing XSV configurations: | Format | Field Separator | List Separator | Null Value | | --------------------------------------- | :-------------: | :------------: | :--------: | | [`csv`](/reference/operators/write_csv) | `,` | `;` | empty | | [`ssv`](/reference/operators/write_ssv) | `<space>` | `,` | `-` | | [`tsv`](/reference/operators/write_tsv) | `\t` | `,` | `-` | Note that nested records have dot-separated field names. ### `field_separator = str` [Section titled â€œfield\_separator = strâ€](#field_separator--str) The string separating different fields. ### `list_separator = str` [Section titled â€œlist\_separator = strâ€](#list_separator--str) The string separating different elements in a list within a single field. ### `null_value = str` [Section titled â€œnull\_value = strâ€](#null_value--str) The string denoting an absent value. ### `no_header=bool (optional)` [Section titled â€œno\_header=bool (optional)â€](#no_headerbool-optional) Whether to not print a header line containing the field names. ## Examples [Section titled â€œExamplesâ€](#examples) ```tql from {x:1, y:true, z: "String"} write_xsv field_separator="/", list_separator=";", null_value="" ``` ```plaintext x/y/z 1/true/String ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`print_xsv`](/reference/functions/print_xsv), [`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv)

# write_yaml

Transforms the input event stream to YAML byte stream. ```tql write_yaml ``` ## Description [Section titled â€œDescriptionâ€](#description) Transforms the input event stream to YAML byte stream. ## Examples [Section titled â€œExamplesâ€](#examples) ### Convert a JSON file into a YAML file [Section titled â€œConvert a JSON file into a YAML fileâ€](#convert-a-json-file-into-a-yaml-file) ```tql load_file "input.json" read_json write_yaml save_file "output.yaml" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_yaml`](/reference/operators/read_yaml), [`parse_yaml`](/reference/functions/parse_yaml), [`print_yaml`](/reference/functions/print_yaml)

# write_zeek_tsv

Transforms event stream into Zeek Tab-Separated Value byte stream. ```tql write_zeek_tsv [set_separator=str, empty_field=str, unset_field=str, disable_timestamp_tags=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The [Zeek](https://zeek.org) network security monitor comes with its own tab-separated value (TSV) format for representing logs. This format includes additional header fields with field names, type annotations, and additional metadata. The `write_zeek_tsv` operator (re)generates the TSV metadata based on Tenzirâ€™s internal schema. Tenzirâ€™s data model is a superset of Zeekâ€™s, so the conversion into Zeek TSV may be lossy. The Zeek types `count`, `real`, and `addr` map to the respective Tenzir types `uint64`, `double`, and `ip`. ### `set_separator = str (optional)` [Section titled â€œset\_separator = str (optional)â€](#set_separator--str-optional) Specifies the set separator. Defaults to `\x09`. ### `empty_field = str (optional)` [Section titled â€œempty\_field = str (optional)â€](#empty_field--str-optional) Specifies the separator for empty fields. Defaults to `(empty)`. ### `unset_field = str (optional)` [Section titled â€œunset\_field = str (optional)â€](#unset_field--str-optional) Specifies the separator for unset â€œnullâ€ fields. Defaults to `-`. ### `disable_timestamp_tags = bool (optional)` [Section titled â€œdisable\_timestamp\_tags = bool (optional)â€](#disable_timestamp_tags--bool-optional) Disables the `#open` and `#close` timestamp tags. Defaults to `false`. ## Examples [Section titled â€œExamplesâ€](#examples) ### Write pipelines results in Zeek TSV format [Section titled â€œWrite pipelines results in Zeek TSV formatâ€](#write-pipelines-results-in-zeek-tsv-format) ```tql subscribe "zeek-logs" where duration > 2s and id.orig_p != 80 write_zeek_tsv save_file "filtered_conn.log" ``` ## See Also [Section titled â€œSee Alsoâ€](#see-also) [`read_zeek_json`](/reference/operators/read_zeek_json), [`read_zeek_tsv`](/reference/operators/read_zeek_tsv)

# yara

Executes YARA rules on byte streams. ```tql yara rule:list<string>, [blockwise=bool, compiled_rules=bool, fast_scan=bool] ``` ## Description [Section titled â€œDescriptionâ€](#description) The `yara` operator applies [YARA](https://virustotal.github.io/yara/) rules to an input of bytes, emitting rule context upon a match. ![YARA Operator](/_astro/yara-operator.excalidraw.CVYOkp4d_19DKCs.svg) We modeled the operator after the official [`yara` command-line utility](https://yara.readthedocs.io/en/stable/commandline.html) to enable a familiar experience for the command users. Similar to the official `yara` command, the operator compiles the rules by default, unless you provide the option `compiled_rules=true`. To quote from the above link: > This is a security measure to prevent users from inadvertently using compiled rules coming from a third-party. Using compiled rules from untrusted sources can lead to the execution of malicious code in your computer. The operator uses a YARA *scanner* under the hood that buffers blocks of bytes incrementally. Even though the input arrives in non-contiguous blocks of memories, the YARA scanner engine support matching across block boundaries. For continuously running pipelines, use the `blockwise=true` option that considers each block as a separate unit. Otherwise the scanner engine would simply accumulate blocks but never trigger a scan. ### `rule: list<string>` [Section titled â€œrule: list\<string>â€](#rule-liststring) The path to the YARA rule(s). If the path is a directory, the operator attempts to recursively add all contained files as YARA rules. ### `blockwise = bool (optional)` [Section titled â€œblockwise = bool (optional)â€](#blockwise--bool-optional) Whether to match on every byte chunk instead of triggering a scan when the input exhausted. This option makes sense for never-ending dataflows where each chunk of bytes constitutes a self-contained unit, such as a single file. ### `compiled_rules = bool (optional)` [Section titled â€œcompiled\_rules = bool (optional)â€](#compiled_rules--bool-optional) Whether to interpret the rules as compiled. When providing this flag, you must exactly provide one rule path as positional argument. ### `fast_scan = bool (optional)` [Section titled â€œfast\_scan = bool (optional)â€](#fast_scan--bool-optional) Enable fast matching mode. ## Examples [Section titled â€œExamplesâ€](#examples) The examples below show how you can scan a single file and how you can create a simple rule scanning service. ### Perform one-shot scanning of files [Section titled â€œPerform one-shot scanning of filesâ€](#perform-one-shot-scanning-of-files) Scan a file with a set of YARA rules: ```tql load_file "evil.exe", mmap=true yara "rule.yara" ``` Letâ€™s unpack a concrete example: ```plaintext rule test { meta: string = "string meta data" integer = 42 boolean = true strings: $foo = "foo" $bar = "bar" $baz = "baz" condition: ($foo and $bar) or $baz } ``` You can produce test matches by feeding bytes into the `yara` operator. You will get one `yara.match` per matching rule: ```tql { rule: { identifier: "test", namespace: "default", tags: [], meta: { string: "string meta data", integer: 42, boolean: true }, strings: { "$foo": "foo", "$bar": "bar", "$baz": "baz" } }, matches: { "$foo": [ { data: "Zm9v", base: 0, offset: 0, match_length: 3 } ], "$bar": [ { data: "YmFy", base: 0, offset: 4, match_length: 3 } ] } } ``` Each match has a `rule` field describing the rule and a `matches` record indexed by string identifier to report a list of matches per rule string.

# Platform command line interface

The Tenzir Platform command-line interface (CLI) allows you to interact with the Tenzir Platform from the command line to manage workspaces and nodes. ## Installation [Section titled â€œInstallationâ€](#installation) Install the [`tenzir-platform`](https://pypi.org/project/tenzir-platform/) package from PyPI. ```text pip install tenzir-platform ``` ## Global Options [Section titled â€œGlobal Optionsâ€](#global-options) The following options are available for all `tenzir-platform` commands: * `-v, --verbose`: Enable verbose logging for additional error information and debugging output. ## Environment Variable Configuration [Section titled â€œEnvironment Variable Configurationâ€](#environment-variable-configuration) The Tenzir Platform CLI supports several environment variables to configure authentication and platform connection settings. All configuration variables for the CLI share the prefix `TENZIR_PLATFORM_CLI_`. ### Supported Settings [Section titled â€œSupported Settingsâ€](#supported-settings) * `TENZIR_PLATFORM_CLI_API_ENDPOINT`: The URL of the Tenzir Platform API instance to connect to. Defaults to `https://rest.tenzir.app/production-v1`. * `TENZIR_PLATFORM_CLI_EXTRA_HEADERS`: Additional headers that the CLI should send with any request to the Tenzir Platform. Must provide as a JSON object. * `TENZIR_PLATFORM_CLI_ISSUER_URL`: The OIDC issuer URL for authentication. Defaults to the issuer URL that the public Tenzir Platform instance uses at <https://app.tenzir.com>. * `TENZIR_PLATFORM_CLI_CLIENT_ID`: The client ID for the CLI client. Defaults to the client id that the public Tenzir Platform instance uses at <https://app.tenzir.com>. * `TENZIR_PLATFORM_CLI_CLIENT_SECRET`: The client secret for the CLI client. When set, the CLI automatically attempts to use the client credentials flow instead of the device code flow for authentication. * `TENZIR_PLATFORM_CLI_CLIENT_SECRET_FILE`: Path to a file containing the client secret. Alternative to providing the secret directly via `TENZIR_PLATFORM_CLI_CLIENT_SECRET`. * `TENZIR_PLATFORM_CLI_ID_TOKEN`: If provided, skip the login workflow completely and use this token for authentication. * `TENZIR_PLATFORM_CLI_AUDIENCE`: Override the OIDC audience parameter. Defaults to the client ID. Required for non-interactive logins with some identity providers like Microsoft Entra. * `TENZIR_PLATFORM_CLI_SCOPE`: Override the default OIDC scopes. Defaults to `openid email` for device code flow and `openid` for client credentials flow. Use this to customize the requested permissions during authentication. ## Authentication [Section titled â€œAuthenticationâ€](#authentication) ### Synopsis [Section titled â€œSynopsisâ€](#synopsis) ```text tenzir-platform auth login tenzir-platform workspace list tenzir-platform workspace select <workspace_id> ``` ### Description [Section titled â€œDescriptionâ€](#description) The `tenzir-platform auth login` command authenticates you with the platform. The `tenzir-platform workspace list` command shows all workspaces available to you. The `tenzir-platform workspace select` command selects a workspace for subsequent operations. #### `<workspace_id>` [Section titled â€œ\<workspace\_id>â€](#workspace_id) The unique ID of the workspace, as shown in `tenzir-platform workspace list`. ## Manage Nodes [Section titled â€œManage Nodesâ€](#manage-nodes) ### Synopsis [Section titled â€œSynopsisâ€](#synopsis-1) ```text tenzir-platform node list tenzir-platform node ping <node_id> tenzir-platform node create [--name <node_name>] tenzir-platform node delete <node_id> tenzir-platform node run [--name <node_name>] [--image <container_image>] ``` ### Description [Section titled â€œDescriptionâ€](#description-1) The following commands interact with the selected workspace: * `tenzir-platform node list` lists all nodes in the selected workspace, including their ID, name, and connection status. * `tenzir-platform node ping` pings the specified node. * `tenzir-platform node create` registers a new node at the platform. This command creates a new API key that a node can use to connect to the platform. It does not start or configure a node. * `tenzir-platform node delete` removes a node from the platform. This command does not stop the node; it only removes it from the platform. * `tenzir-platform node run` creates and registers an ad-hoc node, then starts it on the local host. This command requires Docker Compose. The platform deletes the temporary node when you stop the `run` command. #### `<node_id>` [Section titled â€œ\<node\_id>â€](#node_id) The unique ID of the node, as shown in `tenzir-platform node list`. #### `<node_name>` [Section titled â€œ\<node\_name>â€](#node_name) The name of the node as shown in the app. #### `<container_image>` [Section titled â€œ\<container\_image>â€](#container_image) The Docker image to use for the ad-hoc created node. We recommend using one of the following images: * `tenzir/tenzir:latest` to use the latest release. * `tenzir/tenzir:main` to use the current development version. * `tenzir/tenzir:v5` to pin to a major release. * `tenzir/tenzir:v5.1` to pin to a minor release. * `tenzir/tenzir:v5.1.3` to use a specific release. ## Manage Secrets [Section titled â€œManage Secretsâ€](#manage-secrets) The Tenzir Platform provides secret storage that pipelines running on a Tenzir node can access. ### Synopsis [Section titled â€œSynopsisâ€](#synopsis-2) ```text tenzir-platform secret add <name> [--file=<file>] [--value=<value>] [--env] tenzir-platform secret update <secret_id> [--file=<file>] [--value=<value>] [--env] tenzir-platform secret delete <secret_id> tenzir-platform secret list [--json] ``` ### Description [Section titled â€œDescriptionâ€](#description-2) The following commands manage secrets in the Tenzir Platform: * `tenzir-platform secret add` adds a new secret to the platform. You can provide the secret value directly via the `--value` option, read it from a file using the `--file` option, or source it from an environment variable using the `--env` flag. The platform identifies the secret by the provided `<name>`. * `tenzir-platform secret update` updates an existing secret identified by `<secret_id>`. Like adding a secret, you can provide the new value via `--value`, `--file`, or `--env`. * `tenzir-platform secret delete` removes a secret from the platform. The command identifies the secret to delete by its `<secret_id>`. * `tenzir-platform secret list` lists all secrets available in the platform. Use the `--json` flag to output the list in JSON format for easier integration with other tools. #### `<name>` [Section titled â€œ\<name>â€](#name) The unique name to identify the secret. Pipelines use this name when they access the secret. #### `<secret_id>` [Section titled â€œ\<secret\_id>â€](#secret_id) The unique identifier of the secret, as shown in the output of `tenzir-platform secret list`. #### `--file=<file>` [Section titled â€œ--file=\<file>â€](#--filefile) Specifies a file containing the secret value. The platform securely stores the fileâ€™s contents as the secret value. #### `--value=<value>` [Section titled â€œ--value=\<value>â€](#--valuevalue) Specifies the secret value directly as a command-line argument. #### `--env` [Section titled â€œ--envâ€](#--env) Indicates that the command should source the secret value from an environment variable. You must set the environment variable before you run the command. #### `--json` [Section titled â€œ--jsonâ€](#--json) Outputs the list of secrets in JSON format when used with the `tenzir-platform secret list` command. ## Manage external secret stores [Section titled â€œManage external secret storesâ€](#manage-external-secret-stores) You can configure workspaces to use external secret stores instead of the Tenzir Platformâ€™s built-in secret store. Currently, the platform only supports [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) as an external store. By default, the platform mounts external secret stores as read-only. You canâ€™t add or update secrets from the CLI or web interface. Some external secret store implementations may offer write access options. ### Synopsis [Section titled â€œSynopsisâ€](#synopsis-3) ```text tenzir-platform secret store add aws --region=<region> --assumed-role-arn=<assumed_role_arn> [--name=<name>] [--prefix=<prefix>] [--access-key-id=<key_id>] [--secret-access-key=<key>] tenzir-platform secret store set-default <store_id> tenzir-platform secret store delete <store_id> tenzir-platform secret store list [--json] ``` ### Description [Section titled â€œDescriptionâ€](#description-3) When a node accesses a secret using the `secret("foo")` function in a pipeline, the platform looks up the secret named `foo` in the workspaceâ€™s default secret store and returns the value to the node. When using an external secret store, the platform needs the necessary permissions to read secret values from that store. For AWS Secrets Manager, the Tenzir Platform uses [AWS Security Token Service (STS)](https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html) to assume a role with the necessary permissions. You must create this role and configure one of the following options: 1. Create an IAM user. Download access keys and pass them via the `--access-key-id` and `--secret-access-key` arguments. This approach is the easiest to set up but only acceptable for self-hosted testing or development instances of the Tenzir Platform because it stores long-term credentials in the Tenzir Platform. 2. Assign the permissions to the task role of the websocket gateway. We recommend this option if youâ€™re running the Sovereign Edition of the Platform and deploying it to your own AWS account. 3. Assign the permission to assume the configured role to Tenzirâ€™s AWS account with the ID `660178929208`. This option works only with our publicly hosted platform instance on `https://app.tenzir.com`. We plan to add an OIDC-based option as an alternative to options 1 and 3. ## Manage Alerts [Section titled â€œManage Alertsâ€](#manage-alerts) ### Synopsis [Section titled â€œSynopsisâ€](#synopsis-4) ```text tenzir-platform alert add <node> <duration> <webhook_url> [<webhook_body>] tenzir-platform alert delete <alert_id> tenzir-platform alert list ``` ### Description [Section titled â€œDescriptionâ€](#description-4) The following commands set up alerts for specific nodes. When a node remains disconnected for the configured duration, the alert triggers by sending a POST request to the configured webhook URL. #### `<node>` [Section titled â€œ\<node>â€](#node) The node to monitor. You can provide either a node ID or a node name, as long as the name is unambiguous. #### `<duration>` [Section titled â€œ\<duration>â€](#duration) The amount of time to wait between the node disconnect and triggering the alert. #### `<webhook_url>` [Section titled â€œ\<webhook\_url>â€](#webhook_url) The platform sends a POST request to this URL when the alert triggers. #### `<webhook_body>` [Section titled â€œ\<webhook\_body>â€](#webhook_body) The body to send with the webhook. Must be valid JSON. The body may contain `$NODE_NAME`, which the platform replaces with the name of the node that triggered the alert. Defaults to `{"text": "Node $NODE_NAME disconnected for more than {duration}s"}`, where the platform sets `$NODE_NAME` and `{duration}` dynamically from the CLI parameters. ### Example [Section titled â€œExampleâ€](#example) Consider nodes like this: ```text $ tenzir-platform node list ğŸŸ¢ Node-1 (n-w2tjezz3) ğŸŸ¢ Node-2 (n-kzw21299) ğŸ”´ Node-3 (n-ie2tdgca) ``` We want to receive a Slack notification whenever Node-3 is offline for more than three minutes. First, we create a webhook as described in the [Slack docs](https://api.slack.com/messaging/webhooks). Next, we configure the alert in the Tenzir Platform: ```text tenzir-platform alert add Node-3 3m "https://hooks.slack.com/services/XXXXX/YYYYY/ZZZZZ" '{"text": "Alert! Look after node $NODE_NAME"}' ``` If Node-3 doesnâ€™t reconnect within three minutes, a message appears in the configured Slack channel. ## Manage workspaces [Section titled â€œManage workspacesâ€](#manage-workspaces) On-premise setup required This CLI functionality requires an on-premise platform deployment, available with the [Sovereign Edition](https://tenzir.com/pricing). These CLI commands are available only to local platform administrators. The [`TENZIR_PLATFORM_OIDC_ADMIN_RULES` variable](/guides/platform-setup/configure-identity-provider) defines whoâ€™s an administrator in your platform deployment. ### Synopsis [Section titled â€œSynopsisâ€](#synopsis-5) ```text tenzir-platform admin list-global-workspaces tenzir-platform admin create-workspace <owner_namespace> <owner_id> [--name <workspace_name>] tenzir-platform admin delete-workspace <workspace_id> ``` ### Description [Section titled â€œDescriptionâ€](#description-5) The `tenzir-platform admin list-global-workspaces`, `tenzir-platform admin create-workspace`, and `tenzir-platform admin delete-workspace` commands list, create, or delete workspaces, respectively. #### `<owner_namespace>` [Section titled â€œ\<owner\_namespace>â€](#owner_namespace) Either `user` or `organization`, depending on whether the workspace associates with a user or an organization. The selected namespace determines the *default* access rules for the workspace: * For a user workspace, the platform creates a single access rule that allows access to the user whose user ID matches the given `owner_id`. * For an organization workspace, the platform creates no rules by default. You must manually add them using the `add-auth-rule` subcommand described below. #### `<owner_id>` [Section titled â€œ\<owner\_id>â€](#owner_id) The unique ID of the workspace owner: * If `<owner_namespace>` is `user`, this matches the userâ€™s `sub` claim in the OIDC token. * If `<owner_namespace>` is `organization`, this is an arbitrary string that uniquely identifies the organization the workspace belongs to. #### `--name <workspace_name>` [Section titled â€œ--name \<workspace\_name>â€](#--name-workspace_name) The name of the workspace as shown in the app. #### `<workspace_id>` [Section titled â€œ\<workspace\_id>â€](#workspace_id-1) The unique ID of the workspace, as shown in `tenzir-platform workspace list` or `tenzir-platform admin list-global-workspaces`. ## Configure access rules [Section titled â€œConfigure access rulesâ€](#configure-access-rules) On-premise setup required You can use this CLI functionality only with an on-premise platform deployment, which is available to users of the [Sovereign Edition](https://tenzir.com/pricing). These CLI commands are available only to local platform administrators. The [`TENZIR_PLATFORM_OIDC_ADMIN_RULES` variable](/guides/platform-setup/configure-identity-provider) defines whoâ€™s an administrator in your platform deployment. ### Synopsis [Section titled â€œSynopsisâ€](#synopsis-6) ```text tenzir-platform admin list-auth-rules <workspace_id> tenzir-platform admin add-auth-rule allow-all <workspace_id> tenzir-platform admin add-auth-rule user <workspace_id> <user_id> tenzir-platform admin add-auth-rule email-domain <workspace_id> <connection> <domain> tenzir-platform admin add-auth-rule organization-membership <workspace_id> <connection> <organization_claim> <organization> tenzir-platform admin add-auth-rule organization-role <workspace_id> <connection> <roles_claim> <role> <organization_claim> <organization> tenzir-platform admin delete-auth-rule <workspace_id> <auth_rule_index> ``` ### Description [Section titled â€œDescriptionâ€](#description-6) You can use the `tenzir-platform admin list-auth-rules`, `tenzir-platform admin add-auth-rule`, and `tenzir-platform admin delete-auth-rule` commands to list, create, or delete authentication rules for all users, respectively, if you have admin permissions. Authentication rules allow you to access the workspace with the provided `<workspace_id>` if your `id_token` matches the configured rule. You gain access to a workspace if any configured rule allows access. The following rules exist: * **Email Suffix Rule**: `tenzir-platform admin add-auth-rule email-domain` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>` and an `email` field that ends with the configured `<domain>`. * **Organization Membership**: `tenzir-platform admin add-auth-rule organization-membership` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>` and an `<organization_claim>` field that exactly matches the provided `<organization>`. Note that you can freely choose the `<organization_claim>` and `<organization>`, so you can also repurpose this rule for generic claims that are not necessarily related to organizations. * **Organization Role Rule**: `tenzir-platform admin add-auth-rule organization-role` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>`, an `<organization_claim>` field that exactly matches the provided `<organization>`, and a `<roles_claim>` field that must be a list containing a value exactly matching `<role>`. We recommend using organization role rules to check if you have a specific role with an organization. * **User Rule**: `tenzir-platform admin add-auth-rule user` allows access if the `id_token` contains a `sub` field that exactly matches the provided `<user_id>`. * **Allow all rule**: `tenzir-platform admin add-auth-rule allow-all` allows access to every user. Use this rule to set up a workspace that all users of a platform instance can access.

# Platform Configuration

This page lists configuration settings for the Tenzir Platform. ## Environment variables [Section titled â€œEnvironment variablesâ€](#environment-variables) The Tenzir Platform runs as a set of containers in a Docker Compose stack. Our [example files](https://github.com/tenzir/platform/tree/main/examples) pick up configuration parameters from environment variables. To configure the platform, create a `.env` file in the same directory as your `docker-compose.yaml` file and set the environment variables described below. ### General Settings [Section titled â€œGeneral Settingsâ€](#general-settings) You must configure these settings for every platform instance. ```sh # The docker image tag that you use for platform deployment. TENZIR_PLATFORM_VERSION=latest # By default, the Tenzir UI Frontend communicates directly with the Tenzir # Gateway to get the current status of all connected nodes. When you set this # to true, the UI backend proxies this communication instead. TENZIR_PLATFORM_USE_INTERNAL_WS_PROXY=false # When enabled, this setting allows users to spawn demo nodes that run inside # the same Docker Compose stack as the platform. TENZIR_PLATFORM_DISABLE_LOCAL_DEMO_NODES=true # The Docker image for running demo nodes. TENZIR_PLATFORM_DEMO_NODE_IMAGE=tenzir/tenzir-node:latest # Optional file defining the static workspace configuration for this platform. TENZIR_PLATFORM_CONFIG_FILE= # To configure administrators, provide a list of authentication rules. Every # user matching any of the provided rules becomes an administrator of this # platform instance and can use the `tenzir-platform admin` CLI commands. Use # the `tenzir-platform tools print-auth-rule` CLI command to get valid rules. TENZIR_PLATFORM_ADMIN_RULES=[] # A random string used to encrypt frontend cookies. # Generate with `openssl rand -hex 32`. TENZIR_PLATFORM_INTERNAL_AUTH_SECRET= # A random string used to generate user keys. # Generate with `openssl rand 32 | base64`. TENZIR_PLATFORM_INTERNAL_TENANT_TOKEN_ENCRYPTION_KEY= # A random string the app uses to access restricted API endpoints. # Generate with `openssl rand -hex 32`. TENZIR_PLATFORM_INTERNAL_APP_API_KEY= ``` ### External Connectivity [Section titled â€œExternal Connectivityâ€](#external-connectivity) These settings define the outward-facing interface of the Tenzir Platform. ```sh # The domain where users can reach the Tenzir UI, e.g., # `https://app.tenzir.example`. Route this to the `app` service through your # external HTTPS proxy. TENZIR_PLATFORM_UI_ENDPOINT= # The domain where the API is reachable, e.g., `https://api.tenzir.example`. # Route this to the `platform` service through your external HTTPS proxy. TENZIR_PLATFORM_API_ENDPOINT= # The endpoint where Tenzir nodes connect. Use a URL with `ws://` or `wss://` # scheme, e.g., `wss://nodes.tenzir.example`. Route this to the # `websocket-gateway` service through your external HTTPS proxy. TENZIR_PLATFORM_NODES_ENDPOINT= # The URL where the platform exposes blob storage, e.g., # `https://downloads.tenzir.example`. If you use the bundled blob storage, # route this to the `seaweed` service through your external HTTPS proxy. TENZIR_PLATFORM_DOWNLOADS_ENDPOINT= ``` ### Identity Provider [Section titled â€œIdentity Providerâ€](#identity-provider) Create OAuth clients for the Tenzir Platform in your identity provider and fill in the values below to enable platform connectivity. ```sh # A short identifier for the OIDC provider (e.g., 'auth0', 'keycloak') TENZIR_PLATFORM_OIDC_PROVIDER_NAME= # The OIDC provider for platform authentication. TENZIR_PLATFORM_OIDC_PROVIDER_ISSUER_URL= # A JSON object (or array of objects) containing the OIDC issuer and audiences that the platform # accepts. Example: '{"issuer": "keycloak.example.org", "audiences": ["tenzir_platform"]}' TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES= # The client ID for the Tenzir Platform CLI. TENZIR_PLATFORM_OIDC_CLI_CLIENT_ID= # The client ID and client secret for the Tenzir UI. TENZIR_PLATFORM_OIDC_UI_CLIENT_ID= TENZIR_PLATFORM_OIDC_UI_CLIENT_SECRET= ``` ### Database [Section titled â€œDatabaseâ€](#database) You need to specify the following environment variables so the Tenzir Platform can connect to a postgres instance. ```sh TENZIR_PLATFORM_POSTGRES_USER= TENZIR_PLATFORM_POSTGRES_PASSWORD= TENZIR_PLATFORM_POSTGRES_DB= TENZIR_PLATFORM_POSTGRES_HOSTNAME= ``` ### Blob Storage [Section titled â€œBlob Storageâ€](#blob-storage) ```sh # When using S3 or another external blob storage, create the bucket and provide # a valid access key with read and write permissions. When using the bundled # Seaweed instance, set these values to arbitrary strings. TENZIR_PLATFORM_INTERNAL_BUCKET_NAME= TENZIR_PLATFORM_INTERNAL_ACCESS_KEY_ID= TENZIR_PLATFORM_INTERNAL_SECRET_ACCESS_KEY= ``` ## Configuration File [Section titled â€œConfiguration Fileâ€](#configuration-file) Currently, the configuration file supports only static workspace configuration. ```yaml --- workspaces: static0: # The name of this workspace name: Tenzir # The category for this workspace in the workspace switcher. category: Statically Configured Workspaces # The icon to use for this workspace. icon-url: https://storage.googleapis.com/tenzir-public-data/icons/tenzir-logo-square.svg # Nodes use this token to connect to the workspace as ephemeral nodes. token: wsk_e9ee76d4faf4b213745dd5c99a9be11f501d7009ded63f2d5NmDS38vXR # - or - # token-file: /path/to/token # All users can access this workspace. auth-rules: - { "auth_fn": "auth_allow_all" } # Example dashboard definition. dashboards: dashboard1: name: Example Dashboard cells: - name: Dashboard 1 definition: | partitions where not internal summarize events=sum(events), schema sort -events type: table x: 0 y: 0 w: 12 h: 12 ```

# Test Framework

The [`tenzir-test`](https://github.com/tenzir/test) harness discovers and runs integration tests for pipelines, fixtures, and custom runners. Use this page as a reference for concepts, configuration, and CLI details. For step-by-step walkthroughs, see the guides for [writing tests](/guides/testing/write-tests), [creating fixtures](/guides/testing/create-fixtures), and [adding custom runners](/guides/testing/add-custom-runners). ## Install [Section titled â€œInstallâ€](#install) `tenzir-test` ships as a Python package that requires Python 3.12 or later. Install it with [`uv`](https://docs.astral.sh/uv/) (or `pip`) and verify the console script: ```sh uv add tenzir-test uvx tenzir-test --help ``` ## Core concepts [Section titled â€œCore conceptsâ€](#core-concepts) * **Project root** â€“ Directory passed to `--root`; typically contains `fixtures/`, `inputs/`, `runners/`, and `tests/`. * **Mode** â€“ Auto-detected as *project* or *package*. A `package.yaml` in the current directory (or its parent when you run from `<package>/tests`) switches to package mode. * **Test** â€“ Any supported file under `tests/`; frontmatter controls execution. * **Runner** â€“ Named strategy that executes a test (`tenzir`, `python`, custom entries). * **Fixture** â€“ Reusable environment provider registered under `fixtures/` and requested via frontmatter. * **Suite** â€“ Directory-owned group of tests that share fixtures and run sequentially. Declare it with `suite:` in a `test.yaml`; all descendants join automatically. * **Input** â€“ Data accessed with `TENZIR_INPUTS`; defaults to `<root>/inputs` but you can override it per directory or per test with an `inputs:` setting. * **Scratch directory** â€“ Ephemeral workspace exposed as `TENZIR_TMP_DIR` during each test run. * **Artifact / Baseline** â€“ Runner output persisted next to the test; regenerate with `--update`. * **Configuration sources** â€“ Frontmatter plus inherited `test.yaml` files; `tenzir.yaml` still configures the Tenzir binary. A typical project layout looks like this: ```text project-root/ â”œâ”€â”€ fixtures/ â”‚ â””â”€â”€ __init__.py â”œâ”€â”€ inputs/ â”‚ â””â”€â”€ sample.ndjson â”œâ”€â”€ runners/ â”‚ â””â”€â”€ __init__.py â””â”€â”€ tests/ â”œâ”€â”€ alerts/ â”‚ â”œâ”€â”€ sample.tql â”‚ â””â”€â”€ sample.txt â””â”€â”€ python/ â””â”€â”€ quick-check.py ``` For a package layout (with `package.yaml`), the structure may look like: ```text my-package/ â”œâ”€â”€ package.yaml â”œâ”€â”€ operators/ â”‚ â””â”€â”€ custom-op.tql â”œâ”€â”€ pipelines/ â”‚ â””â”€â”€ smoke.tql â””â”€â”€ tests/ â”œâ”€â”€ inputs/ â”‚ â””â”€â”€ sample.ndjson â”œâ”€â”€ fixtures/ â”‚ â””â”€â”€ __init__.py â”œâ”€â”€ runners/ â”‚ â””â”€â”€ __init__.py â””â”€â”€ pipelines/ â”œâ”€â”€ custom-op.tql â””â”€â”€ custom-op.txt ``` ## Execution modes and packages [Section titled â€œExecution modes and packagesâ€](#execution-modes-and-packages) * The harness treats `--root` as the project root. If that directory (or its parent when named `tests`) contains `package.yaml`, `tenzir-test` switches to **package mode** and exposes: * `TENZIR_PACKAGE_ROOT` â€“ Absolute package directory. * `TENZIR_INPUTS` â€“ `<package>/tests/inputs/` unless a directory `test.yaml` or the test frontmatter overrides it. * `--package-dirs=<package>` â€“ Passed automatically to the `tenzir` binary. * Without a manifest the harness stays in **project mode**, recursively discovers tests under `tests/`, and applies global fixtures, runners, and inputs. ## CLI reference [Section titled â€œCLI referenceâ€](#cli-reference) Run the tests from the project root: ```sh uvx tenzir-test ``` Useful options: * `--tenzir-binary /path/to/tenzir`: Override binary lookup. * `--tenzir-node-binary /path/to/tenzir-node`: Override node binary path. * `--update`: Rewrite reference artifacts next to each test. * `--purge`: Remove generated artifacts (diffs, text outputs) from previous runs. * `--jobs N`: Control concurrency (`4 * CPU cores` by default). * `--coverage` and `--coverage-source-dir`: Enable LLVM coverage. * `-k`, `--keep`: Preserve per-test scratch directories instead of deleting them (same as setting `TENZIR_KEEP_TMP_DIRS=1`). * `--debug`: Emit framework-level diagnostics (fixture lifecycle, discovery notes, comparison targets, etc.). The same mode is available via `TENZIR_TEST_DEBUG=1`. * `--summary`: Print the tabular breakdown and failure tree after each project. * `--diff/--no-diff`: Toggle unified diff output for failed comparisons. Diffs are shown by default; disable them when you only need aggregated statistics. * `--diff-stat/--no-diff-stat`: Show (or suppress) the per-file change counter, which summarises additions and deletions even when the diff body is hidden. * `-p`, `--passthrough`: Stream raw stdout/stderr to the terminal instead of comparing against reference artifacts. The harness forces single-job execution (overriding `--jobs` when necessary) and ignores `--update` while passthrough is active. * `-a`, `--all-projects`: Run the root project together with any satellites provided on the command line. Set `TENZIR_TEST_DEBUG=1` in CI when you want the same diagnostics without passing `--debug` on the command line. ## Selections [Section titled â€œSelectionsâ€](#selections) A *selection* is the ordered list of positional paths you pass after the CLI flags. Each element can point to a single test file, a directory, or an entire project. The harness resolves every element relative to the current working directory first and then relative to the root project. How you shape the selection determines which projects run: * No positional arguments â†’ run every test in the root project. * Paths inside the root project â†’ run only those targets (plus any explicitly named satellites). * Paths that resolve to satellite projects â†’ run those satellites, skipping the root unless you also request it. Use `--all-projects` when you want the root project to execute alongside a selection that only names satellites. This keeps the CLI predictable: the selection lists the exact satellites you care about, and the flag opts the root back in without duplicating its path on the command line. ## Suites [Section titled â€œSuitesâ€](#suites) Suites let you run several tests under one shared fixture lifecycle. Declare a suite in a directory-level `test.yaml`; the definition applies to every test under that directory, including nested subdirectories. tests/http/test.yaml ```yaml suite: smoke-http fixtures: [http] timeout: 45 ``` Key rules: * Suites are directory-owned. Once a `test.yaml` sets `suite`, all descendants belong to that suite. Put tests that should remain independent outside the suite directory or in a sibling directory with a different suite. * Per-test frontmatter may not declare `suite`. * Suite members inherit the directory defaults and can still override most keys on a per-file basis. The exceptions are `fixtures` and `retry`, which must be defined at the directory level once a suite is active so every member agrees on the shared lifecycle. Outside suites you can still set those keys directly in frontmatter. * The harness runs suite members sequentially in lexicographic order of their relative paths. Each suite occupies a single worker, but different suites can run in parallel when `--jobs` allows it. * The CLI executes all suites before any remaining standalone tests so shared fixtures start and stop predictably. * Run the directory that defines the suite (for example `tenzir-test tests/http`) when you want to focus on it. Selecting an individual member now raises an error so every run exercises the full lifecycle and shared fixtures stay in sync. ## Run a subset of tests [Section titled â€œRun a subset of testsâ€](#run-a-subset-of-tests) ```sh uvx tenzir-test tests/alerts/high-severity.tql ``` You can list multiple paths in a single invocation. `tenzir-test` wires every argument into the same runner and fixture registry, so you can mix scenarios from the project and external checkouts: ```sh uvx tenzir-test tests/alerts ../contrib/plugins/*/tests ``` ### Run multiple projects with one command [Section titled â€œRun multiple projects with one commandâ€](#run-multiple-projects-with-one-command) Pass additional project directories after `--root` to execute several projects in one go. Include `--all-projects` so the root executes next to its satellites. The directory given to `--root` acts as the **root project**; all other directories are treated as **satellites**: ```sh uvx tenzir-test --root example-project --all-projects example-satellite ``` Key rules: * The root project provides the baseline configuration (fixtures, runners, `test.yaml` defaults, inputs). Satellites layer their own fixtures and runners on top; duplicate names raise an error so conflicts surface early. * Paths printed in the CLI summary are relative to the working directory. The harness announces each project before running it and lists the runner mix per project for quick insight. * You can target subsets inside each project with additional positional arguments (`tenzir-test --root main --all-projects secondary tests/smoke`). When you skip `--root` entirely and only list satellite directories, the harness runs those satellites in isolation. * Satellites keep their own `tests/`, `inputs/`, `fixtures/`, and `runners/` folders. A root project can host shared assets that satellites reuse without duplicationâ€”for example, the example repository includes an `example-satellite/` directory that consumes the `xxd` runner exported by the root project while defining a satellite-specific fixture. To regenerate baselines while targeting a specific binary and project root: ```sh TENZIR_BINARY=/opt/tenzir/bin/tenzir \ TENZIR_NODE_BINARY=/opt/tenzir/bin/tenzir-node \ uvx tenzir-test --root tests --update ``` ## Runners [Section titled â€œRunnersâ€](#runners) | Runner | Command/behavior | Input extension | Artifact | | -------- | -------------------------------------- | --------------- | -------- | | `tenzir` | `tenzir -f <test>` | `.tql` | `.txt` | | `python` | Execute with the active Python runtime | `.py` | `.txt` | | `shell` | `sh -eu <test>` via the harness helper | `.sh` | varies | Selection flow: 1. The harness chooses the first registered runner that claimed the file extension. 2. Default suffix mapping applies when no runner explicitly claims an extension: `.tql â†’ tenzir`, `.py â†’ python`, `.sh â†’ shell`. 3. A `runner: <name>` frontmatter entry overrides the automatic choice. 4. If no runner claims the extension and none is specified in frontmatter, the harness fails with an error instead of guessing. ### Shell runner [Section titled â€œShell runnerâ€](#shell-runner) Place scripts (for example under `tests/shell/`) with the `.sh` suffix to run them under `bash -eu` via the `shell` runner. The harness also prepends `<root>/_shell` to `PATH` so project-specific helper binaries become discoverable. The runner captures stdout and stderr (like `2>&1`) and compares the combined output with `<test>.txt`; run `tenzir-test --update path/to/test.sh` when you need to refresh the baseline. Register custom runners in `runners/__init__.py` via `tenzir_test.runners.register()` or the `@tenzir_test.runners.startup()` decorator. Use `replace=True` to override a bundled runner or `register_alias()` to publish alternate names. The [runner guide](/guides/testing/add-custom-runners) contains a full example (`XxdRunner`). ### Passthrough-aware subprocesses [Section titled â€œPassthrough-aware subprocessesâ€](#passthrough-aware-subprocesses) When passthrough mode is active the harness streams stdout/stderr directly to the terminal and skips reference comparisons. Runner implementations can respect this automatically by spawning processes through `tenzir_test.run.run_subprocess(...)`. The helper captures output when the harness needs it and inherits the parent descriptors otherwise. Pass `force_capture=True` when your runner must collect stdout even in passthrough mode. If you need to branch on the current behavior, call `tenzir_test.run.get_harness_mode()` or `tenzir_test.run.is_passthrough_enabled()`. The harness cycles between three internal modes: * `HarnessMode.COMPARE` â€“ default behavior; compare actual output with stored baselines. * `HarnessMode.UPDATE` â€“ engaged when you pass `--update`; runners should overwrite reference files. * `HarnessMode.PASSTHROUGH` â€“ enabled via `-p/--passthrough`; stream output directly without touching baselines. `get_harness_mode()` returns the current enum value so custom runners can adapt logic if needed. ## Configuration and frontmatter [Section titled â€œConfiguration and frontmatterâ€](#configuration-and-frontmatter) `tenzir-test` merges configuration sources in this order (later wins): 1. Project defaults (`test.yaml` files, applied per directory). 2. Per-test frontmatter (YAML for `.tql`/`.xxd`, `# key: value` comments for Python and shell scripts). Common frontmatter keys: | Key | Type | Default | Description | | ---------- | --------------- | --------- | ------------------------------------------------------ | | `runner` | string | by suffix | Runner name (`tenzir`, `python`, `shell`, custom). | | `fixtures` | list of strings | `[]` | Requested fixtures; use `fixture` for a single value. | | `timeout` | integer (s) | `30` | Command timeout. (`--coverage` multiplies it by five.) | | `error` | boolean | `false` | Expect a non-zero exit code. | | `skip` | string | unset | Mark the test as skipped (reason required). | | `inputs` | string | project | Override `TENZIR_INPUTS` for this directory or test. | | `retry` | integer | `1` | Total attempt budget for flaky tests (see below). | `test.yaml` files accept the same keys and apply recursively to child directories. A relative `inputs:` value resolves against the file that defines it, so `inputs: ../data` inside `tests/alerts/test.yaml` points at `tests/data/`. Frontmatter values follow the same rule and win over directory defaults. Adjacent `tenzir.yaml` files still configure the Tenzir binary; the harness appends `--config=<file>` automatically. The lookup keeps working even when you point the CLI at extra directories on the command line. `retry` represents the **total number of attempts** the harness should make before declaring the test failed. Intermediate attempts stay quiet; the final outcome line includes `attempts=N/M` whenever the budget exceeds one. Keep the value small and treat it as a temporary guardrail while you fix the underlying flakiness. ### Tenzir configuration files [Section titled â€œTenzir configuration filesâ€](#tenzir-configuration-files) * The harness inspects the directory that owns each test. If it finds `tenzir.yaml`, it appends `--config=<path>` to every invocation of the bundled `tenzir`/`tql`/`diff` runners. The path also seeds `TENZIR_CONFIG` unless you set that variable yourself. Custom runners that call the Tenzir binary should either use `run.get_test_env_and_config_args(test)` or honour the exported environment variables explicitly. * The built-in `node` fixture uses the same discovery process and starts `tenzir-node` from the directory that owns the test file, so relative paths inside `tenzir-node.yaml` resolve against the test location. See the [built-in node fixture](#built-in-node-fixture) section for precedence rules. * This lets you keep one config for CLI-driven scenarios while passing a different config to the embedded node, for example to tweak endpoints or data directories independently. ## Fixtures [Section titled â€œFixturesâ€](#fixtures) ### Declaring fixtures [Section titled â€œDeclaring fixturesâ€](#declaring-fixtures) * List fixture names in frontmatter (`fixtures: [node, http]`). Importing the project `fixtures` package is enough to register custom fixtures thanks to the side effects in `fixtures/__init__.py`. * The harness encodes requests in `TENZIR_TEST_FIXTURES` and exposes helper APIs in `tenzir_test.fixtures`: * `fixtures()` â€“ Read-only view of active fixtures. Attribute access is supported, e.g. `fixtures().node` returns `True` if the fixture was requested and raises `AttributeError` otherwise. * `acquire_fixture("name")` â€“ Manual controller for the named fixture. Use it as a context manager for automatic `start()`/`stop()` or call those methods explicitly to interleave lifecycle steps and optional hooks (for example `kill()` or `restart()`). * `require("name")` â€“ Assert that a fixture was requested. * `Executor()` â€“ Convenience wrapper that runs Tenzir commands with resolved binaries and timeout budget. Example use from a Python helper: ```python from tenzir_test.fixtures import Executor executor = Executor() result = executor.run("from_file 'inputs/events.ndjson' | where severity >= 5\n") assert result.returncode == 0 ``` ### Built-in node fixture [Section titled â€œBuilt-in node fixtureâ€](#built-in-node-fixture) * Request the fixture with `fixtures: [node]`; the harness will start `tenzir-node` with the binaries discovered for the current test. * Configuration precedence: 1. `TENZIR_NODE_CONFIG` in the environment. 2. A `tenzir-node.yaml` placed next to the test file (exported automatically). 3. The Tenzir defaults (no config file). * The node process inherits the test directory as its current working directory, letting `tenzir-node.yaml` reference files with relative paths (for example `state/` or `schemas/`). * Each controller reuses its state and cache directories across `start()`/`stop()` cycles. By default they live under the per-test scratch directory (`TENZIR_TMP_DIR/tenzir-node-*`) and are removed once the fixture context ends. Starting a fresh controller (for example in another test run) yields a brand-new workspace. * The fixture reuses other inherited arguments (for example `--package-dirs=â€¦`) but replaces any existing `--config=` flag so the node process always honours the chosen configuration file. * Tests can read `TENZIR_NODE_CLIENT_ENDPOINT`, `TENZIR_NODE_CLIENT_BINARY`, `TENZIR_NODE_CLIENT_TIMEOUT`, `TENZIR_NODE_STATE_DIRECTORY`, and `TENZIR_NODE_CACHE_DIRECTORY` from the environment to connect to the spawned node and inspect its working tree. * Pipelines launched by the bundled Tenzir runners automatically receive `--endpoint=<value>` when this fixture is active, so they talk to the transient node without additional wiring. * CLI and node configuration are independent: configure the CLI with `tenzir.yaml` and drop a `tenzir-node.yaml` (or set `TENZIR_NODE_CONFIG`) only when the node needs custom settings. ### Registering fixtures [Section titled â€œRegistering fixturesâ€](#registering-fixtures) Implement fixtures in `fixtures/` and register them with `@tenzir_test.fixture()`. Decorate a generator function, yield the environment mapping, and handle cleanup in a `finally` block: ```python from tenzir_test import fixture @fixture() def http(): server = _start_server() try: yield {"HTTP_FIXTURE_URL": server.url} finally: server.stop() ``` `@fixture` also accepts regular callables returning dictionaries, context managers, or `FixtureHandle` instances for advanced scenarios. The [fixture guide](/guides/testing/create-fixtures) demonstrates an HTTP echo server that exposes `HTTP_FIXTURE_URL` and tears down cleanly. ## Environment variables [Section titled â€œEnvironment variablesâ€](#environment-variables) `tenzir-test` recognises the following environment variables: * `TENZIR_TEST_ROOT` â€“ Default test root when `--root` is omitted. * `TENZIR_BINARY` / `TENZIR_NODE_BINARY` â€“ Override binary discovery. * `TENZIR_INPUTS` â€“ Preferred data directory. Defaults to the project inputs folder but reflects any `inputs:` override from `test.yaml` or frontmatter. * `TENZIR_KEEP_TMP_DIRS` â€“ Keep per-test scratch directories (equivalent to `--keep`). * `TENZIR_TEST_DEBUG` â€“ Enable debug logging (equivalent to `--debug`). Fixtures often publish additional variables (for example `TENZIR_NODE_CLIENT_*`, `TENZIR_NODE_STATE_DIRECTORY`, `TENZIR_NODE_CACHE_DIRECTORY`, `HTTP_FIXTURE_URL`). During execution the harness also adds transient variables such as `TENZIR_TMP_DIR` so tests and fixtures can create temporary artifacts without polluting the repository. Combine it with `--keep` (or `TENZIR_KEEP_TMP_DIRS=1`) when you need to inspect the generated files after a run. ## Baselines and artifacts [Section titled â€œBaselines and artifactsâ€](#baselines-and-artifacts) Regenerate reference output whenever behavior changes intentionally: ```sh uvx tenzir-test --update ``` `--purge` removes stale artifacts (diffs, temporary files). Keep generated `.txt` files under version control so future runs can diff against them. ## Troubleshooting [Section titled â€œTroubleshootingâ€](#troubleshooting) * **Missing binaries** â€“ Ensure `tenzir` and `tenzir-node` are on `PATH` or set `TENZIR_BINARY` / `TENZIR_NODE_BINARY` explicitly. * **Unexpected exits** â€“ Set `error: true` in frontmatter when a non-zero exit is expected. * **Skipped tests** â€“ Use `skip: reason` to document temporary skips; baseline files can stay empty. * **Noisy output** â€“ Use `--jobs 1` to serialize worker logs, and enable `--debug` (or set `TENZIR_TEST_DEBUG=1`) when you need to trace comparisons and fixture activity. ## Further reading [Section titled â€œFurther readingâ€](#further-reading) * [Write tests](/guides/testing/write-tests) * [Create fixtures](/guides/testing/create-fixtures) * [Add custom runners](/guides/testing/add-custom-runners)