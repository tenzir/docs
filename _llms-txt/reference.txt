<SYSTEM>Reference</SYSTEM>

# Functions

Functions appear in [expressions](/explanations/language/expressions) and take positional and/or named arguments, producing a value as a result of their computation. Function signatures have the following notation: ```tql f(arg1:<type>, arg2=<type>, [arg3=type]) -> <type> ``` * `arg:<type>`: positional argument * `arg=<type>`: named argument * `[arg=type]`: optional (named) argument * `-> <type>`: function return type ## Aggregation [Section titled “Aggregation”](#aggregation) ### [all](/reference/functions/all) [→](/reference/functions/all) Computes the conjunction (AND) of all grouped boolean values. ```tql all([true,true,false]) ``` ### [any](/reference/functions/any) [→](/reference/functions/any) Computes the disjunction (OR) of all grouped boolean values. ```tql any([true,false,true]) ``` ### [collect](/reference/functions/collect) [→](/reference/functions/collect) Creates a list of all non-null grouped values, preserving duplicates. ```tql collect([1,2,2,3]) ``` ### [count](/reference/functions/count) [→](/reference/functions/count) Counts the events or non-null grouped values. ```tql count([1,2,null]) ``` ### [count\_distinct](/reference/functions/count_distinct) [→](/reference/functions/count_distinct) Counts all distinct non-null grouped values. ```tql count_distinct([1,2,2,3]) ``` ### [count\_if](/reference/functions/count_if) [→](/reference/functions/count_if) Counts the events or non-null grouped values matching a given predicate. ```tql count_if([1,2,null], x => x > 1) ``` ### [distinct](/reference/functions/distinct) [→](/reference/functions/distinct) Creates a sorted list without duplicates of non-null grouped values. ```tql distinct([1,2,2,3]) ``` ### [entropy](/reference/functions/entropy) [→](/reference/functions/entropy) Computes the Shannon entropy of all grouped values. ```tql entropy([1,1,2,3]) ``` ### [first](/reference/functions/first) [→](/reference/functions/first) Takes the first non-null grouped value. ```tql first([null,2,3]) ``` ### [last](/reference/functions/last) [→](/reference/functions/last) Takes the last non-null grouped value. ```tql last([1,2,null]) ``` ### [max](/reference/functions/max) [→](/reference/functions/max) Computes the maximum of all grouped values. ```tql max([1,2,3]) ``` ### [mean](/reference/functions/mean) [→](/reference/functions/mean) Computes the mean of all grouped values. ```tql mean([1,2,3]) ``` ### [median](/reference/functions/median) [→](/reference/functions/median) Computes the approximate median of all grouped values using a t-digest algorithm. ```tql median([1,2,3,4]) ``` ### [min](/reference/functions/min) [→](/reference/functions/min) Computes the minimum of all grouped values. ```tql min([1,2,3]) ``` ### [mode](/reference/functions/mode) [→](/reference/functions/mode) Takes the most common non-null grouped value. ```tql mode([1,1,2,3]) ``` ### [otherwise](/reference/functions/otherwise) [→](/reference/functions/otherwise) Returns a `fallback` value if `primary` is `null`. ```tql x.otherwise(0) ``` ### [quantile](/reference/functions/quantile) [→](/reference/functions/quantile) Computes the specified quantile of all grouped values. ```tql quantile([1,2,3,4], q=0.5) ``` ### [stddev](/reference/functions/stddev) [→](/reference/functions/stddev) Computes the standard deviation of all grouped values. ```tql stddev([1,2,3]) ``` ### [sum](/reference/functions/sum) [→](/reference/functions/sum) Computes the sum of all values. ```tql sum([1,2,3]) ``` ### [value\_counts](/reference/functions/value_counts) [→](/reference/functions/value_counts) Returns a list of all grouped values alongside their frequency. ```tql value_counts([1,2,2,3]) ``` ### [variance](/reference/functions/variance) [→](/reference/functions/variance) Computes the variance of all grouped values. ```tql variance([1,2,3]) ``` ## Bit Operations [Section titled “Bit Operations”](#bit-operations) ### [bit\_and](/reference/functions/bit_and) [→](/reference/functions/bit_and) Computes the bit-wise AND of its arguments. ```tql bit_and(lhs, rhs) ``` ### [bit\_not](/reference/functions/bit_not) [→](/reference/functions/bit_not) Computes the bit-wise NOT of its argument. ```tql bit_not(x) ``` ### [bit\_or](/reference/functions/bit_or) [→](/reference/functions/bit_or) Computes the bit-wise OR of its arguments. ```tql bit_or(lhs, rhs) ``` ### [bit\_xor](/reference/functions/bit_xor) [→](/reference/functions/bit_xor) Computes the bit-wise XOR of its arguments. ```tql bit_xor(lhs, rhs) ``` ### [shift\_left](/reference/functions/shift_left) [→](/reference/functions/shift_left) Performs a bit-wise left shift. ```tql shift_left(lhs, rhs) ``` ### [shift\_right](/reference/functions/shift_right) [→](/reference/functions/shift_right) Performs a bit-wise right shift. ```tql shift_right(lhs, rhs) ``` ## Decoding [Section titled “Decoding”](#decoding) ### [decode\_base64](/reference/functions/decode_base64) [→](/reference/functions/decode_base64) Decodes bytes as Base64. ```tql decode_base64("VGVuemly") ``` ### [decode\_hex](/reference/functions/decode_hex) [→](/reference/functions/decode_hex) Decodes bytes from their hexadecimal representation. ```tql decode_hex("4e6f6E6365") ``` ### [decode\_url](/reference/functions/decode_url) [→](/reference/functions/decode_url) Decodes URL encoded strings. ```tql decode_url("Hello%20World") ``` ## Encoding [Section titled “Encoding”](#encoding) ### [encode\_base64](/reference/functions/encode_base64) [→](/reference/functions/encode_base64) Encodes bytes as Base64. ```tql encode_base64("Tenzir") ``` ### [encode\_hex](/reference/functions/encode_hex) [→](/reference/functions/encode_hex) Encodes bytes into their hexadecimal representation. ```tql encode_hex("Tenzir") ``` ### [encode\_url](/reference/functions/encode_url) [→](/reference/functions/encode_url) Encodes strings using URL encoding. ```tql encode_url("Hello World") ``` ## Hashing [Section titled “Hashing”](#hashing) ### [hash\_md5](/reference/functions/hash_md5) [→](/reference/functions/hash_md5) Computes an MD5 hash digest. ```tql hash_md5("foo") ``` ### [hash\_sha1](/reference/functions/hash_sha1) [→](/reference/functions/hash_sha1) Computes a SHA-1 hash digest. ```tql hash_sha1("foo") ``` ### [hash\_sha224](/reference/functions/hash_sha224) [→](/reference/functions/hash_sha224) Computes a SHA-224 hash digest. ```tql hash_sha224("foo") ``` ### [hash\_sha256](/reference/functions/hash_sha256) [→](/reference/functions/hash_sha256) Computes a SHA-256 hash digest. ```tql hash_sha256("foo") ``` ### [hash\_sha384](/reference/functions/hash_sha384) [→](/reference/functions/hash_sha384) Computes a SHA-384 hash digest. ```tql hash_sha384("foo") ``` ### [hash\_sha512](/reference/functions/hash_sha512) [→](/reference/functions/hash_sha512) Computes a SHA-512 hash digest. ```tql hash_sha512("foo") ``` ### [hash\_xxh3](/reference/functions/hash_xxh3) [→](/reference/functions/hash_xxh3) Computes an XXH3 hash digest. ```tql hash_xxh3("foo") ``` ## IP [Section titled “IP”](#ip) ### [ip\_category](/reference/functions/ip_category) [→](/reference/functions/ip_category) Returns the type classification of an IP address. ```tql ip_category(8.8.8.8) ``` ### [is\_global](/reference/functions/is_global) [→](/reference/functions/is_global) Checks whether an IP address is a global address. ```tql is_global(8.8.8.8) ``` ### [is\_link\_local](/reference/functions/is_link_local) [→](/reference/functions/is_link_local) Checks whether an IP address is a link-local address. ```tql is_link_local(169.254.1.1) ``` ### [is\_loopback](/reference/functions/is_loopback) [→](/reference/functions/is_loopback) Checks whether an IP address is a loopback address. ```tql is_loopback(127.0.0.1) ``` ### [is\_multicast](/reference/functions/is_multicast) [→](/reference/functions/is_multicast) Checks whether an IP address is a multicast address. ```tql is_multicast(224.0.0.1) ``` ### [is\_private](/reference/functions/is_private) [→](/reference/functions/is_private) Checks whether an IP address is a private address. ```tql is_private(192.168.1.1) ``` ### [is\_v4](/reference/functions/is_v4) [→](/reference/functions/is_v4) Checks whether an IP address has version number 4. ```tql is_v4(1.2.3.4) ``` ### [is\_v6](/reference/functions/is_v6) [→](/reference/functions/is_v6) Checks whether an IP address has version number 6. ```tql is_v6(::1) ``` ### [network](/reference/functions/network) [→](/reference/functions/network) Retrieves the network address of a subnet. ```tql 10.0.0.0/8.network() ``` ## List [Section titled “List”](#list) ### [add](/reference/functions/add) [→](/reference/functions/add) Adds an element into a list if it doesn't already exist (set-insertion). ```tql xs.add(y) ``` ### [append](/reference/functions/append) [→](/reference/functions/append) Inserts an element at the back of a list. ```tql xs.append(y) ``` ### [concatenate](/reference/functions/concatenate) [→](/reference/functions/concatenate) Merges two lists. ```tql concatenate(xs, ys) ``` ### [get](/reference/functions/get) [→](/reference/functions/get) Gets a field from a record or an element from a list ```tql xs.get(index, fallback) ``` ### [length](/reference/functions/length) [→](/reference/functions/length) Retrieves the length of a list. ```tql [1,2,3].length() ``` ### [map](/reference/functions/map) [→](/reference/functions/map) Maps each list element to an expression. ```tql xs.map(x => x + 3) ``` ### [prepend](/reference/functions/prepend) [→](/reference/functions/prepend) Inserts an element at the start of a list. ```tql xs.prepend(y) ``` ### [remove](/reference/functions/remove) [→](/reference/functions/remove) Removes all occurrences of an element from a list. ```tql xs.remove(y) ``` ### [sort](/reference/functions/sort) [→](/reference/functions/sort) Sorts lists and record fields. ```tql xs.sort() ``` ### [where](/reference/functions/where) [→](/reference/functions/where) Filters list elements based on a predicate. ```tql xs.where(x => x > 5) ``` ### [zip](/reference/functions/zip) [→](/reference/functions/zip) Combines two lists into a list of pairs. ```tql zip(xs, ys) ``` ## Math [Section titled “Math”](#math) ### [abs](/reference/functions/abs) [→](/reference/functions/abs) Returns the absolute value. ```tql abs(-42) ``` ### [ceil](/reference/functions/ceil) [→](/reference/functions/ceil) Computes the ceiling of a number or a time/duration with a specified unit. ```tql ceil(4.2) ``` ### [floor](/reference/functions/floor) [→](/reference/functions/floor) Computes the floor of a number or a time/duration with a specified unit. ```tql floor(4.8) ``` ### [round](/reference/functions/round) [→](/reference/functions/round) Rounds a number or a time/duration with a specified unit. ```tql round(4.6) ``` ### [sqrt](/reference/functions/sqrt) [→](/reference/functions/sqrt) Computes the square root of a number. ```tql sqrt(49) ``` ## Networking [Section titled “Networking”](#networking) ### [community\_id](/reference/functions/community_id) [→](/reference/functions/community_id) Computes the Community ID for a network connection/flow. ```tql community_id(src_ip=1.2.3.4, dst_ip=4.5.6.7, proto="tcp") ``` ### [decapsulate](/reference/functions/decapsulate) [→](/reference/functions/decapsulate) Decapsulates packet data at link, network, and transport layer. ```tql decapsulate(this) ``` ### [encrypt\_cryptopan](/reference/functions/encrypt_cryptopan) [→](/reference/functions/encrypt_cryptopan) Encrypts an IP address via Crypto-PAn. ```tql encrypt_cryptopan(1.2.3.4) ``` ## OCSF [Section titled “OCSF”](#ocsf) ### [ocsf::category\_name](/reference/functions/ocsf/category_name) [→](/reference/functions/ocsf/category_name) Returns the `category_name` for a given `category_uid`. ```tql ocsf::category_name(2) ``` ### [ocsf::category\_uid](/reference/functions/ocsf/category_uid) [→](/reference/functions/ocsf/category_uid) Returns the `category_uid` for a given `category_name`. ```tql ocsf::category_uid("Findings") ``` ### [ocsf::class\_name](/reference/functions/ocsf/class_name) [→](/reference/functions/ocsf/class_name) Returns the `class_name` for a given `class_uid`. ```tql ocsf::class_name(4003) ``` ### [ocsf::class\_uid](/reference/functions/ocsf/class_uid) [→](/reference/functions/ocsf/class_uid) Returns the `class_uid` for a given `class_name`. ```tql ocsf::class_uid("DNS Activity") ``` ### [ocsf::type\_name](/reference/functions/ocsf/type_name) [→](/reference/functions/ocsf/type_name) Returns the `type_name` for a given `type_uid`. ```tql ocsf::type_name(400704) ``` ### [ocsf::type\_uid](/reference/functions/ocsf/type_uid) [→](/reference/functions/ocsf/type_uid) Returns the `type_uid` for a given `type_name`. ```tql ocsf::type_uid("SSH Activity: Fail") ``` ## Parsing [Section titled “Parsing”](#parsing) ### [parse\_cef](/reference/functions/parse_cef) [→](/reference/functions/parse_cef) Parses a string as a CEF message ```tql string.parse_cef() ``` ### [parse\_csv](/reference/functions/parse_csv) [→](/reference/functions/parse_csv) Parses a string as CSV (Comma-Separated Values). ```tql string.parse_csv(header=["a","b"]) ``` ### [parse\_grok](/reference/functions/parse_grok) [→](/reference/functions/parse_grok) Parses a string according to a grok pattern. ```tql string.parse_grok("%{IP:client} …") ``` ### [parse\_json](/reference/functions/parse_json) [→](/reference/functions/parse_json) Parses a string as a JSON value. ```tql string.parse_json() ``` ### [parse\_kv](/reference/functions/parse_kv) [→](/reference/functions/parse_kv) Parses a string as key-value pairs. ```tql string.parse_kv() ``` ### [parse\_leef](/reference/functions/parse_leef) [→](/reference/functions/parse_leef) Parses a string as a LEEF message ```tql string.parse_leef() ``` ### [parse\_ssv](/reference/functions/parse_ssv) [→](/reference/functions/parse_ssv) Parses a string as space separated values. ```tql string.parse_ssv(header=["a","b"]) ``` ### [parse\_syslog](/reference/functions/parse_syslog) [→](/reference/functions/parse_syslog) Parses a string as a Syslog message. ```tql string.parse_syslog() ``` ### [parse\_tsv](/reference/functions/parse_tsv) [→](/reference/functions/parse_tsv) Parses a string as tab separated values. ```tql string.parse_tsv(header=["a","b"]) ``` ### [parse\_xsv](/reference/functions/parse_xsv) [→](/reference/functions/parse_xsv) Parses a string as delimiter separated values. ```tql string.parse_xsv(",", ";", "", header=["a","b"]) ``` ### [parse\_yaml](/reference/functions/parse_yaml) [→](/reference/functions/parse_yaml) Parses a string as a YAML value. ```tql string.parse_yaml() ``` ## Printing [Section titled “Printing”](#printing) ### [print\_cef](/reference/functions/print_cef) [→](/reference/functions/print_cef) Prints records as Common Event Format (CEF) messages ```tql extension.print_cef(cef_version="0", device_vendor="Tenzir", device_product="Tenzir Node", device_version="5.5.0", signature_id=id, name="description", severity="7") ``` ### [print\_csv](/reference/functions/print_csv) [→](/reference/functions/print_csv) Prints a record as a comma-separated string of values. ```tql record.print_csv() ``` ### [print\_json](/reference/functions/print_json) [→](/reference/functions/print_json) Transforms a value into a JSON string. ```tql record.print_json() ``` ### [print\_kv](/reference/functions/print_kv) [→](/reference/functions/print_kv) Prints records in a key-value format. ```tql record.print_kv() ``` ### [print\_leef](/reference/functions/print_leef) [→](/reference/functions/print_leef) Prints records as LEEF messages ```tql attributes.print_leef(vendor="Tenzir",product_name="Tenzir Node", product_name="5.5.0",event_class_id=id) ``` ### [print\_ndjson](/reference/functions/print_ndjson) [→](/reference/functions/print_ndjson) Transforms a value into a single-line JSON string. ```tql record.print_ndjson() ``` ### [print\_ssv](/reference/functions/print_ssv) [→](/reference/functions/print_ssv) Prints a record as a space-separated string of values. ```tql record.print_ssv() ``` ### [print\_tsv](/reference/functions/print_tsv) [→](/reference/functions/print_tsv) Prints a record as a tab-separated string of values. ```tql record.print_tsv() ``` ### [print\_xsv](/reference/functions/print_xsv) [→](/reference/functions/print_xsv) Prints a record as a delimited sequence of values. ```tql record.print_tsv() ``` ### [print\_yaml](/reference/functions/print_yaml) [→](/reference/functions/print_yaml) Prints a value as a YAML document. ```tql record.print_yaml() ``` ## Record [Section titled “Record”](#record) ### [get](/reference/functions/get) [→](/reference/functions/get) Gets a field from a record or an element from a list ```tql xs.get(index, fallback) ``` ### [has](/reference/functions/has) [→](/reference/functions/has) Checks whether a record has a specified field. ```tql record.has("field") ``` ### [keys](/reference/functions/keys) [→](/reference/functions/keys) Retrieves a list of field names from a record. ```tql record.keys() ``` ### [merge](/reference/functions/merge) [→](/reference/functions/merge) Combines two records into a single record by merging their fields. ```tql merge(foo, bar) ``` ### [sort](/reference/functions/sort) [→](/reference/functions/sort) Sorts lists and record fields. ```tql xs.sort() ``` ## Runtime [Section titled “Runtime”](#runtime) ### [config](/reference/functions/config) [→](/reference/functions/config) Reads Tenzir's configuration file. ```tql config() ``` ### [env](/reference/functions/env) [→](/reference/functions/env) Reads an environment variable. ```tql env("PATH") ``` ### [secret](/reference/functions/secret) [→](/reference/functions/secret) Use the value of a secret. ```tql secret("KEY") ``` ## Subnet [Section titled “Subnet”](#subnet) ### [network](/reference/functions/network) [→](/reference/functions/network) Retrieves the network address of a subnet. ```tql 10.0.0.0/8.network() ``` ## Time & Date [Section titled “Time & Date”](#time--date) ### [count\_days](/reference/functions/count_days) [→](/reference/functions/count_days) Counts the number of `days` in a duration. ```tql count_days(100d) ``` ### [count\_hours](/reference/functions/count_hours) [→](/reference/functions/count_hours) Counts the number of `hours` in a duration. ```tql count_hours(100d) ``` ### [count\_microseconds](/reference/functions/count_microseconds) [→](/reference/functions/count_microseconds) Counts the number of `microseconds` in a duration. ```tql count_microseconds(100d) ``` ### [count\_milliseconds](/reference/functions/count_milliseconds) [→](/reference/functions/count_milliseconds) Counts the number of `milliseconds` in a duration. ```tql count_milliseconds(100d) ``` ### [count\_minutes](/reference/functions/count_minutes) [→](/reference/functions/count_minutes) Counts the number of `minutes` in a duration. ```tql count_minutes(100d) ``` ### [count\_months](/reference/functions/count_months) [→](/reference/functions/count_months) Counts the number of `months` in a duration. ```tql count_months(100d) ``` ### [count\_nanoseconds](/reference/functions/count_nanoseconds) [→](/reference/functions/count_nanoseconds) Counts the number of `nanoseconds` in a duration. ```tql count_nanoseconds(100d) ``` ### [count\_seconds](/reference/functions/count_seconds) [→](/reference/functions/count_seconds) Counts the number of `seconds` in a duration. ```tql count_seconds(100d) ``` ### [count\_weeks](/reference/functions/count_weeks) [→](/reference/functions/count_weeks) Counts the number of `weeks` in a duration. ```tql count_weeks(100d) ``` ### [count\_years](/reference/functions/count_years) [→](/reference/functions/count_years) Counts the number of `years` in a duration. ```tql count_years(100d) ``` ### [day](/reference/functions/day) [→](/reference/functions/day) Extracts the day component from a timestamp. ```tql ts.day() ``` ### [days](/reference/functions/days) [→](/reference/functions/days) Converts a number to equivalent days. ```tql days(100) ``` ### [format\_time](/reference/functions/format_time) [→](/reference/functions/format_time) Formats a time into a string that follows a specific format. ```tql ts.format_time("%d/ %m/%Y") ``` ### [from\_epoch](/reference/functions/from_epoch) [→](/reference/functions/from_epoch) Interprets a duration as Unix time. ```tql from_epoch(time_ms * 1ms) ``` ### [hour](/reference/functions/hour) [→](/reference/functions/hour) Extracts the hour component from a timestamp. ```tql ts.hour() ``` ### [hours](/reference/functions/hours) [→](/reference/functions/hours) Converts a number to equivalent hours. ```tql hours(100) ``` ### [microseconds](/reference/functions/microseconds) [→](/reference/functions/microseconds) Converts a number to equivalent microseconds. ```tql microseconds(100) ``` ### [milliseconds](/reference/functions/milliseconds) [→](/reference/functions/milliseconds) Converts a number to equivalent milliseconds. ```tql milliseconds(100) ``` ### [minute](/reference/functions/minute) [→](/reference/functions/minute) Extracts the minute component from a timestamp. ```tql ts.minute() ``` ### [minutes](/reference/functions/minutes) [→](/reference/functions/minutes) Converts a number to equivalent minutes. ```tql minutes(100) ``` ### [month](/reference/functions/month) [→](/reference/functions/month) Extracts the month component from a timestamp. ```tql ts.month() ``` ### [months](/reference/functions/months) [→](/reference/functions/months) Converts a number to equivalent months. ```tql months(100) ``` ### [nanoseconds](/reference/functions/nanoseconds) [→](/reference/functions/nanoseconds) Converts a number to equivalent nanoseconds. ```tql nanoseconds(100) ``` ### [now](/reference/functions/now) [→](/reference/functions/now) Gets the current wallclock time. ```tql now() ``` ### [parse\_time](/reference/functions/parse_time) [→](/reference/functions/parse_time) Parses a time from a string that follows a specific format. ```tql "10/11/2012".parse_time("%d/%m/%Y") ``` ### [second](/reference/functions/second) [→](/reference/functions/second) Extracts the second component from a timestamp with subsecond precision. ```tql ts.second() ``` ### [seconds](/reference/functions/seconds) [→](/reference/functions/seconds) Converts a number to equivalent seconds. ```tql seconds(100) ``` ### [since\_epoch](/reference/functions/since_epoch) [→](/reference/functions/since_epoch) Interprets a time value as duration since the Unix epoch. ```tql since_epoch(2021-02-24) ``` ### [weeks](/reference/functions/weeks) [→](/reference/functions/weeks) Converts a number to equivalent weeks. ```tql weeks(100) ``` ### [year](/reference/functions/year) [→](/reference/functions/year) Extracts the year component from a timestamp. ```tql ts.year() ``` ### [years](/reference/functions/years) [→](/reference/functions/years) Converts a number to equivalent years. ```tql years(100) ``` ## Utility [Section titled “Utility”](#utility) ### [contains](/reference/functions/contains) [→](/reference/functions/contains) Searches for a value within data structures recursively. ```tql this.contains("value") ``` ### [contains\_null](/reference/functions/contains_null) [→](/reference/functions/contains_null) Checks whether the input contains any `null` values. ```tql {x: 1, y: null}.contains_null() == true ``` ### [is\_empty](/reference/functions/is_empty) [→](/reference/functions/is_empty) Checks whether a value is empty. ```tql "".is_empty() ``` ### [random](/reference/functions/random) [→](/reference/functions/random) Generates a random number in *\[0,1]*. ```tql random() ``` ### [uuid](/reference/functions/uuid) [→](/reference/functions/uuid) Generates a Universally Unique Identifier (UUID) string. ```tql uuid() ``` ## String [Section titled “String”](#string) ### Filesystem [Section titled “Filesystem”](#filesystem) ### [file\_contents](/reference/functions/file_contents) [→](/reference/functions/file_contents) Reads a file's contents. ```tql file_contents("/path/to/file") ``` ### [file\_name](/reference/functions/file_name) [→](/reference/functions/file_name) Extracts the file name from a file path. ```tql file_name("/path/to/log.json") ``` ### [parent\_dir](/reference/functions/parent_dir) [→](/reference/functions/parent_dir) Extracts the parent directory from a file path. ```tql parent_dir("/path/to/log.json") ``` ### Inspection [Section titled “Inspection”](#inspection) ### [ends\_with](/reference/functions/ends_with) [→](/reference/functions/ends_with) Checks if a string ends with a specified substring. ```tql "hello".ends_with("lo") ``` ### [is\_alnum](/reference/functions/is_alnum) [→](/reference/functions/is_alnum) Checks if a string is alphanumeric. ```tql "hello123".is_alnum() ``` ### [is\_alpha](/reference/functions/is_alpha) [→](/reference/functions/is_alpha) Checks if a string contains only alphabetic characters. ```tql "hello".is_alpha() ``` ### [is\_lower](/reference/functions/is_lower) [→](/reference/functions/is_lower) Checks if a string is in lowercase. ```tql "hello".is_lower() ``` ### [is\_numeric](/reference/functions/is_numeric) [→](/reference/functions/is_numeric) Checks if a string contains only numeric characters. ```tql "1234".is_numeric() ``` ### [is\_printable](/reference/functions/is_printable) [→](/reference/functions/is_printable) Checks if a string contains only printable characters. ```tql "hello".is_printable() ``` ### [is\_title](/reference/functions/is_title) [→](/reference/functions/is_title) Checks if a string follows title case. ```tql "Hello World".is_title() ``` ### [is\_upper](/reference/functions/is_upper) [→](/reference/functions/is_upper) Checks if a string is in uppercase. ```tql "HELLO".is_upper() ``` ### [length\_bytes](/reference/functions/length_bytes) [→](/reference/functions/length_bytes) Returns the length of a string in bytes. ```tql "hello".length_bytes() ``` ### [length\_chars](/reference/functions/length_chars) [→](/reference/functions/length_chars) Returns the length of a string in characters. ```tql "hello".length_chars() ``` ### [match\_regex](/reference/functions/match_regex) [→](/reference/functions/match_regex) Checks if a string partially matches a regular expression. ```tql "Hi".match_regex("[Hh]i") ``` ### [slice](/reference/functions/slice) [→](/reference/functions/slice) Slices a string with offsets and strides. ```tql "Hi".slice(begin=2, stride=4) ``` ### [starts\_with](/reference/functions/starts_with) [→](/reference/functions/starts_with) Checks if a string starts with a specified substring. ```tql "hello".starts_with("he") ``` ### Transformation [Section titled “Transformation”](#transformation) ### [capitalize](/reference/functions/capitalize) [→](/reference/functions/capitalize) Capitalizes the first character of a string. ```tql "hello".capitalize() ``` ### [join](/reference/functions/join) [→](/reference/functions/join) Joins a list of strings into a single string using a separator. ```tql join(["a", "b", "c"], ",") ``` ### [pad\_end](/reference/functions/pad_end) [→](/reference/functions/pad_end) Pads a string at the end to a specified length. ```tql "hello".pad_end(10) ``` ### [pad\_start](/reference/functions/pad_start) [→](/reference/functions/pad_start) Pads a string at the start to a specified length. ```tql "hello".pad_start(10) ``` ### [replace](/reference/functions/replace) [→](/reference/functions/replace) Replaces characters within a string. ```tql "hello".replace("o", "a") ``` ### [replace\_regex](/reference/functions/replace_regex) [→](/reference/functions/replace_regex) Replaces characters within a string based on a regular expression. ```tql "hello".replace("l+o", "y") ``` ### [reverse](/reference/functions/reverse) [→](/reference/functions/reverse) Reverses the characters of a string. ```tql "hello".reverse() ``` ### [split](/reference/functions/split) [→](/reference/functions/split) Splits a string into substrings. ```tql split("a,b,c", ",") ``` ### [split\_regex](/reference/functions/split_regex) [→](/reference/functions/split_regex) Splits a string into substrings with a regex. ```tql split_regex("a1b2c", r"\d") ``` ### [to\_lower](/reference/functions/to_lower) [→](/reference/functions/to_lower) Converts a string to lowercase. ```tql "HELLO".to_lower() ``` ### [to\_title](/reference/functions/to_title) [→](/reference/functions/to_title) Converts a string to title case. ```tql "hello world".to_title() ``` ### [to\_upper](/reference/functions/to_upper) [→](/reference/functions/to_upper) Converts a string to uppercase. ```tql "hello".to_upper() ``` ### [trim](/reference/functions/trim) [→](/reference/functions/trim) Trims whitespace or specified characters from both ends of a string. ```tql " hello ".trim() ``` ### [trim\_end](/reference/functions/trim_end) [→](/reference/functions/trim_end) Trims whitespace or specified characters from the end of a string. ```tql "hello ".trim_end() ``` ### [trim\_start](/reference/functions/trim_start) [→](/reference/functions/trim_start) Trims whitespace or specified characters from the start of a string. ```tql " hello".trim_start() ``` ## Type System [Section titled “Type System”](#type-system) ### Conversion [Section titled “Conversion”](#conversion) ### [duration](/reference/functions/duration) [→](/reference/functions/duration) Casts an expression to a duration value. ```tql duration("1.34w") ``` ### [float](/reference/functions/float) [→](/reference/functions/float) Casts an expression to a float. ```tql float(42) ``` ### [int](/reference/functions/int) [→](/reference/functions/int) Casts an expression to an integer. ```tql int(-4.2) ``` ### [ip](/reference/functions/ip) [→](/reference/functions/ip) Casts an expression to an IP address. ```tql ip("1.2.3.4") ``` ### [string](/reference/functions/string) [→](/reference/functions/string) Casts an expression to a string. ```tql string(1.2.3.4) ``` ### [subnet](/reference/functions/subnet) [→](/reference/functions/subnet) Casts an expression to a subnet value. ```tql subnet("1.2.3.4/16") ``` ### [time](/reference/functions/time) [→](/reference/functions/time) Casts an expression to a time value. ```tql time("2020-03-15") ``` ### [uint](/reference/functions/uint) [→](/reference/functions/uint) Casts an expression to an unsigned integer. ```tql uint(4.2) ``` ### Introspection [Section titled “Introspection”](#introspection) ### [type\_id](/reference/functions/type_id) [→](/reference/functions/type_id) Retrieves the type id of an expression. ```tql type_id(1 + 3.2) ``` ### [type\_of](/reference/functions/type_of) [→](/reference/functions/type_of) Retrieves the type definition of an expression. ```tql type_of(this) ``` ### Transposition [Section titled “Transposition”](#transposition) ### [flatten](/reference/functions/flatten) [→](/reference/functions/flatten) Flattens nested data. ```tql flatten(this) ``` ### [unflatten](/reference/functions/unflatten) [→](/reference/functions/unflatten) Unflattens nested data. ```tql unflatten(this) ```

# abs

Returns the absolute value. ```tql abs(x:number) -> number abs(x:duration) -> duration ``` ## Description [Section titled “Description”](#description) This function returns the [absolute value](https://en.wikipedia.org/wiki/Absolute_value) for a number or a duration. ### `x: duration|number` [Section titled “x: duration|number”](#x-durationnumber) The value to compute absolute value for. ## Examples [Section titled “Examples”](#examples) ```tql from {x: -13.3} x = x.abs() ``` ```tql {x: 13.3} ```

# add

Adds an element into a list if it doesn’t already exist (set-insertion). ```tql add(xs:list, x:any) -> list ``` ## Description [Section titled “Description”](#description) The `add` function returns the list `xs` with `x` added at the end, but only if `x` is not already present in the list. This performs a set-insertion operation, ensuring no duplicate values in the resulting list. ### `xs: list` [Section titled “xs: list”](#xs-list) The list to add to. ### `x: any` [Section titled “x: any”](#x-any) An element to add to the list. If this is of a type incompatible with the list, it will be considered as `null`. ## Examples [Section titled “Examples”](#examples) ### Add a new element to a list [Section titled “Add a new element to a list”](#add-a-new-element-to-a-list) ```tql from {xs: [1, 2, 3]} xs = xs.add(4) ``` ```tql {xs: [1, 2, 3, 4]} ``` ### Try to add an existing element [Section titled “Try to add an existing element”](#try-to-add-an-existing-element) ```tql from {xs: [1, 2, 3]} xs = xs.add(2) ``` ```tql {xs: [1, 2, 3]} ``` ### Add to an empty list [Section titled “Add to an empty list”](#add-to-an-empty-list) ```tql from {xs: []} xs = xs.add("hello") ``` ```tql {xs: ["hello"]} ``` ## See Also [Section titled “See Also”](#see-also) [`append`](/reference/functions/append), [`prepend`](/reference/functions/prepend), [`remove`](/reference/functions/remove) [`distinct`](/reference/functions/distinct)

# all

Computes the conjunction (AND) of all grouped boolean values. ```tql all(xs:list) -> bool ``` ## Description [Section titled “Description”](#description) The `all` function returns `true` if all values in `xs` are `true` and `false` otherwise. ### `xs: list` [Section titled “xs: list”](#xs-list) A list of boolean values. ## Examples [Section titled “Examples”](#examples) ### Check if all values are true [Section titled “Check if all values are true”](#check-if-all-values-are-true) ```tql from {x: true}, {x: true}, {x: false} summarize result=all(x) ``` ```tql {result: false} ``` ## See Also [Section titled “See Also”](#see-also) [`any`](/reference/functions/any)

# any

Computes the disjunction (OR) of all grouped boolean values. ```tql any(xs:list) -> bool ``` ## Description [Section titled “Description”](#description) The `any` function returns `true` if any value in `xs` is `true` and `false` otherwise. ### `xs: list` [Section titled “xs: list”](#xs-list) A list of boolean values. ## Examples [Section titled “Examples”](#examples) ### Check if any value is true [Section titled “Check if any value is true”](#check-if-any-value-is-true) ```tql from {x: false}, {x: false}, {x: true} summarize result=any(x) ``` ```tql {result: true} ``` ## See Also [Section titled “See Also”](#see-also) [`all`](/reference/functions/all)

# append

Inserts an element at the back of a list. ```tql append(xs:list, x:any) -> list ``` ## Description [Section titled “Description”](#description) The `append` function returns the list `xs` with `x` inserted at the end. The expression `xs.append(x)` is equivalent to `[...xs, x]`. ## Examples [Section titled “Examples”](#examples) ### Append a number to a list [Section titled “Append a number to a list”](#append-a-number-to-a-list) ```tql from {xs: [1, 2]} xs = xs.append(3) ``` ```tql {xs: [1, 2, 3]} ``` ## See Also [Section titled “See Also”](#see-also) [`add`](/reference/functions/add), [`concatenate`](/reference/functions/concatenate), [`prepend`](/reference/functions/prepend), [`remove`](/reference/functions/remove)

# bit_and

Computes the bit-wise AND of its arguments. ```tql bit_and(lhs:number, rhs:number) -> number ``` ## Description [Section titled “Description”](#description) The `bit_and` function computes the bit-wise AND of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers. ### `lhs: number` [Section titled “lhs: number”](#lhs-number) The left-hand side operand. ### `rhs: number` [Section titled “rhs: number”](#rhs-number) The right-hand side operand. ## Examples [Section titled “Examples”](#examples) ### Perform bit-wise AND on integers [Section titled “Perform bit-wise AND on integers”](#perform-bit-wise-and-on-integers) ```tql from {x: bit_and(5, 3)} ``` ```tql {x: 1} ``` ## See Also [Section titled “See Also”](#see-also) [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_not

Computes the bit-wise NOT of its argument. ```tql bit_not(x:number) -> number ``` ## Description [Section titled “Description”](#description) The `bit_not` function computes the bit-wise NOT of `x`. The operation inverts each bit in the binary representation of the number. ### `x: number` [Section titled “x: number”](#x-number) The number to perform bit-wise NOT on. ## Examples [Section titled “Examples”](#examples) ### Perform bit-wise NOT on an integer [Section titled “Perform bit-wise NOT on an integer”](#perform-bit-wise-not-on-an-integer) ```tql from {x: bit_not(5)} ``` ```tql {x: -6} ``` ## See Also [Section titled “See Also”](#see-also) [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_or

Computes the bit-wise OR of its arguments. ```tql bit_or(lhs:number, rhs:number) -> number ``` ## Description [Section titled “Description”](#description) The `bit_or` function computes the bit-wise OR of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers. ### `lhs: number` [Section titled “lhs: number”](#lhs-number) The left-hand side operand. ### `rhs: number` [Section titled “rhs: number”](#rhs-number) The right-hand side operand. ## Examples [Section titled “Examples”](#examples) ### Perform bit-wise OR on integers [Section titled “Perform bit-wise OR on integers”](#perform-bit-wise-or-on-integers) ```tql from {x: bit_or(5, 3)} ``` ```tql {x: 7} ``` ## See Also [Section titled “See Also”](#see-also) [`bit_and`](/reference/functions/bit_and), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# bit_xor

Computes the bit-wise XOR of its arguments. ```tql bit_xor(lhs:number, rhs:number) -> number ``` ## Description [Section titled “Description”](#description) The `bit_xor` function computes the bit-wise XOR (exclusive OR) of `lhs` and `rhs`. The operation is performed on each corresponding bit position of the two numbers. ### `lhs: number` [Section titled “lhs: number”](#lhs-number) The left-hand side operand. ### `rhs: number` [Section titled “rhs: number”](#rhs-number) The right-hand side operand. ## Examples [Section titled “Examples”](#examples) ### Perform bit-wise XOR on integers [Section titled “Perform bit-wise XOR on integers”](#perform-bit-wise-xor-on-integers) ```tql from {x: bit_xor(5, 3)} ``` ```tql {x: 6} ``` ## See Also [Section titled “See Also”](#see-also) [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_not`](/reference/functions/bit_not), [`shift_left`](/reference/functions/shift_left), [`shift_right`](/reference/functions/shift_right)

# capitalize

Capitalizes the first character of a string. ```tql capitalize(x:string) -> string ``` ## Description [Section titled “Description”](#description) The `capitalize` function returns the input string with the first character converted to uppercase and the rest to lowercase. ## Examples [Section titled “Examples”](#examples) ### Capitalize a lowercase string [Section titled “Capitalize a lowercase string”](#capitalize-a-lowercase-string) ```tql from {x: "hello world".capitalize()} ``` ```tql {x: "Hello world"} ``` ## See Also [Section titled “See Also”](#see-also) [`to_upper`](/reference/functions/to_upper), [`to_lower`](/reference/functions/to_lower), [`to_title`](/reference/functions/to_title)

# ceil

Computes the ceiling of a number or a time/duration with a specified unit. ```tql ceil(x:number) ceil(x:time, unit:duration) ceil(x:duration, unit:duration) ``` ## Description [Section titled “Description”](#description) The `ceil` function takes the [ceiling](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions) of a number `x`. For time and duration values, use the second `unit` argument to define the rounding unit. ## Examples [Section titled “Examples”](#examples) ### Take the ceiling of integers [Section titled “Take the ceiling of integers”](#take-the-ceiling-of-integers) ```tql from { x: ceil(3.4), y: ceil(3.5), z: ceil(-3.4), } ``` ```tql { x: 4, y: 4, z: -3, } ``` ### Round time and duration values up to a unit [Section titled “Round time and duration values up to a unit”](#round-time-and-duration-values-up-to-a-unit) ```tql from { x: ceil(2024-02-24, 1y), y: ceil(10m, 1h) } ``` ```tql { x: 2025-01-01, y: 1h, } ``` ## See Also [Section titled “See Also”](#see-also) [`floor`](/reference/functions/floor), [`round`](/reference/functions/round)

# collect

Creates a list of all non-null grouped values, preserving duplicates. ```tql collect(xs:list) -> list ``` ## Description [Section titled “Description”](#description) The `collect` function returns a list of all non-null values in `xs`, including duplicates. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to collect. ## Examples [Section titled “Examples”](#examples) ### Collect values into a list [Section titled “Collect values into a list”](#collect-values-into-a-list) ```tql from {x: 1}, {x: 2}, {x: 2}, {x: 3} summarize values=collect(x) ``` ```tql {values: [1, 2, 2, 3]} ``` ## See Also [Section titled “See Also”](#see-also) [`distinct`](/reference/functions/distinct), [`sum`](/reference/functions/sum)

# community_id

Computes the Community ID for a network connection/flow. ```tql community_id(src_ip=ip, dst_ip=ip, proto=string, [src_port=int, dst_port=int, seed=int]) -> str ``` ## Description [Section titled “Description”](#description) The `community_id` function computes a unique hash digest of a network connection according to the [Community ID](https://github.com/corelight/community-id-spec) spec. The digest is useful for pivoting between multiple events that belong to the same connection. The `src_ip` and `dst_ip` parameters are required. The `proto` string is also required and must be `tcp`, `udp`, `icmp` or `icmp6`. `src_port` and `dst_port` may only be specified if the other one is. `seed` can be used to set the initial hashing seed. ## Examples [Section titled “Examples”](#examples) ### Compute a Community ID from a flow 5-tuple [Section titled “Compute a Community ID from a flow 5-tuple”](#compute-a-community-id-from-a-flow-5-tuple) ```tql from { x: community_id(src_ip=1.2.3.4, src_port=4584, dst_ip=43.3.132.3, dst_port=3483, proto="tcp") } ``` ```tql {x: "1:koNcqhFRD5kb254ZrLsdv630jCM="} ``` ### Compute a Community ID from a host pair [Section titled “Compute a Community ID from a host pair”](#compute-a-community-id-from-a-host-pair) Because source and destination port are optional, it suffices to provide two IP addreses to compute a valid Community ID. ```tql from {x: community_id(src_ip=1.2.3.4, dst_ip=43.3.132.3, proto="udp")} ``` ```tql {x: "1:7TrrMeH98PrUKC0ySu3RNmpUr48="} ```

# concatenate

Merges two lists. ```tql concatenate(xs:list, ys:list) -> list ``` ## Description [Section titled “Description”](#description) The `concatenate` function returns a list containing all elements from the lists `xs` and `ys` in order. The expression `concatenate(xs, ys)` is equivalent to `[...xs, ...ys]`. ## Examples [Section titled “Examples”](#examples) ### Concatenate two lists [Section titled “Concatenate two lists”](#concatenate-two-lists) ```tql from {xs: [1, 2], ys: [3, 4]} zs = concatenate(xs, ys) ``` ```tql { xs: [1, 2], ys: [3, 4], zs: [1, 2, 3, 4] } ``` ## See Also [Section titled “See Also”](#see-also) [`append`](/reference/functions/append), [`merge`](/reference/functions/merge), [`prepend`](/reference/functions/prepend), [`zip`](/reference/functions/zip)

# config

Reads Tenzir’s configuration file. ```tql config() -> record ``` ## Description [Section titled “Description”](#description) The `config` function retrieves Tenzir’s configuration, including values from various `tenzir.yaml` files, plugin-specific configuration files, environment variables, and command-line options. Note that the `tenzir.secrets`, `tenzir.token` and `caf` options are omitted from the returned record. The former to avoid leaking secrets, the latter as it only contains internal performance-related that are developer-facing and should not be relied upon within TQL. ## Examples [Section titled “Examples”](#examples) ### Provide a name mapping in the config file [Section titled “Provide a name mapping in the config file”](#provide-a-name-mapping-in-the-config-file) /opt/tenzir/etc/tenzir/tenzir.yaml ```yaml flags: de: 🇩🇪 us: 🇺🇸 ``` ```tql let $flags = config().flags from ( {country: "de"}, {country: "us"}, {country: "uk"}, ) select flag = $flags.get(country, "unknown") ``` ```tql {flag: "🇩🇪"} {flag: "🇺🇸"} {flag: "unknown"} ``` ## See also [Section titled “See also”](#see-also) [`env`](/reference/functions/env), [`secret`](/reference/functions/secret)

# contains

Searches for a value within data structures recursively. ```tql contains(input:any, target:any, [exact:bool]) -> bool ``` ## Description [Section titled “Description”](#description) The `contains` function returns `true` if the `target` value is found anywhere within the `input` data structure, and `false` otherwise. The search is performed recursively, meaning it will look inside nested records, lists, and other compound data structures. By default, strings match via substring search and subnets use containment checks. When `exact` is set to `true`, only exact matches are considered. ### `input: any` [Section titled “input: any”](#input-any) The data structure to search within. Can be any type including primitives, records, lists, and nested structures. ### `target: any` [Section titled “target: any”](#target-any) The value to search for. Cannot be a list or record. ### `exact: bool` (optional) [Section titled “exact: bool (optional)”](#exact-bool-optional) Controls the matching behavior: * When `false` (default): strings match via substring search, and subnets/IPs use containment checks * When `true`: only exact equality matches are considered ## Examples [Section titled “Examples”](#examples) ### Search within records [Section titled “Search within records”](#search-within-records) ```tql from {name: "Alice", age: 30, active: true} found_alice = contains(this, "Alice") found_bob = contains(this, "Bob") found_30 = contains(this, 30) ``` ```tql { name: "Alice", age: 30, active: true, found_alice: true, found_bob: false, found_30: true, } ``` ### Search within nested structures [Section titled “Search within nested structures”](#search-within-nested-structures) ```tql from {user: {profile: {name: "John", settings: {theme: "dark"}}}} found_john = contains(this, "John") found_theme = contains(user, "dark") found_missing = contains(this, "light") ``` ```tql { user: { profile: { name: "John", settings: { theme: "dark", }, }, }, found_john: true, found_theme: true, found_missing: false, } ``` ### Search within lists [Section titled “Search within lists”](#search-within-lists) ```tql from {numbers: [1, 2, 3, 42], tags: ["important", "urgent"]} found_42 = contains(numbers, 42) found_important = contains(tags, "important") found_missing = contains(numbers, 99) ``` ```tql { numbers: [1, 2, 3, 42], tags: ["important", "urgent"], found_42: true, found_important: true, found_missing: false, } ``` ### Search with numeric type compatibility [Section titled “Search with numeric type compatibility”](#search-with-numeric-type-compatibility) ```tql from {values: {int_val: 42, uint_val: 42.uint(), double_val: 42.0}} search_int = contains(values, 42) search_uint = contains(values, 42.uint()) search_double = contains(values, 42.0) ``` ```tql { values: { int_val: 42, uint_val: 42, double_val: 42.0, }, search_int: true, search_uint: true, search_double: true, } ``` ### Search in deeply nested structures [Section titled “Search in deeply nested structures”](#search-in-deeply-nested-structures) ```tql from { data: { level1: { level2: { level3: { target: "found" } } } } } deep_search = contains(data, "found") ``` ```tql { data: { level1: { level2: { level3: { target: "found", }, }, }, }, deep_search: true, } ``` ### Substring search in strings [Section titled “Substring search in strings”](#substring-search-in-strings) ```tql from {message: "Hello, World!"} substring_match = contains(message, "World") exact_match = contains(message, "World", exact=true) partial_no_match = contains(message, "Universe") exact_no_match = contains(message, "Hello, World", exact=true) ``` ```tql { message: "Hello, World!", substring_match: true, exact_match: false, partial_no_match: false, exact_no_match: false, } ``` ### Subnet and IP containment [Section titled “Subnet and IP containment”](#subnet-and-ip-containment) ```tql from {subnet: 10.0.0.0/8} contains_ip = contains(subnet, 10.1.2.3) contains_subnet = contains(subnet, 10.0.0.0/16) exact_subnet = contains(subnet, 10.0.0.0/8, exact=true) ``` ```tql { subnet: 10.0.0.0/8, contains_ip: true, contains_subnet: true, exact_subnet: true, } ``` ## See Also [Section titled “See Also”](#see-also) [`has`](/reference/functions/has), [`match_regex`](/reference/functions/match_regex)

# contains_null

Checks whether the input contains any `null` values. ```tql contains_null(x:any) -> bool ``` ## Description [Section titled “Description”](#description) The `contains_null` function checks if the input contains any `null` values recursively. ### `x: any` [Section titled “x: any”](#x-any) The input to check for `null` values. ## Examples [Section titled “Examples”](#examples) ### Check if list has null values [Section titled “Check if list has null values”](#check-if-list-has-null-values) ```tql from {x: [{a: 1}, {}]} contains_null = x.contains_null() ``` ```tql { x: [ { a: 1, }, { a: null, }, ], contains_null: true, } ``` ### Check a record with null values [Section titled “Check a record with null values”](#check-a-record-with-null-values) ```tql from {x: "foo", y: null} contains_null = this.contains_null() ``` ```tql { x: "foo", y: null, contains_null: true, } ``` ## See Also [Section titled “See Also”](#see-also) [`has`](/reference/functions/has), [`is_empty`](/reference/functions/is_empty) [`contains`](/reference/functions/contains)

# count

Counts the events or non-null grouped values. ```tql count(xs:list) -> int ``` ## Description [Section titled “Description”](#description) The `count` function returns the number of non-null values in `xs`. When used without arguments, it counts the total number of events. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to count. ## Examples [Section titled “Examples”](#examples) ### Count the number of non-null values [Section titled “Count the number of non-null values”](#count-the-number-of-non-null-values) ```tql from {x: 1}, {x: null}, {x: 2} summarize total=count(x) ``` ```tql {total: 2} ``` ## See Also [Section titled “See Also”](#see-also) [`count_distinct`](/reference/functions/count_distinct)

# count_days

Counts the number of `days` in a duration. ```tql count_days(x:duration) -> float ``` ## Description This function returns the number of days in a duration, i.e., `duration / 1d`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_distinct

Counts all distinct non-null grouped values. ```tql count_distinct(xs:list) -> int ``` ## Description [Section titled “Description”](#description) The `count_distinct` function returns the number of unique, non-null values in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to count. ## Examples [Section titled “Examples”](#examples) ### Count distinct values [Section titled “Count distinct values”](#count-distinct-values) ```tql from {x: 1}, {x: 2}, {x: 2}, {x: 3} summarize unique=count_distinct(x) ``` ```tql {unique: 3} ``` ## See Also [Section titled “See Also”](#see-also) [`count`](/reference/functions/count), [`distinct`](/reference/functions/distinct)

# count_hours

Counts the number of `hours` in a duration. ```tql count_hours(x:duration) -> float ``` ## Description This function returns the number of hours in a duration, i.e., `duration / 1h`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_if

Counts the events or non-null grouped values matching a given predicate. ```tql count_if(xs:list, predicate:any -> bool) -> int ``` ## Description [Section titled “Description”](#description) The `count_if` function returns the number of non-null values in `xs` that satisfy the given `predicate`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to count. ### `predicate: any -> bool` [Section titled “predicate: any -> bool”](#predicate-any---bool) The predicate to apply to each value to check whether it should be counted. ## Examples [Section titled “Examples”](#examples) ### Count the number of values greater than 1 [Section titled “Count the number of values greater than 1”](#count-the-number-of-values-greater-than-1) ```tql from {x: 1}, {x: null}, {x: 2} summarize total=x.count_if(x => x > 1) ``` ```tql {total: 1} ``` ## See Also [Section titled “See Also”](#see-also) [`count`](/reference/functions/count)

# count_microseconds

Counts the number of `microseconds` in a duration. ```tql count_microseconds(x:duration) -> float ``` ## Description This function returns the number of microseconds in a duration, i.e., `duration / 1us`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_milliseconds

Counts the number of `milliseconds` in a duration. ```tql count_milliseconds(x:duration) -> float ``` ## Description This function returns the number of milliseconds in a duration, i.e., `duration / 1ms`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_minutes

Counts the number of `minutes` in a duration. ```tql count_minutes(x:duration) -> float ``` ## Description This function returns the number of minutes in a duration, i.e., `duration / 1min`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_months

Counts the number of `months` in a duration. ```tql count_months(x:duration) -> float ``` ## Description This function returns the number of months in a duration, i.e., `duration / 1/12 * 1y`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_nanoseconds

Counts the number of `nanoseconds` in a duration. ```tql count_nanoseconds(x:duration) -> int ``` ## Description This function returns the number of nanoseconds in a duration, i.e., `duration / 1ns`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_seconds

Counts the number of `seconds` in a duration. ```tql count_seconds(x:duration) -> float ``` ## Description This function returns the number of seconds in a duration, i.e., `duration / 1s`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_weeks

Counts the number of `weeks` in a duration. ```tql count_weeks(x:duration) -> float ``` ## Description This function returns the number of weeks in a duration, i.e., `duration / 1w`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# count_years

Counts the number of `years` in a duration. ```tql count_years(x:duration) -> float ``` ## Description This function returns the number of years in a duration, i.e., `duration / 1y`. ### `x: duration` The duration to count in. ## See Also [`count_years`](/reference/functions/count_years), [`count_months`](/reference/functions/count_months), [`count_weeks`](/reference/functions/count_weeks), [`count_days`](/reference/functions/count_days), [`count_hours`](/reference/functions/count_hours), [`count_minutes`](/reference/functions/count_minutes), [`count_seconds`](/reference/functions/count_seconds), [`count_milliseconds`](/reference/functions/count_milliseconds), [`count_microseconds`](/reference/functions/count_microseconds), [`count_nanoseconds`](/reference/functions/count_nanoseconds)

# day

Extracts the day component from a timestamp. ```tql day(x: time) -> int ``` ## Description [Section titled “Description”](#description) The `day` function extracts the day component from a timestamp as an integer (1-31). ### `x: time` [Section titled “x: time”](#x-time) The timestamp from which to extract the day. ## Examples [Section titled “Examples”](#examples) ### Extract the day from a timestamp [Section titled “Extract the day from a timestamp”](#extract-the-day-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } day = ts.day() ``` ```tql { ts: 2024-06-15T14:30:45.123456, day: 15, } ``` ## See also [Section titled “See also”](#see-also) [`year`](/reference/functions/year), [`month`](/reference/functions/month), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# days

Converts a number to equivalent days. ```tql days(x:number) -> duration ``` ## Description This function returns days equivalent to a number, i.e., `number * 1d`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# decapsulate

Decapsulates packet data at link, network, and transport layer. ```tql decapsulate(packet:record) -> record ``` ## Description [Section titled “Description”](#description) The `decapsulate` function decodes binary PCAP packet data by extracting link, network, and transport layer information. The function takes a `packet` record as argument as produced by the [`read_pcap`](/reference/operators/read_pcap) operator, which may look like this: ```tql { linktype: 1, timestamp: 2021-11-17T13:32:43.249525, captured_packet_length: 66, original_packet_length: 66, data: "ZJ7zvttmABY88f1tCABFAAA0LzBAAEAGRzjGR/dbgA6GqgBQ4HzXXzhE3N8/r4AQAfyWoQAAAQEICqMYaE9Mw7SY", } ``` This entire record serves as input to `decapsulate` since the `linktype` determines how to intepret the binary `data` field containing the raw packet data. ### VLAN Tags [Section titled “VLAN Tags”](#vlan-tags) The `decapsulate` function also extracts [802.1Q](https://en.wikipedia.org/wiki/IEEE_802.1Q) VLAN tags into a nested `vlan` record, consisting of an `outer` and `inner` field for the respective tags. The value of the VLAN tag corresponds to the 12-bit VLAN identifier (VID). Special values include `0` (frame does not carry a VLAN ID) and `0xFFF` (reserved value; sometimes wildcard match). ## Examples [Section titled “Examples”](#examples) ### Decapsulate packets from a PCAP file [Section titled “Decapsulate packets from a PCAP file”](#decapsulate-packets-from-a-pcap-file) ```tql from "/path/to/trace.pcap" this = decapsulate(this) ``` ```tql { ether: { src: "00-08-02-1C-47-AE", dst: "20-E5-2A-B6-93-F1", type: 2048, }, ip: { src: 10.12.14.101, dst: 92.119.157.10, type: 6, }, tcp: { src_port: 62589, dst_port: 4443, }, community_id: "1:tSl1HyzM7qS0o3OpbOgxQJYCKCc=", udp: null, icmp: null, } ``` If the trace contains 802.1Q traffic, then the output includes a `vlan` record: ```tql { ether: { src: "00-17-5A-ED-7A-F0", dst: "FF-FF-FF-FF-FF-FF", type: 2048, }, vlan: { outer: 1, inner: 20, }, ip: { src: 192.168.1.1, dst: 255.255.255.255, type: 1, }, icmp: { type: 8, code: 0, }, community_id: "1:1eiKaTUjqP9UT1/1yu/o0frHlCk=", } ```

# decode_base64

Decodes bytes as Base64. ```tql decode_base64(bytes: blob|string) -> blob ``` ## Description [Section titled “Description”](#description) Decodes bytes as Base64. ### `bytes: blob|string` [Section titled “bytes: blob|string”](#bytes-blobstring) The value to decode as Base64. ## Examples [Section titled “Examples”](#examples) ### Decode a Base64 encoded string [Section titled “Decode a Base64 encoded string”](#decode-a-base64-encoded-string) ```tql from {bytes: "VGVuemly"} decoded = bytes.decode_base64() ``` ```tql {bytes: "VGVuemly", decoded: "Tenzir"} ``` ## See Also [Section titled “See Also”](#see-also) [`encode_base64`](/reference/functions/encode_base64)

# decode_hex

Decodes bytes from their hexadecimal representation. ```tql decode_hex(bytes: blob|string) -> blob ``` ## Description [Section titled “Description”](#description) Decodes bytes from their hexadecimal representation. ### `bytes: blob|string` [Section titled “bytes: blob|string”](#bytes-blobstring) The value to decode. ## Examples [Section titled “Examples”](#examples) ### Decode a blob from hex [Section titled “Decode a blob from hex”](#decode-a-blob-from-hex) ```tql from {bytes: "54656E7A6972"} decoded = bytes.decode_hex() ``` ```tql {bytes: "54656E7A6972", decoded: "Tenzir"} ``` ### Decode a mixed-case hex string [Section titled “Decode a mixed-case hex string”](#decode-a-mixed-case-hex-string) ```tql from {bytes: "4e6f6E6365"} decoded = bytes.decode_hex() ``` ```tql {bytes: "4e6f6E6365", decoded: "Nonce"} ``` ## See Also [Section titled “See Also”](#see-also) [`encode_hex`](/reference/functions/encode_hex)

# decode_url

Decodes URL encoded strings. ```tql decode_url(string: blob|string) -> blob ``` ## Description [Section titled “Description”](#description) Decodes URL encoded strings or blobs, converting percent-encoded sequences back to their original characters. ### `string: blob|string` [Section titled “string: blob|string”](#string-blobstring) The URL encoded string to decode. ## Examples [Section titled “Examples”](#examples) ### Decode a URL encoded string [Section titled “Decode a URL encoded string”](#decode-a-url-encoded-string) ```tql from {input: "Hello%20World%20%26%20Special%2FChars%3F"} decoded = input.decode_url() ``` ```tql { input: "Hello%20World%20%26%20Special%2FChars%3F", decoded: "Hello World & Special/Chars?", } ``` ## See Also [Section titled “See Also”](#see-also) [`encode_url`](/reference/functions/encode_url)

# distinct

Creates a sorted list without duplicates of non-null grouped values. ```tql distinct(xs:list) -> list ``` ## Description [Section titled “Description”](#description) The `distinct` function returns a sorted list containing unique, non-null values in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to deduplicate. ## Examples [Section titled “Examples”](#examples) ### Get distinct values in a list [Section titled “Get distinct values in a list”](#get-distinct-values-in-a-list) ```tql from {x: 1}, {x: 2}, {x: 2}, {x: 3} summarize unique=distinct(x) ``` ```tql {unique: [1, 2, 3]} ``` ## See Also [Section titled “See Also”](#see-also) [`collect`](/reference/functions/collect), [`count_distinct`](/reference/functions/count_distinct), [`value_counts`](/reference/functions/value_counts)

# duration

Casts an expression to a duration value. ```tql duration(x:string) -> duration ``` ## Description [Section titled “Description”](#description) The `duration` function casts the given string `x` to a duration value. ## Examples [Section titled “Examples”](#examples) ### Cast a string to a duration [Section titled “Cast a string to a duration”](#cast-a-string-to-a-duration) ```tql from {str: "1ms"} dur = duration(str) ``` ```tql {str: "1ms", dur: 1ms} ``` ## See Also [Section titled “See Also”](#see-also) [time](/reference/functions/time)

# encode_base64

Encodes bytes as Base64. ```tql encode_base64(bytes: blob|string) -> string ``` ## Description [Section titled “Description”](#description) Encodes bytes as Base64. ### `bytes: blob|string` [Section titled “bytes: blob|string”](#bytes-blobstring) The value to encode as Base64. ## Examples [Section titled “Examples”](#examples) ### Encode a string as Base64 [Section titled “Encode a string as Base64”](#encode-a-string-as-base64) ```tql from {bytes: "Tenzir"} encoded = bytes.encode_base64() ``` ```tql {bytes: "Tenzir", encoded: "VGVuemly"} ``` ## See Also [Section titled “See Also”](#see-also) [`decode_base64`](/reference/functions/decode_base64)

# encode_hex

Encodes bytes into their hexadecimal representation. ```tql encode_hex(bytes: blob|string) -> string ``` ## Description [Section titled “Description”](#description) Encodes bytes into their hexadecimal representation. ### `bytes: blob|string` [Section titled “bytes: blob|string”](#bytes-blobstring) The value to encode. ## Examples [Section titled “Examples”](#examples) ### Encode a string to hex [Section titled “Encode a string to hex”](#encode-a-string-to-hex) ```tql from {bytes: "Tenzir"} encoded = bytes.encode_hex() ``` ```tql {bytes: "Tenzir", encoded: "54656E7A6972"} ``` ## See Also [Section titled “See Also”](#see-also) [`decode_hex`](/reference/functions/decode_hex)

# encode_url

Encodes strings using URL encoding. ```tql encode_url(bytes: blob|string) -> string ``` ## Description [Section titled “Description”](#description) Encodes strings or blobs using URL encoding, replacing special characters with their percent-encoded equivalents. ### `bytes: blob|string` [Section titled “bytes: blob|string”](#bytes-blobstring) The input to URL encode. ## Examples [Section titled “Examples”](#examples) ### Encode a string as URL encoded [Section titled “Encode a string as URL encoded”](#encode-a-string-as-url-encoded) ```tql from {input: "Hello World & Special/Chars?"} encoded = input.encode_url() ``` ```tql { input: "Hello World & Special/Chars?", encoded: "Hello%20World%20%26%20Special%2FChars%3F", } ``` ## See Also [Section titled “See Also”](#see-also) [`decode_url`](/reference/functions/decode_url)

# encrypt_cryptopan

Encrypts an IP address via Crypto-PAn. ```tql encrypt_cryptopan(address:ip, [seed=string]) ``` ## Description [Section titled “Description”](#description) The `encrypt_cryptopan` function encrypts the IP `address` using the [Crypto-PAn](https://en.wikipedia.org/wiki/Crypto-PAn) algorithm. ### `address: ip` [Section titled “address: ip”](#address-ip) The IP address to encrypt. ### `seed = string (optional)` [Section titled “seed = string (optional)”](#seed--string-optional) A 64-byte seed that describes a hexadecimal value. When the seed is shorter than 64 bytes, the function appends zeros to match the size; when it is longer, it truncates the seed. ## Examples [Section titled “Examples”](#examples) ### Encrypt IP address fields [Section titled “Encrypt IP address fields”](#encrypt-ip-address-fields) ```tql let $seed = "deadbeef" // use secret() function in practice from { src: encrypt_cryptopan(114.13.11.35, seed=$seed), dst: encrypt_cryptopan(114.56.11.200, seed=$seed), } ``` ```tql { src: 117.179.11.60, dst: 117.135.244.180, } ```

# ends_with

Checks if a string ends with a specified substring. ```tql ends_with(x:string, suffix:string) -> bool ``` ## Description [Section titled “Description”](#description) The `ends_with` function returns `true` if `x` ends with `suffix` and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a string ends with a substring [Section titled “Check if a string ends with a substring”](#check-if-a-string-ends-with-a-substring) ```tql from {x: "hello".ends_with("lo")} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`starts_with`](/reference/functions/starts_with)

# entropy

Computes the Shannon entropy of all grouped values. ```tql entropy(xs:list, [normalize=bool]) -> float ``` ## Description [Section titled “Description”](#description) The `entropy` function calculates the Shannon entropy of the values in `xs`, which measures the amount of uncertainty or randomness in the data. Higher entropy values indicate more randomness, while lower values indicate more predictability. The entropy is calculated as: `H(x) = -sum(p(x[i]) * log(p(x[i])))`, where `p(x[i])` is the probability of each unique value. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ### `normalize: bool (optional)` [Section titled “normalize: bool (optional)”](#normalize-bool-optional) Optional parameter to normalize the entropy between 0 and 1. When `true`, the entropy is divided by `log(number of unique values)`. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Compute the entropy of values [Section titled “Compute the entropy of values”](#compute-the-entropy-of-values) ```tql from {x: 1}, {x: 1}, {x: 2}, {x: 3} summarize entropy_value=entropy(x) ``` ```tql { entropy_value: 1.0397207708399179, } ``` ### Compute the normalized entropy [Section titled “Compute the normalized entropy”](#compute-the-normalized-entropy) ```tql from {x: 1}, {x: 1}, {x: 2}, {x: 3} summarize normalized_entropy=entropy(x, normalize=true) ``` ```tql { normalized_entropy: 0.946394630357186, } ``` ## See Also [Section titled “See Also”](#see-also) [`mode`](/reference/functions/mode), [`value_counts`](/reference/functions/value_counts)

# env

Reads an environment variable. ```tql env(x:string) -> string ``` ## Description [Section titled “Description”](#description) The `env` function retrieves the value of an environment variable `x`. If the variable does not exist, it returns `null`. ## Examples [Section titled “Examples”](#examples) ### Read the `PATH` environment variable [Section titled “Read the PATH environment variable”](#read-the-path-environment-variable) ```tql from {x: env("PATH")} ``` ```tql {x: "/usr/local/bin:/usr/bin:/bin"} ``` ## See also [Section titled “See also”](#see-also) [`config`](/reference/functions/config), [`secret`](/reference/functions/secret)

# file_contents

Reads a file’s contents. ```tql file_contents(path:string, [binary=bool]) -> blob|string ``` ## Description [Section titled “Description”](#description) The `file_contents` function reads a file’s contents. ### `path: string` [Section titled “path: string”](#path-string) Absolute path of file to read. ### `binary = bool (optional)` [Section titled “binary = bool (optional)”](#binary--bool-optional) Whether to read the file contents as a `blob`, instead of a `string`. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ```tql let $secops_config = file_contents("/path/to/file.json").parse_json() … to_google_secops client_email=$secops_config.client_email, … ```

# file_name

Extracts the file name from a file path. ```tql file_name(x:string) -> string ``` ## Description [Section titled “Description”](#description) The `file_name` function returns the file name component of a file path, excluding the parent directories. ## Examples [Section titled “Examples”](#examples) ### Extract the file name from a file path [Section titled “Extract the file name from a file path”](#extract-the-file-name-from-a-file-path) ```tql from {x: file_name("/path/to/log.json")} ``` ```tql {x: "log.json"} ``` ## See Also [Section titled “See Also”](#see-also) [`parent_dir`](/reference/functions/parent_dir)

# first

Takes the first non-null grouped value. ```tql first(xs:list) -> any ``` ## Description [Section titled “Description”](#description) The `first` function returns the first non-null value in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to search. ## Examples [Section titled “Examples”](#examples) ### Get the first non-null value [Section titled “Get the first non-null value”](#get-the-first-non-null-value) ```tql from {x: null}, {x: 2}, {x: 3} summarize first_value=first(x) ``` ```tql {first_value: 2} ``` ## See Also [Section titled “See Also”](#see-also) [`last`](/reference/functions/last)

# flatten

Flattens nested data. ```tql flatten(x:record, separtor=string) -> record ``` ## Description [Section titled “Description”](#description) The `flatten` function takes a record and performs actions on contained container types: 1. **Records**: Join nested records with a separator (`.` by default). For example, if a field named `x` is a record with fields `a` and `b`, flattening will lift the nested record into the parent scope by creating two new fields `x.a` and `x.b`. 2. **Lists**: Merge nested lists into a single (flat) list. For example, `[[[2]], [[3, 1]], [[4]]]` becomes `[2, 3, 1, 4]`. For records inside lists, `flatten` “pushes lists down” into one list per record field. For example, the record ```tql { foo: [ { a: 2, b: 1, }, { a: 4, }, ], } ``` becomes ```tql { "foo.a": [ 2, 4, ], "foo.b": [ 1, null, ], } ``` Lists nested in records that are nested in lists will also be flattened. For example, the record ```tql { foo: [ { a: [ [2, 23], [1,16], ], b: [1], }, { a: [[4]], }, ], } ``` becomes ```tql { "foo.a": [ 2, 23, 1, 16, 4 ], "foo.b": [ 1 ] } ``` As you can see from the above examples, flattening also removes `null` values. ### `x: record` [Section titled “x: record”](#x-record) The record you want to flatten. ### `separator: string (optional)` [Section titled “separator: string (optional)”](#separator-string-optional) The separator to use for joining field names. Defaults to `"."`. ## Examples [Section titled “Examples”](#examples) ### Flatten fields with the dot character [Section titled “Flatten fields with the dot character”](#flatten-fields-with-the-dot-character) ```tql from { src_ip: 147.32.84.165, src_port: 1141, dest_ip: 147.32.80.9, dest_port: 53, event_type: "dns", dns: { type: "query", id: 553, rrname: "irc.freenode.net", rrtype: "A", tx_id: 0, grouped: { A: [ "tenzir.com", ], }, }, } this = flatten(this) ``` ```tql { src_ip: 147.32.84.165, src_port: 1141, dest_ip: 147.32.80.9, dest_port: 53, event_type: "dns", "dns.type": "query", "dns.id": 553, "dns.rrname": "irc.freenode.net", "dns.rrtype": "A", "dns.tx_id": 0, "dns.grouped.A": ["tenzir.com"], } ``` ## See Also [Section titled “See Also”](#see-also) [`unflatten`](/reference/functions/unflatten)

# float

Casts an expression to a float. ```tql float(x:any) -> float ``` ## Description [Section titled “Description”](#description) The `float` function converts the given value `x` to a floating-point value. ## Examples [Section titled “Examples”](#examples) ### Cast an integer to a float [Section titled “Cast an integer to a float”](#cast-an-integer-to-a-float) ```tql from {x: float(42)} ``` ```tql {x: 42.0} ``` ### Cast a string to a float [Section titled “Cast a string to a float”](#cast-a-string-to-a-float) ```tql from {x: float("4.2")} ``` ```tql {x: 4.2} ``` ## See Also [Section titled “See Also”](#see-also) [`ip`](/reference/functions/ip), [`string`](/reference/functions/string), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [int](/reference/functions/int)

# floor

Computes the floor of a number or a time/duration with a specified unit. ```tql floor(x:number) floor(x:time, unit:duration) floor(x:duration, unit:duration) ``` ## Description [Section titled “Description”](#description) The `floor` function takes the [floor](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions) of a number `x`. For time and duration values, use the second `unit` argument to define the rounding unit. ## Examples [Section titled “Examples”](#examples) ### Take the floor of integers [Section titled “Take the floor of integers”](#take-the-floor-of-integers) ```tql from { x: floor(3.4), y: floor(3.5), z: floor(-3.4), } ``` ```tql { x: 3, y: 3, z: -4, } ``` ### Round time and duration values down to a unit [Section titled “Round time and duration values down to a unit”](#round-time-and-duration-values-down-to-a-unit) ```tql from { x: floor(2024-02-24, 1y), y: floor(1h52m, 1h) } ``` ```tql { x: 2024-01-01, y: 1h, } ``` ## See Also [Section titled “See Also”](#see-also) [`ceil`](/reference/functions/ceil), [`round`](/reference/functions/round)

# format_time

Formats a time into a string that follows a specific format. ```tql format_time(input: time, format: string) -> string ``` ## Description [Section titled “Description”](#description) The `format_time` function formats the given `input` time into a string by using the given `format`. ### `input: time` [Section titled “input: time”](#input-time) The input time for which a string should be constructed. ### `format: string` [Section titled “format: string”](#format-string) The string that specifies the desired output format, for example `"%m-%d-%Y"`. The allowed format specifiers are the same as for `strftime(3)`: | Specifier | Description | Example | | :-------: | :------------------------------------- | :------------------------ | | `%%` | A literal `%` character | `%` | | `%a` | Abbreviated or full weekday name | `Mon`, `Monday` | | `%A` | Equivalent to `%a` | `Mon`, `Monday` | | `%b` | Abbreviated or full month name | `Jan`, `January` | | `%B` | Equivalent to `%b` | `Jan`, `January` | | `%c` | Date and time representation | `Mon Jan 1 12:00:00 2024` | | `%C` | Century as a decimal number | `20` | | `%d` | Day of the month with zero padding | `01`, `31` | | `%D` | Equivalent to `%m/%d/%y` | `01/31/24` | | `%e` | Day of the month with space padding | `1`, `31` | | `%F` | Equivalent to `%Y-%m-%d` | `2024-01-31` | | `%g` | Last two digits of ISO week-based year | `24` | | `%G` | ISO week-based year | `2024` | | `%h` | Equivalent to `%b` | `Jan` | | `%H` | Hour in 24-hour format | `00`, `23` | | `%I` | Hour in 12-hour format | `01`, `12` | | `%j` | Day of year | `001`, `365` | | `%m` | Month number | `01`, `12` | | `%M` | Minutes | `00`, `59` | | `%n` | Newline character | `\n` | | `%p` | AM/PM designation | `AM`, `PM` | | `%r` | 12-hour clock time | `12:00:00 PM` | | `%R` | Equivalent to `%H:%M` | `23:59` | | `%S` | Seconds | `00`, `59` | | `%t` | Tab character | `\t` | | `%T` | Equivalent to `%H:%M:%S` | `23:59:59` | | `%u` | ISO weekday (Monday=1) | `1`, `7` | | `%U` | Week number (Sunday as first day) | `00`, `52` | | `%V` | ISO week number | `01`, `53` | | `%w` | Weekday (Sunday=0) | `0`, `6` | | `%W` | Week number (Monday as first day) | `00`, `52` | | `%x` | Date representation | `01/31/24` | | `%X` | Time representation | `23:59:59` | | `%y` | Year without century | `24` | | `%Y` | Year with century | `2024` | | `%z` | UTC offset | `+0000`, `-0430` | | `%Z` | Time zone abbreviation | `UTC`, `EST` | ## Examples [Section titled “Examples”](#examples) ### Format a timestamp [Section titled “Format a timestamp”](#format-a-timestamp) ```tql from { x: 2024-12-31T12:59:42, } x = x.format_time("%d.%m.%Y") ``` ```tql {x: "31.12.2024"} ``` ## See Also [Section titled “See Also”](#see-also) [`parse_time`](/reference/functions/parse_time)

# from_epoch

Interprets a duration as Unix time. ```tql from_epoch(x:duration) -> time ``` ## Description [Section titled “Description”](#description) The `from_epoch` function interprets a duration as [Unix time](https://en.wikipedia.org/wiki/Unix_time). ### `x: duration` [Section titled “x: duration”](#x-duration) The duration since the Unix epoch, i.e., 00:00:00 UTC on 1 January 1970. ## Examples [Section titled “Examples”](#examples) ### Convert an integral Unix time [Section titled “Convert an integral Unix time”](#convert-an-integral-unix-time) ```tql from {time: 1736525429} time = from_epoch(time * 1s) ``` ```tql {time: 2025-01-10T16:10:29+00:00} ``` ### Interpret a duration as Unix time [Section titled “Interpret a duration as Unix time”](#interpret-a-duration-as-unix-time) ```tql from {x: from_epoch(50y + 12w + 20m)} ``` ```tql {x: 2020-03-13T00:20:00.000000} ``` ## See Also [Section titled “See Also”](#see-also) [`now`](/reference/functions/now), [`since_epoch`](/reference/functions/since_epoch)

# get

Gets a field from a record or an element from a list ```tql get(x:record, field:string, [fallback:any]) -> any get(x:record|list, index:number, [fallback:any]) -> any ``` ## Description [Section titled “Description”](#description) The `get` function returns the record field with the name `field` or the list element with the index `index`. If `fallback` is provided, the function gracefully returns the fallback value instead of emitting a warning and returning `null`. ### `xs: record|list` [Section titled “xs: record|list”](#xs-recordlist) A `record` or list you want to access. ### `index: int`/`field: string` [Section titled “index: int/field: string”](#index-intfield-string) An index or field to access. If the function’s subject `xs` is a `list`, `index` refers to the position in the list. If the subject `xs` is a `record`, `index` refers to the field index. If the subject is a `record`, you can also use the fields name as a `string` to refer to it. If the given `index` or `field` are do not exist in the subject and no `fallback` was provided, a warning will be raised and the function will return `null`. ### `fallback: any (optional)` [Section titled “fallback: any (optional)”](#fallback-any-optional) A fallback value to return if the given `index` or `field` do not exist in the subject. Providing a `fallback` avoids a warning. ## Examples [Section titled “Examples”](#examples) ### Get the first element of a list, or a fallback value [Section titled “Get the first element of a list, or a fallback value”](#get-the-first-element-of-a-list-or-a-fallback-value) ```tql from ( {xs: [1, 2, 3]}, {xs: []}, } select first = xs.get(0, -1) ``` ```tql {first: 1} {first: -1} ``` ### Access a field of a record, or a fallback value [Section titled “Access a field of a record, or a fallback value”](#access-a-field-of-a-record-or-a-fallback-value) ```tql from ( {x: 1, y: 2}, {x: 3}, } select x = this.get("x", -1), y = this.get("y", -1) ``` ```tql {x: 1, y: 2} {x: 3, y: -1} ``` ## See Also [Section titled “See Also”](#see-also) [`keys`](/reference/functions/keys)

# has

Checks whether a record has a specified field. ```tql has(x:record, field:string) -> bool ``` ## Description [Section titled “Description”](#description) The `has` function returns `true` if the record contains the specified field and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a record has a specific field [Section titled “Check if a record has a specific field”](#check-if-a-record-has-a-specific-field) ```tql from { x: "foo", y: null, } this = { has_x: this.has("x"), has_y: this.has("y"), has_z: this.has("z"), } ``` ```tql { has_x: true, has_y: true, has_z: false, } ``` ## See Also [Section titled “See Also”](#see-also) [`is_empty`](/reference/functions/is_empty), [`keys`](/reference/functions/keys)

# hash_md5

Computes an MD5 hash digest. ```tql hash_md5(x:any, [seed=string]) ``` ## Description [Section titled “Description”](#description) The `hash` function calculates a hash digest of a given value `x`. ### `x: any` [Section titled “x: any”](#x-any) The value to hash. ### `seed = string (optional)` [Section titled “seed = string (optional)”](#seed--string-optional) The seed for the hash. ## Examples [Section titled “Examples”](#examples) ### Compute an MD5 digest of a string [Section titled “Compute an MD5 digest of a string”](#compute-an-md5-digest-of-a-string) ```tql from { x: hash_md5("foo") } ``` ```tql { x: "acbd18db4cc2f85cedef654fccc4a4d8" } ``` ## See Also [Section titled “See Also”](#see-also) [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha1

Computes a SHA-1 hash digest. ```tql hash_sha1(x:any, [seed=string]) -> string ``` ## Description [Section titled “Description”](#description) The `hash_sha1` function calculates a SHA-1 hash digest for the given value `x`. ## Examples [Section titled “Examples”](#examples) ### Compute a SHA-1 digest of a string [Section titled “Compute a SHA-1 digest of a string”](#compute-a-sha-1-digest-of-a-string) ```tql from {x: hash_sha1("foo")} ``` ```tql {x: "0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33"} ``` ## See Also [Section titled “See Also”](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha224

Computes a SHA-224 hash digest. ```tql hash_sha224(x:any, [seed=string]) -> string ``` ## Description [Section titled “Description”](#description) The `hash_sha224` function calculates a SHA-224 hash digest for the given value `x`. ## Examples [Section titled “Examples”](#examples) ### Compute a SHA-224 digest of a string [Section titled “Compute a SHA-224 digest of a string”](#compute-a-sha-224-digest-of-a-string) ```tql from {x: hash_sha224("foo")} ``` ```tql {x: "0808f64e60d58979fcb676c96ec938270dea42445aeefcd3a4e6f8db"} ``` ## See Also [Section titled “See Also”](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha256

Computes a SHA-256 hash digest. ```tql hash_sha256(x:any, [seed=string]) -> string ``` ## Description [Section titled “Description”](#description) The `hash_sha256` function calculates a SHA-256 hash digest for the given value `x`. ## Examples [Section titled “Examples”](#examples) ### Compute a SHA-256 digest of a string [Section titled “Compute a SHA-256 digest of a string”](#compute-a-sha-256-digest-of-a-string) ```tql from {x: hash_sha256("foo")} ``` ```tql {x: "2c26b46b68ffc68ff99b453c1d30413413422e6e6c8ee90c3abeac38044e8a8c1b0"} ``` ## See Also [Section titled “See Also”](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha384

Computes a SHA-384 hash digest. ```tql hash_sha384(x:any, [seed=string]) -> string ``` ## Description [Section titled “Description”](#description) The `hash_sha384` function calculates a SHA-384 hash digest for the given value `x`. ## Examples [Section titled “Examples”](#examples) ### Compute a SHA-384 digest of a string [Section titled “Compute a SHA-384 digest of a string”](#compute-a-sha-384-digest-of-a-string) ```tql from {x: hash_sha384("foo")} ``` ```tql {x: "98c11ffdfdd540676b1a137cb1a22b2a70350c9a44171d6b1180c6be5cbb2ee3f79d532c8a1dd9ef2e8e08e752a3babb"} ``` ## See Also [Section titled “See Also”](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha512`](/reference/functions/hash_sha512), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_sha512

Computes a SHA-512 hash digest. ```tql hash_sha512(x:any, [seed=string]) -> string ``` ## Description [Section titled “Description”](#description) The `hash_sha512` function calculates a SHA-512 hash digest for the given value `x`. ## Examples [Section titled “Examples”](#examples) ### Compute a SHA-512 digest of a string [Section titled “Compute a SHA-512 digest of a string”](#compute-a-sha-512-digest-of-a-string) ```tql from {x: hash_sha512("foo")} ``` ```tql {x: "f7fbba6e0636f890e56fbbf3283e524c6fa3204ae298382d624741d0dc6638326e282c41be5e4254d8820772c5518a2c5a8c0c7f7eda19594a7eb539453e1ed7"} ``` ## See Also [Section titled “See Also”](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_xxh3`](/reference/functions/hash_xxh3)

# hash_xxh3

Computes an XXH3 hash digest. ```tql hash_xxh3(x:any, [seed=string]) -> string ``` ## Description [Section titled “Description”](#description) The `hash_xxh3` function calculates a 64-bit XXH3 hash digest for the given value `x`. ## Examples [Section titled “Examples”](#examples) ### Compute an XXH3 digest of a string [Section titled “Compute an XXH3 digest of a string”](#compute-an-xxh3-digest-of-a-string) ```tql from {x: hash_xxh3("foo")} ``` ```tql {x: "ab6e5f64077e7d8a"} ``` ## See Also [Section titled “See Also”](#see-also) [`hash_md5`](/reference/functions/hash_md5), [`hash_sha1`](/reference/functions/hash_sha1), [`hash_sha224`](/reference/functions/hash_sha224), [`hash_sha256`](/reference/functions/hash_sha256), [`hash_sha384`](/reference/functions/hash_sha384), [`hash_sha512`](/reference/functions/hash_sha512)

# hour

Extracts the hour component from a timestamp. ```tql hour(x: time) -> int ``` ## Description [Section titled “Description”](#description) The `hour` function extracts the hour component from a timestamp as an integer (0-23). ### `x: time` [Section titled “x: time”](#x-time) The timestamp from which to extract the hour. ## Examples [Section titled “Examples”](#examples) ### Extract the hour from a timestamp [Section titled “Extract the hour from a timestamp”](#extract-the-hour-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } hour = ts.hour() ``` ```tql { ts: 2024-06-15T14:30:45.123456, hour: 14, } ``` ## See also [Section titled “See also”](#see-also) [`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# hours

Converts a number to equivalent hours. ```tql hours(x:number) -> duration ``` ## Description This function returns hours equivalent to a number, i.e., `number * 1h`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# int

Casts an expression to an integer. ```tql int(x:number|string, base=int) -> int ``` ## Description [Section titled “Description”](#description) The `int` function casts the provided value `x` to an integer. Non-integer values are truncated. ### `x: number|string` [Section titled “x: number|string”](#x-numberstring) The input to convert. ### `base = int` [Section titled “base = int”](#base--int) Base (radix) to parse a string as. Can be `10` or `16`. If `16`, the string inputs may be optionally prefixed by `0x` or `0X`, e.g., `-0x134`. Defaults to `10`. ## Examples [Section titled “Examples”](#examples) ### Cast a floating-point number to an integer [Section titled “Cast a floating-point number to an integer”](#cast-a-floating-point-number-to-an-integer) ```tql from {x: int(4.2)} ``` ```tql {x: 4} ``` ### Convert a string to an integer [Section titled “Convert a string to an integer”](#convert-a-string-to-an-integer) ```tql from {x: int("42")} ``` ```tql {x: 42} ``` ### Parse a hexadecimal number [Section titled “Parse a hexadecimal number”](#parse-a-hexadecimal-number) ```tql from {x: int("0x42", base=16)} ``` ```tql {x: 66} ``` ## See Also [Section titled “See Also”](#see-also) [`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [string](/reference/functions/string)

# ip

Casts an expression to an IP address. ```tql ip(x:string) -> ip ``` ## Description [Section titled “Description”](#description) The `ip` function casts the provided string `x` to an IP address. ## Examples [Section titled “Examples”](#examples) ### Cast a string to an IP address [Section titled “Cast a string to an IP address”](#cast-a-string-to-an-ip-address) ```tql from {x: ip("1.2.3.4")} ``` ```tql {x: 1.2.3.4} ``` ## See Also [Section titled “See Also”](#see-also) [`subnet`](/reference/functions/subnet), [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [int](/reference/functions/int), [string](/reference/functions/string)

# ip_category

Returns the type classification of an IP address. ```tql ip_category(x:ip) -> string ``` ## Description [Section titled “Description”](#description) The `ip_category` function returns the category classification of a given IP address `x` as a string. The possible return values are: * `"unspecified"` - Unspecified address (0.0.0.0 or ::) * `"loopback"` - Loopback address (127.0.0.0/8 or ::1) * `"link_local"` - Link-local address (169.254.0.0/16 or fe80::/10) * `"multicast"` - Multicast address (224.0.0.0/4 or ff00::/8) * `"broadcast"` - Broadcast address (255.255.255.255, IPv4 only) * `"private"` - Private address (RFC 1918 for IPv4, RFC 4193 for IPv6) * `"global"` - Global (publicly routable) address The function returns the most specific classification that applies to the address. For example, 127.0.0.1 is classified as `"loopback"` rather than just non-global. ## Examples [Section titled “Examples”](#examples) ### Get IP address types [Section titled “Get IP address types”](#get-ip-address-types) ```tql from { global_ipv4: ip_category(8.8.8.8), private_ipv4: ip_category(192.168.1.1), loopback: ip_category(127.0.0.1), multicast: ip_category(224.0.0.1), link_local: ip_category(169.254.1.1), } ``` ```tql { global_ipv4: "global", private_ipv4: "private", loopback: "loopback", multicast: "multicast", link_local: "link_local", } ``` ## See Also [Section titled “See Also”](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local)

# is_alnum

Checks if a string is alphanumeric. ```tql is_alnum(x:string) -> bool ``` ## Description [Section titled “Description”](#description) The `is_alnum` function returns `true` if `x` contains only alphanumeric characters and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a string is alphanumeric [Section titled “Check if a string is alphanumeric”](#check-if-a-string-is-alphanumeric) ```tql from {x: "hello123".is_alnum()} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`is_alpha`](/reference/functions/is_alpha), [`is_numeric`](/reference/functions/is_numeric), [`is_printable`](/reference/functions/is_printable)

# is_alpha

Checks if a string contains only alphabetic characters. ```tql is_alpha(x:string) -> bool ``` ## Description [Section titled “Description”](#description) The `is_alpha` function returns `true` if `x` contains only alphabetic characters and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a string is alphabetic [Section titled “Check if a string is alphabetic”](#check-if-a-string-is-alphabetic) ```tql from {x: "hello".is_alpha()} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`is_alnum`](/reference/functions/is_alnum), [`is_lower`](/reference/functions/is_lower), [`is_numeric`](/reference/functions/is_numeric), [`is_printable`](/reference/functions/is_printable), [`is_upper`](/reference/functions/is_upper)

# is_empty

Checks whether a value is empty. ```tql is_empty(x:string|list|record) -> bool ``` ## Description [Section titled “Description”](#description) The `is_empty` function returns `true` if the input value is empty and `false` otherwise. ### `x: string|list|record` [Section titled “x: string|list|record”](#x-stringlistrecord) The value to check for emptiness. The function works on three types: * **Strings**: Returns `true` for empty strings (`""`) * **Lists**: Returns `true` for empty lists (`[]`) * **Records**: Returns `true` for empty records (`{}`) For `null` values, the function returns `null`. For unsupported types, the function emits a warning and returns `null`. ## Examples [Section titled “Examples”](#examples) ### Check if a string is empty [Section titled “Check if a string is empty”](#check-if-a-string-is-empty) ```tql from { empty: "".is_empty(), not_empty: "hello".is_empty(), } ``` ```tql { empty: true, not_empty: false, } ``` ### Check if a list is empty [Section titled “Check if a list is empty”](#check-if-a-list-is-empty) ```tql from { empty: [].is_empty(), not_empty: [1, 2, 3].is_empty(), } ``` ```tql { empty: true, not_empty: false, } ``` ### Check if a record is empty [Section titled “Check if a record is empty”](#check-if-a-record-is-empty) ```tql from { empty: {}.is_empty(), not_empty: {a: 1, b: 2}.is_empty(), } ``` ```tql { empty: true, not_empty: false, } ``` ### Null handling [Section titled “Null handling”](#null-handling) ```tql from { result: null.is_empty(), } ``` ```tql { result: null, } ``` ## See Also [Section titled “See Also”](#see-also) [`length`](/reference/functions/length), [`has`](/reference/functions/has)

# is_global

Checks whether an IP address is a global address. ```tql is_global(x:ip) -> bool ``` ## Description [Section titled “Description”](#description) The `is_global` function checks whether a given IP address `x` is a global address. A global address is a publicly routable address that is not: * Loopback (127.0.0.0/8 for IPv4, ::1 for IPv6) * Private (RFC 1918 for IPv4, RFC 4193 for IPv6) * Link-local (169.254.0.0/16 for IPv4, fe80::/10 for IPv6) * Multicast (224.0.0.0/4 for IPv4, ff00::/8 for IPv6) * Broadcast (255.255.255.255 for IPv4) * Unspecified (0.0.0.0 for IPv4, :: for IPv6) ## Examples [Section titled “Examples”](#examples) ### Check if an IP is global [Section titled “Check if an IP is global”](#check-if-an-ip-is-global) ```tql from { global_ipv4: 8.8.8.8.is_global(), global_ipv6: 2001:4860:4860::8888.is_global(), private: 192.168.1.1.is_global(), loopback: 127.0.0.1.is_global(), link_local: 169.254.1.1.is_global(), } ``` ```tql { global_ipv4: true, global_ipv6: true, private: false, loopback: false, link_local: false, } ``` ## See Also [Section titled “See Also”](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_link_local

Checks whether an IP address is a link-local address. ```tql is_link_local(x:ip) -> bool ``` ## Description [Section titled “Description”](#description) The `is_link_local` function checks whether a given IP address `x` is a link-local address. For IPv4, link-local addresses are in the range 169.254.0.0 to 169.254.255.255 (169.254.0.0/16). For IPv6, link-local addresses are in the range fe80::/10. Link-local addresses are used for communication between nodes on the same network segment and are not routable on the internet. ## Examples [Section titled “Examples”](#examples) ### Check if an IP is link-local [Section titled “Check if an IP is link-local”](#check-if-an-ip-is-link-local) ```tql from { ipv4_link_local: 169.254.1.1.is_link_local(), ipv6_link_local: fe80::1.is_link_local(), not_link_local: 192.168.1.1.is_link_local(), } ``` ```tql { ipv4_link_local: true, ipv6_link_local: true, not_link_local: false, } ``` ## See Also [Section titled “See Also”](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`ip_category`](/reference/functions/ip_category)

# is_loopback

Checks whether an IP address is a loopback address. ```tql is_loopback(x:ip) -> bool ``` ## Description [Section titled “Description”](#description) The `is_loopback` function checks whether a given IP address `x` is a loopback address. For IPv4, loopback addresses are in the range 127.0.0.0 to 127.255.255.255 (127.0.0.0/8). For IPv6, the loopback address is ::1. ## Examples [Section titled “Examples”](#examples) ### Check if an IP is loopback [Section titled “Check if an IP is loopback”](#check-if-an-ip-is-loopback) ```tql from { ipv4_loopback: 127.0.0.1.is_loopback(), ipv6_loopback: ::1.is_loopback(), not_loopback: 8.8.8.8.is_loopback(), } ``` ```tql { ipv4_loopback: true, ipv6_loopback: true, not_loopback: false, } ``` ## See Also [Section titled “See Also”](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_lower

Checks if a string is in lowercase. ```tql is_lower(x:string) -> bool ``` ## Description [Section titled “Description”](#description) The `is_lower` function returns `true` if `x` is entirely in lowercase and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a string is lowercase [Section titled “Check if a string is lowercase”](#check-if-a-string-is-lowercase) ```tql from {x: "hello".is_lower()} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`is_alpha`](/reference/functions/is_alpha), [`is_upper`](/reference/functions/is_upper), [`to_lower`](/reference/functions/to_lower)

# is_multicast

Checks whether an IP address is a multicast address. ```tql is_multicast(x:ip) -> bool ``` ## Description [Section titled “Description”](#description) The `is_multicast` function checks whether a given IP address `x` is a multicast address. For IPv4, multicast addresses are in the range 224.0.0.0 to 239.255.255.255 (224.0.0.0/4). For IPv6, multicast addresses start with the prefix ff00::/8. ## Examples [Section titled “Examples”](#examples) ### Check if an IP is multicast [Section titled “Check if an IP is multicast”](#check-if-an-ip-is-multicast) ```tql from { ipv4_multicast: 224.0.0.1.is_multicast(), ipv6_multicast: ff02::1.is_multicast(), not_multicast: 8.8.8.8.is_multicast(), } ``` ```tql { ipv4_multicast: true, ipv6_multicast: true, not_multicast: false, } ``` ## See Also [Section titled “See Also”](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_numeric

Checks if a string contains only numeric characters. ```tql is_numeric(x:string) -> bool ``` ## Description [Section titled “Description”](#description) The `is_numeric` function returns `true` if `x` contains only numeric characters and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a string is numeric [Section titled “Check if a string is numeric”](#check-if-a-string-is-numeric) ```tql from {x: "1234".is_numeric()} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`is_alpha`](/reference/functions/is_alpha), [`is_alnum`](/reference/functions/is_alnum)

# is_printable

Checks if a string contains only printable characters. ```tql is_printable(x:string) -> bool ``` ## Description [Section titled “Description”](#description) The `is_printable` function returns `true` if `x` contains only printable characters and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a string is printable [Section titled “Check if a string is printable”](#check-if-a-string-is-printable) ```tql from {x: "hello".is_printable()} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`is_alnum`](/reference/functions/is_alnum), [`is_alpha`](/reference/functions/is_alpha)

# is_private

Checks whether an IP address is a private address. ```tql is_private(x:ip) -> bool ``` ## Description [Section titled “Description”](#description) The `is_private` function checks whether a given IP address `x` is a private address according to RFC 1918 (IPv4) and RFC 4193 (IPv6). For IPv4, private addresses are: * 10.0.0.0/8 (10.0.0.0 - 10.255.255.255) * 172.16.0.0/12 (172.16.0.0 - 172.31.255.255) * 192.168.0.0/16 (192.168.0.0 - 192.168.255.255) For IPv6, private addresses are: * fc00::/7 (Unique Local Addresses) Note: Link-local addresses (169.254.0.0/16 for IPv4 and fe80::/10 for IPv6) are **not** considered private addresses by this function. ## Examples [Section titled “Examples”](#examples) ### Check if an IP is private [Section titled “Check if an IP is private”](#check-if-an-ip-is-private) ```tql from { private_10: 10.0.0.1.is_private(), private_172: 172.16.0.1.is_private(), private_192: 192.168.1.1.is_private(), private_ipv6: fc00::1.is_private(), link_local: 169.254.1.1.is_private(), public: 8.8.8.8.is_private(), } ``` ```tql { private_10: true, private_172: true, private_192: true, private_ipv6: true, link_local: false, public: false, } ``` ## See Also [Section titled “See Also”](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_title

Checks if a string follows title case. ```tql is_title(x:string) -> bool ``` ## Description [Section titled “Description”](#description) The `is_title` function returns `true` if `x` is in title case and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a string is in title case [Section titled “Check if a string is in title case”](#check-if-a-string-is-in-title-case) ```tql from {x: "Hello World".is_title()} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`to_title`](/reference/functions/to_title)

# is_upper

Checks if a string is in uppercase. ```tql is_upper(x:string) -> bool ``` ## Description [Section titled “Description”](#description) The `is_upper` function returns `true` if `x` is entirely in uppercase; otherwise, it returns `false`. ## Examples [Section titled “Examples”](#examples) ### Check if a string is uppercase [Section titled “Check if a string is uppercase”](#check-if-a-string-is-uppercase) ```tql from {x: "HELLO".is_upper()} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`is_alpha`](/reference/functions/is_alpha), [`is_lower`](/reference/functions/is_lower), [`to_upper`](/reference/functions/to_upper)

# is_v4

Checks whether an IP address has version number 4. ```tql is_v4(x:ip) -> bool ``` ## Description [Section titled “Description”](#description) The `ipv4` function checks whether the version number of a given IP address `x` is 4. ## Examples [Section titled “Examples”](#examples) ### Check if an IP is IPv4 [Section titled “Check if an IP is IPv4”](#check-if-an-ip-is-ipv4) ```tql from { x: 1.2.3.4.is_v4(), y: ::1.is_v4(), } ``` ```tql { x: true, y: false, } ``` ## See Also [Section titled “See Also”](#see-also) [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# is_v6

Checks whether an IP address has version number 6. ```tql is_v6(x:ip) -> bool ``` ## Description [Section titled “Description”](#description) The `is_v6` function checks whether the version number of a given IP address `x` is 6. ## Examples [Section titled “Examples”](#examples) ### Check if an IP is IPv6 [Section titled “Check if an IP is IPv6”](#check-if-an-ip-is-ipv6) ```tql from { x: 1.2.3.4.is_v6(), y: ::1.is_v6(), } ``` ```tql { x: false, y: true, } ``` ## See Also [Section titled “See Also”](#see-also) [`is_v4`](/reference/functions/is_v4), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category)

# join

Joins a list of strings into a single string using a separator. ```tql join(xs:list, [separator:string]) -> string ``` ## Description [Section titled “Description”](#description) The `join` function concatenates the elements of the input list `xs` into a single string, separated by the specified `separator`. ### `xs: list` [Section titled “xs: list”](#xs-list) A list of strings to join. ### `separator: string (optional)` [Section titled “separator: string (optional)”](#separator-string-optional) The string used to separate elements in the result. Defaults to `""`. ## Examples [Section titled “Examples”](#examples) ### Join a list of strings with a comma [Section titled “Join a list of strings with a comma”](#join-a-list-of-strings-with-a-comma) ```tql from {x: join(["a", "b", "c"], "-")} ``` ```tql {x: "a-b-c"} ``` ## See Also [Section titled “See Also”](#see-also) [`split`](/reference/functions/split), [`split_regex`](/reference/functions/split_regex)

# keys

Retrieves a list of field names from a record. ```tql keys(x:record) -> list<string> ``` ## Description [Section titled “Description”](#description) The `keys` function returns a list of strings containing all field names from the input record `x`. ### `x: record` [Section titled “x: record”](#x-record) The record whose field names you want to retrieve. ## Examples [Section titled “Examples”](#examples) ### Get all field names from a record [Section titled “Get all field names from a record”](#get-all-field-names-from-a-record) ```tql from {x: 1, y: "hello", z: true} select field_names = this.keys() ``` ```tql { field_names: ["x", "y", "z"], } ``` ### Use keys to dynamically access record fields [Section titled “Use keys to dynamically access record fields”](#use-keys-to-dynamically-access-record-fields) You can combine `keys` with sorting and element access to dynamically select fields from records. For example, the following collects the values from the element with the first key alphabetically in each record: ```tql from ( {foo: 10, bar: 20, baz: 30}, {foo: 100, bar: 200}, {baz: 300, qux: 400}, {}, ) summarize first_sorted_key = this[this.keys().sort().first()]?.collect() ``` ```tql { first_sorted_key: [20, 200, 300], } ``` ### Use keys to get distribution of available fields [Section titled “Use keys to get distribution of available fields”](#use-keys-to-get-distribution-of-available-fields) ```tql from ( {foo: 10, bar: 20, baz: 30}, {foo: 100, bar: 200}, {baz: 300, qux: 400}, {}, ) select key = this.keys() unroll key top key ``` ```tql {name: "foo", count: 2} {name: "bar", count: 2} {name: "baz", count: 2} {name: "qux", count: 1} ``` ## See also [Section titled “See also”](#see-also) [`has`](/reference/functions/has), [`get`](/reference/functions/get)

# last

Takes the last non-null grouped value. ```tql last(xs:list) -> any ``` ## Description [Section titled “Description”](#description) The `last` function returns the last non-null value in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to search. ## Examples [Section titled “Examples”](#examples) ### Get the last non-null value [Section titled “Get the last non-null value”](#get-the-last-non-null-value) ```tql from {x: 1}, {x: 2}, {x: null} summarize last_value=last(x) ``` ```tql {last_value: 2} ``` ## See Also [Section titled “See Also”](#see-also) [`first`](/reference/functions/first)

# length

Retrieves the length of a list. ```tql length(xs:list) -> int ``` ## Description [Section titled “Description”](#description) The `length` function returns the number of elements in the list `xs`. ## Examples [Section titled “Examples”](#examples) ### Get the length of a list [Section titled “Get the length of a list”](#get-the-length-of-a-list) ```tql from {n: [1, 2, 3].length()} ``` ```tql {n: 3} ``` ## See Also [Section titled “See Also”](#see-also) [`is_empty`](/reference/functions/is_empty), [`length_bytes`](/reference/functions/length_bytes), [`length_chars`](/reference/functions/length_chars)

# length_bytes

Returns the length of a string in bytes. ```tql length_bytes(x:string) -> int ``` ## Description [Section titled “Description”](#description) The `length_bytes` function returns the byte length of the `x` string. ## Examples [Section titled “Examples”](#examples) ### Get the byte length of a string [Section titled “Get the byte length of a string”](#get-the-byte-length-of-a-string) For ASCII strings, the byte length is the same as the number of characters: ```tql from {x: "hello".length_bytes()} ``` ```tql {x: 5} ``` For Unicode, this may not be the case: ```tql from {x: "👻".length_bytes()} ``` ```tql {x: 4} ``` ## See Also [Section titled “See Also”](#see-also) [`length`](/reference/functions/length), [`length_chars`](/reference/functions/length_chars)

# length_chars

Returns the length of a string in characters. ```tql length_chars(x:string) -> int ``` ## Description [Section titled “Description”](#description) The `length_chars` function returns the character length of the `x` string. ## Examples [Section titled “Examples”](#examples) ### Get the character length of a string [Section titled “Get the character length of a string”](#get-the-character-length-of-a-string) For ASCII strings, the character length is the same as the number of bytes: ```tql from {x: "hello".length_chars()} ``` ```tql {x: 5} ``` For Unicode, this may not be the case: ```tql from {x: "👻".length_chars()} ``` ```tql {x: 1} ``` ## See Also [Section titled “See Also”](#see-also) [`length`](/reference/functions/length), [`length_bytes`](/reference/functions/length_bytes)

# map

Maps each list element to an expression. ```tql map(xs:list, capture:field, any->any) -> list ``` ## Description [Section titled “Description”](#description) The `map` function applies an expression to each element within a list, returning a list of the same length. ### `xs: list` [Section titled “xs: list”](#xs-list) A list of values. ### `function: any -> any` [Section titled “function: any -> any”](#function-any---any) A lambda function that is applied to each list element. If the lambda evaluates to different but compatible types for the elements of the list a unification is performed. For example, records are compatible with other records, and the resulting record will have the keys of both. If the lambda evaluates to incompatible types for different elements of the list, the largest possible group of compatible values will be chosen and all other values will be `null`. ## Examples [Section titled “Examples”](#examples) ### Check a predicate for all members of a list [Section titled “Check a predicate for all members of a list”](#check-a-predicate-for-all-members-of-a-list) ```tql from { hosts: [1.2.3.4, 127.0.0.1, 10.0.0.127] } hosts = hosts.map(x => x in 10.0.0.0/8) ``` ```tql { hosts: [false, false, true] } ``` ### Reshape a record inside a list [Section titled “Reshape a record inside a list”](#reshape-a-record-inside-a-list) ```tql from { answers: [ { rdata: 76.76.21.21, rrname: "tenzir.com" } ] } answers = answers.map(x => {hostname: x.rrname, ip: x.rdata}) ``` ```tql { answers: [ { hostname: "tenzir.com", ip: "76.76.21.21", } ] } ``` ### Null values [Section titled “Null values”](#null-values) In the below example, the first entry does not match the given grok pattern, causing `parse_grok` to emit a `null`. `map` will promote `null` values to typed null values, allowing you to still get all valid parts of the list mapped. ```tql let $pattern = "%{WORD:w} %{NUMBER:n}" from { l: ["hello", "world 42"] } l = l.map(str => str.parse_grok($pattern)) ``` ```tql { l: [ null, { w: "world", n: 42, }, ], } ``` ### Incompatible types between elements [Section titled “Incompatible types between elements”](#incompatible-types-between-elements) In the below example the list `l` contains three strings. Two of those are JSON objects and one is a JSON list. While all three can be parsed as JSON by `parse_json`, the resulting `record` and `list` are incompatible types. `map` will resolve this by picking the “largest compatible group”, in this case preferring the two `record`s over one `list`. ```tql from { l: [ r#"{ "x": 0 }"#, r#"{ "y": 0 }"#, r#"[ 3 ]"#, ] } l = l.map(str => str.parse_json()) ``` ```tql { l: [ {x: 0, y: null}, {x: null, y: 0}, null, ] } ``` ## See Also [Section titled “See Also”](#see-also) [`where`](/reference/functions/where), [`zip`](/reference/functions/zip)

# match_regex

Checks if a string partially matches a regular expression. ```tql match_regex(input:string, regex:string) -> bool ``` ## Description [Section titled “Description”](#description) The `match_regex` function returns `true` if `regex` matches a substring of `input`. To check whether the full string matches, you can use `^` and `$` to signify start and end of the string. ### `input: string` [Section titled “input: string”](#input-string) The string to partially match. ### `regex: string` [Section titled “regex: string”](#regex-string) The regular expression try and match. The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `match_regex` at the moment. ## Examples [Section titled “Examples”](#examples) ### Check contains a matching substring [Section titled “Check contains a matching substring”](#check-contains-a-matching-substring) ```tql from {input: "Hello There World"}, {input: "hi there!"}, {input: "Good Morning" } output = input.match_regex("[T|t]here") ``` ```tql {input: "Hello There World", output: true} {input: "hi there!", output: true} {input: "Good Morning", output: false} ``` ### Check if a string matches fully [Section titled “Check if a string matches fully”](#check-if-a-string-matches-fully) ```tql from {input: "example"}, {input: "Example!"}, {input: "example?" } output = input.match_regex("^[E|e]xample[!]?$") ``` ```tql {input: "example", output: true} {input: "example!", output: true} {input: "example?", output: false} ```

# max

Computes the maximum of all grouped values. ```tql max(xs:list) -> number ``` ## Description [Section titled “Description”](#description) The `max` function returns the largest numeric value in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ## Examples [Section titled “Examples”](#examples) ### Find the maximum value [Section titled “Find the maximum value”](#find-the-maximum-value) ```tql from {x: 1}, {x: 2}, {x: 3} summarize max_value=max(x) ``` ```tql {max_value: 3} ``` ## See Also [Section titled “See Also”](#see-also) [`min`](/reference/functions/min), [`mean`](/reference/functions/mean), [`sum`](/reference/functions/sum)

# mean

Computes the mean of all grouped values. ```tql mean(xs:list) -> float ``` ## Description [Section titled “Description”](#description) The `mean` function returns the average of all numeric values in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to average. ## Examples [Section titled “Examples”](#examples) ### Compute the mean value [Section titled “Compute the mean value”](#compute-the-mean-value) ```tql from {x: 1}, {x: 2}, {x: 3} summarize avg=mean(x) ``` ```tql {avg: 2.0} ``` ## See Also [Section titled “See Also”](#see-also) [`max`](/reference/functions/max), [`median`](/reference/functions/median), [`min`](/reference/functions/min), [`quantile`](/reference/functions/quantile), [`stddev`](/reference/functions/stddev), [`sum`](/reference/functions/sum), [`variance`](/reference/functions/variance)

# median

Computes the approximate median of all grouped values using a t-digest algorithm. ```tql median(xs:list) -> float ``` ## Description [Section titled “Description”](#description) The `median` function returns an approximate median of all numeric values in `xs`, computed with a t-digest algorithm. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ## Examples [Section titled “Examples”](#examples) ### Compute the median value [Section titled “Compute the median value”](#compute-the-median-value) ```tql from {x: 1}, {x: 2}, {x: 3}, {x: 4} summarize median_value=median(x) ``` ```tql {median_value: 2.5} ``` ## See Also [Section titled “See Also”](#see-also) [`mean`](/reference/functions/mean), [`mode`](/reference/functions/mode), [`quantile`](/reference/functions/quantile)

# merge

Combines two records into a single record by merging their fields. ```tql merge(x: record, y: record) -> record ``` ## Description [Section titled “Description”](#description) The `merge` function takes two records and returns a new record containing all fields from both records. If both records contain the same field, the value from the second record takes precedence. ## Examples [Section titled “Examples”](#examples) ### Basic record merging [Section titled “Basic record merging”](#basic-record-merging) ```tql from {x: {a: 1, b: 2}, y: {c: 3, d: 4}} select result = merge(x, y) ``` ```tql { result: { a: 1, b: 2, c: 3, d: 4 } } ``` ### Handling overlapping fields [Section titled “Handling overlapping fields”](#handling-overlapping-fields) When fields exist in both records, the second record’s values take precedence: ```tql from { r1: {name: "Alice", age: 30}, r2: {name: "Bob", location: "NY"}, } select result = merge(r1, r2) ``` ```tql { result: { name: "Bob", age: 30, location: "NY" } } ``` ### Handling null values [Section titled “Handling null values”](#handling-null-values) If either input is null, the input will be ignored. ```tql from {x: {a: 1}, y: null} select result = merge(x, y) ``` ```tql { result: { a: 1 } } ``` ## See Also [Section titled “See Also”](#see-also) [`concatenate`](/reference/functions/concatenate)

# microseconds

Converts a number to equivalent microseconds. ```tql microseconds(x:number) -> duration ``` ## Description This function returns microseconds equivalent to a number, i.e., `number * 1us`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# milliseconds

Converts a number to equivalent milliseconds. ```tql milliseconds(x:number) -> duration ``` ## Description This function returns milliseconds equivalent to a number, i.e., `number * 1ms`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# min

Computes the minimum of all grouped values. ```tql min(xs:list) -> number ``` ## Description [Section titled “Description”](#description) The `min` function returns the smallest numeric value in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ## Examples [Section titled “Examples”](#examples) ### Find the minimum value [Section titled “Find the minimum value”](#find-the-minimum-value) ```tql from {x: 1}, {x: 2}, {x: 3} summarize min_value=min(x) ``` ```tql {min_value: 1} ``` ## See Also [Section titled “See Also”](#see-also) [`max`](/reference/functions/max), [`mean`](/reference/functions/mean), [`sum`](/reference/functions/sum)

# minute

Extracts the minute component from a timestamp. ```tql minute(x: time) -> int ``` ## Description [Section titled “Description”](#description) The `minute` function extracts the minute component from a timestamp as an integer (0-59). ### `x: time` [Section titled “x: time”](#x-time) The timestamp from which to extract the minute. ## Examples [Section titled “Examples”](#examples) ### Extract the minute from a timestamp [Section titled “Extract the minute from a timestamp”](#extract-the-minute-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } minute = ts.minute() ``` ```tql { ts: 2024-06-15T14:30:45.123456, minute: 30, } ``` ## See also [Section titled “See also”](#see-also) [`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`second`](/reference/functions/second)

# minutes

Converts a number to equivalent minutes. ```tql minutes(x:number) -> duration ``` ## Description This function returns minutes equivalent to a number, i.e., `number * 1min`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# mode

Takes the most common non-null grouped value. ```tql mode(xs:list) -> any ``` ## Description [Section titled “Description”](#description) The `mode` function returns the most frequently occurring non-null value in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ## Examples [Section titled “Examples”](#examples) ### Find the mode of values [Section titled “Find the mode of values”](#find-the-mode-of-values) ```tql from {x: 1}, {x: 1}, {x: 2}, {x: 3} summarize mode_value=mode(x) ``` ```tql {mode_value: 1} ``` ## See Also [Section titled “See Also”](#see-also) [`median`](/reference/functions/median), [`value_counts`](/reference/functions/value_counts)

# month

Extracts the month component from a timestamp. ```tql month(x: time) -> int ``` ## Description [Section titled “Description”](#description) The `month` function extracts the month component from a timestamp as an integer (1-12). ### `x: time` [Section titled “x: time”](#x-time) The timestamp from which to extract the month. ## Examples [Section titled “Examples”](#examples) ### Extract the month from a timestamp [Section titled “Extract the month from a timestamp”](#extract-the-month-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } month = ts.month() ``` ```tql { ts: 2024-06-15T14:30:45.123456, month: 6, } ``` ## See also [Section titled “See also”](#see-also) [`year`](/reference/functions/year), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# months

Converts a number to equivalent months. ```tql months(x:number) -> duration ``` ## Description This function returns months equivalent to a number, i.e., `number * 1/12 * 1y`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# nanoseconds

Converts a number to equivalent nanoseconds. ```tql nanoseconds(x:number) -> duration ``` ## Description This function returns nanoseconds equivalent to a number, i.e., `number * 1ns`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# network

Retrieves the network address of a subnet. ```tql network(x:subnet) -> ip ``` ## Description [Section titled “Description”](#description) The `network` function returns the network address of a subnet. ## Examples [Section titled “Examples”](#examples) ### Get the network address of a subnet [Section titled “Get the network address of a subnet”](#get-the-network-address-of-a-subnet) ```tql from {subnet: 192.168.0.0/16} select ip = subnet.network() ``` ```tql {ip: 192.168.0.0} ```

# now

Gets the current wallclock time. ```tql now() -> time ``` ## Description [Section titled “Description”](#description) The `now` function returns the current wallclock time. ## Examples [Section titled “Examples”](#examples) ### Get the time in UTC [Section titled “Get the time in UTC”](#get-the-time-in-utc) ```tql let $now = now() from { x: $now } ``` ```tql {x: 2024-10-28T13:27:33.957987} ``` ### Compute a field with the current time [Section titled “Compute a field with the current time”](#compute-a-field-with-the-current-time) ```tql subscribe "my-topic" select ts=now() ``` ```tql {ts: 2024-10-30T15:03:04.85298} {ts: 2024-10-30T15:03:06.31878} {ts: 2024-10-30T15:03:07.59813} ``` ## See Also [Section titled “See Also”](#see-also) [`from_epoch`](/reference/functions/from_epoch), [`since_epoch`](/reference/functions/since_epoch)

# ocsf::category_name

Returns the `category_name` for a given `category_uid`. ```tql ocsf::category_uid(id:int) -> string ``` ## Description [Section titled “Description”](#description) ### `id: int` [Section titled “id: int”](#id-int) The `category_uid` for which `category_name` should be returned. ## See Also [Section titled “See Also”](#see-also) [`ocsf::category_uid`](/reference/functions/ocsf/category_uid)

# ocsf::category_uid

Returns the `category_uid` for a given `category_name`. ```tql ocsf::category_uid(name:string) -> int ``` ## Description [Section titled “Description”](#description) ### `name: string` [Section titled “name: string”](#name-string) The `category_name` for which `category_uid` should be returned. ## See Also [Section titled “See Also”](#see-also) [`ocsf::category_name`](/reference/functions/ocsf/category_name)

# ocsf::class_name

Returns the `class_name` for a given `class_uid`. ```tql ocsf::class_uid(id:int) -> string ``` ## Description [Section titled “Description”](#description) ### `id: int` [Section titled “id: int”](#id-int) The `class_uid` for which `class_name` should be returned. ## See Also [Section titled “See Also”](#see-also) [`ocsf::class_uid`](/reference/functions/ocsf/class_uid)

# ocsf::class_uid

Returns the `class_uid` for a given `class_name`. ```tql ocsf::class_uid(name:string) -> int ``` ## Description [Section titled “Description”](#description) ### `name: string` [Section titled “name: string”](#name-string) The `class_name` for which `class_uid` should be returned. ## See Also [Section titled “See Also”](#see-also) [`ocsf::class_name`](/reference/functions/ocsf/class_name)

# ocsf::type_name

Returns the `type_name` for a given `type_uid`. ```tql ocsf::type_name(id:int) -> string ``` ## Description [Section titled “Description”](#description) ### `id: int` [Section titled “id: int”](#id-int) The `type_uid` for which `type_name` should be returned. ## See Also [Section titled “See Also”](#see-also) [`ocsf::type_uid`](/reference/functions/ocsf/type_uid)

# ocsf::type_uid

Returns the `type_uid` for a given `type_name`. ```tql ocsf::type_uid(name:string) -> int ``` ## Description [Section titled “Description”](#description) ### `name: string` [Section titled “name: string”](#name-string) The `type_name` for which `type_uid` should be returned. ## See Also [Section titled “See Also”](#see-also) [`ocsf::type_name`](/reference/functions/ocsf/type_name)

# otherwise

Returns a `fallback` value if `primary` is `null`. ```tql otherwise(primary:any, fallback:any) -> any ``` ## Description [Section titled “Description”](#description) The `otherwise` function evaluates its arguments and replaces `primary` with `fallback` where `primary` would be `null`. ### `primary: any` [Section titled “primary: any”](#primary-any) The expression to return if not `null`. ### `fallback: any` [Section titled “fallback: any”](#fallback-any) The expression to return if `primary` evaluates to `null`. ## Examples [Section titled “Examples”](#examples) ### Set a default value for a key [Section titled “Set a default value for a key”](#set-a-default-value-for-a-key) ```tql from {x: 1}, {x: 2}, {} x = x.otherwise(-1) ``` ```tql {x: 1} {x: 2} {x: -1} ```

# pad_end

Pads a string at the end to a specified length. ```tql pad_end(x:string, length:int, [pad_char:string]) -> string ``` ## Description [Section titled “Description”](#description) The `pad_end` function pads the string `x` at the end with `pad_char` (default: space) until it reaches the specified `length`. If the string is already longer than or equal to the specified length, it returns the original string unchanged. ### `x: string` [Section titled “x: string”](#x-string) The string to pad. ### `length: int` [Section titled “length: int”](#length-int) The target length of the resulting string. ### `pad_char: string` [Section titled “pad\_char: string”](#pad_char-string) The character to use for padding. Must be a single character. Defaults to a space. Defaults to `" "`. ## Examples [Section titled “Examples”](#examples) ### Pad with spaces [Section titled “Pad with spaces”](#pad-with-spaces) ```tql from {x: "hello".pad_end(10)} ``` ```tql {x: "hello "} ``` ### Pad with custom character [Section titled “Pad with custom character”](#pad-with-custom-character) ```tql from {x: "hello".pad_end(10, ".")} ``` ```tql {x: "hello....."} ``` ### String already long enough [Section titled “String already long enough”](#string-already-long-enough) ```tql from {x: "hello world".pad_end(5)} ``` ```tql {x: "hello world"} ``` ## See Also [Section titled “See Also”](#see-also) [`pad_start`](/reference/functions/pad_start), [`trim`](/reference/functions/trim), [`trim_end`](/reference/functions/trim_end)

# pad_start

Pads a string at the start to a specified length. ```tql pad_start(x:string, length:int, [pad_char:string]) -> string ``` ## Description [Section titled “Description”](#description) The `pad_start` function pads the string `x` at the start with `pad_char` (default: space) until it reaches the specified `length`. If the string is already longer than or equal to the specified length, it returns the original string unchanged. ### `x: string` [Section titled “x: string”](#x-string) The string to pad. ### `length: int` [Section titled “length: int”](#length-int) The target length of the resulting string. ### `pad_char: string` [Section titled “pad\_char: string”](#pad_char-string) The character to use for padding. Must be a single character. Defaults to a space. Defaults to `" "`. ## Examples [Section titled “Examples”](#examples) ### Pad with spaces [Section titled “Pad with spaces”](#pad-with-spaces) ```tql from {x: "hello".pad_start(10)} ``` ```tql {x: " hello"} ``` ### Pad with custom character [Section titled “Pad with custom character”](#pad-with-custom-character) ```tql from {x: "42".pad_start(5, "0")} ``` ```tql {x: "00042"} ``` ### String already long enough [Section titled “String already long enough”](#string-already-long-enough) ```tql from {x: "hello world".pad_start(5)} ``` ```tql {x: "hello world"} ``` ## See Also [Section titled “See Also”](#see-also) [`pad_end`](/reference/functions/pad_end), [`trim`](/reference/functions/trim), [`trim_start`](/reference/functions/trim_start)

# parent_dir

Extracts the parent directory from a file path. ```tql parent_dir(x:string) -> string ``` ## Description [Section titled “Description”](#description) The `parent_dir` function returns the parent directory path of the given file path, excluding the file name. ## Examples [Section titled “Examples”](#examples) ### Extract the parent directory from a file path [Section titled “Extract the parent directory from a file path”](#extract-the-parent-directory-from-a-file-path) ```tql from {x: parent_dir("/path/to/log.json")} ``` ```tql {x: "/path/to"} ``` ## See Also [Section titled “See Also”](#see-also) [`file_name`](/reference/functions/file_name)

# parse_cef

Parses a string as a CEF message ```tql parse_cef(input:string, [schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled “Description”](#description) The `parse_cef` function parses a string as a CEF message ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ```tql from { x: "CEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8|key=value" } y = x.parse_cef() ``` ```tql { cef_version: 0, device_vendor: "Cynet", device_product: "Cynet 360", device_version: "4.5.4.22139", signature_id: "0", name: "Memory Pattern - Cobalt Strike Beacon ReflectiveLoader", severity: "8", extension: { key: "value" } } ``` # See Also [Section titled “See Also”](#see-also) [`read_cef`](/reference/operators/read_cef), [`print_cef`](/reference/functions/print_cef), [`read_syslog`](/reference/operators/read_syslog)

# parse_csv

Parses a string as CSV (Comma-Separated Values). ```tql parse_csv(input:string, header=list<string>|string, [list_separator=string, null_value=string, auto_expand=bool, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled “Description”](#description) The `parse_csv` function parses a string as [CSV](https://en.wikipedia.org/wiki/Comma-separated_values). ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `header = list<string>|string` [Section titled “header = list\<string>|string”](#header--liststringstring) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. ### `list_separator = string (optional)` [Section titled “list\_separator = string (optional)”](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `;`. ### `null_value = string (optional)` [Section titled “null\_value = string (optional)”](#null_value--string-optional) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ```tql from { input: "1,2,3" } output = input.parse_csv(header=["a","b","c"]) ``` ```tql { input: "1,2,3", output: { a: 1, b: 2, c: 3, }, } ``` ## See Also [Section titled “See Also”](#see-also) [`read_csv`](/reference/operators/read_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_tsv`](/reference/functions/parse_tsv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_grok

Parses a string according to a grok pattern. ```tql parse_grok(input:string, pattern:string, [pattern_definitions=record|string, indexed_captures=bool, include_unnamed=bool, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled “Description”](#description) `parse_grok` uses a regular expression based parser similar to the [Logstash `grok` plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) in Elasticsearch. Tenzir ships with the same built-in patterns as Elasticsearch, found [here](https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns/ecs-v1). In short, `pattern` consists of replacement fields, that look like `%{SYNTAX[:SEMANTIC[:CONVERSION]]}`, where: * `SYNTAX` is a reference to a pattern, either built-in or user-defined through the `pattern_defintions` option. * `SEMANTIC` is an identifier that names the field in the parsed record. * `CONVERSION` is either `infer` (default), `string` (default with `raw=true`), `int`, or `float`. The supported regular expression syntax is the one supported by [Boost.Regex](https://www.boost.org/doc/libs/1_81_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html), which is effectively Perl-compatible. ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `pattern: string` [Section titled “pattern: string”](#pattern-string) The `grok` pattern used for matching. Must match the input in its entirety. ### `pattern_definitions = record|string (optional)` [Section titled “pattern\_definitions = record|string (optional)”](#pattern_definitions--recordstring-optional) New pattern definitions to use. This may be a record of the form ```tql { pattern_name: "pattern", … } ``` For example, the built-in pattern `INT` would be defined as ```tql { INT: "(?:[+-]?(?:[0-9]+))" } ``` Alternatively, this may be a user-defined newline-delimited list of patterns, where a line starts with the pattern name, followed by a space, and the `grok`-pattern for that pattern. For example, the built-in pattern `INT` is defined as follows: ```plaintext INT (?:[+-]?(?:[0-9]+)) ``` ### `indexed_captures = bool (optional)` [Section titled “indexed\_captures = bool (optional)”](#indexed_captures--bool-optional) All subexpression captures are included in the output, with the `SEMANTIC` used as the field name if possible, and the capture index otherwise. ### `include_unnamed = bool (optional)` [Section titled “include\_unnamed = bool (optional)”](#include_unnamed--bool-optional) By default, only fields that were given a name with `SEMANTIC`, or with the regular expression named capture syntax `(?<name>...)` are included in the resulting record. With `include_unnamed=true`, replacement fields without a `SEMANTIC` are included in the output, using their `SYNTAX` value as the record field name. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ```tql let $pattern = "%{IP:client} %{WORD} %{URIPATHPARAM:req} %{NUMBER:bytes} %{NUMBER:dur}" from { input: "55.3.244.1 GET /index.html 15824 0.043" } output = input.parse_grok($pattern) output.dur = output.dur * 1s ``` ```tql { input: "55.3.244.1 GET /index.html 15824 0.043", output: { client: 55.3.244.1, req: "/index.html", bytes: 15824, dur: 43.0ms } } ``` ## See Also [Section titled “See Also”](#see-also) [`read_grok`](/reference/operators/read_grok)

# parse_json

Parses a string as a JSON value. ```tql parse_json(input:string, [schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> any ``` ## Description [Section titled “Description”](#description) The `parse_json` function parses a string as a JSON value. ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Parse a JSON record [Section titled “Parse a JSON record”](#parse-a-json-record) ```tql from { input: r#"{ "a": 42, "b": "text"}"# } output = input.parse_json() ``` ```tql { input: "{ \"a\": 42, \"b\": \"text\"}", output: { a: 42, b: "text" } } ``` ### Parse a JSON list [Section titled “Parse a JSON list”](#parse-a-json-list) ```tql from { input: "[0,1]" } output = input.parse_json() ``` ```tql { input: "[0,1]", output: [0, 1] } ``` ## See Also [Section titled “See Also”](#see-also) [`read_json`](/reference/operators/read_json)

# parse_kv

Parses a string as key-value pairs. ```tql parse_kv(input:string, [field_split=string, value_split=string, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled “Description”](#description) The `parse_kv` function parses a string as key-value pairs. The string is first split into fields according to `field_split`. This can be a regular expression. For example, the input `foo: bar, baz: 42` can be split into `foo: bar` and `baz: 42` with the regular expression `r",\s*"` (a comma, followed by any amount of whitespace) as the field splitter. Note that the matched separators are removed when splitting a string. Afterwards, the extracted fields are split into their key and value by `value_split`, which can again be a regular expression. In our example, `r":\s*"` could be used to split `foo: bar` into the key `foo` and its value `bar`, and similarly `baz: 42` into `baz` and `42`. The result would thus be `{"foo": "bar", "baz": 42}`. If the regex matches multiple substrings, only the first match is used. If no match is found, the “field” is considered an extension of the previous fields value. The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `parse_kv` at the moment. However, if the regular expression has a capture group, it is assumed that only the content of the capture group shall be used as the separator. This means that unsupported regular expressions such as `(?=foo)bar(?<=baz)` can be effectively expressed as `foo(bar)baz` instead. ### Quoted Values [Section titled “Quoted Values”](#quoted-values) The parser is aware of double-quotes (`"`). If the `field_split` or `value_split` are found within enclosing quotes, they are not considered matches. This means that both the key and the value may be enclosed in double-quotes. For example, given `field_split` `\s*,\s*` and `value_split` `=`, the input ```plaintext "key"="nested = value",key2="value, and more" ``` will parse as ```tql { key: "nested = value", key2: "value, and more", } ``` ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `field_split = string (optional)` [Section titled “field\_split = string (optional)”](#field_split--string-optional) The regular expression used to separate individual fields. Defaults to `r"\s"`. ### `value_split = string (optional)` [Section titled “value\_split = string (optional)”](#value_split--string-optional) The regular expression used to separate a key from its value. Defaults to `"="`. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Parse comma-separated key-value pairs [Section titled “Parse comma-separated key-value pairs”](#parse-comma-separated-key-value-pairs) ```tql from { input: "surname: John, family_name: Smith, date_of_birth: 1995-05-26" } output = input.parse_kv(field_split=r"\s*,\s*", value_split=r"\s*:\s*") ``` ```tql { input: "surname: John, family_name: Smith, date_of_birth: 1995-05-26", output: { surname: "John", family_name: "Smith", date_of_birth: 1995-05-26, }, } ``` ### Fields without a `value_split` [Section titled “Fields without a value\_split”](#fields-without-a-value_split) ```tql from { input: "x=1 y=2 z=3 4 5 a=6" } this = { ...input.parse_kv() } ``` ```tql { x: 1, y: 2, z: "3 4 5", a: 6, } ``` # See Also [Section titled “See Also”](#see-also) [`read_kv`](/reference/operators/read_kv), [`print_kv`](/reference/functions/print_kv)

# parse_leef

Parses a string as a LEEF message ```tql parse_leef(input:string, [schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled “Description”](#description) The `parse_leef` function parses a string as a LEEF message ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ```tql from { input: "LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1 dst=2.10.20.20 spt=1200" } output = input.parse_leef() ``` ```tql { input: "LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1\tdst=2.10.20.20\tspt=1200", output: { leef_version: "1.0", vendor: "Microsoft", product_name: "MSExchange", product_version: "2016", event_class_id: "15345", attributes: { src: 10.50.1.1, dst: 2.10.20.20, spt: 1200, }, }, } ``` # See Also [Section titled “See Also”](#see-also) [`read_leef`](/reference/operators/read_leef), [`print_leef`](/reference/functions/print_leef), [`parse_cef`](/reference/functions/parse_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# parse_ssv

Parses a string as space separated values. ```tql parse_ssv(input:string, header=list<string>|string, [list_separator:string, null_value:string, auto_expand=bool, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled “Description”](#description) The `parse_ssv` function parses a string as space separated values. ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `header = list<string>|string` [Section titled “header = list\<string>|string”](#header--liststringstring) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. ### `list_separator = string (optional)` [Section titled “list\_separator = string (optional)”](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `,`. ### `null_value = string (optional)` [Section titled “null\_value = string (optional)”](#null_value--string-optional) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ```tql from { input: "1 2 3" } output = input.parse_ssv(header=["a","b","c"]) ``` ```tql { input: "1 2 3", output: { a: 1, b: 2, c: 3, }, } ``` ## See Also [Section titled “See Also”](#see-also) [`read_ssv`](/reference/operators/read_ssv), [`parse_csv`](/reference/functions/parse_csv), [`parse_tsv`](/reference/functions/parse_tsv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_syslog

Parses a string as a Syslog message. ```tql parse_syslog [raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) Parses a string as a [Syslog](https://en.wikipedia.org/wiki/Syslog) message. Tenzir supports reading syslog messages in both the standardized “Syslog Protocol” format ([RFC 5424](https://tools.ietf.org/html/rfc5424)), and the older “BSD syslog Protocol” format ([RFC 3164](https://tools.ietf.org/html/rfc3164)). ## `input: string` [Section titled “input: string”](#input-string) ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Parse a RFC5424 syslog string [Section titled “Parse a RFC5424 syslog string”](#parse-a-rfc5424-syslog-string) ```tql from { input: r#"<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource="Application" eventID="1011"] Event log entry"#} output = input.parse_syslog() ``` ```tql { input: "<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource=\"Application\" eventID=\"1011\"] Event log entry", output: { facility: 20, severity: 5, version: 8, timestamp: 2023-10-11T22:14:15.003Z, hostname: "mymachineexamplecom", app_name: "evntslog", process_id: "1370", message_id: "ID47", structured_data: { "exampleSDID@32473": { eventSource: "Application", eventID: 1011, }, }, message: "Event log entry", }, } ```

# parse_time

Parses a time from a string that follows a specific format. ```tql parse_time(input: string, format: string) -> time ``` ## Description [Section titled “Description”](#description) The `parse_time` function matches the given `input` string against the `format` to construct a timestamp. ### `input: string` [Section titled “input: string”](#input-string) The input string from which the timestamp should be extracted. ### `format: string` [Section titled “format: string”](#format-string) The string that specifies the format of `input`, for example `"%m-%d-%Y"`. The allowed format specifiers are the same as for `strptime(3)`: | Specifier | Description | Example | | :-------: | :------------------------------------- | :------------------------ | | `%%` | A literal `%` character | `%` | | `%a` | Abbreviated weekday name | `Mon` | | `%A` | Full weekday name | `Monday` | | `%b` | Abbreviated month name | `Jan` | | `%B` | Full month name | `January` | | `%c` | Date and time representation | `Mon Jan 1 12:00:00 2024` | | `%C` | Century (year divided by 100) | `20` | | `%d` | Day of month with zero padding | `01`, `31` | | `%D` | Equivalent to `%m/%d/%y` | `01/31/24` | | `%e` | Day of month with space padding | `1`, `31` | | `%F` | Equivalent to `%Y-%m-%d` | `2024-01-31` | | `%g` | Last two digits of ISO week-based year | `24` | | `%G` | ISO week-based year | `2024` | | `%h` | Equivalent to `%b` | `Jan` | | `%H` | Hour in 24-hour format | `00`, `23` | | `%I` | Hour in 12-hour format | `01`, `12` | | `%j` | Day of year | `001`, `365` | | `%m` | Month number | `01`, `12` | | `%M` | Minute | `00`, `59` | | `%n` | Newline character | `\n` | | `%p` | AM/PM designation | `AM`, `PM` | | `%r` | 12-hour clock time | `12:00:00 PM` | | `%R` | Equivalent to `%H:%M` | `23:59` | | `%S` | Seconds | `00`, `59` | | `%t` | Tab character | `\t` | | `%T` | Equivalent to `%H:%M:%S` | `23:59:59` | | `%u` | ISO weekday (Monday=1) | `1`, `7` | | `%U` | Week number (Sunday as first day) | `00`, `52` | | `%V` | ISO week number | `01`, `53` | | `%w` | Weekday (Sunday=0) | `0`, `6` | | `%W` | Week number (Monday as first day) | `00`, `52` | | `%x` | Date representation | `01/31/24` | | `%X` | Time representation | `23:59:59` | | `%y` | Year without century | `24` | | `%Y` | Year with century | `2024` | | `%z` | UTC offset | `+0000`, `-0430` | | `%Z` | Time zone abbreviation | `UTC`, `EST` | ## Examples [Section titled “Examples”](#examples) ### Parse a timestamp [Section titled “Parse a timestamp”](#parse-a-timestamp) ```tql from { x: "2024-12-31+12:59:42", } x = x.parse_time("%Y-%m-%d+%H:%M:%S") ``` ```tql {x: 2024-12-31T12:59:42.000000} ``` ## See Also [Section titled “See Also”](#see-also) [`format_time`](/reference/functions/format_time)

# parse_tsv

Parses a string as tab separated values. ```tql parse_tsv(input:string, header=list<string>|string, [list_separator:string, null_value:string, auto_expand=bool, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled “Description”](#description) The `parse_tsv` function parses a string as [TSV](https://en.wikipedia.org/wiki/Tab-separated_values). ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `header = list<string>|string` [Section titled “header = list\<string>|string”](#header--liststringstring) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. ### `list_separator = string (optional)` [Section titled “list\_separator = string (optional)”](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `,`. ### `null_value = string (optional)` [Section titled “null\_value = string (optional)”](#null_value--string-optional) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ```tql from { input: "1\t2\t3" } output = input.parse_tsv(header=["a","b","c"]) ``` ```tql { input: "1\t2\t3", output: { a: 1, b: 2, c: 3, }, } ``` ## See Also [Section titled “See Also”](#see-also) [`read_tsv`](/reference/operators/read_tsv), [`parse_csv`](/reference/functions/parse_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_xsv`](/reference/functions/parse_xsv)

# parse_xsv

Parses a string as delimiter separated values. ```tql parse_xsv(input:string, field_separator=string, list_separator=string, null_value=string, header=list<string>|string, [auto_expand=bool, quotes=string, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> record ``` ## Description [Section titled “Description”](#description) The `parse_xsv` function parses a string as [XSV](https://en.wikipedia.org/wiki/Delimiter-separated_values), a generalization of CSV with a more flexible separator specification. The following table lists existing XSV configurations: | Format | Field Separator | List Separator | Null Value | | --------------------------------------- | :-------------: | :------------: | :--------: | | [`csv`](/reference/functions/parse_csv) | `,` | `;` | empty | | [`ssv`](/reference/functions/parse_ssv) | `<space>` | `,` | `-` | | [`tsv`](/reference/functions/parse_tsv) | `\t` | `,` | `-` | ### `header = list<string>|string` [Section titled “header = list\<string>|string”](#header--liststringstring) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. ### `field_separator = string` [Section titled “field\_separator = string”](#field_separator--string) The string separating different fields. ### `list_separator = string` [Section titled “list\_separator = string”](#list_separator--string) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. ### `null_value = string` [Section titled “null\_value = string”](#null_value--string) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ```tql from { input: "1,2,3" } output = input.parse_xsv( field_separator=",", list_separator= ";", null_value="", header=["a","b","c"], ) ``` ```tql { input: "1,2,3", output: { a: 1, b: 2, c: 3, }, } ``` ## See Also [Section titled “See Also”](#see-also) [`read_xsv`](/reference/operators/read_xsv), [`parse_csv`](/reference/functions/parse_csv), [`parse_ssv`](/reference/functions/parse_ssv), [`parse_tsv`](/reference/functions/parse_tsv)

# parse_yaml

Parses a string as a YAML value. ```tql parse_yaml(input:string, [schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string]) -> any ``` ## Description [Section titled “Description”](#description) The `parse_yaml` function parses a string as a YAML value. ### `input: string` [Section titled “input: string”](#input-string) The string to parse. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Parse a YAML map containing a list [Section titled “Parse a YAML map containing a list”](#parse-a-yaml-map-containing-a-list) ```tql from { x: r#"yarp: - darp - larp"# } x = x.parse_yaml() ``` ```tql { x: { yarp: [ "darp", "larp", ], }, } ``` ## See Also [Section titled “See Also”](#see-also) [`read_yaml`](/reference/operators/read_yaml), [`print_yaml`](/reference/functions/print_yaml)

# prepend

Inserts an element at the start of a list. ```tql prepend(xs:list, x:any) -> list ``` ## Description [Section titled “Description”](#description) The `prepend` function returns the list `xs` with `x` inserted at the front. The expression `xs.prepend(y)` is equivalent to `[x, ...xs]`. ## Examples [Section titled “Examples”](#examples) ### Prepend a number to a list [Section titled “Prepend a number to a list”](#prepend-a-number-to-a-list) ```tql from {xs: [1, 2]} xs = xs.prepend(3) ``` ```tql {xs: [3, 1, 2]} ``` ## See Also [Section titled “See Also”](#see-also) [`add`](/reference/functions/add), [`append`](/reference/functions/append), [`concatenate`](/reference/functions/concatenate), [`remove`](/reference/functions/remove)

# print_cef

Prints records as Common Event Format (CEF) messages ```tql print_cef(extension:record, cef_version=str, device_vendor=str, device_product=str, device_version=str, signature_id=str, name=str, severity=str, [flatten_separator=str, null_value=str]) -> str ``` ## Description [Section titled “Description”](#description) Prints records as the attributes of a CEF message. ### `extension: record` [Section titled “extension: record”](#extension-record) The record to print as the extension of the CEF message ### `cef_version = str` [Section titled “cef\_version = str”](#cef_version--str) The CEF version in the CEF header. ### `device_vendor = str` [Section titled “device\_vendor = str”](#device_vendor--str) The vendor in the CEF header. ### `device_product = str` [Section titled “device\_product = str”](#device_product--str) The product name in the CEF header. ### `device_version = str` [Section titled “device\_version = str”](#device_version--str) The product version in the CEF header. ### `signature_id = str` [Section titled “signature\_id = str”](#signature_id--str) The event (class) ID in the CEF header. ### `name = str` [Section titled “name = str”](#name--str) The name field in the CEF header, i.e. the human readable description. ### `severity = str` [Section titled “severity = str”](#severity--str) The severity in the CEF header. ### `null_value = str (optional)` [Section titled “null\_value = str (optional)”](#null_value--str-optional) A string to use if any of the values in `extension` are `null`. Defaults to the empty string. ### `flatten_separator = str (optional)` [Section titled “flatten\_separator = str (optional)”](#flatten_separator--str-optional) A string used to flatten nested records in `attributes`. Defaults to `"."`. ## Examples [Section titled “Examples”](#examples) ### Write a CEF [Section titled “Write a CEF”](#write-a-cef) ```tql from { extension: { a: 42, b: "Hello" }, signature_id: "MyCustomSignature", severity: "8" } r = extension.print_cef( cef_version="0", device_vendor="Tenzir", device_product="Tenzir Node", device_version="5.5.0", signature_id=signature_id, severity=severity, name= signature_id + " written by Tenzir" ) select r write_lines ``` ```txt CEF:0|Tenzir|Tenzir Node|5.5.0|MyCustomSignature|MyCustomSignature written by Tenzir|8|a=42 b=Hello ``` ### Upgrade a nested CEF message in Syslog [Section titled “Upgrade a nested CEF message in Syslog”](#upgrade-a-nested-cef-message-in-syslog) ```tql from "my.log" { read_syslog // produces the expected shape for `write_syslog` } // read the message into a structured form message = message.parse_cef() // re-write the message with modifications message = message.extension.print_cef( cef_version=message.cef_version, device_vendor=message.device_vendor, device_product=message.device_product, device_version=message.device_version, signature_id=signature_id, severity="9" name=message.name ) write_syslog ``` ## See Also [Section titled “See Also”](#see-also) [`parse_cef`](/reference/functions/parse_cef), [`read_cef`](/reference/operators/read_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# print_csv

Prints a record as a comma-separated string of values. ```tql print_csv(input:record, [list_separator=str, null_value=str]) -> string ``` ## Description The `print_csv` function prints a record’s values as a comma separated string. ### `input: record` The record you want to print. ### `list_separator = str (optional)` The string separating the elements in list fields. Defaults to `";"`. ### `null_value = str (optional)` The string denoting an absent value. Defaults to `""`. ## Examples [Section titled “Examples”](#examples) ```tql from { x:1, y:true, z: "String" } output = this.print_csv() ``` ```tql { x: 1, y: true, z: "String", output: "1,true,String", } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_csv`](/reference/functions/parse_csv), [`write_csv`](/reference/operators/write_csv)

# print_json

Transforms a value into a JSON string. ```tql print_json(input:any, [strip=bool, color=bool, arrays_of_objects=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool]) -> string ``` ## Description [Section titled “Description”](#description) Transforms a value into a JSON string. ### `input: any` The value to print as a JSON value. ### `strip = bool (optional)` Enables all `strip_*` options. Defaults to `false`. ### `color = bool (optional)` Colorize the output. Defaults to `false`. ### `strip_null_fields = bool (optional)` Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Print without null fields [Section titled “Print without null fields”](#print-without-null-fields) ```tql from {x: 0}, {x:null}, {x: { x: 0, y: 1 } }, { x: [0,1,2,] } x = x.print_json(strip_null_fields=true) ``` ```tql { x: "0", } { x: null, } { x: "{\n \"x\": 0,\n \"y\": 1\n}", } { x: "[\n 0,\n 1,\n 2\n]", } ``` ## See also [Section titled “See also”](#see-also) [`write_json`](/reference/operators/write_json), [`print_ndjson`](/reference/functions/print_ndjson), [`parse_json`](/reference/functions/parse_json)

# print_kv

Prints records in a key-value format. ```tql print_kv( input:record, [field_separator=str, value_separator=str, list_separator=str, flatten_separator=str, null_value=str] ) -> str ``` ## Description [Section titled “Description”](#description) Prints records in a Key-Value format. Nested data will be flattend, keys or values containing the given separators will be quoted and the special characters `\n`, `\r`, `\` and `"` will be escaped. ### `input: record` [Section titled “input: record”](#input-record) The record to print as a string. ### `field_separator = str (optional)` [Section titled “field\_separator = str (optional)”](#field_separator--str-optional) A string that shall separate the key-value pairs. Must not be an empty string. Defaults to `" "`. ### `value_separator = str (optional)` [Section titled “value\_separator = str (optional)”](#value_separator--str-optional) A string that shall separate key and value within key-value pair. Must not be an empty string. Defaults to `"="`. ### `list_separator = str (optional)` [Section titled “list\_separator = str (optional)”](#list_separator--str-optional) Must not be an empty string. Defaults to `","`. ### `flatten_separator = str (optional)` [Section titled “flatten\_separator = str (optional)”](#flatten_separator--str-optional) A string to join the keys of nested records with. For example, given `flatten="."` Defaults to `"."`. ### `null_value = str (optional)` [Section titled “null\_value = str (optional)”](#null_value--str-optional) A string to represent null values. Defaults to the empty string. ## Examples [Section titled “Examples”](#examples) ### Format a record as key-value pair [Section titled “Format a record as key-value pair”](#format-a-record-as-key-value-pair) ```tql from { input: {key: "value"} } output = input.print_kv() ``` ```tql { input: { key: "value", }, output: "key=value", } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_kv`](/reference/functions/parse_kv), [`write_kv`](/reference/operators/read_kv), [`write_kv`](/reference/operators/write_kv)

# print_leef

Prints records as LEEF messages ```tql print_leef(attributes:record, vendor=str, product_name=str, product_version=str, event_class_id=str, [delimiter=str, null_value=str, flatten_separator=str]) -> str ``` ## Description [Section titled “Description”](#description) Prints records as the attributes of a [LEEF](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) message. ### `attributes: record` [Section titled “attributes: record”](#attributes-record) The record to print as the attributes of a LEEF message ### `vendor = str` [Section titled “vendor = str”](#vendor--str) The vendor in the LEEF header. ### `product_name = str` [Section titled “product\_name = str”](#product_name--str) The product name in the LEEF header. ### `product_version = str` [Section titled “product\_version = str”](#product_version--str) The product version in the LEEF header. ### `event_class_id = str` [Section titled “event\_class\_id = str”](#event_class_id--str) The event (class) ID in the LEEF header. ### `delimiter = str (optional)` [Section titled “delimiter = str (optional)”](#delimiter--str-optional) This delimiter will be used to separate the key-value pairs in the attributes. It must be a single character. If the chosen delimiter is not `"\t"`, the message will be a LEEF:2.0 message, otherwise it will be LEEF:1.0. Defaults to `"\t"`. ### `null_value = str (optional)` [Section titled “null\_value = str (optional)”](#null_value--str-optional) A string to use if any of the header values evaluate to null. Defaults to an empty string. ### `flatten_separator = str (optional)` [Section titled “flatten\_separator = str (optional)”](#flatten_separator--str-optional) A string used to flatten nested records in `attributes`. Defaults to `"."`. ## Examples [Section titled “Examples”](#examples) ### Write a LEEF:1.0 message [Section titled “Write a LEEF:1.0 message”](#write-a-leef10-message) ```tql from { attributes: { a: 42, b: "Hello" }, event_class_id: "critical" } r = attributes.print_leef( vendor="Tenzir", product_name="Tenzir Node", product_version="5.5.0", event_class_id=event_class_id) select r write_lines ``` ```txt LEEF:1.0|Tenzir Node|5.5.0|critical|a=42 b=Hello ``` ### Reformat a nested LEEF message [Section titled “Reformat a nested LEEF message”](#reformat-a-nested-leef-message) ```tql from "my.log" { read_syslog // produces the expected shape for `write_syslog` } message = message.parse_leef() message = message.attributes.print_leef( vendor=message.vendor, product_name=message.product_name, product_version=message.product_version, event_class_id=message.event_class_id, delimiter="^" ) write_syslog ``` ## See Also [Section titled “See Also”](#see-also) [`parse_leef`](/reference/functions/parse_leef), [`read_leef`](/reference/operators/read_leef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# print_ndjson

Transforms a value into a single-line JSON string. ```tql print_ndjson(input:any, [strip=bool, color=bool, arrays_of_objects=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool]) -> string ``` ## Description [Section titled “Description”](#description) Transforms a value into a single-line JSON string. ### `input: any` The value to print as a JSON value. ### `strip = bool (optional)` Enables all `strip_*` options. Defaults to `false`. ### `color = bool (optional)` Colorize the output. Defaults to `false`. ### `strip_null_fields = bool (optional)` Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Print without null fields [Section titled “Print without null fields”](#print-without-null-fields) ```tql from {x: 0}, {x:null}, {x: {x: 0, y: 1}}, {x: [0,1,2,]} x = x.print_ndjson(strip_null_fields=true) ``` ```tql { x: "0", } { x: null, } { x: "{\"x\": 0, \"y\": 1}", } { x: "[0, 1, 2]", } ``` ## See also [Section titled “See also”](#see-also) [`write_ndjson`](/reference/operators/write_ndjson), [`print_json`](/reference/functions/print_json), [`parse_json`](/reference/functions/parse_json)

# print_ssv

Prints a record as a space-separated string of values. ```tql print_ssv(input:record, [list_separator=str, null_value=str]) -> string ``` ## Description The `print_ssv` function prints a record’s values as a space separated string. ### `input: record` The record you want to print. ### `list_separator = str (optional)` The string separating the elements in list fields. Defaults to `","`. ### `null_value = str (optional)` The string denoting an absent value. Defaults to `"-"`. ## Examples [Section titled “Examples”](#examples) ### Print a record as space [Section titled “Print a record as space”](#print-a-record-as-space) ```tql from {x:1, y:true, z: "String"} output = this.print_ssv() ``` ```tql { x: 1, y: true, z: "String", output: "1 true String", } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_ssv`](/reference/functions/parse_ssv), [`write_ssv`](/reference/operators/write_ssv)

# print_tsv

Prints a record as a tab-separated string of values. ```tql print_tsv(input:record, [list_separator=str, null_value=str]) -> string ``` ## Description The `print_tsv` function prints a record’s values as a tab separated string. ### `input: record` The record you want to print. ### `list_separator = str (optional)` The string separating the elements in list fields. Defaults to `","`. ### `null_value = str (optional)` The string denoting an absent value. Defaults to `"-"`. ## Examples [Section titled “Examples”](#examples) ```tql from { x:1, y:true, z: "String" } output = this.print_tsv() ``` ```tql { x: 1, y: true, z: "String", output: "1\ttrue\tString", } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_tsv`](/reference/functions/parse_tsv), [`write_tsv`](/reference/operators/write_tsv)

# print_xsv

Prints a record as a delimited sequence of values. ```tql print_xsv(input:record, field_separator=str, list_separator=str, null_value=str) -> string ``` ## Description [Section titled “Description”](#description) The `parse_xsv` function prints a record’s values as delimiter separated string. The following table lists existing XSV configurations: | Format | Field Separator | List Separator | Null Value | | --------------------------------------- | :-------------: | :------------: | :--------: | | [`csv`](/reference/functions/print_csv) | `,` | `;` | empty | | [`ssv`](/reference/functions/print_ssv) | `<space>` | `,` | `-` | | [`tsv`](/reference/functions/print_tsv) | `\t` | `,` | `-` | ### `field_separator = str` [Section titled “field\_separator = str”](#field_separator--str) The string separating different fields. ### `list_separator = str` [Section titled “list\_separator = str”](#list_separator--str) The string separating different elements in a list within a single field. ### `null_value = str` [Section titled “null\_value = str”](#null_value--str) The string denoting an absent value. ## Examples [Section titled “Examples”](#examples) ```tql from { x:1, y:true, z: "String", } output = this.print_xsv( field_separator=",", list_separator=";", null_value="null") ``` ```tql { x: 1, y: true, z: "String", output: "1,true,String", } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_xsv`](/reference/functions/parse_xsv), [`write_xsv`](/reference/operators/write_xsv)

# print_yaml

Prints a value as a YAML document. ```tql print_yaml( input:any, [include_document_markers=bool] ) ``` ## Description [Section titled “Description”](#description) ### `input:any` [Section titled “input:any”](#inputany) The value to print as YAML. ### `include_document_markers = bool (optional)` [Section titled “include\_document\_markers = bool (optional)”](#include_document_markers--bool-optional) Includes the “start of document” and “end of document” markers in the result. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ```tql from {x: { x: 0, y: 1 } }, { x: [0,1,2,] } x = x.print_yaml() ``` ```tql { x: "x: 0\ny: 1", } { x: "- 0\n- 1\n- 2", } ``` ## See Also [Section titled “See Also”](#see-also) [`write_yaml`](/reference/operators/write_yaml), [`parse_yaml`](/reference/functions/parse_yaml)

# quantile

Computes the specified quantile of all grouped values. ```tql quantile(xs:list, q=float) -> float ``` ## Description [Section titled “Description”](#description) The `quantile` function returns the quantile of all numeric values in `xs`, specified by the argument `q`, which should be a value between 0 and 1. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ### `q: float` [Section titled “q: float”](#q-float) The quantile to compute, where `q=0.5` represents the median. ## Examples [Section titled “Examples”](#examples) ### Compute the 0.5 quantile (median) of values [Section titled “Compute the 0.5 quantile (median) of values”](#compute-the-05-quantile-median-of-values) ```tql from {x: 1}, {x: 2}, {x: 3}, {x: 4} summarize median_value=quantile(x, q=0.5) ``` ```tql {median_value: 2.5} ``` ## See Also [Section titled “See Also”](#see-also) [`median`](/reference/functions/median), [`mean`](/reference/functions/mean)

# random

Generates a random number in *\[0,1]*. ```tql random() -> float ``` ## Description [Section titled “Description”](#description) The `random` function generates a random number by drawing from a [uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) in the interval *\[0,1]*. ## Examples [Section titled “Examples”](#examples) ### Generate a random number [Section titled “Generate a random number”](#generate-a-random-number) ```tql from {x: random()} ``` ```tql {x: 0.19634716885782455} ``` ## See Also [Section titled “See Also”](#see-also) [`uuid`](/reference/functions/uuid)

# remove

Removes all occurrences of an element from a list. ```tql remove(xs:list, x:any) -> list ``` ## Description [Section titled “Description”](#description) The `remove` function returns the list `xs` with all occurrences of `x` removed. If `x` is not present in the list, the original list is returned unchanged. ### `xs: list` [Section titled “xs: list”](#xs-list) A list to remove elements from. ### `x: any` [Section titled “x: any”](#x-any) The value to remove from the list. ## Examples [Section titled “Examples”](#examples) ### Remove an element from a list [Section titled “Remove an element from a list”](#remove-an-element-from-a-list) ```tql from {xs: [1, 2, 3, 2, 4]} xs = xs.remove(2) ``` ```tql {xs: [1, 3, 4]} ``` ### Remove a non-existent element [Section titled “Remove a non-existent element”](#remove-a-non-existent-element) ```tql from {xs: [1, 2, 3]} xs = xs.remove(5) ``` ```tql {xs: [1, 2, 3]} ``` ### Remove from a list with strings [Section titled “Remove from a list with strings”](#remove-from-a-list-with-strings) ```tql from {xs: ["apple", "banana", "apple", "orange"]} xs = xs.remove("apple") ``` ```tql {xs: ["banana", "orange"]} ``` ## See Also [Section titled “See Also”](#see-also) [`add`](/reference/functions/add), [`append`](/reference/functions/append), [`prepend`](/reference/functions/prepend) [`distinct`](/reference/functions/distinct)

# replace

Replaces characters within a string. ```tql replace(x:string, pattern:string, replacement:string, [max=int]) -> string ``` ## Description [Section titled “Description”](#description) The `replace` function returns a new string where occurrences of `pattern` in `x` are replaced with `replacement`, up to `max` times. If `max` is omitted, all occurrences are replaced. ### `x: string` [Section titled “x: string”](#x-string) The subject to replace the action on. ### `pattern: string` [Section titled “pattern: string”](#pattern-string) The pattern to replace in `x`. ### `replacement: string` [Section titled “replacement: string”](#replacement-string) The replacement value for `pattern`. ### `max = int (optional)` [Section titled “max = int (optional)”](#max--int-optional) The maximum number of replacements to perform. If the option is not set, all occurrences are replaced. ## Examples [Section titled “Examples”](#examples) ### Replace all occurrences of a character [Section titled “Replace all occurrences of a character”](#replace-all-occurrences-of-a-character) ```tql from {x: "hello".replace("l", "r")} ``` ```tql {x: "herro"} ``` ### Replace a limited number of occurrences [Section titled “Replace a limited number of occurrences”](#replace-a-limited-number-of-occurrences) ```tql from {x: "hello".replace("l", "r", max=1)} ``` ```tql {x: "herlo"} ``` ## See Also [Section titled “See Also”](#see-also) [`replace_regex`](/reference/functions/replace_regex), [`replace`](/reference/operators/replace)

# replace_regex

Replaces characters within a string based on a regular expression. ```tql replace_regex(x:string, pattern:string, replacement:string, [max=int]) -> string ``` ## Description [Section titled “Description”](#description) The `replace_regex` function returns a new string where substrings in `x` that match `pattern` are replaced with `replacement`, up to `max` times. If `max` is omitted, all matches are replaced. ### `x: string` [Section titled “x: string”](#x-string) The subject to replace the action on. ### `pattern: string` [Section titled “pattern: string”](#pattern-string) The pattern (as regular expression) to replace in `x`. ### `replacement: string` [Section titled “replacement: string”](#replacement-string) The replacement value for `pattern`. ### `max = int (optional)` [Section titled “max = int (optional)”](#max--int-optional) The maximum number of replacements to perform. If the option is not set, all occurrences are replaced. ## Examples [Section titled “Examples”](#examples) ### Replace all matches of a regular expression [Section titled “Replace all matches of a regular expression”](#replace-all-matches-of-a-regular-expression) ```tql from {x: replace_regex("hello", "l+", "y")} ``` ```tql {x: "heyo"} ``` ### Replace a limited number of matches [Section titled “Replace a limited number of matches”](#replace-a-limited-number-of-matches) ```tql from {x: replace_regex("hellolo", "l+", "y", max=1)} ``` ```tql {x: "heyolo"} ``` ## See Also [Section titled “See Also”](#see-also) [`replace`](/reference/functions/replace)

# reverse

Reverses the characters of a string. ```tql reverse(x:string) -> string ``` ## Description [Section titled “Description”](#description) The `reverse` function returns a new string with the characters of `x` in reverse order. This function operates on Unicode codepoints, not grapheme clusters. Hence, it will not correctly reverse grapheme clusters composed of multiple codepoints. ## Examples [Section titled “Examples”](#examples) ### Reverse a string [Section titled “Reverse a string”](#reverse-a-string) ```tql from {x: reverse("hello")} ``` ```tql {x: "olleh"} ```

# round

Rounds a number or a time/duration with a specified unit. ```tql round(x:number) round(x:time, unit:duration) round(x:duration, unit:duration) ``` ## Description [Section titled “Description”](#description) The `round` function rounds a number `x` to an integer. For time and duration values, use the second `unit` argument to define the rounding unit. ## Examples [Section titled “Examples”](#examples) ### Round integers [Section titled “Round integers”](#round-integers) ```tql from { x: round(3.4), y: round(3.5), z: round(-3.4), } ``` ```tql { x: 3, y: 4, z: -3, } ``` ### Round time and duration values [Section titled “Round time and duration values”](#round-time-and-duration-values) ```tql from { x: round(2024-08-23, 1y), y: round(42m, 1h) } ``` ```tql { x: 2025-01-01, y: 1h, } ``` ## See Also [Section titled “See Also”](#see-also) [`ceil`](/reference/functions/ceil), [`floor`](/reference/functions/floor)

# second

Extracts the second component from a timestamp with subsecond precision. ```tql second(x: time) -> float ``` ## Description [Section titled “Description”](#description) The `second` function extracts the second component from a timestamp as a floating-point number (0-59.999…) that includes subsecond precision. ### `x: time` [Section titled “x: time”](#x-time) The timestamp from which to extract the second. ## Examples [Section titled “Examples”](#examples) ### Extract the second from a timestamp [Section titled “Extract the second from a timestamp”](#extract-the-second-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } second = ts.second() ``` ```tql { ts: 2024-06-15T14:30:45.123456, second: 45.123456, } ``` ### Extract only the full second component without subsecond precision [Section titled “Extract only the full second component without subsecond precision”](#extract-only-the-full-second-component-without-subsecond-precision) ```tql from { ts: 2024-06-15T14:30:45.123456, } full_second = ts.second().floor() ``` ```tql { ts: 2024-06-15T14:30:45.123456, full_second: 45, } ``` ## See also [Section titled “See also”](#see-also) [`year`](/reference/functions/year), [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute)

# seconds

Converts a number to equivalent seconds. ```tql seconds(x:number) -> duration ``` ## Description This function returns seconds equivalent to a number, i.e., `number * 1s`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# secret

Use the value of a secret. ```tql secret(name:string) -> secret ``` ## Description [Section titled “Description”](#description) An operator accepting a secret will first try and lookup the value in the environment or configuration of the Tenzir Node. A `tenzir` client process can use secrets only if it has a Tenzir Node to connect to. If the secret is not found in the node, a request is made to the Tenzir Platform. Should the platform also not be able to find the secret, an error is raised. See the [explanation page for secrets](/explanations/secrets) for more details. ### `name: string` [Section titled “name: string”](#name-string) The name of the secret to use. This must be a constant. ## Legacy Model [Section titled “Legacy Model”](#legacy-model) The configuration option `tenzir.legacy-secret-model` changes the behavior of the `secret` function to return a `string` instead of a `secret`. The legacy model only allows using secrets from the Tenzir Node’s configuration. No secrets from the Tenzir Platform’s secret store will be available. We do not recommend enabling this option. ## Examples [Section titled “Examples”](#examples) ### Using secrets in an operator [Section titled “Using secrets in an operator”](#using-secrets-in-an-operator) ```tql load_tcp "127.0.0.1:4000" { read_ndjson } to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token") ``` ### Secrets are not rendered in output [Section titled “Secrets are not rendered in output”](#secrets-are-not-rendered-in-output) ```tql from {x: secret("geheim")} ``` ```tql {x: "***" } ``` ## See also [Section titled “See also”](#see-also) [`config`](/reference/functions/config), [`env`](/reference/functions/env)

# shift_left

Performs a bit-wise left shift. ```tql shift_left(lhs:number, rhs:number) -> number ``` ## Description [Section titled “Description”](#description) The `shift_left` function performs a bit-wise left shift of `lhs` by `rhs` bit positions. Each left shift multiplies the number by 2. ### `lhs: number` [Section titled “lhs: number”](#lhs-number) The number to be shifted. ### `rhs: number` [Section titled “rhs: number”](#rhs-number) The number of bit positions to shift to the left. ## Examples [Section titled “Examples”](#examples) ### Shift bits to the left [Section titled “Shift bits to the left”](#shift-bits-to-the-left) ```tql from {x: shift_left(5, 2)} ``` ```tql {x: 20} ``` ## See Also [Section titled “See Also”](#see-also) [`shift_right`](/reference/functions/shift_right), [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not)

# shift_right

Performs a bit-wise right shift. ```tql shift_right(lhs:number, rhs:number) -> number ``` ## Description [Section titled “Description”](#description) The `shift_right` function performs a bit-wise right shift of `lhs` by `rhs` bit positions. Each right shift divides the number by 2, truncating any fractional part. ### `lhs: number` [Section titled “lhs: number”](#lhs-number) The number to be shifted. ### `rhs: number` [Section titled “rhs: number”](#rhs-number) The number of bit positions to shift to the right. ## Examples [Section titled “Examples”](#examples) ### Shift bits to the right [Section titled “Shift bits to the right”](#shift-bits-to-the-right) ```tql from {x: shift_right(20, 2)} ``` ```tql {x: 5} ``` ## See Also [Section titled “See Also”](#see-also) [`shift_left`](/reference/functions/shift_left), [`bit_and`](/reference/functions/bit_and), [`bit_or`](/reference/functions/bit_or), [`bit_xor`](/reference/functions/bit_xor), [`bit_not`](/reference/functions/bit_not)

# since_epoch

Interprets a time value as duration since the Unix epoch. ```tql since_epoch(x:time) -> duration ``` ## Description [Section titled “Description”](#description) The `since_epoch` function turns a time value into a duration since the [Unix epoch](https://en.wikipedia.org/wiki/Unix_time), i.e., since 00:00:00 UTC on January 1970. ## Examples [Section titled “Examples”](#examples) ### Retrive the Unix time for a given date [Section titled “Retrive the Unix time for a given date”](#retrive-the-unix-time-for-a-given-date) ```tql from { x: since_epoch(2021-02-24) } ``` ```tql {x: 18682.0d} ``` ## See Also [Section titled “See Also”](#see-also) [`from_epoch`](/reference/functions/from_epoch), [`now`](/reference/functions/now)

# slice

Slices a string with offsets and strides. ```tql slice(x:string, [begin=int, end=int, stride=int]) ``` ## Description [Section titled “Description”](#description) The `slice` function takes a string as input and selects parts from it. ### `x: string` [Section titled “x: string”](#x-string) The string to slice. ### `begin = int (optional)` [Section titled “begin = int (optional)”](#begin--int-optional) The offset to start slice from. If negative, offset is calculated from the end of string. Defaults to `0`. ### `end = int (optional)` [Section titled “end = int (optional)”](#end--int-optional) The offset to end the slice at. If negative, offset is calculated from the end of string. If unspecified, ends at the `input`’s end. ### `stride = int (optional)` [Section titled “stride = int (optional)”](#stride--int-optional) The difference between the current character to take and the next character to take. If negative, characters are chosen in reverse. Defaults to `1`. ## Examples [Section titled “Examples”](#examples) ### Get the first 3 characters of a string [Section titled “Get the first 3 characters of a string”](#get-the-first-3-characters-of-a-string) ```tql from {x: "123456789"} x = x.slice(end=3) ``` ```tql {x: "123"} ``` ### Get the 1st, 3rd, and 5th characters [Section titled “Get the 1st, 3rd, and 5th characters”](#get-the-1st-3rd-and-5th-characters) ```tql from {x: "1234567890"} x = x.slice(stride=2, end=6) ``` ```tql {x: "135"} ``` ### Select a substring from the 2nd character up to the 8th character [Section titled “Select a substring from the 2nd character up to the 8th character”](#select-a-substring-from-the-2nd-character-up-to-the-8th-character) ```tql from {x: "1234567890"} x = x.slice(begin=1, end=8) ``` ```tql {x: "2345678"} ```

# sort

Sorts lists and record fields. ```tql sort(xs:list|record) -> list|record ``` ## Description [Section titled “Description”](#description) The `sort` function takes either a list or record as input, ordering lists by value and records by their field name. ### `xs: list|record` [Section titled “xs: list|record”](#xs-listrecord) The list or record to sort. ## Examples [Section titled “Examples”](#examples) ### Sort values in a list [Section titled “Sort values in a list”](#sort-values-in-a-list) ```tql from {xs: [1, 3, 2]} xs = xs.sort() ``` ```tql {xs: [1, 2, 3]} ``` ### Sort a record by its field names [Section titled “Sort a record by its field names”](#sort-a-record-by-its-field-names) ```tql from {a: 1, c: 3, b: {y: true, x: false}} this = this.sort() ``` ```tql {a: 1, b: {y: true, x: false}, c: 3} ``` Note that nested records are not automatically sorted. Use `b = b.sort()` to sort it manually.

# split

Splits a string into substrings. ```tql split(x:string, pattern:string, [max:int], [reverse:bool]) -> list ``` ## Description [Section titled “Description”](#description) The `split` function splits the input string `x` into a list of substrings using the specified `pattern`. Optional arguments allow limiting the number of splits (`max`) and reversing the splitting direction (`reverse`). ### `x: string` [Section titled “x: string”](#x-string) The string to split. ### `pattern: string` [Section titled “pattern: string”](#pattern-string) The delimiter or pattern used for splitting. ### `max: int (optional)` [Section titled “max: int (optional)”](#max-int-optional) The maximum number of splits to perform. Defaults to `0`, meaning no limit. ### `reverse: bool (optional)` [Section titled “reverse: bool (optional)”](#reverse-bool-optional) If `true`, splits from the end of the string. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Split a string by a delimiter [Section titled “Split a string by a delimiter”](#split-a-string-by-a-delimiter) ```tql from {xs: split("a,b,c", ",")} ``` ```tql {xs: ["a", "b", "c"]} ``` ### Limit the number of splits [Section titled “Limit the number of splits”](#limit-the-number-of-splits) ```tql from {xs: split("a-b-c", "-", max=1)} ``` ```tql {xs: ["a", "b-c"]} ``` ## See Also [Section titled “See Also”](#see-also) [`split_regex`](/reference/functions/split_regex), [`join`](/reference/functions/join)

# split_regex

Splits a string into substrings with a regex. ```tql split_regex(x:string, pattern:string, [max:int], [reverse:bool]) -> list ``` ## Description [Section titled “Description”](#description) The `split_regex` function splits the input string `x` into a list of substrings using the specified regular expression `pattern`. Optional arguments allow limiting the number of splits (`max`) and reversing the splitting direction (`reverse`). ### `x: string` [Section titled “x: string”](#x-string) The string to split. ### `pattern: string` [Section titled “pattern: string”](#pattern-string) The regular expression used for splitting. ### `max: int (optional)` [Section titled “max: int (optional)”](#max-int-optional) The maximum number of splits to perform. Defaults to `0`, meaning no limit. ### `reverse: bool (optional)` [Section titled “reverse: bool (optional)”](#reverse-bool-optional) If `true`, splits from the end of the string. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Split a string using a regex pattern [Section titled “Split a string using a regex pattern”](#split-a-string-using-a-regex-pattern) ```tql from {xs: split_regex("a1b2c", r"\d")} ``` ```tql {xs: ["a", "b", "c", ""]} ``` ### Limit the number of splits [Section titled “Limit the number of splits”](#limit-the-number-of-splits) ```tql from {xs: split_regex("a1b2c3", r"\d", max=1)} ``` ```tql {xs: ["a", "b2c3"]} ``` ## See Also [Section titled “See Also”](#see-also) [`split`](/reference/functions/split), [`join`](/reference/functions/join)

# sqrt

Computes the square root of a number. ```tql sqrt(x:number) -> float ``` ## Description [Section titled “Description”](#description) The `sqrt` function computes the [square root](https://en.wikipedia.org/wiki/Square_root) of any non-negative number `x`. ## Examples [Section titled “Examples”](#examples) ### Compute the square root of an integer [Section titled “Compute the square root of an integer”](#compute-the-square-root-of-an-integer) ```tql from {x: sqrt(49)} ``` ```tql {x: 7.0} ``` ### Fail to compute the square root of a negative number [Section titled “Fail to compute the square root of a negative number”](#fail-to-compute-the-square-root-of-a-negative-number) ```tql from {x: sqrt(-1)} ``` ```tql {x: null} ```

# starts_with

Checks if a string starts with a specified substring. ```tql starts_with(x:string, prefix:string) -> bool ``` ## Description [Section titled “Description”](#description) The `starts_with` function returns `true` if `x` starts with `prefix` and `false` otherwise. ## Examples [Section titled “Examples”](#examples) ### Check if a string starts with a substring [Section titled “Check if a string starts with a substring”](#check-if-a-string-starts-with-a-substring) ```tql from {x: "hello".starts_with("he")} ``` ```tql {x: true} ``` ## See Also [Section titled “See Also”](#see-also) [`ends_with`](/reference/functions/ends_with)

# stddev

Computes the standard deviation of all grouped values. ```tql stddev(xs:list) -> float ``` ## Description [Section titled “Description”](#description) The `stddev` function returns the standard deviation of all numeric values in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ## Examples [Section titled “Examples”](#examples) ### Compute the standard deviation of values [Section titled “Compute the standard deviation of values”](#compute-the-standard-deviation-of-values) ```tql from {x: 1}, {x: 2}, {x: 3} summarize stddev_value=stddev(x) ``` ```tql {stddev_value: 0.816} ``` ## See Also [Section titled “See Also”](#see-also) [`variance`](/reference/functions/variance), [`mean`](/reference/functions/mean)

# string

Casts an expression to a string. ```tql string(x:any) -> string ``` ## Description [Section titled “Description”](#description) The `string` function casts the given value `x` to a string. ## Examples [Section titled “Examples”](#examples) ### Cast an IP address to a string [Section titled “Cast an IP address to a string”](#cast-an-ip-address-to-a-string) ```tql from {x: string(1.2.3.4)} ``` ```tql {x: "1.2.3.4"} ``` ## See Also [Section titled “See Also”](#see-also) [`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [int](/reference/functions/int)

# subnet

Casts an expression to a subnet value. ```tql subnet(x:string) -> subnet ``` ## Description [Section titled “Description”](#description) The `subnet` function casts an expression to a subnet. ### `x: string` [Section titled “x: string”](#x-string) The string expression to cast. ## Examples [Section titled “Examples”](#examples) ### Cast a string to a subnet [Section titled “Cast a string to a subnet”](#cast-a-string-to-a-subnet) ```tql from {x: subnet("1.2.3.4/16")} ``` ```tql {x: 1.2.0.0/16} ``` ## See Also [Section titled “See Also”](#see-also) [`int`](/reference/functions/int), [`ip`](/reference/functions/ip), [`is_v4`](/reference/functions/is_v4), [`is_v6`](/reference/functions/is_v6), [`is_multicast`](/reference/functions/is_multicast), [`is_loopback`](/reference/functions/is_loopback), [`is_private`](/reference/functions/is_private), [`is_global`](/reference/functions/is_global), [`is_link_local`](/reference/functions/is_link_local), [`ip_category`](/reference/functions/ip_category), [`time`](/reference/functions/time), [`uint`](/reference/functions/uint), [float](/reference/functions/float), [string](/reference/functions/string)

# sum

Computes the sum of all values. ```tql sum(xs:list) -> int ``` ## Description [Section titled “Description”](#description) The `sum` function computes the total of all number values. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to aggregate. ## Examples [Section titled “Examples”](#examples) ### Compute a sum over a group of events [Section titled “Compute a sum over a group of events”](#compute-a-sum-over-a-group-of-events) ```tql from {x: 1}, {x: 2}, {x: 3} summarize n=sum(x) ``` ```tql {n: 6} ``` ## See Also [Section titled “See Also”](#see-also) [`collect`](/reference/functions/collect), [`max`](/reference/functions/max), [`mean`](/reference/functions/mean), [`min`](/reference/functions/min)

# time

Casts an expression to a time value. ```tql time(x:any) -> time ``` ## Description [Section titled “Description”](#description) The `time` function casts the given string or number `x` to a time value. ## Examples [Section titled “Examples”](#examples) ### Cast a string to a time value [Section titled “Cast a string to a time value”](#cast-a-string-to-a-time-value) ```tql from {x: time("2020-03-15")} ``` ```tql {x: 2020-03-15T00:00:00.000000} ``` ## See Also [Section titled “See Also”](#see-also) [`ip`](/reference/functions/ip), [`string`](/reference/functions/string), [`subnet`](/reference/functions/subnet), [`uint`](/reference/functions/uint), [duration](/reference/functions/duration), [float](/reference/functions/float), [int](/reference/functions/int)

# to_lower

Converts a string to lowercase. ```tql to_lower(x:string) -> string ``` ## Description [Section titled “Description”](#description) The `to_lower` function converts all characters in `x` to lowercase. ## Examples [Section titled “Examples”](#examples) ### Convert a string to lowercase [Section titled “Convert a string to lowercase”](#convert-a-string-to-lowercase) ```tql from {x: "HELLO".to_lower()} ``` ```tql {x: "hello"} ``` ## See Also [Section titled “See Also”](#see-also) [`capitalize`](/reference/functions/capitalize), [`is_lower`](/reference/functions/is_lower), [`to_title`](/reference/functions/to_title), [`to_upper`](/reference/functions/to_upper)

# to_title

Converts a string to title case. ```tql to_title(x:string) -> string ``` ## Description [Section titled “Description”](#description) The `to_title` function converts all words in `x` to title case. ## Examples [Section titled “Examples”](#examples) ### Convert a string to title case [Section titled “Convert a string to title case”](#convert-a-string-to-title-case) ```tql from {x: "hello world".to_title()} ``` ```tql {x: "Hello World"} ``` ## See Also [Section titled “See Also”](#see-also) [`capitalize`](/reference/functions/capitalize), [`is_title`](/reference/functions/is_title), [`to_lower`](/reference/functions/to_lower), [`to_upper`](/reference/functions/to_upper)

# to_upper

Converts a string to uppercase. ```tql to_upper(x:string) -> string ``` ## Description [Section titled “Description”](#description) The `to_upper` function converts all characters in `x` to uppercase. ## Examples [Section titled “Examples”](#examples) ### Convert a string to uppercase [Section titled “Convert a string to uppercase”](#convert-a-string-to-uppercase) ```tql from {x: "hello".to_upper()} ``` ```tql {x: "HELLO"} ``` ## See Also [Section titled “See Also”](#see-also) [`capitalize`](/reference/functions/capitalize), [`is_upper`](/reference/functions/is_upper), [`to_lower`](/reference/functions/to_lower), [`to_title`](/reference/functions/to_title)

# trim

Trims whitespace or specified characters from both ends of a string. ```tql trim(x:string, [chars:string]) -> string ``` ## Description [Section titled “Description”](#description) The `trim` function removes characters from both ends of `x`. When called with one argument, it removes leading and trailing whitespace. When called with two arguments, it removes any characters found in `chars` from both ends of the string. ### `x: string` [Section titled “x: string”](#x-string) The string to trim. ### `chars: string (optional)` [Section titled “chars: string (optional)”](#chars-string-optional) A string where each character represents a character to remove. Any character found in this string will be trimmed from both ends. Defaults to whitespace characters. ## Examples [Section titled “Examples”](#examples) ### Trim whitespace from both ends [Section titled “Trim whitespace from both ends”](#trim-whitespace-from-both-ends) ```tql from {x: " hello ".trim()} ``` ```tql {x: "hello"} ``` ### Trim specific characters [Section titled “Trim specific characters”](#trim-specific-characters) ```tql from {x: "/path/to/file/".trim("/")} ``` ```tql {x: "path/to/file"} ``` ### Trim multiple characters [Section titled “Trim multiple characters”](#trim-multiple-characters) ```tql from {x: "--hello--world--".trim("-")} ``` ```tql {x: "hello--world"} ``` ## See Also [Section titled “See Also”](#see-also) [`trim_start`](/reference/functions/trim_start), [`trim_end`](/reference/functions/trim_end)

# trim_end

Trims whitespace or specified characters from the end of a string. ```tql trim_end(x:string, [chars:string]) -> string ``` ## Description [Section titled “Description”](#description) The `trim_end` function removes characters from the end of `x`. When called with one argument, it removes trailing whitespace. When called with two arguments, it removes any characters found in `chars` from the end of the string. ### `x: string` [Section titled “x: string”](#x-string) The string to trim. ### `chars: string (optional)` [Section titled “chars: string (optional)”](#chars-string-optional) A string where each character represents a character to remove. Any character found in this string will be trimmed from the end. Defaults to whitespace characters. ## Examples [Section titled “Examples”](#examples) ### Trim whitespace from the end [Section titled “Trim whitespace from the end”](#trim-whitespace-from-the-end) ```tql from {x: "hello ".trim_end()} ``` ```tql {x: "hello"} ``` ### Trim specific characters [Section titled “Trim specific characters”](#trim-specific-characters) ```tql from {x: "/path/to/file/".trim_end("/")} ``` ```tql {x: "/path/to/file"} ``` ### Trim multiple characters [Section titled “Trim multiple characters”](#trim-multiple-characters) ```tql from {x: "hello/-/".trim_end("/-")} ``` ```tql {x: "hello"} ``` ## See Also [Section titled “See Also”](#see-also) [`trim`](/reference/functions/trim), [`trim_start`](/reference/functions/trim_start)

# trim_start

Trims whitespace or specified characters from the start of a string. ```tql trim_start(x:string, [chars:string]) -> string ``` ## Description [Section titled “Description”](#description) The `trim_start` function removes characters from the beginning of `x`. When called with one argument, it removes leading whitespace. When called with two arguments, it removes any characters found in `chars` from the start of the string. ### `x: string` [Section titled “x: string”](#x-string) The string to trim. ### `chars: string (optional)` [Section titled “chars: string (optional)”](#chars-string-optional) A string where each character represents a character to remove. Any character found in this string will be trimmed from the start. Defaults to whitespace characters. ## Examples [Section titled “Examples”](#examples) ### Trim whitespace from the start [Section titled “Trim whitespace from the start”](#trim-whitespace-from-the-start) ```tql from {x: " hello".trim_start()} ``` ```tql {x: "hello"} ``` ### Trim specific characters [Section titled “Trim specific characters”](#trim-specific-characters) ```tql from {x: "/path/to/file".trim_start("/")} ``` ```tql {x: "path/to/file"} ``` ### Trim multiple characters [Section titled “Trim multiple characters”](#trim-multiple-characters) ```tql from {x: "/-/hello".trim_start("/-")} ``` ```tql {x: "hello"} ``` ## See Also [Section titled “See Also”](#see-also) [`trim`](/reference/functions/trim), [`trim_end`](/reference/functions/trim_end)

# type_id

Retrieves the type id of an expression. ```tql type_id(x:any) -> string ``` ## Description [Section titled “Description”](#description) The `type_id` function returns the type id of the given value `x`. ## Examples [Section titled “Examples”](#examples) ### Retrieve the type of a numeric expression [Section titled “Retrieve the type of a numeric expression”](#retrieve-the-type-of-a-numeric-expression) ```tql from {x: type_id(1 + 3.2)} ``` ```tql {x: "41615fdb30a38aaf"} ``` ## See also [Section titled “See also”](#see-also) [`type_of`](/reference/functions/type_of)

# type_of

Retrieves the type definition of an expression. ```tql type_of(x:any) -> record ``` ## Description [Section titled “Description”](#description) The `type_of` function returns the type definition of the given value `x`. Subject to change This function is designed for internal use of the Tenzir Platform and its output format is subject to change without notice. ## Examples [Section titled “Examples”](#examples) ### Retrieve the type definition of a schema [Section titled “Retrieve the type definition of a schema”](#retrieve-the-type-definition-of-a-schema) ```tql from {x: 1, y: "2"} this = type_of(this) ``` ```tql { name: "tenzir.from", kind: "record", attributes: [], state: { fields: [ { name: "x", type: { name: null, kind: "int64", attributes: [], state: null, }, }, { name: "y", type: { name: null, kind: "string", attributes: [], state: null, }, }, ], }, } ``` ## See also [Section titled “See also”](#see-also) [`type_id`](/reference/functions/type_id)

# uint

Casts an expression to an unsigned integer. ```tql uint(x:number|string, base=int) -> uint ``` ## Description [Section titled “Description”](#description) The `uint` function casts the provided value `x` to an unsigned integer. Non-integer values are truncated. ### `x: number|string` [Section titled “x: number|string”](#x-numberstring) The input to convert. ### `base = int` [Section titled “base = int”](#base--int) Base (radix) to parse a string as. Can be `10` or `16`. If `16`, the string inputs may be optionally prefixed by `0x` or `0X`, e.g., `0x134`. Defaults to `10`. ## Examples [Section titled “Examples”](#examples) ### Cast a floating-point number to an unsigned integer [Section titled “Cast a floating-point number to an unsigned integer”](#cast-a-floating-point-number-to-an-unsigned-integer) ```tql from {x: uint(4.2)} ``` ```tql {x: 4} ``` ### Parse a hexadecimal number [Section titled “Parse a hexadecimal number”](#parse-a-hexadecimal-number) ```tql from {x: uint("0x42", base=16)} ``` ```tql {x: 66} ``` ## See Also [Section titled “See Also”](#see-also) [`ip`](/reference/functions/ip), [`subnet`](/reference/functions/subnet), [`time`](/reference/functions/time), [float](/reference/functions/float), [int](/reference/functions/int), [string](/reference/functions/string)

# unflatten

Unflattens nested data. ```tql unflatten(x:record, [separator=string]) -> record ``` ## Description [Section titled “Description”](#description) The `unflatten` function creates nested records out of fields whose names include a separator. ### `x: record` [Section titled “x: record”](#x-record) The record you want to unflatten. ### `separator: string (optional)` [Section titled “separator: string (optional)”](#separator-string-optional) The separator to use for splitting field names. Defaults to `"."`. ## Examples [Section titled “Examples”](#examples) ### Unflatten fields at the dot character [Section titled “Unflatten fields at the dot character”](#unflatten-fields-at-the-dot-character) ```tql // Note the fields in double quotes that are single fields that contain a // literal "." in their field name, as opposed to nested records. from { src_ip: 147.32.84.165, src_port: 1141, dest_ip: 147.32.80.9, dest_port: 53, event_type: "dns", "dns.type": "query", "dns.id": 553, "dns.rrname": "irc.freenode.net", "dns.rrtype": "A", "dns.tx_id": 0, "dns.grouped.A": ["tenzir.com"], } this = unflatten(this) ``` ```tql { src_ip: 147.32.84.165, src_port: 1141, dest_ip: 147.32.80.9, dest_port: 53, event_type: "dns", dns: { type: "query", id: 553, rrname: "irc.freenode.net", rrtype: "A", tx_id: 0, grouped: { A: [ "tenzir.com", ], }, }, } ``` ## See Also [Section titled “See Also”](#see-also) [`flatten`](/reference/functions/flatten)

# uuid

Generates a Universally Unique Identifier (UUID) string. ```tql uuid([version=string]) -> string ``` ## Description [Section titled “Description”](#description) The `uuid` function generates a [Universally Unique Identifier (UUID)](https://en.wikipedia.org/wiki/Universally_unique_identifier) string. UUIDs, 128-bit numbers, uniquely identify information in computer systems. This function generates several UUID versions based on relevant standards like [RFC 4122](https://www.rfc-editor.org/rfc/rfc4122.html) and the newer [RFC 9562](https://www.rfc-editor.org/rfc/rfc9562.html) which defines versions 6 and 7. ### `version = string (optional)` [Section titled “version = string (optional)”](#version--string-optional) Specifies the version of the UUID to generate. If you omit this argument, the function uses `"v4"` by default. It supports the following values: * `"v1"`: Generates a time-based UUID using a timestamp and node ID. * `"v4"`: Generates a randomly generated UUID using a cryptographically strong random number generator (see RFC 4122). **This is the default.** * `"v6"`: Generates a time-based UUID, similar to v1 but reordered for better database index locality and lexical sorting (see RFC 9562). * `"v7"`: Generates a time-based UUID using a Unix timestamp and random bits, designed to be monotonically increasing (suitable for primary keys, see RFC 9562). * `"nil"`: Generates the special “nil” UUID, which consists entirely of zeros: `00000000-0000-0000-0000-000000000000`. Defaults to `"v4"`. ## Examples [Section titled “Examples”](#examples) ### Generate a random default (v4) UUID [Section titled “Generate a random default (v4) UUID”](#generate-a-random-default-v4-uuid) ```tql from {guid: uuid()} ``` ```tql {guid: "f47ac10b-58cc-4372-a567-0e02b2c3d479"} ``` ### Generate a random version 7 UUID [Section titled “Generate a random version 7 UUID”](#generate-a-random-version-7-uuid) ```tql from {guid: uuid(version="v7")} ``` ```tql {guid: "018ecb4f-abc1-7123-8def-0123456789ab"} ``` ### Generate the nil UUID [Section titled “Generate the nil UUID”](#generate-the-nil-uuid) ```tql from {guid: uuid(version="nil")} ``` ```tql {guid: "00000000-0000-0000-0000-000000000000"} ``` ## See Also [Section titled “See Also”](#see-also) [`random`](/reference/functions/random)

# value_counts

Returns a list of all grouped values alongside their frequency. ```tql value_counts(xs:list) -> list ``` ## Description [Section titled “Description”](#description) The `value_counts` function returns a list of all unique non-null values in `xs` alongside their occurrence count. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ## Examples [Section titled “Examples”](#examples) ### Get value counts [Section titled “Get value counts”](#get-value-counts) ```tql from {x: 1}, {x: 2}, {x: 2}, {x: 3} summarize counts=value_counts(x) ``` ```tql {counts: [{value: 1, count: 1}, {value: 2, count: 2}, {value: 3, count: 1}]} ``` ## See Also [Section titled “See Also”](#see-also) [`mode`](/reference/functions/mode), [`distinct`](/reference/functions/distinct)

# variance

Computes the variance of all grouped values. ```tql variance(xs:list) -> float ``` ## Description [Section titled “Description”](#description) The `variance` function returns the variance of all numeric values in `xs`. ### `xs: list` [Section titled “xs: list”](#xs-list) The values to evaluate. ## Examples [Section titled “Examples”](#examples) ### Compute the variance of values [Section titled “Compute the variance of values”](#compute-the-variance-of-values) ```tql from {x: 1}, {x: 2}, {x: 3} summarize variance_value=variance(x) ``` ```tql {variance_value: 0.666} ``` ## See Also [Section titled “See Also”](#see-also) [`stddev`](/reference/functions/stddev), [`mean`](/reference/functions/mean)

# weeks

Converts a number to equivalent weeks. ```tql weeks(x:number) -> duration ``` ## Description This function returns weeks equivalent to a number, i.e., `number * 1w`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# where

Filters list elements based on a predicate. ```tql where(xs:list, any->bool) -> list ``` ## Description [Section titled “Description”](#description) The `where` function keeps only elements of a list for which a predicate evaluates to `true`. ### `xs: list` [Section titled “xs: list”](#xs-list) A list of values. ### `function: any -> bool` [Section titled “function: any -> bool”](#function-any---bool) A lambda function that is evaluated for each list element. ## Examples [Section titled “Examples”](#examples) ### Keep only elements greater than 3 [Section titled “Keep only elements greater than 3”](#keep-only-elements-greater-than-3) ```tql from { xs: [1, 2, 3, 4, 5] } xs = xs.where(x => x > 3) ``` ```tql { xs: [4, 5] } ``` ## See Also [Section titled “See Also”](#see-also) [`map`](/reference/functions/map)

# year

Extracts the year component from a timestamp. ```tql year(x: time) -> int ``` ## Description [Section titled “Description”](#description) The `year` function extracts the year component from a timestamp as an integer. ### `x: time` [Section titled “x: time”](#x-time) The timestamp from which to extract the year. ## Examples [Section titled “Examples”](#examples) ### Extract the year from a timestamp [Section titled “Extract the year from a timestamp”](#extract-the-year-from-a-timestamp) ```tql from { ts: 2024-06-15T14:30:45.123456, } year = ts.year() ``` ```tql { ts: 2024-06-15T14:30:45.123456, year: 2024, } ``` ## See also [Section titled “See also”](#see-also) [`month`](/reference/functions/month), [`day`](/reference/functions/day), [`hour`](/reference/functions/hour), [`minute`](/reference/functions/minute), [`second`](/reference/functions/second)

# years

Converts a number to equivalent years. ```tql years(x:number) -> duration ``` ## Description This function returns years equivalent to a number, i.e., `number * 1y`. ### `x: number` The number to convert. ## See Also [`years`](/reference/functions/years), [`months`](/reference/functions/months), [`weeks`](/reference/functions/weeks), [`days`](/reference/functions/days), [`hours`](/reference/functions/hours), [`minutes`](/reference/functions/minutes), [`seconds`](/reference/functions/seconds), [`milliseconds`](/reference/functions/milliseconds), [`microseconds`](/reference/functions/microseconds), [`nanoseconds`](/reference/functions/nanoseconds)

# zip

Combines two lists into a list of pairs. ```tql zip(xs:list, ys:list) -> list ``` ## Description [Section titled “Description”](#description) The `zip` function returns a list containing records with two fields `left` and `right`, each containing the respective elements of the input lists. If both lists are null, `zip` returns null. If one of the lists is null or has a mismatching length, missing values are filled in with nulls, using the longer list’s length, and a warning is emitted. ## Examples [Section titled “Examples”](#examples) ### Combine two lists [Section titled “Combine two lists”](#combine-two-lists) ```tql from {xs: [1, 2], ys: [3, 4]} select zs = zip(xs, ys) ``` ```tql { zs: [ {left: 1, right: 3}, {left: 2, right: 4} ] } ``` ## See Also [Section titled “See Also”](#see-also) [`concatenate`](/reference/functions/concatenate), [`map`](/reference/functions/map)

# MCP Server

The [Tenzir MCP Server](https://github.com/tenzir/mcp) enables AI assistants to interact with Tenzir through the [Model Context Protocol](https://modelcontextprotocol.io) (MCP). ## Tools [Section titled “Tools”](#tools) The MCP server provides several *tools* that your AI agent can call. Work in progress The MCP server is changing rapidly at the moment. Stay tuned for more content here soon!

# Node Configuration

The below example configuration ships with every Tenzir package. Head over to the [explanation of the configuration](/explanations/configuration) for details on how the various settings work. tenzir.yaml ```yaml # This is an example configuration file for Tenzir that shows all available # options. Options in angle brackets have their default value determined at # runtime. # Options that concern Tenzir. tenzir: # The token that is offered when connecting to the Tenzir Platform. # It is used to identify the node and assign it to the correct workspace. # This setting is ignored in the open-source edition of Tenzir, which does # not contain the platform plugin. token: tnz_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX # The host and port to listen at for node-to-node connections in the form # `<host>:<port>`. Host or port may be emitted to use their defaults, which # are localhost and 5158, respectively. Set the port to zero to automatically # choose a port. Set to false to disable exposing an endpoint. endpoint: localhost:5158 # The timeout for connecting to a Tenzir server. Set to 0 seconds to wait # indefinitely. connection-timeout: 5m # The delay between two connection attempts. Set to 0s to try connecting # without retries. connection-retry-delay: 3s # Configure retention policies. retention: # How long to keep metrics for. Set to 0s to disable metrics retention # entirely. # WARNING: A low retention period may negatively impact the usability of # pipeline activity in the Tenzir Platform. #metrics: 7d # How long to keep diagnostics for. Set to 0s to disable diagnostics # retention entirely. # WARNING: A low retention period may negatively impact the usability of # diagnostics in the Tenzir Platform. #diagnostics: 30d # Configure the behavior of the `cache` operator. The Tenzir Platform uses the # cache operator to store and retrieve data efficiently. cache: # Specifies the default lifetime for the `cache` operator. #lifetime: 10min # Specifies an upper bound for the total memory usage in bytes across all # caches in a node. If the memory usage exceeds this limit, the node will # start evicting caches to make room for new data. The node requires a # minimum total cache capacity of 64MiB. #capacity: 1Gi # A certificate file used as the default for operators accepting a `cacert` # option. This will default to an appropriate directory for the system. For # example: # - /etc/ssl/certs/ca-bundle.crt on RedHat # - /etc/ssl/certs/ca-certificates.crt on Ubuntu #cacert: # The file system path used for persistent state. # Defaults to one of the following paths, selecting the first that is # available: # - $STATE_DIRECTORY # - $PWD/tenzir.db #state-directory: # The file system path used for recoverable state. # In a node process, defaults to the first of the following paths that is # available: # - $CACHE_DIRECTORY # - $XDG_CACHE_HOME # - $XDG_HOME_DIR/.cache/tenzir (linux) or $XDG_HOME_DIR/Libraries/caches/tenzir (mac) # - $HOME/.cache/tenzir (linux) or $HOME/Libraries/caches/tenzir (mac) # - $TEMPORARY_DIRECTORY/tenzir-cache-<uid> # To determine $TEMPORARY_DIRECTORY, the values of TMPDIR, TMP, TEMP, TEMPDIR are # checked in that order, and as a last resort "/tmp" is used. # In a client process, this setting is ignored and # `$TEMPORARY_DIRECTORY/tenzir-client-cache-<uid>` is used as cache directory. #cache-directory: # The file system path used for log files. # Defaults to one of the following paths, selecting the first that is # available: # - $LOGS_DIRECTORY/server.log # - <state-directory>/server.log #log-file: # The file system path used for client log files relative to the current # working directory of the client. Note that this is disabled by default. # If not specified no log files are written for clients at all. client-log-file: "client.log" # Format for printing individual log entries to the log-file. # For a list of valid format specifiers, see spdlog format specification # at https://github.com/gabime/spdlog/wiki/3.-Custom-formatting. file-format: "[%Y-%m-%dT%T.%e%z] [%n] [%l] [%s:%#] %v" # Configures the minimum severity of messages written to the log file. # Possible values: quiet, error, warning, info, verbose, debug, trace. # File logging is only available for commands that start a node (e.g., # tenzir-node). The levels above 'verbose' are usually not available in # release builds. file-verbosity: debug # Whether to enable automatic log rotation. If set to false, a new log file # will be created when the size of the current log file exceeds 10 MiB. disable-log-rotation: false # The size limit when a log file should be rotated. log-rotation-threshold: 10MiB # Maximum number of log messages in the logger queue. log-queue-size: 1000000 # The sink type to use for console logging. Possible values: stderr, # syslog, journald. Note that 'journald' can only be selected on linux # systems, and only if Tenzir was built with journald support. # The journald sink is used as default if Tenzir is started as a systemd # service and the service is configured to use the journal for stderr, # otherwise the default is the unstructured stderr sink. #console-sink: stderr/journald # Mode for console log output generation. Automatic renders color only when # writing to a tty. # Possible values: always, automatic, never. (default automatic) console: automatic # Format for printing individual log entries to the console. For a list # of valid format specifiers, see spdlog format specification at # https://github.com/gabime/spdlog/wiki/3.-Custom-formatting. console-format: "%^[%T.%e] %v%$" # Configures the minimum severity of messages written to the console. # For a list of valid log levels, see file-verbosity. console-verbosity: info # List of directories to look for schema files in ascending order of # priority. schema-dirs: [] # Additional directories to load plugins specified using `tenzir.plugins` # from. plugin-dirs: [] # List of paths that contain statically configured packages. # This setting is ignored unless the package manager plugin is enabled. package-dirs: [] # The plugins to load at startup. For relative paths, Tenzir tries to find # the files in the specified `tenzir.plugin-dirs`. The special values # 'bundled' and 'all' enable autoloading of bundled and all plugins # respectively. Note: Add `example` or `/path/to/libtenzir-plugin-example.so` # to load the example plugin. plugins: [] # Names of plugins and builtins to explicitly forbid from being used in # Tenzir. For example, adding `shell` will prohibit use of the `shell` # operator builtin, and adding `kafka` will prohibit use of the `kafka` # connector plugin. disable-plugins: [] # Forbid unsafe location overrides for pipelines with the 'local' and 'remote' # keywords, e.g., remotely reading from a file. no-location-overrides: false # Prevent all pipelines from automatically starting when the node starts. no-autostart: false # Do not move pipeline operators to subprocesses. disable-pipeline-subprocesses: false # The size of an index shard, expressed in number of events. This should # be a power of 2. max-partition-size: 4Mi # Timeout after which the importer forwards events to subscribers like `export # live=true` or `metrics live=true`. Set to 0s for an unbuffered mode. A # higher value increases performance, and a lower value reduces latency. import-buffer-timeout: 1s # Timeout after which an active partition is forcibly flushed, regardless of # its size. active-partition-timeout: 30s # Maximum number of events across all active partitions. This indirectly # controls the maximum memory usage when importing events. max-buffered-events: 12Mi # Automatically rebuild undersized and outdated partitions in the background. # The given number controls how much resources to spend on it. Set to 0 to # disable. automatic-rebuild: 1 # Timeout after which an automatic rebuild is triggered. rebuild-interval: 30min # Zstd compression level applied to the Feather store backend. # zstd-compression-level: <default> # The URL of the control endpoint when connecting to a self-hosted # instance of the Tenzir Platform. platform-control-endpoint: wss://ws.tenzir.app/production # Whether to undermine the security of the TLS connection to the # Tenzir Platform by disabling certificate validation. # Setting this to `true` is strongly discouraged. platform-skip-peer-verification: false # The name to use when connecting to the platform as an ephemeral node. # This setting is ignored unless a workspace token is used to connect to # the platform. Workspace tokens are currently only available for the # Sovereign Edition of the Tenzir Platform. platform-ephemeral-node-name: Ephemeral Node # Control how operator's calculate demand from their upstream operator. Note # that this is an expert feature and should only be changed if you know what # you are doing. The configured values can also be changed per operator by # using the `_tune` operator. demand: # Issue demand only if room for at least this many elements is available. # Must be greater than zero. Values may either be set to a number, or to a # record containing `bytes` and `events` fields with numbers depending on # the operator's input type. min-elements: bytes: 128Ki events: 8Ki # Controls how many elements may be buffered until the operator stops # issuing demand. Must be greater or equal to min-elements. Values may # either be set to a number, or to a record containing `bytes` and `events` # fields with numbers depending on the operator's input type. max-elements: bytes: 4Mi events: 254Ki # Controls how many batches of elements may be buffered until the operator # stops issuing demand. Must be greater than zero. max-batches: 20 # Controls the minimum backoff duration after an operator is detected to be # idle. Must be at least 10ms. min-backoff: 10ms # Controls the maximum backoff duration after an operator is detected to be # idle. Must be at least 10ms. max-backoff: 1s # Controls the growth rate of the backoff duration for operators that # continue to be idle. Must be at least 1.0. Note that setting a growth rate # of 1.0 causes the `max-backoff` duration to be ignored, replacing the # exponential growth with a constant value. backoff-rate: 2.0 # Context configured as part of the configuration that are always available. contexts: # A unique name for the context that's used in the context, enrich, and # lookup operators to refer to the context. indicators: # The type of the context. type: bloom-filter # Arguments for creating the context, depending on the type. Refer to the # documentation of the individual context types to see the arguments they # require. Note that changes to these arguments to not apply to any # contexts that were previously created. arguments: capacity: 1B fp-probability: 0.001 # The `index` key is used to adjust the false-positive rate of # the first-level lookup data structures (called synopses) in the # catalog. The lower the false-positive rate the more space will be # required, so this setting can be used to manually tune the trade-off # of performance vs. space. index: # The default false-positive rate for type synopses. default-fp-rate: 0.01 # rules: # Every rule adjusts the behaviour of Tenzir for a set of targets. # Tenzir creates one synopsis per target. Targets can be either types # or field names. # # fp-rate - false positive rate. Has effect on string and address type # targets # # partition-index - Tenzir will not create dense index when set to false # - targets: [:ip] # fp-rate: 0.01 # The `tenzir-ctl start` command starts a new Tenzir server process. start: # Prints the endpoint for clients when the server is ready to accept # connections. This comes in handy when letting the OS choose an # available random port, i.e., when specifying 0 as port value. print-endpoint: false # Writes the endpoint for clients when the server is ready to accept # connections to the specified destination. This comes in handy when letting # the OS choose an available random port, i.e., when specifying 0 as port # value, and `print-endpoint` is not sufficient. #write-endpoint: /tmp/tenzir-node-endpoint # An ordered list of commands to run inside the node after starting. # As an example, to configure an auto-starting PCAP source that listens # on the interface 'en0' and lives inside the Tenzir node, add `spawn # source pcap -i en0`. # Note that commands are not executed sequentially but in parallel. commands: [] # Triggers removal of old data when the disk budget is exceeded. disk-budget-high: 0GiB # When the budget was exceeded, data is erased until the disk space is # below this value. disk-budget-low: 0GiB # Seconds between successive disk space checks. disk-budget-check-interval: 90 # When erasing, how many partitions to erase in one go before rechecking # the size of the database directory. disk-budget-step-size: 1 # Binary to use for checking the size of the database directory. If left # unset, Tenzir will recursively add up the size of all files in the # database directory to compute the size. Mainly useful for e.g. # compressed filesystem where raw file size is not the correct metric. # Must be the absolute path to an executable file, which will get passed # the database directory as its first and only argument. #disk-budget-check-binary: /opt/tenzir/libexec/tenzir-df-percent.sh # User-defined operators. operators: # The Zeek operator is an example that takes raw bytes in the form of a # PCAP and then parses Zeek's output via the `zeek-json` format to generate # a stream of events. zeek: | shell "zeek -r - LogAscii::output_to_stdout=T JSONStreaming::disable_default_logs=T JSONStreaming::enable_log_rotation=F json-streaming-logs" read_zeek_json # The Suricata operator is analogous to the above Zeek example, with the # difference that we are using Suricata. The commmand line configures # Suricata such that it reads PCAP on stdin and produces EVE JSON logs on # stdout, which we then parse with the `suricata` format. suricata: | shell "suricata -r /dev/stdin --set outputs.1.eve-log.filename=/dev/stdout --set logging.outputs.0.console.enabled=no" read_suricata # In addition to running pipelines interactively, you can also deploy # *Pipelines as Code*. This infrastrucutre-as-code-like method differs from # pipelines run on the command-line or through app.tenzir.com in two ways: # 1. Pipelines deployed as code always start alongside the Tenzir node. # 2. Deletion via the user interface is not allowed for pipelines configured # as code. pipelines: # A unique identifier for the pipeline that's used for metrics, diagnostics, # and API calls interacting with the pipeline. publish-suricata: # An optional user-facing name for the pipeline. Defaults to the id. name: Import Suricata from TCP # The definition of the pipeline. Configured pipelines that fail to start # cause the node to fail to start. definition: | load_tcp "0.0.0.0:34343" { read_suricata schema_only=true } | where event_type != "stats" | publish "suricata" # Pipelines that encounter an error stop running and show an error state. # This option causes pipelines to automatically restart when they # encounter an error instead. The first restart happens immediately, and # subsequent restarts after the configured delay, defaulting to 1 minute. # The following values are valid for this option: # - Omit the option, or set it to null or false to disable. # - Set the option to true to enable with the default delay of 1 minute. # - Set the option to a valid duration to enable with a custom delay. restart-on-error: 1 minute # Pipelines that are unstoppable will run automatically and indefinitely. # They are not able to pause or stop. # If they do complete, they will end up in a failed state. # If `restart-on-error` is enabled, they will restart after the specified # duration. unstoppable: false # Use the legacy secret model. Under this model, the `secret` function yields # plain `string`s and can only look up secrets from the `tenzir.secrets` # section in this config, but not from the Tenzir Platform's secret store. legacy-secret-model: false # Enables the `secret_assert` operator. This operator can be used for our # integration tests and may be useful to test local setups. # Since it theoretically allows for brute-forcing a secret's value, it is # disabled by default. enable-assert-secret-operator: false # Local secrets, defined as key - value pairs. The values must be strings secrets: # my-secret-name: my-secret-value # The below settings are internal to CAF, and aren't checked by Tenzir directly. # Please be careful when changing these options. Note that some CAF options may # be in conflict with Tenzir options, and are only listed here for completeness. caf: # Options affecting the internal scheduler. scheduler: # Accepted alternative: "sharing". policy: stealing # Configures whether the scheduler generates profiling output. enable-profiling: false # Output file for profiler data (only if profiling is enabled). #profiling-output-file: </dev/null> # Measurement resolution in milliseconds (only if profiling is enabled). profiling-resolution: 100ms # Forces a fixed number of threads if set. Defaults to the number of # available CPU cores if starting a Tenzir node, or *2* for client commands. #max-threads: <number of cores> # Maximum number of messages actors can consume in one run. max-throughput: 500 # When using "stealing" as scheduler policy. work-stealing: # Number of zero-sleep-interval polling attempts. aggressive-poll-attempts: 100 # Frequency of steal attempts during aggressive polling. aggressive-steal-interval: 10 # Number of moderately aggressive polling attempts. moderate-poll-attempts: 500 # Frequency of steal attempts during moderate polling. moderate-steal-interval: 5 # Sleep interval between poll attempts. moderate-sleep-duration: 50us # Frequency of steal attempts during relaxed polling. relaxed-steal-interval: 1 # Sleep interval between poll attempts. relaxed-sleep-duration: 10ms stream: # Maximum delay for partial batches. max-batch-delay: 15ms # Selects an implementation for credit computation. # Accepted alternative: "token-based". credit-policy: token-based # When using "size-based" as credit-policy. size-based-policy: # Desired batch size in bytes. bytes-per-batch: 32 # Maximum input buffer size in bytes. buffer-capacity: 256 # Frequency of collecting batch sizes. sampling-rate: 100 # Frequency of re-calibrations. calibration-interval: 1 # Factor for discounting older samples. smoothing-factor: 2.5 # When using "token-based" as credit-policy. token-based-policy: # Number of elements per batch. batch-size: 1 # Max. number of elements in the input buffer. buffer-size: 64 # Collecting metrics can be resource consuming. This section is used for # filtering what should and what should not be collected metrics-filters: # Rules for actor based metrics filtering. actors: # List of selected actors for run-time metrics. includes: [] # List of excluded actors from run-time metrics. excludes: [] # Configure using OpenSSL for node-to-node connections. # NOTE: Use the tenzir.endpoint variable to configure the endpoint. openssl: # Path to the PEM-formatted certificate file. certificate: # Path to the private key file for this node. key: # Passphrase to decrypt the private key. passphrase: # Path to an OpenSSL-style directory of trusted certificates. capath: # Path to a file of concatenated PEM-formatted certificates. cafile: # Colon-separated list of OpenSSL cipher strings to use. cipher-list: ```

# Operators

Tenzir comes with a wide range of built-in pipeline operators. ## Analyze [Section titled “Analyze”](#analyze) ### [rare](/reference/operators/rare) [→](/reference/operators/rare) Shows the least common values. ```tql rare auth.token ``` ### [reverse](/reference/operators/reverse) [→](/reference/operators/reverse) Reverses the event order. ```tql reverse ``` ### [sort](/reference/operators/sort) [→](/reference/operators/sort) Sorts events by the given expressions. ```tql sort name, -abs(transaction) ``` ### [summarize](/reference/operators/summarize) [→](/reference/operators/summarize) Groups events and applies aggregate functions to each group. ```tql summarize name, sum(amount) ``` ### [top](/reference/operators/top) [→](/reference/operators/top) Shows the most common values. ```tql top user ``` ## Charts [Section titled “Charts”](#charts) ### [chart\_area](/reference/operators/chart_area) [→](/reference/operators/chart_area) Plots events on an area chart. ```tql chart_area … ``` ### [chart\_bar](/reference/operators/chart_bar) [→](/reference/operators/chart_bar) Plots events on an bar chart. ```tql chart_bar … ``` ### [chart\_line](/reference/operators/chart_line) [→](/reference/operators/chart_line) Plots events on an line chart. ```tql chart_line … ``` ### [chart\_pie](/reference/operators/chart_pie) [→](/reference/operators/chart_pie) Plots events on an pie chart. ```tql chart_pie … ``` ## Connecting Pipelines [Section titled “Connecting Pipelines”](#connecting-pipelines) ### [publish](/reference/operators/publish) [→](/reference/operators/publish) Publishes events to a channel with a topic. ```tql publish "topic" ``` ### [subscribe](/reference/operators/subscribe) [→](/reference/operators/subscribe) Subscribes to events from a channel with a topic. ```tql subscribe "topic" ``` ## Contexts [Section titled “Contexts”](#contexts) ### [context::create\_bloom\_filter](/reference/operators/context/create_bloom_filter) [→](/reference/operators/context/create_bloom_filter) Creates a Bloom filter context. ```tql context::create_bloom_filter "ctx", capacity=1Mi, fp_probability=0.01 ``` ### [context::create\_geoip](/reference/operators/context/create_geoip) [→](/reference/operators/context/create_geoip) Creates a GeoIP context. ```tql context::create_geoip "ctx", db_path="GeoLite2-City.mmdb" ``` ### [context::create\_lookup\_table](/reference/operators/context/create_lookup_table) [→](/reference/operators/context/create_lookup_table) Creates a lookup table context. ```tql context::create_lookup_table "ctx" ``` ### [context::enrich](/reference/operators/context/enrich) [→](/reference/operators/context/enrich) Enriches events with data from a context. ```tql context::enrich "ctx", key=x ``` ### [context::erase](/reference/operators/context/erase) [→](/reference/operators/context/erase) Removes entries from a context. ```tql context::erase "ctx", key=x ``` ### [context::inspect](/reference/operators/context/inspect) [→](/reference/operators/context/inspect) Resets a context. ```tql context::inspect "ctx" ``` ### [context::list](/reference/operators/context/list) [→](/reference/operators/context/list) Lists all contexts ```tql context::list ``` ### [context::load](/reference/operators/context/load) [→](/reference/operators/context/load) Loads context state. ```tql context::load "ctx" ``` ### [context::remove](/reference/operators/context/remove) [→](/reference/operators/context/remove) Deletes a context. ```tql context::remove "ctx" ``` ### [context::reset](/reference/operators/context/reset) [→](/reference/operators/context/reset) Resets a context. ```tql context::reset "ctx" ``` ### [context::save](/reference/operators/context/save) [→](/reference/operators/context/save) Saves context state. ```tql context::save "ctx" ``` ### [context::update](/reference/operators/context/update) [→](/reference/operators/context/update) Updates a context with new data. ```tql context::update "ctx", key=x, value=y ``` ## Detection [Section titled “Detection”](#detection) ### [sigma](/reference/operators/sigma) [→](/reference/operators/sigma) Filter the input with Sigma rules and output matching events. ```tql sigma "/tmp/rules/" ``` ### [yara](/reference/operators/yara) [→](/reference/operators/yara) Executes YARA rules on byte streams. ```tql yara "/path/to/rules", blockwise=true ``` ## Encode & Decode [Section titled “Encode & Decode”](#encode--decode) ### [compress](/reference/operators/compress) [→](/reference/operators/compress) Compresses a stream of bytes. ```tql compress "zstd" ``` ### [compress\_brotli](/reference/operators/compress_brotli) [→](/reference/operators/compress_brotli) Compresses a stream of bytes using Brotli compression. ```tql compress_brotli, level=10 ``` ### [compress\_bz2](/reference/operators/compress_bz2) [→](/reference/operators/compress_bz2) Compresses a stream of bytes using bz2 compression. ```tql compress_bz2, level=9 ``` ### [compress\_gzip](/reference/operators/compress_gzip) [→](/reference/operators/compress_gzip) Compresses a stream of bytes using gzip compression. ```tql compress_gzip, level=8 ``` ### [compress\_lz4](/reference/operators/compress_lz4) [→](/reference/operators/compress_lz4) Compresses a stream of bytes using lz4 compression. ```tql compress_lz4, level=7 ``` ### [compress\_zstd](/reference/operators/compress_zstd) [→](/reference/operators/compress_zstd) Compresses a stream of bytes using zstd compression. ```tql compress_zstd, level=6 ``` ### [decompress](/reference/operators/decompress) [→](/reference/operators/decompress) Decompresses a stream of bytes. ```tql decompress "gzip" ``` ### [decompress\_brotli](/reference/operators/decompress_brotli) [→](/reference/operators/decompress_brotli) Decompresses a stream of bytes in the Brotli format. ```tql decompress_brotli ``` ### [decompress\_bz2](/reference/operators/decompress_bz2) [→](/reference/operators/decompress_bz2) Decompresses a stream of bytes in the Bzip2 format. ```tql decompress_bz2 ``` ### [decompress\_gzip](/reference/operators/decompress_gzip) [→](/reference/operators/decompress_gzip) Decompresses a stream of bytes in the Gzip format. ```tql decompress_gzip ``` ### [decompress\_lz4](/reference/operators/decompress_lz4) [→](/reference/operators/decompress_lz4) Decompresses a stream of bytes in the Lz4 format. ```tql decompress_lz4 ``` ### [decompress\_zstd](/reference/operators/decompress_zstd) [→](/reference/operators/decompress_zstd) Decompresses a stream of bytes in the Zstd format. ```tql decompress_zstd ``` ## Escape Hatches [Section titled “Escape Hatches”](#escape-hatches) ### [python](/reference/operators/python) [→](/reference/operators/python) Executes Python code against each event of the input. ```tql python "self.x = self.y" ``` ### [shell](/reference/operators/shell) [→](/reference/operators/shell) Executes a system command and hooks its stdin and stdout into the pipeline. ```tql shell "echo hello" ``` ## Filter [Section titled “Filter”](#filter) ### [assert](/reference/operators/assert) [→](/reference/operators/assert) Drops events and emits a warning if the invariant is violated. ```tql assert name.starts_with("John") ``` ### [assert\_throughput](/reference/operators/assert_throughput) [→](/reference/operators/assert_throughput) Emits a warning if the pipeline does not have the expected throughput ```tql assert_throughput 1000, within=1s ``` ### [deduplicate](/reference/operators/deduplicate) [→](/reference/operators/deduplicate) Removes duplicate events based on a common key. ```tql deduplicate src_ip ``` ### [head](/reference/operators/head) [→](/reference/operators/head) Limits the input to the first `n` events. ```tql head 20 ``` ### [sample](/reference/operators/sample) [→](/reference/operators/sample) Dynamically samples events from a event stream. ```tql sample 30s, max_samples=2k ``` ### [slice](/reference/operators/slice) [→](/reference/operators/slice) Keeps a range of events within the interval `[begin, end)` stepping by `stride`. ```tql slice begin=10, end=30 ``` ### [tail](/reference/operators/tail) [→](/reference/operators/tail) Limits the input to the last `n` events. ```tql tail 20 ``` ### [taste](/reference/operators/taste) [→](/reference/operators/taste) Limits the input to `n` events per unique schema. ```tql taste 1 ``` ### [where](/reference/operators/where) [→](/reference/operators/where) Keeps only events for which the given predicate is true. ```tql where name.starts_with("John") ``` ## Flow Control [Section titled “Flow Control”](#flow-control) ### [cron](/reference/operators/cron) [→](/reference/operators/cron) Runs a pipeline periodically according to a cron expression. ```tql cron "* */10 * * * MON-FRI" { from "https://example.org" } ``` ### [delay](/reference/operators/delay) [→](/reference/operators/delay) Delays events relative to a given start time, with an optional speedup. ```tql delay ts, speed=2.5 ``` ### [discard](/reference/operators/discard) [→](/reference/operators/discard) Discards all incoming events. ```tql discard ``` ### [every](/reference/operators/every) [→](/reference/operators/every) Runs a pipeline periodically at a fixed interval. ```tql every 10s { summarize sum(amount) } ``` ### [fork](/reference/operators/fork) [→](/reference/operators/fork) Executes a subpipeline with a copy of the input. ```tql fork { to "copy.json" } ``` ### [load\_balance](/reference/operators/load_balance) [→](/reference/operators/load_balance) Routes the data to one of multiple subpipelines. ```tql load_balance $over { publish $over } ``` ### [pass](/reference/operators/pass) [→](/reference/operators/pass) Does nothing with the input. ```tql pass ``` ### [repeat](/reference/operators/repeat) [→](/reference/operators/repeat) Repeats the input a number of times. ```tql repeat 100 ``` ### [throttle](/reference/operators/throttle) [→](/reference/operators/throttle) Limits the bandwidth of a pipeline. ```tql throttle 100M, within=1min ``` ## Host Inspection [Section titled “Host Inspection”](#host-inspection) ### [files](/reference/operators/files) [→](/reference/operators/files) Shows file information for a given directory. ```tql files "/var/log/", recurse=true ``` ### [nics](/reference/operators/nics) [→](/reference/operators/nics) Shows a snapshot of available network interfaces. ```tql nics ``` ### [processes](/reference/operators/processes) [→](/reference/operators/processes) Shows a snapshot of running processes. ```tql processes ``` ### [sockets](/reference/operators/sockets) [→](/reference/operators/sockets) Shows a snapshot of open sockets. ```tql sockets ``` ## Internals [Section titled “Internals”](#internals) ### [api](/reference/operators/api) [→](/reference/operators/api) Use Tenzir's REST API directly from a pipeline. ```tql api "/pipeline/list" ``` ### [batch](/reference/operators/batch) [→](/reference/operators/batch) The `batch` operator controls the batch size of events. ```tql batch timeout=1s ``` ### [buffer](/reference/operators/buffer) [→](/reference/operators/buffer) An in-memory buffer to improve handling of data spikes in upstream operators. ```tql buffer 10M, policy="drop" ``` ### [cache](/reference/operators/cache) [→](/reference/operators/cache) An in-memory cache shared between pipelines. ```tql cache "w01wyhTZm3", ttl=10min ``` ### [local](/reference/operators/local) [→](/reference/operators/local) Forces a pipeline to run locally. ```tql local { sort foo } ``` ### [measure](/reference/operators/measure) [→](/reference/operators/measure) Replaces the input with metrics describing the input. ```tql measure ``` ### [remote](/reference/operators/remote) [→](/reference/operators/remote) Forces a pipeline to run remotely at a node. ```tql remote { version } ``` ### [serve](/reference/operators/serve) [→](/reference/operators/serve) Make events available under the `/serve` REST API endpoint ```tql serve "abcde12345" ``` ### [strict](/reference/operators/strict) [→](/reference/operators/strict) Treats all warnings as errors. ```tql strict { assert false } ``` ### [unordered](/reference/operators/unordered) [→](/reference/operators/unordered) Removes ordering assumptions from a pipeline. ```tql unordered { read_ndjson } ``` ## Modify [Section titled “Modify”](#modify) ### [dns\_lookup](/reference/operators/dns_lookup) [→](/reference/operators/dns_lookup) Performs DNS lookups to resolve IP addresses to hostnames or hostnames to IP addresses. ```tql dns_lookup ip_address, result=dns_info ``` ### [drop](/reference/operators/drop) [→](/reference/operators/drop) Removes fields from the event. ```tql drop name, metadata.id ``` ### [drop\_null\_fields](/reference/operators/drop_null_fields) [→](/reference/operators/drop_null_fields) Removes fields containing null values from the event. ```tql drop_null_fields name, metadata.id ``` ### [enumerate](/reference/operators/enumerate) [→](/reference/operators/enumerate) Add a field with the number of preceding events. ```tql enumerate num ``` ### [http](/reference/operators/http) [→](/reference/operators/http) Sends HTTP/1.1 requests and forwards the response. ```tql http "example.com" ``` ### [move](/reference/operators/move) [→](/reference/operators/move) Moves values from one field to another, removing the original field. ```tql move id=parsed_id, ctx.message=incoming.status ``` ### [replace](/reference/operators/replace) [→](/reference/operators/replace) Replaces all occurrences of a value with another value. ```tql replace what=42, with=null ``` ### [select](/reference/operators/select) [→](/reference/operators/select) Selects some values and discards the rest. ```tql select name, id=metadata.id ``` ### [set](/reference/operators/set) [→](/reference/operators/set) Assigns a value to a field, creating it if necessary. ```tql name = "Tenzir" ``` ### [timeshift](/reference/operators/timeshift) [→](/reference/operators/timeshift) Adjusts timestamps relative to a given start time, with an optional speedup. ```tql timeshift ts, start=2020-01-01 ``` ### [unroll](/reference/operators/unroll) [→](/reference/operators/unroll) Returns a new event for each member of a list or a record in an event, duplicating the surrounding event. ```tql unroll names ``` ## OCSF [Section titled “OCSF”](#ocsf) ### [ocsf::apply](/reference/operators/ocsf/apply) [→](/reference/operators/ocsf/apply) Casts incoming events to their OCSF type. ```tql ocsf::apply ``` ### [ocsf::derive](/reference/operators/ocsf/derive) [→](/reference/operators/ocsf/derive) Automatically assigns enum strings from their integer counterparts and vice versa. ```tql ocsf::derive ``` ### [ocsf::trim](/reference/operators/ocsf/trim) [→](/reference/operators/ocsf/trim) Drops fields from OCSF events to reduce their size. ```tql ocsf::trim ``` ## Packages [Section titled “Packages”](#packages) ### [package::add](/reference/operators/package/add) [→](/reference/operators/package/add) Installs a package. ```tql package::add "suricata-ocsf" ``` ### [package::list](/reference/operators/package/list) [→](/reference/operators/package/list) Shows installed packages. ```tql package::list ``` ### [package::remove](/reference/operators/package/remove) [→](/reference/operators/package/remove) Uninstalls a package. ```tql package::remove "suricata-ocsf" ``` ## Parsing [Section titled “Parsing”](#parsing) ### [read\_all](/reference/operators/read_all) [→](/reference/operators/read_all) Parses an incoming bytes stream into a single event. ```tql read_all binary=true ``` ### [read\_bitz](/reference/operators/read_bitz) [→](/reference/operators/read_bitz) Parses bytes as *BITZ* format. ```tql read_bitz ``` ### [read\_cef](/reference/operators/read_cef) [→](/reference/operators/read_cef) Parses an incoming Common Event Format (CEF) stream into events. ```tql read_cef ``` ### [read\_csv](/reference/operators/read_csv) [→](/reference/operators/read_csv) Read CSV (Comma-Separated Values) from a byte stream. ```tql read_csv null_value="-" ``` ### [read\_delimited](/reference/operators/read_delimited) [→](/reference/operators/read_delimited) Parses an incoming bytes stream into events using a string as delimiter. ```tql read_delimited "|" ``` ### [read\_delimited\_regex](/reference/operators/read_delimited_regex) [→](/reference/operators/read_delimited_regex) Parses an incoming bytes stream into events using a regular expression as delimiter. ```tql read_delimited_regex r"\s+" ``` ### [read\_feather](/reference/operators/read_feather) [→](/reference/operators/read_feather) Parses an incoming Feather byte stream into events. ```tql read_feather ``` ### [read\_gelf](/reference/operators/read_gelf) [→](/reference/operators/read_gelf) Parses an incoming GELF stream into events. ```tql read_gelf ``` ### [read\_grok](/reference/operators/read_grok) [→](/reference/operators/read_grok) Parses lines of input with a grok pattern. ```tql read_grok "%{IP:client} %{WORD:action}" ``` ### [read\_json](/reference/operators/read_json) [→](/reference/operators/read_json) Parses an incoming JSON stream into events. ```tql read_json arrays_of_objects=true ``` ### [read\_kv](/reference/operators/read_kv) [→](/reference/operators/read_kv) Read Key-Value pairs from a byte stream. ```tql read_kv r"(\s+)[A-Z_]+:", r":\s*" ``` ### [read\_leef](/reference/operators/read_leef) [→](/reference/operators/read_leef) Parses an incoming \[LEEF]\[leef] stream into events. ```tql read_leef ``` ### [read\_lines](/reference/operators/read_lines) [→](/reference/operators/read_lines) Parses an incoming bytes stream into events. ```tql read_lines ``` ### [read\_ndjson](/reference/operators/read_ndjson) [→](/reference/operators/read_ndjson) Parses an incoming NDJSON (newline-delimited JSON) stream into events. ```tql read_ndjson ``` ### [read\_parquet](/reference/operators/read_parquet) [→](/reference/operators/read_parquet) Reads events from a Parquet byte stream. ```tql read_parquet ``` ### [read\_pcap](/reference/operators/read_pcap) [→](/reference/operators/read_pcap) Reads raw network packets in PCAP file format. ```tql read_pcap ``` ### [read\_ssv](/reference/operators/read_ssv) [→](/reference/operators/read_ssv) Read SSV (Space-Separated Values) from a byte stream. ```tql read_ssv header="name count" ``` ### [read\_suricata](/reference/operators/read_suricata) [→](/reference/operators/read_suricata) Parse an incoming \[Suricata EVE JSON]\[eve-json] stream into events. ```tql read_suricata ``` ### [read\_syslog](/reference/operators/read_syslog) [→](/reference/operators/read_syslog) Parses an incoming Syslog stream into events. ```tql read_syslog ``` ### [read\_tsv](/reference/operators/read_tsv) [→](/reference/operators/read_tsv) Read TSV (Tab-Separated Values) from a byte stream. ```tql read_tsv auto_expand=true ``` ### [read\_xsv](/reference/operators/read_xsv) [→](/reference/operators/read_xsv) Read XSV from a byte stream. ```tql read_xsv ";", ":", "N/A" ``` ### [read\_yaml](/reference/operators/read_yaml) [→](/reference/operators/read_yaml) Parses an incoming YAML stream into events. ```tql read_yaml ``` ### [read\_zeek\_json](/reference/operators/read_zeek_json) [→](/reference/operators/read_zeek_json) Parse an incoming Zeek JSON stream into events. ```tql read_zeek_json ``` ### [read\_zeek\_tsv](/reference/operators/read_zeek_tsv) [→](/reference/operators/read_zeek_tsv) Parses an incoming `Zeek TSV` stream into events. ```tql read_zeek_tsv ``` ## Pipelines [Section titled “Pipelines”](#pipelines) ### [pipeline::activity](/reference/operators/pipeline/activity) [→](/reference/operators/pipeline/activity) Summarizes the activity of pipelines. ```tql pipeline::activity range=1d, interval=1h ``` ### [pipeline::detach](/reference/operators/pipeline/detach) [→](/reference/operators/pipeline/detach) Starts a pipeline in the node. ```tql pipeline::detach { … } ``` ### [pipeline::list](/reference/operators/pipeline/list) [→](/reference/operators/pipeline/list) Shows managed pipelines. ```tql pipeline::list ``` ### [pipeline::run](/reference/operators/pipeline/run) [→](/reference/operators/pipeline/run) Starts a pipeline in the node and waits for it to complete. ```tql pipeline::run { … } ``` ## Printing [Section titled “Printing”](#printing) ### [write\_bitz](/reference/operators/write_bitz) [→](/reference/operators/write_bitz) Writes events in *BITZ* format. ```tql write_bitz ``` ### [write\_csv](/reference/operators/write_csv) [→](/reference/operators/write_csv) Transforms event stream to CSV (Comma-Separated Values) byte stream. ```tql write_csv ``` ### [write\_feather](/reference/operators/write_feather) [→](/reference/operators/write_feather) Transforms the input event stream to Feather byte stream. ```tql write_feather ``` ### [write\_json](/reference/operators/write_json) [→](/reference/operators/write_json) Transforms the input event stream to a JSON byte stream. ```tql write_json ``` ### [write\_kv](/reference/operators/write_kv) [→](/reference/operators/write_kv) Writes events in a Key-Value format. ```tql write_kv ``` ### [write\_lines](/reference/operators/write_lines) [→](/reference/operators/write_lines) Writes events as key-value pairsthe *values* of an event. ```tql write_lines ``` ### [write\_ndjson](/reference/operators/write_ndjson) [→](/reference/operators/write_ndjson) Transforms the input event stream to a Newline-Delimited JSON byte stream. ```tql write_ndjson ``` ### [write\_parquet](/reference/operators/write_parquet) [→](/reference/operators/write_parquet) Transforms event stream to a Parquet byte stream. ```tql write_parquet ``` ### [write\_pcap](/reference/operators/write_pcap) [→](/reference/operators/write_pcap) Transforms event stream to PCAP byte stream. ```tql write_pcap ``` ### [write\_ssv](/reference/operators/write_ssv) [→](/reference/operators/write_ssv) Transforms event stream to SSV (Space-Separated Values) byte stream. ```tql write_ssv ``` ### [write\_syslog](/reference/operators/write_syslog) [→](/reference/operators/write_syslog) Writes events as syslog. ```tql write_syslog ``` ### [write\_tql](/reference/operators/write_tql) [→](/reference/operators/write_tql) Transforms the input event stream to a TQL notation byte stream. ```tql write_tql ``` ### [write\_tsv](/reference/operators/write_tsv) [→](/reference/operators/write_tsv) Transforms event stream to TSV (Tab-Separated Values) byte stream. ```tql write_tsv ``` ### [write\_xsv](/reference/operators/write_xsv) [→](/reference/operators/write_xsv) Transforms event stream to XSV byte stream. ```tql write_xsv ``` ### [write\_yaml](/reference/operators/write_yaml) [→](/reference/operators/write_yaml) Transforms the input event stream to YAML byte stream. ```tql write_yaml ``` ### [write\_zeek\_tsv](/reference/operators/write_zeek_tsv) [→](/reference/operators/write_zeek_tsv) Transforms event stream into Zeek Tab-Separated Value byte stream. ```tql write_zeek_tsv ``` ## Inputs [Section titled “Inputs”](#inputs) ### Bytes [Section titled “Bytes”](#bytes) ### [load\_amqp](/reference/operators/load_amqp) [→](/reference/operators/load_amqp) Loads a byte stream via AMQP messages. ```tql load_amqp ``` ### [load\_azure\_blob\_storage](/reference/operators/load_azure_blob_storage) [→](/reference/operators/load_azure_blob_storage) Loads bytes from Azure Blob Storage. ```tql load_azure_blob_storage "abfs://container/file" ``` ### [load\_file](/reference/operators/load_file) [→](/reference/operators/load_file) Loads the contents of the file at `path` as a byte stream. ```tql load_file "/tmp/data.json" ``` ### [load\_ftp](/reference/operators/load_ftp) [→](/reference/operators/load_ftp) Loads a byte stream via FTP. ```tql load_ftp "ftp.example.org" ``` ### [load\_gcs](/reference/operators/load_gcs) [→](/reference/operators/load_gcs) Loads bytes from a Google Cloud Storage object. ```tql load_gcs "gs://bucket/object.json" ``` ### [load\_google\_cloud\_pubsub](/reference/operators/load_google_cloud_pubsub) [→](/reference/operators/load_google_cloud_pubsub) Subscribes to a Google Cloud Pub/Sub subscription and obtains bytes. ```tql load_google_cloud_pubsub project_id="my-project" ``` ### [load\_http](/reference/operators/load_http) [→](/reference/operators/load_http) Loads a byte stream via HTTP. ```tql load_http "example.org", params={n: 5} ``` ### [load\_kafka](/reference/operators/load_kafka) [→](/reference/operators/load_kafka) Loads a byte stream from an Apache Kafka topic. ```tql load_kafka topic="example" ``` ### [load\_nic](/reference/operators/load_nic) [→](/reference/operators/load_nic) Loads bytes from a network interface card (NIC). ```tql load_nic "eth0" ``` ### [load\_s3](/reference/operators/load_s3) [→](/reference/operators/load_s3) Loads from an Amazon S3 object. ```tql load_s3 "s3://my-bucket/obj.csv" ``` ### [load\_sqs](/reference/operators/load_sqs) [→](/reference/operators/load_sqs) Loads bytes from \[Amazon SQS]\[sqs] queues. ```tql load_sqs "sqs://tenzir" ``` ### [load\_stdin](/reference/operators/load_stdin) [→](/reference/operators/load_stdin) Accepts bytes from standard input. ```tql load_stdin ``` ### [load\_tcp](/reference/operators/load_tcp) [→](/reference/operators/load_tcp) Loads bytes from a TCP or TLS connection. ```tql load_tcp "0.0.0.0:8090" { read_json } ``` ### [load\_udp](/reference/operators/load_udp) [→](/reference/operators/load_udp) Loads bytes from a UDP socket. ```tql load_udp "0.0.0.0:8090" ``` ### [load\_zmq](/reference/operators/load_zmq) [→](/reference/operators/load_zmq) Receives ZeroMQ messages. ```tql load_zmq ``` ### Events [Section titled “Events”](#events) ### [from](/reference/operators/from) [→](/reference/operators/from) Obtains events from an URI, inferring the source, compression and format. ```tql from "data.json" ``` ### [from\_azure\_blob\_storage](/reference/operators/from_azure_blob_storage) [→](/reference/operators/from_azure_blob_storage) Reads one or multiple files from Azure Blob Storage. ```tql from_azure_blob_storage "abfs://container/data/**.json" ``` ### [from\_file](/reference/operators/from_file) [→](/reference/operators/from_file) Reads one or multiple files from a filesystem. ```tql from_file "s3://data/**.json" ``` ### [from\_fluent\_bit](/reference/operators/from_fluent_bit) [→](/reference/operators/from_fluent_bit) Receives events via Fluent Bit. ```tql from_fluent_bit "opentelemetry" ``` ### [from\_gcs](/reference/operators/from_gcs) [→](/reference/operators/from_gcs) Reads one or multiple files from Google Cloud Storage. ```tql from_gcs "gs://my-bucket/data/**.json" ``` ### [from\_http](/reference/operators/from_http) [→](/reference/operators/from_http) Sends and receives HTTP/1.1 requests. ```tql from_http "0.0.0.0:8080" ``` ### [from\_opensearch](/reference/operators/from_opensearch) [→](/reference/operators/from_opensearch) Receives events via Opensearch Bulk API. ```tql from_opensearch ``` ### [from\_s3](/reference/operators/from_s3) [→](/reference/operators/from_s3) Reads one or multiple files from Amazon S3. ```tql from_s3 "s3://my-bucket/data/**.json" ``` ### [from\_udp](/reference/operators/from_udp) [→](/reference/operators/from_udp) Receives UDP datagrams and outputs structured events. ```tql from_udp "0.0.0.0:8090" ``` ### [from\_velociraptor](/reference/operators/from_velociraptor) [→](/reference/operators/from_velociraptor) Submits VQL to a Velociraptor server and returns the response as events. ```tql from_velociraptor subscribe="Windows" ``` ## Node [Section titled “Node”](#node) ### Inspection [Section titled “Inspection”](#inspection) ### [diagnostics](/reference/operators/diagnostics) [→](/reference/operators/diagnostics) Retrieves diagnostic events from a Tenzir node. ```tql diagnostics ``` ### [metrics](/reference/operators/metrics) [→](/reference/operators/metrics) Retrieves metrics events from a Tenzir node. ```tql metrics "cpu" ``` ### [openapi](/reference/operators/openapi) [→](/reference/operators/openapi) Shows the node's OpenAPI specification. ```tql openapi ``` ### [plugins](/reference/operators/plugins) [→](/reference/operators/plugins) Shows all available plugins and built-ins. ```tql plugins ``` ### [version](/reference/operators/version) [→](/reference/operators/version) Shows the current version. ```tql version ``` ### Storage Engine [Section titled “Storage Engine”](#storage-engine) ### [export](/reference/operators/export) [→](/reference/operators/export) Retrieves events from a Tenzir node. ```tql export ``` ### [fields](/reference/operators/fields) [→](/reference/operators/fields) Retrieves all fields stored at a node. ```tql fields ``` ### [import](/reference/operators/import) [→](/reference/operators/import) Imports events into a Tenzir node. ```tql import ``` ### [partitions](/reference/operators/partitions) [→](/reference/operators/partitions) Retrieves metadata about events stored at a node. ```tql partitions src_ip == 1.2.3.4 ``` ### [schemas](/reference/operators/schemas) [→](/reference/operators/schemas) Retrieves all schemas for events stored at a node. ```tql schemas ``` ## Outputs [Section titled “Outputs”](#outputs) ### Bytes [Section titled “Bytes”](#bytes-1) ### [save\_amqp](/reference/operators/save_amqp) [→](/reference/operators/save_amqp) Saves a byte stream via AMQP messages. ```tql save_amqp ``` ### [save\_azure\_blob\_storage](/reference/operators/save_azure_blob_storage) [→](/reference/operators/save_azure_blob_storage) Saves bytes to Azure Blob Storage. ```tql save_azure_blob_storage "abfs://container/file" ``` ### [save\_email](/reference/operators/save_email) [→](/reference/operators/save_email) Saves bytes through an SMTP server. ```tql save_email "user@example.org" ``` ### [save\_file](/reference/operators/save_file) [→](/reference/operators/save_file) Writes a byte stream to a file. ```tql save_file "/tmp/out.json" ``` ### [save\_ftp](/reference/operators/save_ftp) [→](/reference/operators/save_ftp) Saves a byte stream via FTP. ```tql save_ftp "ftp.example.org" ``` ### [save\_gcs](/reference/operators/save_gcs) [→](/reference/operators/save_gcs) Saves bytes to a Google Cloud Storage object. ```tql save_gcs "gs://bucket/object.json" ``` ### [save\_google\_cloud\_pubsub](/reference/operators/save_google_cloud_pubsub) [→](/reference/operators/save_google_cloud_pubsub) Publishes to a Google Cloud Pub/Sub topic. ```tql save_google_cloud_pubsub project_id="my-project" ``` ### [save\_http](/reference/operators/save_http) [→](/reference/operators/save_http) Sends a byte stream via HTTP. ```tql save_http "example.org/api" ``` ### [save\_kafka](/reference/operators/save_kafka) [→](/reference/operators/save_kafka) Saves a byte stream to a Apache Kafka topic. ```tql save_kafka topic="example" ``` ### [save\_s3](/reference/operators/save_s3) [→](/reference/operators/save_s3) Saves bytes to an Amazon S3 object. ```tql save_s3 "s3://my-bucket/obj.csv" ``` ### [save\_sqs](/reference/operators/save_sqs) [→](/reference/operators/save_sqs) Saves bytes to \[Amazon SQS]\[sqs] queues. ```tql save_sqs "sqs://tenzir" ``` ### [save\_stdout](/reference/operators/save_stdout) [→](/reference/operators/save_stdout) Writes a byte stream to standard output. ```tql save_stdout ``` ### [save\_tcp](/reference/operators/save_tcp) [→](/reference/operators/save_tcp) Saves bytes to a TCP or TLS connection. ```tql save_tcp "0.0.0.0:8090", tls=true ``` ### [save\_udp](/reference/operators/save_udp) [→](/reference/operators/save_udp) Saves bytes to a UDP socket. ```tql save_udp "0.0.0.0:8090" ``` ### [save\_zmq](/reference/operators/save_zmq) [→](/reference/operators/save_zmq) Sends bytes as ZeroMQ messages. ```tql save_zmq ``` ### Events [Section titled “Events”](#events-1) ### [to](/reference/operators/to) [→](/reference/operators/to) Saves to an URI, inferring the destination, compression and format. ```tql to "output.json" ``` ### [to\_amazon\_security\_lake](/reference/operators/to_amazon_security_lake) [→](/reference/operators/to_amazon_security_lake) Sends OCSF events to Amazon Security Lake. ```tql to_amazon_security_lake "s3://…" ``` ### [to\_azure\_log\_analytics](/reference/operators/to_azure_log_analytics) [→](/reference/operators/to_azure_log_analytics) Sends events to the Microsoft Azure Logs Ingestion API. ```tql to_azure_log_analytics tenant_id="...", workspace_id="..." ``` ### [to\_clickhouse](/reference/operators/to_clickhouse) [→](/reference/operators/to_clickhouse) Sends events to a ClickHouse table. ```tql to_clickhouse table="my_table" ``` ### [to\_fluent\_bit](/reference/operators/to_fluent_bit) [→](/reference/operators/to_fluent_bit) Sends events via Fluent Bit. ```tql to_fluent_bit "elasticsearch" … ``` ### [to\_google\_cloud\_logging](/reference/operators/to_google_cloud_logging) [→](/reference/operators/to_google_cloud_logging) Sends events to Google Cloud Logging. ```tql to_google_cloud_logging … ``` ### [to\_google\_secops](/reference/operators/to_google_secops) [→](/reference/operators/to_google_secops) Sends unstructured events to a Google SecOps Chronicle instance. ```tql to_google_secops … ``` ### [to\_hive](/reference/operators/to_hive) [→](/reference/operators/to_hive) Writes events to a URI using hive partitioning. ```tql to_hive "s3://…", partition_by=[x] ``` ### [to\_kafka](/reference/operators/to_kafka) [→](/reference/operators/to_kafka) Sends messages to an Apache Kafka topic. ```tql to_kafka "topic", message=this.print_json() ``` ### [to\_opensearch](/reference/operators/to_opensearch) [→](/reference/operators/to_opensearch) Sends events to an OpenSearch-compatible Bulk API. ```tql to_opensearch "localhost:9200", … ``` ### [to\_sentinelone\_data\_lake](/reference/operators/to_sentinelone_data_lake) [→](/reference/operators/to_sentinelone_data_lake) Sends security events to SentinelOne Singularity Data Lake via REST API. ```tql to_sentinelone_data_lake "https://…", … ``` ### [to\_snowflake](/reference/operators/to_snowflake) [→](/reference/operators/to_snowflake) Sends events to a Snowflake database. ```tql to_snowflake account_identifier="… ``` ### [to\_splunk](/reference/operators/to_splunk) [→](/reference/operators/to_splunk) Sends events to a Splunk \[HTTP Event Collector (HEC)]\[hec]. ```tql to_splunk "localhost:8088", … ```

# api

Use Tenzir’s REST API directly from a pipeline. ```tql api endpoint:string, [request_body:string] ``` ## Description [Section titled “Description”](#description) The `api` operator interacts with Tenzir’s REST API without needing to spin up a web server, making all APIs accessible from within pipelines. ### `endpoint: string` [Section titled “endpoint: string”](#endpoint-string) The endpoint to request, e.g., `/pipeline/list` to list all managed pipelines. Tenzir’s [REST API specification](/reference/node/api) lists all available endpoints. ### `request_body: string (optional)` [Section titled “request\_body: string (optional)”](#request_body-string-optional) A single string containing the JSON request body to send with the request. ## Examples [Section titled “Examples”](#examples) ### List all running pipelines [Section titled “List all running pipelines”](#list-all-running-pipelines) ```tql api "/pipeline/list" ``` ### Create a new pipeline and start it immediately [Section titled “Create a new pipeline and start it immediately”](#create-a-new-pipeline-and-start-it-immediately) ```tql api "/pipeline/create", { name: "Suricata Import", definition: "from file /tmp/eve.sock read suricata", autostart: { created: true }, } ``` ## See Also [Section titled “See Also”](#see-also) [`openapi`](/reference/operators/openapi), [`serve`](/reference/operators/serve)

# assert

Drops events and emits a warning if the invariant is violated. ```tql assert invariant:bool, [message=any] ``` ## Description [Section titled “Description”](#description) The `assert` operator asserts that `invariant` is `true` for events. In case an event does not satisfy the invariant, it is dropped and a warning is emitted. ### `invariant: bool` [Section titled “invariant: bool”](#invariant-bool) Condition to assert being `true`. ### `message = any (optional)` [Section titled “message = any (optional)”](#message--any-optional) Context to associate with the assertion failure. ## Examples [Section titled “Examples”](#examples) ### Make sure that `x != 2` [Section titled “Make sure that x != 2”](#make-sure-that-x--2) ```tql from {x: 1}, {x: 2}, {x: 3} assert x != 2 ``` ```tql {x: 1} // warning: assertion failure {x: 3} ``` ### Check that a topic only contains certain events [Section titled “Check that a topic only contains certain events”](#check-that-a-topic-only-contains-certain-events) ```tql subscribe "network" assert @name == "ocsf.network_activity" // continue processing ``` ## See Also [Section titled “See Also”](#see-also) [`assert_throughput`](/reference/operators/assert_throughput), [`where`](/reference/operators/where)

# assert_throughput

Emits a warning if the pipeline does not have the expected throughput ```tql assert_throughput min_events:int, within=duration, [retries=int] ``` ## Description [Section titled “Description”](#description) The `assert_throughput` operator checks a pipeline’s throughput, emitting a warning if the minimum specified throughput is unmet, and optionally an error if the number of retries is exceeded. ## Examples [Section titled “Examples”](#examples) ### Require 1,000 events per second, failing if the issue persists for 30s [Section titled “Require 1,000 events per second, failing if the issue persists for 30s”](#require-1000-events-per-second-failing-if-the-issue-persists-for-30s) ```tql from "udp://0.0.0.0:514" { read_syslog } assert_throughput 1k, within=1s, retries=30 ``` ## See Also [Section titled “See Also”](#see-also) [`assert`](/reference/operators/assert)

# batch

The `batch` operator controls the batch size of events. ```tql batch [limit:int, timeout=duration] ``` ## Description [Section titled “Description”](#description) The `batch` operator takes its input and rewrites it into batches of up to the desired size. Expert Operator The `batch` operator is a lower-level building block that lets users explicitly control batching, which otherwise is controlled automatically by Tenzir’s underlying pipeline execution engine. Use with caution! ### `limit: int (optional)` [Section titled “limit: int (optional)”](#limit-int-optional) How many events to put into one batch at most. Defaults to `65536`. ### `timeout = duration (optional)` [Section titled “timeout = duration (optional)”](#timeout--duration-optional) Specifies a maximum latency for events passing through the batch operator. When unspecified, an infinite duration is used.

# buffer

An in-memory buffer to improve handling of data spikes in upstream operators. ```tql buffer [capacity:int, policy=string] ``` ## Description [Section titled “Description”](#description) The `buffer` operator buffers up to the specified number of events or bytes in memory. By default, operators in a pipeline run only when their downstream operators want to receive input. This mechanism is called back pressure. The `buffer` operator effectively breaks back pressure by storing up to the specified number of events in memory, always requesting more input, which allows upstream operators to run uninterruptedly even in case the downstream operators of the buffer are unable to keep up. This allows pipelines to handle data spikes more easily. ### `capacity: int (optional)` [Section titled “capacity: int (optional)”](#capacity-int-optional) The number of events or bytes that may be kept at most in the buffer. Note that every operator already buffers up to `254Ki` events before it starts applying back pressure. Smaller buffers may decrease performance. ### `policy = string (optional)` [Section titled “policy = string (optional)”](#policy--string-optional) Specifies what the operator does when the buffer runs full. * `"drop"`: Drop events that do not fit into the buffer. This policy is not supported for bytes inputs. * `"block"`: Use back pressure to slow down upstream operators. When buffering events, this option defaults to `"block"` for pipelines visible on the overview page on [app.tenzir.com](https://app.tenzir.com), and to `"drop"` otherwise. When buffering bytes, this option always defaults to `"block"`. ## Examples [Section titled “Examples”](#examples) ### Buffer up to 10 million events or bytes [Section titled “Buffer up to 10 million events or bytes”](#buffer-up-to-10-million-events-or-bytes) Buffer and drop events if downstream cannot keep up: ```tql buffer 10M, policy="drop" ``` ## See Also [Section titled “See Also”](#see-also) [`cache`](/reference/operators/cache)

# cache

An in-memory cache shared between pipelines. ```tql cache id:string, [mode=string, capacity=int, read_timeout=duration, write_timeout=duration] ``` ## Description [Section titled “Description”](#description) The `cache` operator caches events in an in-memory buffer at a node. Caches must have a user-provided unique ID. The first pipeline to use a cache writes into the cache. All further pipelines using the same cache will read from the cache instead of executing the operators before the `cache` operator in the same pipeline. ### `id: string` [Section titled “id: string”](#id-string) An arbitrary string that uniquely identifies the cache. ### `mode = string (optional)` [Section titled “mode = string (optional)”](#mode--string-optional) Configures whether the operator is used an input, an output, or a transformation. The following modes are available currently: * `"read"`: The operators acts as an input operator reading from a cache that is requires to already exist. * `"write"`: The operator acts as an output operator writing into a cache that must not already exist. * `"readwrite"`: The operator acts as a transformation passing through events, lazily creating a cache if it does not already exist. If a cache exists, upstream operators will not be run and instead the cache is read. Defaults to `"readwrite"`. ### `capacity = int (optional)` [Section titled “capacity = int (optional)”](#capacity--int-optional) Stores how many events the cache can hold. Caches stop accepting events if the capacity is reached and emit a warning. Defaults to unlimited. ### `read_timeout = duration (optional)` [Section titled “read\_timeout = duration (optional)”](#read_timeout--duration-optional) Defines the maximum inactivity time until the cache is evicted from memory. The timer starts when writing the cache completes (or runs into the capacity limit), and resets whenever the cache is read from. Defaults to `10min`, or the value specified in the `tenzir.cache.lifetime` option. ### `write_timeout = duration (optional)` [Section titled “write\_timeout = duration (optional)”](#write_timeout--duration-optional) If set, defines an upper bound for the lifetime of the cache. Unlike the `read_timeout` option, this does not refresh when the cache is accessed. ## Examples [Section titled “Examples”](#examples) ### Cache the results of an expensive query [Section titled “Cache the results of an expensive query”](#cache-the-results-of-an-expensive-query) ```tql export where @name == "suricata.flow" summarize total=sum(bytes_toserver), src_ip, dest_ip cache "some-unique-identifier" ``` ### Get high-level statistics about a query [Section titled “Get high-level statistics about a query”](#get-high-level-statistics-about-a-query) This calculates the cache again only if the query does not exist anymore, and delete the cache if it’s unused for more than a minute. ```tql export where @name == "suricata.flow" summarize src_ip, total=sum(bytes_toserver), dest_ip cache "some-unique-identifier", read_timeout=1min summarize src_ip, total=sum(total), destinations=count(dest_ip) ``` Get the same statistics, assuming the cache still exists: ```tql cache "some-unique-identifier", mode="read" summarize src_ip, total=sum(total), destinations=count(dest_ip) ``` ## See Also [Section titled “See Also”](#see-also) [`buffer`](/reference/operators/buffer)

# chart_area

Plots events on an area chart. ```tql chart_area x=field, y=any, [x_min=any, x_max=any, y_min=any, y_max=any, resolution=duration, fill=any, x_log=bool, y_log=bool, group=any, position=string] ``` ## Description [Section titled “Description”](#description) Visualizes events with an area chart on the [Tenzir Platform](https://app.tenzir.com). ### `x = field` [Section titled “x = field”](#x--field) Positions on the x-axis for each data point. ### `y = any` [Section titled “y = any”](#y--any) Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation). Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`. For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`. ### `x_min = any (optional)` [Section titled “x\_min = any (optional)”](#x_min--any-optional) If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket. ### `x_max = any (optional)` [Section titled “x\_max = any (optional)”](#x_max--any-optional) If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket. ### `y_min = any (optional)` [Section titled “y\_min = any (optional)”](#y_min--any-optional) If specified, any `y` values less than `y_min` will appear clipped out of the chart. ### `y_max = any (optional)` [Section titled “y\_max = any (optional)”](#y_max--any-optional) If specified, any `y` values greater than `y_max` will appear clipped out of the chart. ### `resolution = duration (optional)` [Section titled “resolution = duration (optional)”](#resolution--duration-optional) This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified. For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket. ### `fill = any (optional)` [Section titled “fill = any (optional)”](#fill--any-optional) Optional value to fill gaps and replace `null`s with. ### `x_log = bool (optional)` [Section titled “x\_log = bool (optional)”](#x_log--bool-optional) If `true`, use a logarithmic scale for the x-axis. Defaults to `false`. ### `y_log = bool (optional)` [Section titled “y\_log = bool (optional)”](#y_log--bool-optional) If `true`, use a logarithmic scale for the y-axis. Defaults to `false`. ### `group = any (optional)` [Section titled “group = any (optional)”](#group--any-optional) Optional expression to group the aggregations with. ### `position = string (optional)` [Section titled “position = string (optional)”](#position--string-optional) Determines how the `y` values are displayed. Possible values: * `grouped` * `stacked` Defaults to `grouped`. ## Examples [Section titled “Examples”](#examples) ### Chart TCP metrics [Section titled “Chart TCP metrics”](#chart-tcp-metrics) This pipeline charts MBs read and written by different pipelines over TCP in hourly intervals for the past 24 hours. ```tql metrics "tcp" chart_area x=timestamp, y={tx: sum(bytes_written/1M), rx: sum(bytes_read/1M)}, x_min=now()-1d, resolution=1h, group=pipeline_id ``` ## See Also [Section titled “See Also”](#see-also) [`chart_bar`](/reference/operators/chart_bar), [`chart_line`](/reference/operators/chart_line), [`chart_pie`](/reference/operators/chart_pie)

# chart_bar

Plots events on an bar chart. ```tql chart_bar x|label=field, y|value=any, [x_min=any, x_max=any, y_min=any, y_max=any, resolution=duration, fill=any, x_log=bool, y_log=bool, group=any, position=string] ``` ## Description [Section titled “Description”](#description) Visualizes events with an bar chart on the [Tenzir Platform](https://app.tenzir.com). ### `x|label = field` [Section titled “x|label = field”](#xlabel--field) Label for each bar. ### `y|value = any` [Section titled “y|value = any”](#yvalue--any) Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation). Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`. For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`. ### `x_min = any (optional)` [Section titled “x\_min = any (optional)”](#x_min--any-optional) If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket. ### `x_max = any (optional)` [Section titled “x\_max = any (optional)”](#x_max--any-optional) If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket. ### `y_min = any (optional)` [Section titled “y\_min = any (optional)”](#y_min--any-optional) If specified, any `y` values less than `y_min` will appear clipped out of the chart. ### `y_max = any (optional)` [Section titled “y\_max = any (optional)”](#y_max--any-optional) If specified, any `y` values greater than `y_max` will appear clipped out of the chart. ### `resolution = duration (optional)` [Section titled “resolution = duration (optional)”](#resolution--duration-optional) This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified. For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket. ### `fill = any (optional)` [Section titled “fill = any (optional)”](#fill--any-optional) Optional value to fill gaps and replace `null`s with. ### `x_log = bool (optional)` [Section titled “x\_log = bool (optional)”](#x_log--bool-optional) If `true`, use a logarithmic scale for the x-axis. Defaults to `false`. ### `y_log = bool (optional)` [Section titled “y\_log = bool (optional)”](#y_log--bool-optional) If `true`, use a logarithmic scale for the y-axis. Defaults to `false`. ### `group = any (optional)` [Section titled “group = any (optional)”](#group--any-optional) Optional expression to group the aggregations with. ### `position = string (optional)` [Section titled “position = string (optional)”](#position--string-optional) Determines how the `y` values are displayed. Possible values: * `grouped` * `stacked` Defaults to `grouped`. ## Examples [Section titled “Examples”](#examples) ### Chart count of events imported for every unique schema [Section titled “Chart count of events imported for every unique schema”](#chart-count-of-events-imported-for-every-unique-schema) ```tql metrics "import" chart_bar x=schema, y=sum(events), x_min=now()-1d ``` ## See Also [Section titled “See Also”](#see-also) [`chart_area`](/reference/operators/chart_area), [`chart_line`](/reference/operators/chart_line), [`chart_pie`](/reference/operators/chart_pie)

# chart_line

Plots events on an line chart. ```tql chart_line x=field, y=any, [x_min=any, x_max=any, y_min=any, y_max=any, resolution=duration, fill=any, x_log=bool, y_log=bool, group=any] ``` ## Description [Section titled “Description”](#description) Visualizes events with an line chart on the [Tenzir Platform](https://app.tenzir.com). ### `x = field` [Section titled “x = field”](#x--field) Positions on the x-axis for each data point. ### `y = any` [Section titled “y = any”](#y--any) Positions on the y-axis for each data point. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation). Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`. For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`. ### `x_min = any (optional)` [Section titled “x\_min = any (optional)”](#x_min--any-optional) If specified, only charts events where `x >= x_min`. If `resolution` is specified, `x_min` is *floored* to create a full bucket. ### `x_max = any (optional)` [Section titled “x\_max = any (optional)”](#x_max--any-optional) If specified, only charts events where `x <= x_max`. If `resolution` is specified, `x_max` is *ceiled* to create a full bucket. ### `y_min = any (optional)` [Section titled “y\_min = any (optional)”](#y_min--any-optional) If specified, any `y` values less than `y_min` will appear clipped out of the chart. ### `y_max = any (optional)` [Section titled “y\_max = any (optional)”](#y_max--any-optional) If specified, any `y` values greater than `y_max` will appear clipped out of the chart. ### `resolution = duration (optional)` [Section titled “resolution = duration (optional)”](#resolution--duration-optional) This option can be specified to create buckets of the given resolution on the x-axis. An aggregation function must be specified to combine values in the same bucket when `resolution` is specified. For example, if the resolution is set to `15min`, the `x` values are *floored* to create buckets of 15 minutes. Any aggregations specified act on that bucket. ### `fill = any (optional)` [Section titled “fill = any (optional)”](#fill--any-optional) Optional value to fill gaps and replace `null`s with. ### `x_log = bool (optional)` [Section titled “x\_log = bool (optional)”](#x_log--bool-optional) If `true`, use a logarithmic scale for the x-axis. Defaults to `false`. ### `y_log = bool (optional)` [Section titled “y\_log = bool (optional)”](#y_log--bool-optional) If `true`, use a logarithmic scale for the y-axis. Defaults to `false`. ### `group = any (optional)` [Section titled “group = any (optional)”](#group--any-optional) Optional expression to group the aggregations with. ## Examples [Section titled “Examples”](#examples) ### Chart published events [Section titled “Chart published events”](#chart-published-events) This pipeline charts number of events published by each pipeline over 30 minute intervals for the past 24 hours. ```tql metrics "publish" chart_line x=timestamp, y=sum(events), x_min=now()-1d, group=pipeline_id, resolution=30min ``` ## See Also [Section titled “See Also”](#see-also) [`chart_area`](/reference/operators/chart_area), [`chart_bar`](/reference/operators/chart_bar), [`chart_pie`](/reference/operators/chart_pie)

# chart_pie

Plots events on an pie chart. ```tql chart_pie x|label=field, y|value=any, [group=any] ``` ## Description [Section titled “Description”](#description) Visualizes events with an pie chart on the [Tenzir Platform](https://app.tenzir.com). ### `x|label = field` [Section titled “x|label = field”](#xlabel--field) Name of each slice on the chart. ### `y|value = any` [Section titled “y|value = any”](#yvalue--any) Value of each slice on the chart. Multiple data points for the same group can be be aggregated using an [aggregation function](/reference/functions#aggregation). Multiple `y` values and their labels can be specified by using the record syntax: `{name: value, ...}`. For example, `y = {"Avg. Load": mean(load)}` calculates the [mean](/reference/functions/mean) of the `load` field and labels it as `Avg. Load`. ### `group = any (optional)` [Section titled “group = any (optional)”](#group--any-optional) Optional expression to group the aggregations with. ## Examples [Section titled “Examples”](#examples) ### Chart count of events imported for every unique schema [Section titled “Chart count of events imported for every unique schema”](#chart-count-of-events-imported-for-every-unique-schema) ```tql metrics "import" where timestamp > now() - 1d chart_pie label=schema, value=sum(events) ``` ## See Also [Section titled “See Also”](#see-also) [`chart_area`](/reference/operators/chart_area), [`chart_bar`](/reference/operators/chart_bar), [`chart_line`](/reference/operators/chart_line)

# compress

Compresses a stream of bytes. ```tql compress codec:string, [level=int] ``` Deprecated The `compress` operator is deprecated. You should use the [bespoke operators](/reference/operators#encode--decode) instead. These operators offer more options for some of the formats. ## Description [Section titled “Description”](#description) The `compress` operator compresses bytes in a pipeline incrementally with a known codec. ### `codec: string` [Section titled “codec: string”](#codec-string) An identifier of the codec to use. Currently supported are `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`. ### `level = int (optional)` [Section titled “level = int (optional)”](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ## Examples [Section titled “Examples”](#examples) ### Export all events in a Gzip-compressed NDJSON file [Section titled “Export all events in a Gzip-compressed NDJSON file”](#export-all-events-in-a-gzip-compressed-ndjson-file) ```tql export write_ndjson compress "gzip" save_file "/tmp/backup.json.gz" ``` ### Recompress a Zstd-compressed file at a higher compression level [Section titled “Recompress a Zstd-compressed file at a higher compression level”](#recompress-a-zstd-compressed-file-at-a-higher-compression-level) ```tql load_file "in.zst" decompress "zstd" compress "zstd", level=18 save_file "out.zst" ```

# compress_brotli

Compresses a stream of bytes using Brotli compression. ```tql compress_brotli [level=int, window_bits=int] ``` ## Description [Section titled “Description”](#description) The `compress_brotli` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled “level = int (optional)”](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ### `window_bits = int (optional)` [Section titled “window\_bits = int (optional)”](#window_bits--int-optional) A number representing the encoder window bits. ## Examples [Section titled “Examples”](#examples) ### Export all events in a Brotli-compressed NDJSON file [Section titled “Export all events in a Brotli-compressed NDJSON file”](#export-all-events-in-a-brotli-compressed-ndjson-file) ```tql export write_ndjson compress_brotli save_file "/tmp/backup.json.bt" ``` ### Recompress a Brotli-compressed file at a different compression level [Section titled “Recompress a Brotli-compressed file at a different compression level”](#recompress-a-brotli-compressed-file-at-a-different-compression-level) ```tql load_file "in.brotli" decompress_brotli compress_brotli level=18 save_file "out.brotli" ``` ## See Also [Section titled “See Also”](#see-also) [`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_brotli`](/reference/operators/decompress_brotli)

# compress_bz2

Compresses a stream of bytes using bz2 compression. ```tql compress_bz2 [level=int] ``` ## Description [Section titled “Description”](#description) The `compress_bz2` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled “level = int (optional)”](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ## Examples [Section titled “Examples”](#examples) ### Export all events in a Bzip2-compressed NDJSON file [Section titled “Export all events in a Bzip2-compressed NDJSON file”](#export-all-events-in-a-bzip2-compressed-ndjson-file) ```tql export write_ndjson compress_bz2 save_file "/tmp/backup.json.bz2" ``` ### Recompress a Bzip2-compressed file at a different compression level [Section titled “Recompress a Bzip2-compressed file at a different compression level”](#recompress-a-bzip2-compressed-file-at-a-different-compression-level) ```tql load_file "in.bz2" decompress_bz2 compress_bz2 level=18 save_file "out.bz2" ``` ## See Also [Section titled “See Also”](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_bz2`](/reference/operators/decompress_bz2)

# compress_gzip

Compresses a stream of bytes using gzip compression. ```tql compress_gzip [level=int, window_bits=int, format=string] ``` ## Description [Section titled “Description”](#description) The `compress_gzip` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled “level = int (optional)”](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ### `window_bits = int (optional)` [Section titled “window\_bits = int (optional)”](#window_bits--int-optional) A number representing the encoder window bits. ### `format = string (optional)` [Section titled “format = string (optional)”](#format--string-optional) A string representing the used format. Possible values are `zlib`, `deflate` and `gzip`. Defaults to `gzip`. ## Examples [Section titled “Examples”](#examples) ### Export all events in a Gzip-compressed NDJSON file [Section titled “Export all events in a Gzip-compressed NDJSON file”](#export-all-events-in-a-gzip-compressed-ndjson-file) ```tql export write_ndjson compress_gzip save_file "/tmp/backup.json.gz" ``` ### Compress using Gzip deflate [Section titled “Compress using Gzip deflate”](#compress-using-gzip-deflate) ```tql export write_ndjson compress_gzip format="deflate" ``` ### Recompress a Gzip-compressed file at a different compression level [Section titled “Recompress a Gzip-compressed file at a different compression level”](#recompress-a-gzip-compressed-file-at-a-different-compression-level) ```tql load_file "in.gzip" decompress_gzip compress_gzip level=18 save_file "out.gzip" ``` ## See Also [Section titled “See Also”](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_lz4`](/reference/operators/compress_lz4), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_gzip`](/reference/operators/decompress_gzip)

# compress_lz4

Compresses a stream of bytes using lz4 compression. ```tql compress_lz4 [level=int] ``` ## Description [Section titled “Description”](#description) The `compress_lz4` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled “level = int (optional)”](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ## Examples [Section titled “Examples”](#examples) ### Export all events in a Lz4-compressed NDJSON file [Section titled “Export all events in a Lz4-compressed NDJSON file”](#export-all-events-in-a-lz4-compressed-ndjson-file) ```tql export write_ndjson compress_lz4 save_file "/tmp/backup.json.lz4" ``` ### Recompress a Lz4-compressed file at a different compression level [Section titled “Recompress a Lz4-compressed file at a different compression level”](#recompress-a-lz4-compressed-file-at-a-different-compression-level) ```tql load_file "in.lz4" decompress_lz4 compress_lz4 level=18 save_file "out.lz4" ``` ## See Also [Section titled “See Also”](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_lz4`](/reference/operators/decompress_lz4)

# compress_zstd

Compresses a stream of bytes using zstd compression. ```tql compress_zstd [level=int] ``` ## Description [Section titled “Description”](#description) The `compress_zstd` operator compresses bytes in a pipeline incrementally. ### `level = int (optional)` [Section titled “level = int (optional)”](#level--int-optional) The compression level to use. The supported values depend on the codec used. If omitted, the default level for the codec is used. ## Examples [Section titled “Examples”](#examples) ### Export all events in a Zstd-compressed NDJSON file [Section titled “Export all events in a Zstd-compressed NDJSON file”](#export-all-events-in-a-zstd-compressed-ndjson-file) ```tql export write_ndjson compress_zstd save_file "/tmp/backup.json.zstd" ``` ### Recompress a Zstd-compressed file at a different compression level [Section titled “Recompress a Zstd-compressed file at a different compression level”](#recompress-a-zstd-compressed-file-at-a-different-compression-level) ```tql load_file "in.zstd" decompress_zstd compress_zstd level=18 save_file "out.zstd" ``` ## See Also [Section titled “See Also”](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`compress_bz2`](/reference/operators/compress_bz2), [`compress_gzip`](/reference/operators/compress_gzip), [`compress_lz4`](/reference/operators/compress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# context::create_bloom_filter

Creates a Bloom filter context. ```tql context::create_bloom_filter name:string, capacity=int, fp_probability=float ``` ## Description [Section titled “Description”](#description) The `context::create_bloom_filter` operator constructs a new context of type [Bloom filter](/explanations/enrichment#bloom-filter). To find suitable values for the capacity and false-positive probability, consult Thomas Hurst’s [Bloom Filter Calculator](https://hur.st/bloomfilter/). The parameter `n` corresponds to `capacity` and `p` to `fp_probability`. You can also create a Bloom filter context as code by adding it to `tenzir.contexts` in your `tenzir.yaml`: \<prefix>/etc/tenzir/tenzir.yaml ```yaml tenzir: contexts: my-iocs: type: bloom-filter arguments: capacity: 1B fp-probability: 0.001 ``` Making changes to `arguments` of an already created context has no effect. ### `name: string` [Section titled “name: string”](#name-string) The name of the new Bloom filter. ### `capacity = uint` [Section titled “capacity = uint”](#capacity--uint) The maximum number of items in the filter that maintain the false positive probability. Adding more elements does not yield an error, but lookups will more likely return false positives. ### `fp_probability = float` [Section titled “fp\_probability = float”](#fp_probability--float) The false-positive probability of the Bloom filter. ## Examples [Section titled “Examples”](#examples) ### Create a new Bloom filter context [Section titled “Create a new Bloom filter context”](#create-a-new-bloom-filter-context) ```tql context::create_bloom_filter "ctx", capacity=1B, fp_probability=0.001 ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::create_geoip

Creates a GeoIP context. ```tql context::create_geoip name:string, [db_path=string] ``` ## Description [Section titled “Description”](#description) The `context::create_geoip` operator constructs a new context of type [GeoIP](/explanations/enrichment#geoip-database). You must either provide a database with the `db_path` argument or use [`context::load`](/reference/operators/context/load) to populate the context after creation. You can also create a GeoIP context as code by adding it to `tenzir.contexts` in your `tenzir.yaml`: \<prefix>/etc/tenzir/tenzir.yaml ```yaml tenzir: contexts: my-geoips: type: geoip arguments: db-path: /usr/local/share/stuff/high-res-geoips.mmdb ``` Making changes to `arguments` of an already created context has no effect. ### `name: string` [Section titled “name: string”](#name-string) The name of the new GeoIP context. ### `db_path = string (optional)` [Section titled “db\_path = string (optional)”](#db_path--string-optional) The path to the [MMDB](https://maxmind.github.io/MaxMind-DB/) database, relative to the node’s working directory. ## Examples [Section titled “Examples”](#examples) ### Create a new GeoIP context [Section titled “Create a new GeoIP context”](#create-a-new-geoip-context) ```tql context::create_geoip "ctx", db_path="GeoLite2-City.mmdb" ``` ### Populate a GeoIP context from a remote location [Section titled “Populate a GeoIP context from a remote location”](#populate-a-geoip-context-from-a-remote-location) Load [CIRCL’s Geo Open](https://data.public.lu/en/datasets/geo-open-ip-address-geolocation-per-country-in-mmdb-format/) dataset from November 12, 2024: ```tql load_http "https://data.public.lu/fr/datasets/r/69064b5d-bf46-4244-b752-2096b16917a4" context::load "ctx" ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::create_lookup_table

Creates a lookup table context. ```tql context::create_lookup_table name:string ``` ## Description [Section titled “Description”](#description) The `context::create_lookup_table` operator constructs a new context of type [lookup table](/explanations/enrichment#lookup-table). You can also create a lookup table as code by adding it to `tenzir.contexts` in your `tenzir.yaml`: \<prefix>/etc/tenzir/tenzir.yaml ```yaml tenzir: contexts: my-table: type: lookup-table ``` ### `name: string` [Section titled “name: string”](#name-string) The name of the new lookup table. ## Examples [Section titled “Examples”](#examples) ### Create a new lookup table context [Section titled “Create a new lookup table context”](#create-a-new-lookup-table-context) ```tql context::create_lookup_table "ctx" ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::enrich

Enriches events with data from a context. ```tql context::enrich name:string, key=any, [into=field, mode=string, format=string] ``` ## Description [Section titled “Description”](#description) The `context::enrich` operator enriches events with data from a context. ### `name: string` [Section titled “name: string”](#name-string) The name of the context to enrich with. ### `key = any` [Section titled “key = any”](#key--any) The field to use for the context lookup. ### `into = field (optional)` [Section titled “into = field (optional)”](#into--field-optional) The field into which to write the enrichment. Defaults to the context name (`name`). ### `mode = string (optional)` [Section titled “mode = string (optional)”](#mode--string-optional) The mode of the enrichment operation: * `set`: overwrites the field specified by `into`. * `append`: appends into the list specified by `into`. If `into` is `null` or an `empty` list, a new list is created. If `into` is not a list, the enrichment will fail with a warning. Defaults to `set`. ### `format = string (optional)` [Section titled “format = string (optional)”](#format--string-optional) The style of the enriched value: * `plain`: formats the enrichment as retrieved from the context. * `ocsf`: formats the enrichment as an [OCSF Enrichment](https://schema.ocsf.io/1.4.0-dev/objects/enrichment?extensions=) object with fields `data`, `provider`, `type`, and `value`. Defaults to `plain`. ## Examples [Section titled “Examples”](#examples) ### Enrich with a lookup table [Section titled “Enrich with a lookup table”](#enrich-with-a-lookup-table) Create a lookup table: ```tql context::create_lookup_table "ctx" ``` Add data to the lookup table: ```tql from {x:1, y:"a"}, {x:2, y:"b"} context::update "ctx", key=x, value=y ``` Enrich with the table: ```tql from {x:1} context::enrich "ctx", key=x ``` ```tql { x: 1, ctx: "a", } ``` ### Enrich as OCSF Enrichment [Section titled “Enrich as OCSF Enrichment”](#enrich-as-ocsf-enrichment) Assume the same table preparation as above, but followed by a different call to `context::enrich` using the `format` option: ```tql from {x:1} context::enrich "ctx", key=x, format="ocsf" ``` ```tql { x: 1, ctx: { created_time: 2024-11-18T16:35:48.069981, name: "x", value: 1, data: "a", } } ``` ### Enrich by appending to an array [Section titled “Enrich by appending to an array”](#enrich-by-appending-to-an-array) Enrich twice with the same context and accumulate enrichments into an array: ```tql from {x:1} context::enrich "ctx", key=x, into=enrichments, mode="append" context::enrich "ctx", key=x, into=enrichments, mode="append" ``` ```tql { x: 1, enrichments: [ "a", "a", ] } ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::erase

Removes entries from a context. ```tql context::erase name:string, key=any ``` ## Description [Section titled “Description”](#description) The `context::erase` operator removes data from a context. Use the `key` argument to specify the field in the input that should be deleted from the context. ### `name: string` [Section titled “name: string”](#name-string) The name of the context to remove entries from. ### `key = any` [Section titled “key = any”](#key--any) The field that represents the enrichment key in the data. ## Examples [Section titled “Examples”](#examples) ### Delete entries from a context [Section titled “Delete entries from a context”](#delete-entries-from-a-context) ```plaintext from {network: 10.0.0.1/16} context::erase "network-classification", key=network ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::inspect

Resets a context. ```tql context::inspect name:string ``` ## Description [Section titled “Description”](#description) The `context::inspect` operator shows details about a specified context. ### `name: string` [Section titled “name: string”](#name-string) The name of the context to inspect. ## Examples [Section titled “Examples”](#examples) ### Inspect a context [Section titled “Inspect a context”](#inspect-a-context) Add data to the lookup table: ```tql from {x:1, y:"a"}, {x:2, y:"b"} context::update "ctx", key=x, value=y ``` Retrieve the lookup table contents: ```tql context::inspect "ctx" ``` ```tql {key: 2, value: "b"} {key: 1, value: "a"} ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::erase`](/reference/operators/context/enrich), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::list

Lists all contexts ```tql context::list ``` ## Description [Section titled “Description”](#description) The `context::list` operator retrieves all contexts. ## Examples [Section titled “Examples”](#examples) ### Show all contexts [Section titled “Show all contexts”](#show-all-contexts) ```tql context::list ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::load

Loads context state. ```tql context::load name:string ``` ## Description [Section titled “Description”](#description) The `context::load` operator replaces the state of the specified context with its (binary) input. ### `name: string` [Section titled “name: string”](#name-string) The name of the context whose state to update. ## Examples [Section titled “Examples”](#examples) ### Replace the database of a GeoIP context [Section titled “Replace the database of a GeoIP context”](#replace-the-database-of-a-geoip-context) ```tql load_file "ultra-high-res.mmdb", mmap=true context::load "ctx" ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::remove

Deletes a context. ```tql context::remove name:string ``` ## Description [Section titled “Description”](#description) The `context::remove` operator deletes the specified context. ### `name: string` [Section titled “name: string”](#name-string) The name of the context to delete. ## Examples [Section titled “Examples”](#examples) ### Delete a context [Section titled “Delete a context”](#delete-a-context) ```tql context::delete "ctx" ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::inspect`](/reference/operators/context/inspect), [`context::load`](/reference/operators/context/load), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`list`](/reference/operators/context/list), [`reset`](/reference/operators/context/reset), [`update`](/reference/operators/context/update)

# context::reset

Resets a context. ```tql context::reset name:string ``` ## Description [Section titled “Description”](#description) The `context::reset` operator erases all data that has been added with `context::update`. ### `name: string` [Section titled “name: string”](#name-string) The name of the context to reset. ## Examples [Section titled “Examples”](#examples) ### Reset a context [Section titled “Reset a context”](#reset-a-context) ```tql context::reset "ctx" ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_geoip`](/reference/operators/context/create_geoip), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::save`](/reference/operators/context/save), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::save

Saves context state. ```tql context::save name:string ``` ## Description [Section titled “Description”](#description) The `context::save` operator dumps the state of the specified context into its (binary) output. ### `name: string` [Section titled “name: string”](#name-string) The name of the context whose state to save. ## Examples [Section titled “Examples”](#examples) ### Store the database of a GeoIP context [Section titled “Store the database of a GeoIP context”](#store-the-database-of-a-geoip-context) ```tql context::save "ctx" save_file "snapshot.mmdb" ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list), [`update`](/reference/operators/context/update)

# context::update

Updates a context with new data. ```tql context::update name:string, key=any, [value=any, create_timeout=duration, write_timeout=duration, read_timeout=duration] ``` ## Description [Section titled “Description”](#description) The `context::update` operator adds new data to a specified context. Use the `key` argument to specify the field in the input that should be associated with the context. The [`context::enrich`](/reference/operators/context/enrich) operator uses this key to access the context. For contexts that support assigning a value with a given key, you can provide an expression to customize what’s being associated with the given key. The three arguments `create_timeout`, `write_timeout`, and `read_timeout` only work with lookup tables and set the respective timeouts per table entry. ### `name: string` [Section titled “name: string”](#name-string) The name of the context to update. ### `key = any` [Section titled “key = any”](#key--any) The field that represents the enrichment key in the data. ### `value = any (optional)` [Section titled “value = any (optional)”](#value--any-optional) The field that represents the enrichment value to associate with `key`. Defaults to `this`. ### `create_timeout = duration (optional)` [Section titled “create\_timeout = duration (optional)”](#create_timeout--duration-optional) Expires a context entry after a given duration since entry creation. ### `write_timeout = duration (optional)` [Section titled “write\_timeout = duration (optional)”](#write_timeout--duration-optional) Expires a context entry after a given duration since the last update time. Every Every call to `context::update` resets the timeout for the respective key. ### `read_timeout = duration (optional)` [Section titled “read\_timeout = duration (optional)”](#read_timeout--duration-optional) Expires a context entry after a given duration since the last access time. Every call to `context::enrich` resets the timeout for the respective key. ## Examples [Section titled “Examples”](#examples) ### Populate a lookup table with data [Section titled “Populate a lookup table with data”](#populate-a-lookup-table-with-data) Create a lookup table: ```tql context::create_lookup_table "ctx" ``` Add data to the lookup table via `context::update`: ```tql from {x:1, y:"a"}, {x:2, y:"b"} context::update "ctx", key=x, value=y ``` Retrieve the lookup table contents: ```tql context::inspect "ctx" ``` ```tql {key: 2, value: "b"} {key: 1, value: "a"} ``` ### Use a custom value as lookup table [Section titled “Use a custom value as lookup table”](#use-a-custom-value-as-lookup-table) ```tql from {x:1}, {x:2} context::update "ctx", key=x, value=x*x ``` ```tql {key: 2, value: 4} {key: 1, value: 1} ``` ## See Also [Section titled “See Also”](#see-also) [`context::create_bloom_filter`](/reference/operators/context/create_bloom_filter), [`context::create_lookup_table`](/reference/operators/context/create_lookup_table), [`context::load`](/reference/operators/context/load), [`context::remove`](/reference/operators/context/remove), [`context::reset`](/reference/operators/context/reset), [`context::save`](/reference/operators/context/save), [`create_geoip`](/reference/operators/context/create_geoip), [`enrich`](/reference/operators/context/enrich), [`erase`](/reference/operators/context/erase), [`inspect`](/reference/operators/context/inspect), [`list`](/reference/operators/context/list)

# cron

Runs a pipeline periodically according to a cron expression. ```tql cron schedule:string { … } ``` ## Description [Section titled “Description”](#description) The `cron` operator performs scheduled execution of a pipeline indefinitely according to a [cron expression](https://en.wikipedia.org/wiki/Cron). The executor spawns a new pipeline according to the cadence given by `schedule`. If the pipeline runs longer than the interval to the next scheduled time point, the next run immediately starts. ### `schedule: string` [Section titled “schedule: string”](#schedule-string) The cron expression with the following syntax: ```plaintext <seconds> <minutes> <hours> <days of month> <months> <days of week> ``` The 6 fields are separated by a space. Allowed values for each field are: | Field | Value range\* | Special characters | Alternative Literals | | ------------ | ------------- | ----------------------- | -------------------- | | seconds | 0-59 | `*` `,` `-` | | | minutes | 0-59 | `*` `,` `-` | | | hours | 0-23 | `*` `,` `-` | | | days of | 1-31 | `*` `,` `-` `?` `L` `W` | | | months | 1-12 | `*` `,` `-` | `JAN` … `DEC` | | days of week | 0-6 | `*` `,` `-` `?` `L` `#` | `SUN` … `SAT` | The special characters have the following meaning: | Special character | Meaning | Description | | ----------------- | ----------------- | ------------------------------------------------- | | `*` | all values | selects all values within a field | | `?` | no specific value | specify one field and leave the other unspecified | | `-` | range | specify ranges | | `,` | comma | specify additional values | | `/` | slash | specify increments | | `L` | last | last day of the month or last day of the week | | `W` | weekday | the weekday nearest to the given day | | `#` | nth | specify the Nth day of the month | ## Examples [Section titled “Examples”](#examples) ### Fetch the results from an API every 10 minutes [Section titled “Fetch the results from an API every 10 minutes”](#fetch-the-results-from-an-api-every-10-minutes) Pull an endpoint on every 10th minute, Monday through Friday: ```tql cron "* */10 * * * MON-FRI" { from "https://example.org/api" } publish "api" ``` ## See Also [Section titled “See Also”](#see-also) [`every`](/reference/operators/every)

# decompress

Decompresses a stream of bytes. ```tql decompress codec:string ``` Deprecated The `decompress` operator is deprecated. You should use the [bespoke operators](/reference/operators#encode--decode) instead. ## Description [Section titled “Description”](#description) The `decompress` operator decompresses bytes in a pipeline incrementally with a known codec. The operator supports decompressing multiple concatenated streams of the same codec transparently. ### `codec: string` [Section titled “codec: string”](#codec-string) An identifier of the codec to use. Currently supported are `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`. ## Examples [Section titled “Examples”](#examples) ### Import Suricata events from a Zstd-compressed file [Section titled “Import Suricata events from a Zstd-compressed file”](#import-suricata-events-from-a-zstd-compressed-file) ```tql load_file "eve.json.zst" decompress "zstd" read_suricata import ``` ### Convert a Zstd-compressed file into an LZ4-compressed file [Section titled “Convert a Zstd-compressed file into an LZ4-compressed file”](#convert-a-zstd-compressed-file-into-an-lz4-compressed-file) ```tql load_file "in.zst" decompress "zstd" compress "lz4" save_file "out.lz4" ```

# decompress_brotli

Decompresses a stream of bytes in the Brotli format. ```tql decompress_brotli ``` ## Description [Section titled “Description”](#description) The `decompress_brotli` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled “Examples”](#examples) ### Import Suricata events from a Brotli-compressed file [Section titled “Import Suricata events from a Brotli-compressed file”](#import-suricata-events-from-a-brotli-compressed-file) ```tql load_file "eve.json.brotli" decompress_brotli read_suricata import ``` ## See Also [Section titled “See Also”](#see-also) [`compress_brotli`](/reference/operators/compress_brotli), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_bz2

Decompresses a stream of bytes in the Bzip2 format. ```tql decompress_bz2 ``` ## Description [Section titled “Description”](#description) The `decompress_bz2` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled “Examples”](#examples) ### Import Suricata events from a Bzip2-compressed file [Section titled “Import Suricata events from a Bzip2-compressed file”](#import-suricata-events-from-a-bzip2-compressed-file) ```tql load_file "eve.json.bz" decompress_bz2 read_suricata import ``` ## See Also [Section titled “See Also”](#see-also) [`compress_bz2`](/reference/operators/compress_bz2), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_gzip

Decompresses a stream of bytes in the Gzip format. ```tql decompress_gzip ``` ## Description [Section titled “Description”](#description) The `decompress_gzip` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled “Examples”](#examples) ### Import Suricata events from a Gzip-compressed file [Section titled “Import Suricata events from a Gzip-compressed file”](#import-suricata-events-from-a-gzip-compressed-file) ```tql load_file "eve.json.gz" decompress_brotli decompress_gzip import ``` ## See Also [Section titled “See Also”](#see-also) [`compress_gzip`](/reference/operators/compress_gzip), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_lz4

Decompresses a stream of bytes in the Lz4 format. ```tql decompress_lz4 ``` ## Description [Section titled “Description”](#description) The `decompress_lz4` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled “Examples”](#examples) ### Import Suricata events from a LZ4-compressed file [Section titled “Import Suricata events from a LZ4-compressed file”](#import-suricata-events-from-a-lz4-compressed-file) ```tql load_file "eve.json.lz4" decompress_lz4 read_suricata import ``` ## See Also [Section titled “See Also”](#see-also) [`compress_lz4`](/reference/operators/compress_lz4), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# decompress_zstd

Decompresses a stream of bytes in the Zstd format. ```tql decompress_zstd ``` ## Description [Section titled “Description”](#description) The `decompress_zstd` operator decompresses bytes in a pipeline incrementally. The operator supports decompressing multiple concatenated streams of the same codec transparently. ## Examples [Section titled “Examples”](#examples) ### Import Suricata events from a Zstd-compressed file [Section titled “Import Suricata events from a Zstd-compressed file”](#import-suricata-events-from-a-zstd-compressed-file) ```tql load_file "eve.json.zstd" decompress_zstd read_suricata import ``` ## See Also [Section titled “See Also”](#see-also) [`compress_zstd`](/reference/operators/compress_zstd), [`decompress_brotli`](/reference/operators/decompress_brotli), [`decompress_bz2`](/reference/operators/decompress_bz2), [`decompress_gzip`](/reference/operators/decompress_gzip), [`decompress_lz4`](/reference/operators/decompress_lz4), [`decompress_zstd`](/reference/operators/decompress_zstd)

# deduplicate

Removes duplicate events based on a common key. ```tql deduplicate [key:any, limit=int, distance=int, create_timeout=duration, write_timeout=duration, read_timeout=duration] ``` ## Description [Section titled “Description”](#description) The `deduplicate` operator removes duplicates from a stream of events, based on the value of one or more fields. ### `key: any (optional)` [Section titled “key: any (optional)”](#key-any-optional) The key to deduplicate. To deduplicate multiple fields, use a record expression like `{foo: bar, baz: qux}`. Defaults to `this`, i.e., deduplicating entire events. ### `limit = int (optional)` [Section titled “limit = int (optional)”](#limit--int-optional) The number of duplicate keys allowed before an event is suppressed. Defaults to `1`, which is equivalent to removing all duplicates. ### `distance = int (optional)` [Section titled “distance = int (optional)”](#distance--int-optional) Distance between two events that can be considered duplicates. A value of `1` means that only adjacent events can be considered duplicate. When unspecified, the distance is infinite. ### `create_timeout = duration (optional)` [Section titled “create\_timeout = duration (optional)”](#create_timeout--duration-optional) The time that needs to pass until a surpressed event is no longer considered a duplicate. The timeout resets when the first event for a given key is let through. ### `write_timeout = duration (optional)` [Section titled “write\_timeout = duration (optional)”](#write_timeout--duration-optional) The time that needs to pass until a suppressed event is no longer considered a duplicate. The timeout resets when any event for a given key is let through. For a limit of `1`, the write timeout is equivalent to the create timeout. The write timeout must be smaller than the create timeout. ### `read_timeout = duration (optional)` [Section titled “read\_timeout = duration (optional)”](#read_timeout--duration-optional) The time that needs to pass until a suppressed event is no longer considered a duplicate. The timeout resets when a key is seen, even if the event is suppressed. The read timeout must be smaller than the write and create timeouts. ## Examples [Section titled “Examples”](#examples) ### Simple deduplication [Section titled “Simple deduplication”](#simple-deduplication) Consider the following data: ```tql {foo: 1, bar: "a"} {foo: 1, bar: "a"} {foo: 1, bar: "a"} {foo: 1, bar: "b"} {foo: null, bar: "b"} {bar: "b"} {foo: null, bar: "b"} {foo: null, bar: "b"} ``` For `deduplicate`, all duplicate events are removed: ```tql {foo: 1, bar: "a"} {foo: 1, bar: "b"} {foo: null, bar: "b"} {bar: "b"} ``` If `deduplicate bar` is used, only the field `bar` is considered when determining whether an event is a duplicate: ```tql {foo: 1, bar: "a"} {foo: 1, bar: "b"} ``` And for `deduplicate foo`, only the field `foo` is considered. Note, how the missing `foo` field is treated as if it had the value `null`, i.e., it’s not included in the output. ```tql {foo: 1, bar: "a"} {foo: null, bar: "b"} ``` ### Get up to 10 warnings per hour for each run of a pipeline [Section titled “Get up to 10 warnings per hour for each run of a pipeline”](#get-up-to-10-warnings-per-hour-for-each-run-of-a-pipeline) ```tql diagnostics live=true deduplicate {id: pipeline_id, run: run}, limit=10, create_timeout=1h ``` ### Get an event whenever the node disconnected from the Tenzir Platform [Section titled “Get an event whenever the node disconnected from the Tenzir Platform”](#get-an-event-whenever-the-node-disconnected-from-the-tenzir-platform) ```tql metrics "platform", live=true deduplicate connected, distance=1 where not connected ``` ## See Also [Section titled “See Also”](#see-also) [`sample`](/reference/operators/sample)

# delay

Delays events relative to a given start time, with an optional speedup. ```tql delay field:time, [start=time, speed=double] ``` ## Description [Section titled “Description”](#description) The `delay` operator replays a dataflow according to a time field by introducing sleeping periods proportional to the inter-arrival times of the events. With the `speed` option, you can adjust the sleep time of the time series induced by `field` with a multiplicative factor. This has the effect of making the time series “faster” for values great than 1 and “slower” for values less than 1. Unless you provide a start time with `start`, the operator will anchor the timestamps in `field` to begin with the current wall clock time, as if you provided `start=now()`. The diagram below illustrates the effect of applying `delay` to dataflow. If an event in the stream has a timestamp the precedes the previous event, `delay` emits it instantly. Otherwise `delay` sleeps the amount of time to reach the next timestamp. As shown in the last illustration, the `speed` factor has a scaling effect on the inter-arrival times. ![Delay](/_astro/delay.excalidraw.BSSAawE0_19DKCs.svg) The options `start` and `speed` work independently, i.e., you can use them separately or both together. ### `field: time` [Section titled “field: time”](#field-time) The field in the event containing the timestamp values. ### `start = time (optional)` [Section titled “start = time (optional)”](#start--time-optional) The timestamp to anchor the time values around. Defaults to the first non-null timestamp in `field`. ### `speed = double (optional)` [Section titled “speed = double (optional)”](#speed--double-optional) A constant factor to be divided by the inter-arrival time. For example, 2.0 decreases the event gaps by a factor of two, resulting a twice as fast dataflow. A value of 0.1 creates dataflow that spans ten times the original time frame. Defaults to 1.0. ## Examples [Section titled “Examples”](#examples) ### Replay logs in real time [Section titled “Replay logs in real time”](#replay-logs-in-real-time) Replay the M57 Zeek logs with real-world inter-arrival times from the `ts` field. For example, if an event arrives at time *t* and the next event at time *u*, then the `delay` operator will wait time *u - t* between emitting the two events. If *t > u* then the operator immediately emits next event. ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" read_zeek_tsv delay ts ``` ### Replay logs at 10.5 times the original speed [Section titled “Replay logs at 10.5 times the original speed”](#replay-logs-at-105-times-the-original-speed) ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" read_zeek_tsv delay ts, speed=10.5 ``` ### Replay and delay after a given timestamp [Section titled “Replay and delay after a given timestamp”](#replay-and-delay-after-a-given-timestamp) Replay and start delaying only after `ts` exceeds `2021-11-17T16:35` and emit all events prior to that timestamp immediately. ```tql load_file "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" read_zeek_tsv delay ts, start=2021-11-17T16:35, speed=10.0 ``` Adjust the timestamp to the present, and then start replaying in 2 hours from now: ```tql load_file "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" decompress "zstd" read_zeek_tsv timeshift ts delay ts, start=now()+2h ``` ## See Also [Section titled “See Also”](#see-also) [`timeshift`](/reference/operators/timeshift)

# diagnostics

Retrieves diagnostic events from a Tenzir node. ```tql diagnostics [live=bool, retro=bool] ``` ## Description [Section titled “Description”](#description) The `diagnostics` operator retrieves diagnostic events from a Tenzir node. ### `live = bool (optional)` [Section titled “live = bool (optional)”](#live--bool-optional) If `true`, emits diagnostic events as they are generated in real-time. Unless `retro=true` is also given, this makes it so that previous diagnostics events are not returned. ### `retro = bool (optional)` [Section titled “retro = bool (optional)”](#retro--bool-optional) Return diagnostic events that were generated in the past. Unless `live=true` is given, this is the default. If both are set to `true`, all previous events are returned before beginning with the live events. ## Schemas [Section titled “Schemas”](#schemas) Tenzir emits diagnostic information with the following schema: ### `tenzir.diagnostic` [Section titled “tenzir.diagnostic”](#tenzirdiagnostic) Contains detailed information about the diagnostic. | Field | Type | Description | | :------------ | :------------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline that created the diagnostic. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `timestamp` | `time` | The exact timestamp of the diagnostic creation. | | `message` | `string` | The diagnostic message. | | `severity` | `string` | The diagnostic severity. | | `notes` | `list<record>` | The diagnostic notes. Can be empty. | | `annotations` | `list<record>` | The diagnostic annotations. Can be empty. | | `rendered` | `string` | The rendered diagnostic, as printed on the command-line. | The records in `notes` have the following schema: | Field | Type | Description | | :-------- | :------- | :------------------------------------------------------------ | | `kind` | `string` | The kind of note, which is `note`, `usage`, `hint` or `docs`. | | `message` | `string` | The message of this note. | The records in `annotations` have the following schema: | Field | Type | Description | | :-------- | :------- | :----------------------------------------------------------------------------------------------------------- | | `primary` | `bool` | True if the `source` represents the underlying reason for the diagnostic, false if it is only related to it. | | `text` | `string` | A message for explanations. Can be empty. | | `source` | `string` | The character range in the pipeline string that this annotation is associated to. | ## Examples [Section titled “Examples”](#examples) ### View all diagnostics generated in the past 5 minutes [Section titled “View all diagnostics generated in the past 5 minutes”](#view-all-diagnostics-generated-in-the-past-5-minutes) ```tql diagnostics where timestamp > now() - 5min ``` ### Get a live feed of error diagnostics [Section titled “Get a live feed of error diagnostics”](#get-a-live-feed-of-error-diagnostics) ```tql diagnostics live=true where severity == "error" ``` ## See Also [Section titled “See Also”](#see-also) [`metrics`](/reference/operators/metrics)

# discard

Discards all incoming events. ```tql discard ``` ## Description [Section titled “Description”](#description) The `discard` operator discards all the incoming events immediately, without rendering them or doing any additional processing. This operator is mainly used to test or benchmark pipelines. ## Examples [Section titled “Examples”](#examples) ### Benchmark to see how long it takes to export everything [Section titled “Benchmark to see how long it takes to export everything”](#benchmark-to-see-how-long-it-takes-to-export-everything) ```tql export discard ```

# dns_lookup

Performs DNS lookups to resolve IP addresses to hostnames or hostnames to IP addresses. ```tql dns_lookup field, [result=field] ``` ## Description [Section titled “Description”](#description) The `dns_lookup` operator performs DNS resolution on the specified field. It automatically detects whether to perform a forward lookup (hostname to IP) or reverse lookup (IP to hostname) based on the field’s content. * **Reverse lookup**: When the field contains an IP address, the operator performs a PTR query to find the associated hostname. * **Forward lookup**: When the field contains a string, the operator performs A and AAAA queries to find associated IP addresses. The result is stored as a record in the specified result field. ### `field: ip|string` [Section titled “field: ip|string”](#field-ipstring) The field containing either an IP address or hostname to look up. ### `result = field (optional)` [Section titled “result = field (optional)”](#result--field-optional) The field where the DNS lookup result will be stored. Defaults to `dns_lookup`. The result is a record with the following structure: For reverse lookups (IP to hostname): ```tql { hostname: string } ``` For forward lookups (hostname to IP): ```tql list<record> ``` Where each record has the structure: ```tql { address: ip, type: string, ttl: duration } ``` If the lookup fails or times out, the result field will be `null`. ## Examples [Section titled “Examples”](#examples) ### Reverse DNS lookup [Section titled “Reverse DNS lookup”](#reverse-dns-lookup) Resolve an IP address to its hostname: ```tql from {src_ip: 8.8.8.8, dst_ip: 192.168.1.1} dns_lookup src_ip, result=src_dns ``` ```tql { src_ip: 8.8.8.8, dst_ip: 192.168.1.1, src_dns: { hostname: "dns.google" } } ``` ### Forward DNS lookup [Section titled “Forward DNS lookup”](#forward-dns-lookup) Resolve a hostname to its IP addresses: ```tql from {domain: "example.com", timestamp: 2024-01-15T10:30:00} dns_lookup domain, result=ip_info ``` ```tql { domain: "example.com", timestamp: 2024-01-15T10:30:00, ip_info: [ {address: 93.184.215.14, type: "A", ttl: 5m}, {address: 2606:2800:21f:cb07:6820:80da:af6b:8b2c, type: "AAAA", ttl: 5m} ] } ``` ### Handling lookup failures [Section titled “Handling lookup failures”](#handling-lookup-failures) When a DNS lookup fails, the result field is set to `null`: ```tql from {ip: 192.168.1.123} dns_lookup ip, result=hostname_info ``` ```tql { ip: 192.168.1.123, hostname_info: null } ``` ### Multiple lookups in a pipeline [Section titled “Multiple lookups in a pipeline”](#multiple-lookups-in-a-pipeline) ```tql from { source: 1.1.1.1, destination: "tenzir.com" } dns_lookup source, result=source_dns dns_lookup destination, result=dest_ips ``` ```tql { source: 1.1.1.1, destination: "tenzir.com", source_dns: { hostname: "one.one.one.one" }, dest_ips: [ {address: 185.199.108.153, type: "A", ttl: 1h}, {address: 185.199.109.153, type: "A", ttl: 1h}, {address: 185.199.110.153, type: "A", ttl: 1h}, {address: 185.199.111.153, type: "A", ttl: 1h} ] } ``` ## See Also [Section titled “See Also”](#see-also) [`set`](/reference/operators/set)

# drop

Removes fields from the event. ```tql drop field... ``` ## Description [Section titled “Description”](#description) Removes the given fields from the events. Issues a warning if a field is not present. ## Examples [Section titled “Examples”](#examples) ### Drop fields from the input [Section titled “Drop fields from the input”](#drop-fields-from-the-input) ```tql from { src: 192.168.0.4, dst: 192.168.0.31, role: "admin", info: { id: "cR32kdMD9", msg: 8411, }, } drop role, info.id ``` ```tql { src: 192.168.0.4, dst: 192.168.0.31, info: { msg: 8411, }, } ``` ## See Also [Section titled “See Also”](#see-also) [`select`](/reference/operators/select), [`where`](/reference/operators/where)

# drop_null_fields

Removes fields containing null values from the event. ```tql drop_null_fields [field...] ``` ## Description [Section titled “Description”](#description) The `drop_null_fields` operator removes fields that have `null` values from events. Without arguments, it removes all fields with `null` values from the entire event. When provided with specific field paths, it removes those fields if they contain null values, and for record fields, it also recursively removes any null fields within them. ### `field... (optional)` [Section titled “field... (optional)”](#field-optional) A comma-separated list of field paths to process. When specified: * If a field contains `null`, it will be removed * If a field is a record, all null fields within it will be removed recursively * Other null fields outside the specified paths will be preserved ## Examples [Section titled “Examples”](#examples) ### Drop all null fields from the input [Section titled “Drop all null fields from the input”](#drop-all-null-fields-from-the-input) ```tql from { src: 192.168.0.4, dst: null, role: "admin", info: { id: null, msg: 8411, }, } drop_null_fields ``` ```tql { src: 192.168.0.4, role: "admin", info: { msg: 8411, }, } ``` ### Drop specific null fields [Section titled “Drop specific null fields”](#drop-specific-null-fields) ```tql from { src: 192.168.0.4, dst: null, role: null, info: { id: null, msg: 8411, }, } drop_null_fields dst, info.id ``` ```tql { src: 192.168.0.4, role: null, info: { msg: 8411, }, } ``` ### Drop null fields within a record field [Section titled “Drop null fields within a record field”](#drop-null-fields-within-a-record-field) When specifying a record field, all null fields within it are removed recursively: ```tql from { metadata: { created: "2024-01-01", updated: null, tags: null, author: "admin" }, data: { value: 42, comment: null } } drop_null_fields metadata ``` ```tql { metadata: { created: "2024-01-01", author: "admin", }, data: { value: 42, comment: null, }, } ``` Note that `data.comment` remains null because only `metadata` was specified. ### Behavior with records inside lists [Section titled “Behavior with records inside lists”](#behavior-with-records-inside-lists) The `drop_null_fields` operator does not remove fields from records that are inside lists: ```tql from { id: 1, items: [{name: "a", value: 1}, {name: "b", value: null}], metadata: null, tags: ["x", null, "y"] } drop_null_fields ``` ```tql { id: 1, items: [ { name: "a", value: 1, }, { name: "b", value: null, }, ], tags: [ "x", null, "y", ], } ``` In this example: * The `metadata` field is removed because it contains `null` * The `items` field is kept with all its internal structure intact * The `tags` field is kept even though it contains `null` elements * Fields within records inside lists (like `value`) are not dropped even if they contain `null` ## See Also [Section titled “See Also”](#see-also) [`drop`](/reference/operators/drop), [`select`](/reference/operators/select), [`where`](/reference/operators/where)

# enumerate

Add a field with the number of preceding events. ```tql enumerate [out:field, group=any] ``` ## Description [Section titled “Description”](#description) The `enumerate` operator adds a new field with the number of preceding events to the beginning of the input record. ### `out: field (optional)` [Section titled “out: field (optional)”](#out-field-optional) Sets the name of the output field. Defaults to `"#"`. ### `group: any (optional)` [Section titled “group: any (optional)”](#group-any-optional) Groups events by the specified expression and enumerates within each group. When provided, an independent enumeration counter is used for each unique value of the grouping expression. ## Examples [Section titled “Examples”](#examples) ### Enumerate the input by prepending row numbers [Section titled “Enumerate the input by prepending row numbers”](#enumerate-the-input-by-prepending-row-numbers) ```tql from {x: "a"}, {x: "b"}, {x: "c"} enumerate ``` ```tql {"#": 0, x: "a"} {"#": 1, x: "b"} {"#": 2, x: "c"} ``` ### Use a custom field for the row numbers [Section titled “Use a custom field for the row numbers”](#use-a-custom-field-for-the-row-numbers) ```tql from {x: true}, {x: false} enumerate index ``` ```tql {index: 0, x: true} {index: 1, x: false} ``` ### Count within groups [Section titled “Count within groups”](#count-within-groups) ```tql from {x: 1}, {x: 2}, {x: 1}, {x: 2} enumerate count, group=x count = count + 1 ``` ```tql {count: 1, x: 1} {count: 1, x: 2} {count: 2, x: 1} {count: 2, x: 2} ```

# every

Runs a pipeline periodically at a fixed interval. ```tql every interval:duration { … } ``` ## Description [Section titled “Description”](#description) The `every` operator repeats running a pipeline indefinitely at a fixed interval. The first run is starts directly when the outer pipeline itself starts. Every `interval`, the executor spawns a new pipeline that runs to completion. If the pipeline runs longer than `interval`, the next run immediately starts. ## Examples [Section titled “Examples”](#examples) ### Produce one event per second and enumerate the result [Section titled “Produce one event per second and enumerate the result”](#produce-one-event-per-second-and-enumerate-the-result) ```tql every 1s { from {} } enumerate ``` ```tql {"#": 0} // immediately {"#": 1} // after 1s {"#": 2} // after 2s {"#": 3} // after 3s // … continues like this ``` ### Fetch the results from an API every 10 minutes [Section titled “Fetch the results from an API every 10 minutes”](#fetch-the-results-from-an-api-every-10-minutes) ```tql every 10min { load_http "example.org/api/threats" read_json } publish "threat-feed" ``` ## See Also [Section titled “See Also”](#see-also) [`cron`](/reference/operators/cron)

# export

Retrieves events from a Tenzir node. ```tql export [live=bool, retro=bool, internal=bool, parallel=int] ``` ## Description [Section titled “Description”](#description) The `export` operator retrieves events from a Tenzir node. This operator is the dual to [`import`](/reference/operators/import). ### `live = bool (optional)` [Section titled “live = bool (optional)”](#live--bool-optional) Work on all events that are imported with `import` operators in real-time instead of on events persisted at a Tenzir node. Note that live exports may drop events if the following pipeline fails to keep up. To connect pipelines with back pressure, use the [`publish`](/reference/operators/publish) and [`subscribe`](/reference/operators/subscribe) operators. ### `retro = bool (optional)` [Section titled “retro = bool (optional)”](#retro--bool-optional) Export persistent events at a Tenzir node. Unless `live=true` is given, this is implied. Use `retro=true, live=true` to export past events, and live events afterwards. ### `internal = bool (optional)` [Section titled “internal = bool (optional)”](#internal--bool-optional) Export internal events, such as metrics or diagnostics, instead. By default, `export` only returns events that were previously imported with `import`. In contrast, `export internal=true` exports internal events such as operator metrics. ### `parallel = int (optional)` [Section titled “parallel = int (optional)”](#parallel--int-optional) The parallel level controls how many worker threads the operator uses at most for querying historical events. Defaults to 3. ## Examples [Section titled “Examples”](#examples) ### Export all stored events as JSON [Section titled “Export all stored events as JSON”](#export-all-stored-events-as-json) ```tql export write_json ``` ### Get a subset of matching events [Section titled “Get a subset of matching events”](#get-a-subset-of-matching-events) ```tql export where src_ip == 1.2.3.4 head 20 ``` ## See Also [Section titled “See Also”](#see-also) [`import`](/reference/operators/import), [`subscribe`](/reference/operators/subscribe)

# fields

Retrieves all fields stored at a node. ```tql fields ``` ## Description [Section titled “Description”](#description) The `fields` operator shows a list of all fields stored at a node across all available schemas. ## Examples [Section titled “Examples”](#examples) ### Get the top-5 most frequently used fields across schemas [Section titled “Get the top-5 most frequently used fields across schemas”](#get-the-top-5-most-frequently-used-fields-across-schemas) ```tql fields summarize field, count=count_distinct(schema), schemas=distinct(schema) sort -count head 5 ```

# files

Shows file information for a given directory. ```tql files [dir:string, recurse=bool, follow_symlinks=bool, skip_permission_denied=bool] ``` ## Description [Section titled “Description”](#description) The `files` operator shows file information for all files in the given directory. ### `dir: string (optional)` [Section titled “dir: string (optional)”](#dir-string-optional) The directory to list files in. Defaults to the current working directory. ### `recurse = bool (optional)` [Section titled “recurse = bool (optional)”](#recurse--bool-optional) Recursively list files in subdirectories. ### `follow_symlinks = bool (optional)` [Section titled “follow\_symlinks = bool (optional)”](#follow_symlinks--bool-optional) Follow directory symlinks. ### `skip_permission_denied = bool (optional)` [Section titled “skip\_permission\_denied = bool (optional)”](#skip_permission_denied--bool-optional) Skip directories that would otherwise result in permission denied errors. ## Schemas [Section titled “Schemas”](#schemas) Tenzir emits file information with the following schema. ### `tenzir.file` [Section titled “tenzir.file”](#tenzirfile) Contains detailed information about the file. | Field | Type | Description | | :---------------- | :------- | :--------------------------------------- | | `path` | `string` | The file path. | | `type` | `string` | The type of the file (see below). | | `permissions` | `record` | The permissions of the file (see below). | | `owner` | `string` | The file’s owner. | | `group` | `string` | The file’s group. | | `file_size` | `uint64` | The file size in bytes. | | `hard_link_count` | `uint64` | The number of hard links to the file. | | `last_write_time` | `time` | The time of the last write to the file. | The `type` field can have one of the following values: | Value | Description | | :---------- | :------------------------------ | | `regular` | The file is a regular file. | | `directory` | The file is a directory. | | `symlink` | The file is a symbolic link. | | `block` | The file is a block device. | | `character` | The file is a character device. | | `fifo` | The file is a named IPC pipe. | | `socket` | The file is a named IPC socket. | | `not_found` | The file does not exist. | | `unknown` | The file has an unknown type. | The `permissions` record contains the following fields: | Field | Type | Description | | :------- | :------- | :---------------------------------- | | `owner` | `record` | The file permissions for the owner. | | `group` | `record` | The file permissions for the group. | | `others` | `record` | The file permissions for others. | The `owner`, `group`, and `others` records contain the following fields: | Field | Type | Description | | :-------- | :----- | :------------------------------ | | `read` | `bool` | Whether the file is readable. | | `write` | `bool` | Whether the file is writeable. | | `execute` | `bool` | Whether the file is executable. | ## Examples [Section titled “Examples”](#examples) ### Compute the total file size of the current directory [Section titled “Compute the total file size of the current directory”](#compute-the-total-file-size-of-the-current-directory) ```tql files recurse=true summarize total_size=sum(file_size) ``` ### Find all named pipes in `/tmp` [Section titled “Find all named pipes in /tmp”](#find-all-named-pipes-in-tmp) ```tql files "/tmp", recurse=true, skip_permission_denied=true where type == "fifo" ``` ## See Also [Section titled “See Also”](#see-also) [`load_file`](/reference/operators/load_file), [`processes`](/reference/operators/processes), [`save_file`](/reference/operators/save_file), [`sockets`](/reference/operators/sockets)

# fork

Executes a subpipeline with a copy of the input. ```tql fork { … } ``` ## Description [Section titled “Description”](#description) The `fork` operator executes a subpipeline with a copy of its input, that is: whenever an event arrives, it is sent both to the given pipeline and forwarded at the same time to the next operator. ### `{ … }` [Section titled “{ … }”](#-) The pipeline to execute. Must have a sink. ## Examples [Section titled “Examples”](#examples) ### Publish incoming events while importing them simultaneously [Section titled “Publish incoming events while importing them simultaneously”](#publish-incoming-events-while-importing-them-simultaneously) ```tql fork { publish "imported-events" } import ```

# from

Obtains events from an URI, inferring the source, compression and format. ```tql from uri:string, [loader_args… { … }] from events… ``` ## Description [Section titled “Description”](#description) The `from` operator is an easy way to get data into Tenzir. It will try to infer the connector, compression and format based on the given URI. Alternatively, it can be used to create events from records. ### `uri: string` [Section titled “uri: string”](#uri-string) The URI to load from. ### `loader_args… (optional)` [Section titled “loader\_args… (optional)”](#loader_args-optional) An optional set of arguments passed to the loader. This can be used to e.g. pass credentials to a connector: ```tql from "https://example.org/file.json", headers={Token: "XYZ"} ``` ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) The optional pipeline argument allows for explicitly specifying how `from` decompresses and parses data. By default, the pipeline is inferred based on a set of [rules](#explanation). If inference is not possible, or not sufficient, this argument can be used to control the decompression and parsing. Providing this pipeline disables the inference. [Examples](#load-a-file-with-parser-arguments) ### `events…` [Section titled “events…”](#events) Instead of a URI, you can also provide one or more records, which will be the operators output. This is mostly useful for testing pipelines without loading actual data. ## Explanation [Section titled “Explanation”](#explanation) Loading a resource into tenzir consists of three steps: * [**Loading**](#loading) the raw bytes * [**Decompressing**](#decompressing) (optional) * [**Reading**](#reading) the bytes as structured data The `from` operator tries to infer all three steps from the given URI. ### Loading [Section titled “Loading”](#loading) The connector is inferred based on the URI `scheme://`. See the [URI schemes section](#uri-schemes) for supported schemes. If no scheme is present, the connector attempts to load from the filesystem. ### Decompressing [Section titled “Decompressing”](#decompressing) The compression is inferred from the “file-ending” in the URI. Under the hood, this uses the [`decompress_*` operators](/reference/operators#encode--decode). Supported compressions can be found in the [list of compression extensions](#compression). The decompression step is optional and will only happen if a compression could be inferred. If you know that the source is compressed and the compression cannot be inferred, you can use the [pipeline argument](#---optional) to specify the decompression manually. ### Reading [Section titled “Reading”](#reading) The format to read is, just as the compression, inferred from the file-ending. Supported file formats are the common file endings for our [`read_*` operators](/reference/operators#parsing). If you want to provide additional arguments to the parser, you can use the [pipeline argument](#---optional) to specify the parsing manually. This can be useful, if you e.g. know that the input is `suricata` or `ndjson` instead of just plain `json`. ### The pipeline argument & its relation to the loader [Section titled “The pipeline argument & its relation to the loader”](#the-pipeline-argument--its-relation-to-the-loader) Some loaders, such as the [`load_tcp`](/reference/operators/load_tcp) operator, accept a sub-pipeline directly. If the selected loader accepts a sub-pipeline, the `from` operator will dispatch decompression and parsing into that sub-pipeline. If a an explicit pipeline argument is provided it is forwarded as-is. If the loader does not accept a sub-pipeline, the decompression and parsing steps are simply performed as part of the regular pipeline. #### Example transformation: [Section titled “Example transformation:”](#example-transformation) from operator ```tql from "myfile.json.gz" ``` Effective pipeline ```tql load_file "myfile.json.gz" decompress_gzip read_json ``` #### Example with pipeline argument: [Section titled “Example with pipeline argument:”](#example-with-pipeline-argument) from operator ```tql from "tcp://0.0.0.0:12345", parallel=10 { read_gelf } ``` Effective pipeline ```tql load_tcp "tcp://0.0.0.0:12345", parallel=10 { read_gelf } ``` ## Supported Deductions [Section titled “Supported Deductions”](#supported-deductions) ### URI schemes [Section titled “URI schemes”](#uri-schemes) | Scheme | Operator | Example | | :-------------- | :-------------------------------------------------------------------------- | :----------------------------------------------- | | `abfs`,`abfss` | [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage) | `from "abfs://path/to/file.json"` | | `amqp` | [`load_amqp`](/reference/operators/load_amqp) | `from "amqp://…` | | `elasticsearch` | [`from_opensearch`](/reference/operators/from_opensearch) | `from "elasticsearch://1.2.3.4:9200` | | `file` | [`load_file`](/reference/operators/load_file) | `from "file://path/to/file.json"` | | `fluent-bit` | [`from_fluent_bit`](/reference/operators/from_fluent_bit) | `from "fluent-bit://elasticsearch"` | | `ftp`, `ftps` | [`load_ftp`](/reference/operators/load_ftp) | `from "ftp://example.com/file.json"` | | `gcps` | [`load_google_cloud_pubsub`](/reference/operators/load_google_cloud_pubsub) | `from "gcps://project_id/subscription_id" { … }` | | `gs` | [`load_gcs`](/reference/operators/load_gcs) | `from "gs://bucket/object.json"` | | `http`, `https` | [`load_http`](/reference/operators/load_http) | `from "http://example.com/file.json"` | | `inproc` | [`load_zmq`](/reference/operators/load_zmq) | `from "inproc://127.0.0.1:56789" { read_json }` | | `kafka` | [`load_kafka`](/reference/operators/load_kafka) | `from "kafka://topic" { read_json }` | | `opensearch` | [`from_opensearch`](/reference/operators/from_opensearch) | `from "opensearch://1.2.3.4:9200` | | `s3` | [`load_s3`](/reference/operators/load_s3) | `from "s3://bucket/file.json"` | | `sqs` | [`load_sqs`](/reference/operators/load_sqs) | `from "sqs://my-queue" { read_json }` | | `tcp` | [`load_tcp`](/reference/operators/load_tcp) | `from "tcp://127.0.0.1:13245" { read_json }` | | `udp` | [`load_udp`](/reference/operators/load_udp) | `from "udp://127.0.0.1:56789" { read_json }` | | `zmq` | [`load_zmq`](/reference/operators/load_zmq) | `from "zmq://127.0.0.1:56789" { read_json }` | Please see the respective operator pages for details on the URI’s locator format. ### File extensions [Section titled “File extensions”](#file-extensions) #### Format [Section titled “Format”](#format) The `from` operator can deduce the file format based on these file-endings: | Format | File Endings | Operator | | :------ | :------------------- | :-------------------------------------------------- | | CSV | `.csv` | [`read_csv`](/reference/operators/read_csv) | | Feather | `.feather`, `.arrow` | [`read_feather`](/reference/operators/read_feather) | | JSON | `.json` | [`read_json`](/reference/operators/read_json) | | NDJSON | `.ndjson`, `.jsonl` | [`read_ndjson`](/reference/operators/read_ndjson) | | Parquet | `.parquet` | [`read_parquet`](/reference/operators/read_parquet) | | Pcap | `.pcap` | [`read_pcap`](/reference/operators/read_pcap) | | SSV | `.ssv` | [`read_ssv`](/reference/operators/read_ssv) | | TSV | `.tsv` | [`read_tsv`](/reference/operators/read_tsv) | | YAML | `.yaml` | [`read_yaml`](/reference/operators/read_yaml) | #### Compression [Section titled “Compression”](#compression) The `from` operator can deduce the following compressions based on these file-endings: | Compression | File Endings | | :---------- | :--------------- | | Brotli | `.br`, `.brotli` | | Bzip2 | `.bz2` | | Gzip | `.gz`, `.gzip` | | LZ4 | `.lz4` | | Zstd | `.zst`, `.zstd` | ## Examples [Section titled “Examples”](#examples) ### Load a local file [Section titled “Load a local file”](#load-a-local-file) ```tql from "path/to/my/load/file.csv" ``` ### Load a compressed file [Section titled “Load a compressed file”](#load-a-compressed-file) ```tql from "path/to/my/load/file.json.bz2" ``` ### Load a file with parser arguments [Section titled “Load a file with parser arguments”](#load-a-file-with-parser-arguments) Provide an explicit header to the CSV parser: ```tql from "path/to/my/load/file.csv.bz2" { decompress_brotli // this is now necessary due to the pipeline argument read_csv header="col1,col2,col3" } ``` ### Pick a more suitable parser [Section titled “Pick a more suitable parser”](#pick-a-more-suitable-parser) The file `eve.json` contains Suricata logs, but the `from` operator does not know this. We provide an explicit `read_suricata` instead: ```tql from "path/to/my/load/eve.json" { read_suricata } ``` ### Load from HTTP with a header [Section titled “Load from HTTP with a header”](#load-from-http-with-a-header) ```tql from "https://example.org/file.json", headers={Token: "1234"} ``` ### Create events from records [Section titled “Create events from records”](#create-events-from-records) ```tql from {message: "Value", endpoint: {ip: 127.0.0.1, port: 42}}, {message: "Value", endpoint: {ip: 127.0.0.1, port: 42}, raw: "text"}, {message: "Value", endpoint: null} ``` ```tql { message: "Value", endpoint: { ip: 127.0.0.1, port: 42 } } { message: "Value", endpoint: { ip: 127.0.0.1, port: 42 }, raw: "text" } { message: "Value", endpoint: null } ``` ## See Also [Section titled “See Also”](#see-also) [`from_file`](/reference/operators/from_file), [`to`](/reference/operators/to)

# from_azure_blob_storage

Reads one or multiple files from Azure Blob Storage. ```tql from_azure_blob_storage url:string, [account_key=string, watch=bool, remove=bool, rename=string->string, path_field=field] { … } ``` ## Description [Section titled “Description”](#description) The `from_azure_blob_storage` operator reads files from Azure Blob Storage, with support for glob patterns, automatic format detection, and file monitoring. By default, authentication is handled by the Azure SDK’s credential chain which may read from multiple environment variables, such as: * `AZURE_TENANT_ID` * `AZURE_CLIENT_ID` * `AZURE_CLIENT_SECRET` * `AZURE_AUTHORITY_HOST` * `AZURE_CLIENT_CERTIFICATE_PATH` * `AZURE_FEDERATED_TOKEN_FILE` ### `url: string` [Section titled “url: string”](#url-string) URL identifying the Azure Blob Storage location where data should be read from. The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `container/**/data` matches `container/data`. Supported URI formats: 1. `abfs[s]://<account>.blob.core.windows.net[/<container>[/<path>]]` 2. `abfs[s]://<container>@<account>.dfs.core.windows.net[/<path>]` 3. `abfs[s]://[<account>@]<host>[.<domain>][:<port>][/<container>[/<path>]]` 4. `abfs[s]://[<account>@]<container>[/<path>]` (1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2). ### `account_key = string (optional)` [Section titled “account\_key = string (optional)”](#account_key--string-optional) Account key for authenticating with Azure Blob Storage. ### `watch = bool (optional)` [Section titled “watch = bool (optional)”](#watch--bool-optional) In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s. Defaults to `false`. ### `remove = bool (optional)` [Section titled “remove = bool (optional)”](#remove--bool-optional) Deletes files after they have been read completely. Defaults to `false`. ### `rename = string -> string (optional)` [Section titled “rename = string -> string (optional)”](#rename--string---string-optional) Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path. If the target path already exists, the operator will overwrite the file. The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path. ### `path_field = field (optional)` [Section titled “path\_field = field (optional)”](#path_field--field-optional) This makes the operator insert the path to the file where an event originated from before emitting it. By default, paths will not be inserted into the outgoing events. ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. ## Examples [Section titled “Examples”](#examples) ### Read every JSON file from a container [Section titled “Read every JSON file from a container”](#read-every-json-file-from-a-container) ```tql from_azure_blob_storage "abfs://my-container/data/**.json" ``` ### Read CSV files using account key authentication [Section titled “Read CSV files using account key authentication”](#read-csv-files-using-account-key-authentication) ```tql from_azure_blob_storage "abfs://container/data.csv", account_key="your-account-key" ``` ### Read Suricata EVE JSON logs continuously [Section titled “Read Suricata EVE JSON logs continuously”](#read-suricata-eve-json-logs-continuously) ```tql from_azure_blob_storage "abfs://logs/suricata/**.json", watch=true { read_suricata } ``` ### Process files and move them to an archive container [Section titled “Process files and move them to an archive container”](#process-files-and-move-them-to-an-archive-container) ```tql from_azure_blob_storage "abfs://input/**.json", rename=(path => "/archive/" + path) ``` ### Add source path to events [Section titled “Add source path to events”](#add-source-path-to-events) ```tql from_azure_blob_storage "abfs://data/**.json", path_field=source_file ``` ## See Also [Section titled “See Also”](#see-also) [`from_file`](/reference/operators/from_file), [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage), [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage)

# from_file

Reads one or multiple files from a filesystem. ```tql from_file url:string, [watch=bool, remove=bool, rename=string->string, path_field=field] { … } ``` ## Description [Section titled “Description”](#description) The `from_file` operator reads files from local filesystems or cloud storage, with support for glob patterns, automatic format detection, and file monitoring. ### `url: string` [Section titled “url: string”](#url-string) URL or local filesystem path where data should be read from. The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `foo/**/bar` matches `foo/bar`. The URL can include additional options. For `s3://`, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`. For `gs://`, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`. ### `watch = bool (optional)` [Section titled “watch = bool (optional)”](#watch--bool-optional) In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s. Defaults to `false`. ### `remove = bool (optional)` [Section titled “remove = bool (optional)”](#remove--bool-optional) Deletes files after they have been read completely. Defaults to `false`. ### `rename = string -> string (optional)` [Section titled “rename = string -> string (optional)”](#rename--string---string-optional) Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path. If the target path already exists, the operator will overwrite the file. The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path. ### `path_field = field (optional)` [Section titled “path\_field = field (optional)”](#path_field--field-optional) This makes the operator insert the path to the file where an event originated from before emitting it. By default, paths will not be inserted into the outgoing events. ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. This is using the same logic as [`from`](/reference/operators/from). ## Examples [Section titled “Examples”](#examples) ### Read every `.csv` file from S3 [Section titled “Read every .csv file from S3”](#read-every-csv-file-from-s3) ```tql from_file "s3://my-bucket/**.csv" ``` ### Read every `.json` file in `/data` as Suricata EVE JSON [Section titled “Read every .json file in /data as Suricata EVE JSON”](#read-every-json-file-in-data-as-suricata-eve-json) ```tql from_file "/data/**.json" { read_suricata } ``` ### Read all files from S3 continuously and delete them afterwards [Section titled “Read all files from S3 continuously and delete them afterwards”](#read-all-files-from-s3-continuously-and-delete-them-afterwards) ```tql from_file "s3://my-bucket/**", watch=true, remove=true ``` ### Move files to a directory, preserving filenames [Section titled “Move files to a directory, preserving filenames”](#move-files-to-a-directory-preserving-filenames) ```tql // The trailing slash automatically appends the original filename from_file "/input/*.json", rename=path => "/output/" ``` ## See Also [Section titled “See Also”](#see-also) [`from`](/reference/operators/from), [`load_file`](/reference/operators/load_file)

# from_fluent_bit

Receives events via Fluent Bit. ```tql from_fluent_bit plugin:string, [options=record, fluent_bit_options=record, schema=string, selector=string, schema_only=bool, merge=bool, raw=bool, unflatten=string, tls=bool, cacert=string, certfile=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled “Description”](#description) The `from_fluent_bit` operator acts as a bridge into the [Fluent Bit](https://docs.fluentbit.io) ecosystem, making it possible to acquire events from a Fluent Bit [input plugin](https://docs.fluentbit.io/manual/pipeline/inputs). An invocation of the `fluent-bit` commandline utility ```bash fluent-bit -o plugin -p key1=value1 -p key2=value2 -p… ``` translates to our `from_fluent_bit` operator as follows: ```tql from_fluent_bit "plugin", options={key1: value1, key2: value2, …} ``` ### `plugin: string` [Section titled “plugin: string”](#plugin-string) The name of the Fluent Bit plugin. Run `fluent-bit -h` and look under the **Inputs** section of the help text for available plugin names. The web documentation often comes with an example invocation near the bottom of the page, which also provides a good idea how you could use the operator. ### `options = record (optional)` [Section titled “options = record (optional)”](#options--record-optional) Sets plugin configuration properties. The key-value pairs in this record are equivalent to `-p key=value` for the `fluent-bit` executable. ### `fluent_bit_options = record (optional)` [Section titled “fluent\_bit\_options = record (optional)”](#fluent_bit_options--record-optional) Sets global properties of the Fluent Bit service., e.g., `fluent_bit_options={flush:1, grace:3}`. Consult the list of available [key-value pairs](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file#config_section) to configure Fluent Bit according to your needs. We recommend factoring these options into the plugin-specific `fluent-bit.yaml` so that they are independent of the `fluent-bit` operator arguments. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## URI support & integration with `from` [Section titled “URI support & integration with from”](#uri-support--integration-with-from) The `from_fluent_bit` operator can also be used from the [`from`](/reference/operators/from) operator. For this, the `fluentbit://` scheme can be used. The URI is then translated: ```tql from "fluentbit://plugin" ``` ```tql from_fluent_bit "plugin" ``` ## Examples [Section titled “Examples”](#examples) ### OpenTelemetry [Section titled “OpenTelemetry”](#opentelemetry) Ingest [OpenTelemetry](https://docs.fluentbit.io/manual/pipeline/inputs/slack) logs, metrics, and traces: ```tql from_fluent_bit "opentelemetry" ``` You can then send JSON-encoded log data to a freshly created API endpoint: ```bash curl \ --header "Content-Type: application/json" \ --request POST \ --data '{"resourceLogs":[{"resource":{},"scopeLogs":[{"scope":{},"logRecords":[{"timeUnixNano":"1660296023390371588","body":{"stringValue":"{\"message\":\"dummy\"}"},"traceId":"","spanId":""}]}]}]}' \ http://0.0.0.0:4318/v1/logs ``` ### Splunk [Section titled “Splunk”](#splunk) Handle [Splunk](https://docs.fluentbit.io/manual/pipeline/inputs/splunk) HEC requests: ```tql from_fluent_bit "splunk", options={port: 8088} ```

# from_gcs

Reads one or multiple files from Google Cloud Storage. ```tql from_gcs url:string, [anonymous=bool, watch=bool, remove=bool, rename=string->string, path_field=field] { … } ``` ## Description [Section titled “Description”](#description) The `from_gcs` operator reads files from Google Cloud Storage, with support for glob patterns, automatic format detection, and file monitoring. By default, authentication is handled by Google’s Application Default Credentials (ADC) chain, which may read from multiple sources: * `GOOGLE_APPLICATION_CREDENTIALS` environment variable pointing to a service account key file * User credentials from `gcloud auth application-default login` * Service account attached to the compute instance (Compute Engine, GKE) * Google Cloud SDK credentials ### `url: string` [Section titled “url: string”](#url-string) URL identifying the Google Cloud Storage location where data should be read from. The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `bucket/**/data` matches `bucket/data`. The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist: > For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`. ### `anonymous = bool (optional)` [Section titled “anonymous = bool (optional)”](#anonymous--bool-optional) Use anonymous credentials instead of any configured authentication. This only works for publicly readable buckets and objects. Defaults to `false`. ### `watch = bool (optional)` [Section titled “watch = bool (optional)”](#watch--bool-optional) In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s. Defaults to `false`. ### `remove = bool (optional)` [Section titled “remove = bool (optional)”](#remove--bool-optional) Deletes files after they have been read completely. Defaults to `false`. ### `rename = string -> string (optional)` [Section titled “rename = string -> string (optional)”](#rename--string---string-optional) Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path. If the target path already exists, the operator will overwrite the file. The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path. ### `path_field = field (optional)` [Section titled “path\_field = field (optional)”](#path_field--field-optional) This makes the operator insert the path to the file where an event originated from before emitting it. By default, paths will not be inserted into the outgoing events. ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. ## Examples [Section titled “Examples”](#examples) ### Read every JSON file from a bucket [Section titled “Read every JSON file from a bucket”](#read-every-json-file-from-a-bucket) ```tql from_gcs "gs://my-bucket/data/**.json" ``` ### Read CSV files from a public bucket [Section titled “Read CSV files from a public bucket”](#read-csv-files-from-a-public-bucket) ```tql from_gcs "gs://public-dataset/data.csv", anonymous=true ``` ### Read Zeek logs continuously [Section titled “Read Zeek logs continuously”](#read-zeek-logs-continuously) ```tql from_gcs "gs://logs/zeek/**.log", watch=true { read_zeek_tsv } ``` ### Add source path to events [Section titled “Add source path to events”](#add-source-path-to-events) ```tql from_gcs "gs://data-bucket/**.json", path_field=source_file ``` ### Read Suricata EVE JSON logs with custom parsing [Section titled “Read Suricata EVE JSON logs with custom parsing”](#read-suricata-eve-json-logs-with-custom-parsing) ```tql from_gcs "gs://security-logs/suricata/**.json" { read_suricata } ```

# from_http

Sends and receives HTTP/1.1 requests. ```tql from_http url:string, [method=string, body=record|string|blob, encode=string, headers=record, metadata_field=field, error_field=field, paginate=record->string, paginate_delay=duration, connection_timeout=duration, max_retry_count=int, retry_delay=duration, tls=bool, certfile=string, keyfile=string, password=string { … }] from_http url:string, server=true, [metadata_field=field, responses=record, max_request_size=int, tls=bool, certfile=string, keyfile=string, password=string { … }] ``` ## Description [Section titled “Description”](#description) The `from_http` operator issues HTTP requests or spins up an HTTP/1.1 server on a given address and forwards received requests as events. ### `url: string` [Section titled “url: string”](#url-string) URL to listen on or to connect to. Must have the form `<host>:<port>` when `server=true`. ### `method = string (optional)` [Section titled “method = string (optional)”](#method--string-optional) One of the following HTTP methods to use when using the client: * `get` * `head` * `post` * `put` * `del` * `connect` * `options` * `trace` Defaults to `get`, or `post` if `body` is specified. ### `body = blob|record|string (optional)` [Section titled “body = blob|record|string (optional)”](#body--blobrecordstring-optional) Body to send with the HTTP request. If the value is a `record`, then the body is encoded according to the `encode` option and an appropriate `Content-Type` is set for the request. ### `encode = string (optional)` [Section titled “encode = string (optional)”](#encode--string-optional) Specifies how to encode `record` bodies. Supported values: * `json` * `form` Defaults to `json`. ### `headers = record (optional)` [Section titled “headers = record (optional)”](#headers--record-optional) Record of headers to send with the request. ### `metadata_field = field (optional)` [Section titled “metadata\_field = field (optional)”](#metadata_field--field-optional) Field to insert metadata into when using the parsing pipeline. The response metadata (when using the client mode) has the following schema: | Field | Type | Description | | :-------- | :------- | :------------------------------------ | | `code` | `uint64` | The HTTP status code of the response. | | `headers` | `record` | The response headers. | The request metadata (when using the server mode) has the following schema: | Field | Type | Description | | :--------- | :------- | :----------------------------------- | | `headers` | `record` | The request headers. | | `query` | `record` | The query parameters of the request. | | `path` | `string` | The path requested. | | `fragment` | `string` | The URI fragment of the request. | | `method` | `string` | The HTTP method of the request. | | `version` | `string` | The HTTP version of the request. | ### `error_field = field (optional)` [Section titled “error\_field = field (optional)”](#error_field--field-optional) Field to insert the response body for HTTP error responses (status codes not in the 2xx or 3xx range). When set, any HTTP response with a status code outside the 200–399 range will have its body stored in this field as a `blob`. Otherwise, error responses, alongside the original event, are skipped and an error is emitted. ### `paginate = record -> string (optional)` [Section titled “paginate = record -> string (optional)”](#paginate--record---string-optional) A lambda expression to evaluate against the result of the request (optionally parsed by the given pipeline). If the expression evaluation is successful and non-null, the resulting string is used as the URL for a new GET request with the same headers. ### `paginate_delay = duration (optional)` [Section titled “paginate\_delay = duration (optional)”](#paginate_delay--duration-optional) The duration to wait between consecutive pagination requests. Defaults to `0s`. ### `connection_timeout = duration (optional)` [Section titled “connection\_timeout = duration (optional)”](#connection_timeout--duration-optional) Timeout for the connection. Defaults to `5s`. ### `max_retry_count = int (optional)` [Section titled “max\_retry\_count = int (optional)”](#max_retry_count--int-optional) The maximum times to retry a failed request. Every request has its own retry count. Defaults to `0`. ### `retry_delay = duration (optional)` [Section titled “retry\_delay = duration (optional)”](#retry_delay--duration-optional) The duration to wait between each retry. Defaults to `1s`. ### `server = bool (optional)` [Section titled “server = bool (optional)”](#server--bool-optional) Whether to spin up an HTTP server or act as an HTTP client. Defaults to `false`, i.e., the HTTP client. ### `responses = record (optional)` [Section titled “responses = record (optional)”](#responses--record-optional) Specify custom responses for endpoints on the server. For example, ```tql responses = { "/resource/create": { code: 200, content_type: "text/html", body: "Created!" }, "/resource/delete": { code: 401, content_type: "text/html", body: "Unauthorized!" } } ``` creates two special routes on the server with different responses. Requests to an unspecified endpoint are responded with HTTP Status `200 OK`. ### `max_request_size = int (optional)` [Section titled “max\_request\_size = int (optional)”](#max_request_size--int-optional) The maximum size of an incoming request to accept. Defaults to `10MiB`. ### `tls = bool (optional)` [Section titled “tls = bool (optional)”](#tls--bool-optional) Enables TLS. Defaults to `false`. ### `certfile = string (optional)` [Section titled “certfile = string (optional)”](#certfile--string-optional) Path to the client certificate. Required for server if `tls` is `true`. ### `keyfile = string (optional)` [Section titled “keyfile = string (optional)”](#keyfile--string-optional) Path to the key for the client certificate. Required for server if `tls` is `true`. ### `password = string (optional)` [Section titled “password = string (optional)”](#password--string-optional) Password for keyfile. ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) A pipeline that receives the response body as bytes, allowing parsing per request. This is especially useful in scenarios where the response body can be parsed into multiple events. If not provided, the operator will attempt to infer the parsing operator from the `Content-Type` header. Should this inference fail (e.g., unsupported or missing `Content-Type`), the operator raises an error. ## Examples [Section titled “Examples”](#examples) ### Make a GET request [Section titled “Make a GET request”](#make-a-get-request) Make a request to [urlscan.io](https://urlscan.io/docs/api#search) to search for scans for `tenzir.com` and get the first result. ```tql from_http "https://urlscan.io/api/v1/search?q=tenzir.com" unroll results head 1 ``` ```tql { results: { submitter: { ... }, task: { ... }, stats: { ... }, page: { ... }, _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30", _score: null, sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ], result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/", screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png", }, total: 9, took: 296, has_more: false, } ``` ### Paginated API Requests [Section titled “Paginated API Requests”](#paginated-api-requests) Use the `paginate` parameter to handle paginated APIs: ```tql from_http "https://api.example.com/data", paginate=(x => x.next_url?) ``` This sends a GET request to the initial URL and evaluates the `x.next_url` field in the response to determine the next URL for subsequent requests. ### Retry Failed Requests [Section titled “Retry Failed Requests”](#retry-failed-requests) Configure retries for failed requests: ```tql from_http "https://api.example.com/data", max_retry_count=3, retry_delay=2s ``` This tries up to 3 times, waiting 2 seconds between each retry. ### Listen on port 8080 [Section titled “Listen on port 8080”](#listen-on-port-8080) Spin up a server with: ```tql from_http "0.0.0.0:8080", server=true, metadata_field=metadata ``` Send a request to the HTTP endpoint via `curl`: ```sh echo '{"key": "value"}' | gzip | curl localhost:8080 --data-binary @- -H 'Content-Encoding: gzip' -H 'Content-Type: application/json' ``` Observe the request in the Tenzir pipeline, parsed and decompressed: ```tql { key: "value", metadata: { headers: { Host: "localhost:8080", "User-Agent": "curl/8.13.0", Accept: "*/*", "Content-Encoding": "gzip", "Content-Length": "37", "Content-Type": "application/json", }, path: "/", method: "post", version: "HTTP/1.1", }, } ``` ## See Also [Section titled “See Also”](#see-also) [`http`](/reference/operators/http), [`serve`](/reference/operators/serve)

# from_opensearch

Receives events via Opensearch Bulk API. ```tql from_opensearch [url:string, keep_actions=bool, max_request_size=int, tls=bool, certfile=string, keyfile=string, password=string] ``` ## Description [Section titled “Description”](#description) The `from_opensearch` operator emulates simple situations for the [Opensearch Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/). ### `url: string (optional)` [Section titled “url: string (optional)”](#url-string-optional) URL to listen on. Must have the form `host[:port]`. Defaults to `"0.0.0.0:9200"`. ### `keep_actions = bool (optional)` [Section titled “keep\_actions = bool (optional)”](#keep_actions--bool-optional) Whether to keep the command objects such as `{"create": ...}`. Defaults to `false`. ### `max_request_size = int (optional)` [Section titled “max\_request\_size = int (optional)”](#max_request_size--int-optional) The maximum size of an incoming request to accept. Defaults to `10Mib`. ### `tls = bool (optional)` [Section titled “tls = bool (optional)”](#tls--bool-optional) Enables TLS. Defaults to `false`. ### `certfile = string (optional)` [Section titled “certfile = string (optional)”](#certfile--string-optional) Path to the client certificate. Required if `tls` is `true`. ### `keyfile = string (optional)` [Section titled “keyfile = string (optional)”](#keyfile--string-optional) Path to the key for the client certificate. Required if `tls` is `true`. ### `password = string (optional)` [Section titled “password = string (optional)”](#password--string-optional) Password for keyfile. ## Examples [Section titled “Examples”](#examples) ### Listen on port 8080 on an interface with IP 1.2.3.4 [Section titled “Listen on port 8080 on an interface with IP 1.2.3.4”](#listen-on-port-8080-on-an-interface-with-ip-1234) ```tql from_opensearch "1.2.3.4:8080" ``` ### Listen with TLS [Section titled “Listen with TLS”](#listen-with-tls) ```tql from_opensearch tls=true, certfile="server.crt", keyfile="private.key" ``` ## See also [Section titled “See also”](#see-also) [`to_opensearch`](/reference/operators/to_opensearch)

# from_s3

Reads one or multiple files from Amazon S3. ```tql from_s3 url:string, [anonymous=bool, access_key=string, secret_key=string, session_token=string, role=string, external_id=string, watch=bool, remove=bool, rename=string->string, path_field=field] { … } ``` ## Description [Section titled “Description”](#description) The `from_s3` operator reads files from Amazon S3, with support for glob patterns, automatic format detection, and file monitoring. By default, authentication is handled by AWS’s default credentials provider chain, which may read from multiple environment variables and credential files: * `~/.aws/credentials` and `~/.aws/config` * `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` * `AWS_SESSION_TOKEN` * EC2 instance metadata service * ECS container credentials ### `url: string` [Section titled “url: string”](#url-string) URL identifying the S3 location where data should be read from. The characters `*` and `**` have a special meaning. `*` matches everything except `/`. `**` matches everything including `/`. The sequence `/**/` can also match nothing. For example, `bucket/**/data` matches `bucket/data`. Supported URI format: `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)` Options can be appended to the path as query parameters: * `region`: AWS region (e.g., `us-east-1`) * `scheme`: Connection scheme (`http` or `https`) * `endpoint_override`: Custom S3-compatible endpoint * `allow_bucket_creation`: Allow creating buckets if they don’t exist * `allow_bucket_deletion`: Allow deleting buckets ### `anonymous = bool (optional)` [Section titled “anonymous = bool (optional)”](#anonymous--bool-optional) Use anonymous credentials instead of any configured authentication. Defaults to `false`. ### `access_key = string (optional)` [Section titled “access\_key = string (optional)”](#access_key--string-optional) AWS access key ID for authentication. ### `secret_key = string (optional)` [Section titled “secret\_key = string (optional)”](#secret_key--string-optional) AWS secret access key for authentication. Required if `access_key` is provided. ### `session_token = string (optional)` [Section titled “session\_token = string (optional)”](#session_token--string-optional) AWS session token for temporary credentials. ### `role = string (optional)` [Section titled “role = string (optional)”](#role--string-optional) IAM role to assume when accessing S3. ### `external_id = string (optional)` [Section titled “external\_id = string (optional)”](#external_id--string-optional) External ID to use when assuming the specified `role`. ### `watch = bool (optional)` [Section titled “watch = bool (optional)”](#watch--bool-optional) In addition to processing all existing files, this option keeps the operator running, watching for new files that also match the given URL. Currently, this scans the filesystem up to every 10s. Defaults to `false`. ### `remove = bool (optional)` [Section titled “remove = bool (optional)”](#remove--bool-optional) Deletes files after they have been read completely. Defaults to `false`. ### `rename = string -> string (optional)` [Section titled “rename = string -> string (optional)”](#rename--string---string-optional) Renames files after they have been read completely. The lambda function receives the original path as an argument and must return the new path. If the target path already exists, the operator will overwrite the file. The operator automatically creates any intermediate directories required for the target path. If the target path ends with a trailing slash (`/`), the original filename will be automatically appended to create the final path. ### `path_field = field (optional)` [Section titled “path\_field = field (optional)”](#path_field--field-optional) This makes the operator insert the path to the file where an event originated from before emitting it. By default, paths will not be inserted into the outgoing events. ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) Pipeline to use for parsing the file. By default, this pipeline is derived from the path of the file, and will not only handle parsing but also decompression if applicable. ## Examples [Section titled “Examples”](#examples) ### Read every JSON file from a bucket [Section titled “Read every JSON file from a bucket”](#read-every-json-file-from-a-bucket) ```tql from_s3 "s3://my-bucket/data/**.json" ``` ### Read CSV files using explicit credentials [Section titled “Read CSV files using explicit credentials”](#read-csv-files-using-explicit-credentials) ```tql from_s3 "s3://my-bucket/data.csv", access_key=secret("AWS_ACCESS_KEY"), secret_key=secret("AWS_SECRET_KEY") ``` ### Read from S3-compatible service with custom endpoint [Section titled “Read from S3-compatible service with custom endpoint”](#read-from-s3-compatible-service-with-custom-endpoint) ```tql from_s3 "s3://my-bucket/data/**.json?endpoint_override=minio.example.com:9000&scheme=http" ``` ### Read files continuously and assume IAM role [Section titled “Read files continuously and assume IAM role”](#read-files-continuously-and-assume-iam-role) ```tql from_s3 "s3://logs/application/**.json", watch=true, role="arn:aws:iam::123456789012:role/LogReaderRole" ``` ### Process files and move them to an archive bucket [Section titled “Process files and move them to an archive bucket”](#process-files-and-move-them-to-an-archive-bucket) ```tql from_s3 "s3://input-bucket/**.json", rename=(path => "archive/" + path) ``` ### Add source path to events [Section titled “Add source path to events”](#add-source-path-to-events) ```tql from_s3 "s3://data-bucket/**.json", path_field=source_file ``` ### Read Zeek logs with anonymous access [Section titled “Read Zeek logs with anonymous access”](#read-zeek-logs-with-anonymous-access) ```tql from_s3 "s3://public-bucket/zeek/**.log", anonymous=true { read_zeek_tsv } ``` ## See Also [Section titled “See Also”](#see-also) [`from_file`](/reference/operators/from_file), [`load_s3`](/reference/operators/load_s3), [`save_s3`](/reference/operators/save_s3)

# from_udp

Receives UDP datagrams and outputs structured events. ```tql from_udp endpoint:string, [resolve_hostnames=bool], [binary=bool] ``` ## Description [Section titled “Description”](#description) Listens for UDP datagrams on the specified endpoint and outputs each datagram as a structured event containing the data and peer information. Unlike [`load_udp`](/reference/operators/load_udp), which outputs raw bytes, `from_udp` produces structured events with metadata about the sender. ### `endpoint: string` [Section titled “endpoint: string”](#endpoint-string) The address to listen on. Must be of the format: `[udp://]host:port`. Use `0.0.0.0` as the host to accept datagrams on all interfaces. The [`nics`](/reference/operators/nics) operator lists all available interfaces. ### `resolve_hostnames = bool (optional)` [Section titled “resolve\_hostnames = bool (optional)”](#resolve_hostnames--bool-optional) Perform DNS lookups to resolve hostnames for sender IP addresses. Defaults to `false` since DNS lookups can be slow and may impact performance when receiving many datagrams. ### `binary = bool (optional)` [Section titled “binary = bool (optional)”](#binary--bool-optional) Output datagram data as binary (`blob`) instead of text (`string`). Defaults to `false`. When `false`, the data field contains a UTF-8 string. When `true`, the data field contains raw bytes as a blob. ## Output Schema [Section titled “Output Schema”](#output-schema) Each UDP datagram produces one event with the following structure: ```json { "data": <string|blob>, // string by default, blob when binary=true "peer": { "ip": <ip>, "port": <uint64>, "hostname": <string> // Does not exist when `resolve_hostnames=false` } } ``` ## Examples [Section titled “Examples”](#examples) ### Receive UDP datagrams with sender information [Section titled “Receive UDP datagrams with sender information”](#receive-udp-datagrams-with-sender-information) ```tql from_udp "0.0.0.0:1234" ``` This might output events like: ```json { "data": "Hello World", "peer": { "ip": "192.168.1.10", "port": 5678 } } ``` ### Parse JSON data from UDP datagrams [Section titled “Parse JSON data from UDP datagrams”](#parse-json-data-from-udp-datagrams) ```tql from_udp "127.0.0.1:8080" select data = data.parse_json() ``` ### Filter by sender and decode data [Section titled “Filter by sender and decode data”](#filter-by-sender-and-decode-data) ```tql from_udp "0.0.0.0:9999" where peer.ip == 192.168.1.100 select data ``` ## See Also [Section titled “See Also”](#see-also) [`load_udp`](/reference/operators/load_udp), [`save_udp`](/reference/operators/save_udp)

# from_velociraptor

Submits VQL to a Velociraptor server and returns the response as events. ```tql from_velociraptor [request_name=string, org_id=string, max_rows=int, subscribe=string, query=string, max_wait=duration, profile=string] ``` ## Description [Section titled “Description”](#description) The `from_velociraptor` input operator provides a request-response interface to a [Velociraptor](https://docs.velociraptor.app) server: ![Velociraptor](/_astro/velociraptor.excalidraw.DqyMhvp8_19DKCs.svg) The pipeline operator is the client and it establishes a connection to a Velociraptor server. The client request contains a query written in the [Velociraptor Query Language (VQL)](https://docs.velociraptor.app/docs/vql), a SQL-inspired language with a `SELECT .. FROM .. WHERE` structure. You can either send a raw VQL query via `from_velociraptor query "<vql>"` to a server and processs the response, or hook into a continuous feed of artifacts via `from_velociraptor subscribe <artifact>`. Whenever a hunt runs that contains this artifact, the server will forward it to the pipeline and emit the artifact payload in the response field `HuntResults`. All Velociraptor client-to-server communication is mutually authenticated and encrypted via TLS certificates. This means you must provide client-side certificate, which you can generate as follows. (Velociraptor ships as a static binary that we refer to as `velociraptor-binary` here.) 1. Create a server configuration `server.yaml`: ```bash velociraptor-binary config generate > server.yaml ``` 2. Create an API client: ```bash velociraptor-binary -c server.yaml config api_client name tenzir client.yaml ``` Copy the generated `client.yaml` to your Tenzir plugin configuration directory as `velociraptor.yaml` so that the operator can find it: ```bash cp client.yaml /etc/tenzir/plugin/velociraptor.yaml ``` 3. Run the frontend with the server configuration: ```bash velociraptor-binary -c server.yaml frontend ``` Now you are ready to run VQL queries! ### `request_name = string (optional)` [Section titled “request\_name = string (optional)”](#request_name--string-optional) An identifier for the request to the Velociraptor server. Defaults to a randoum UUID. ### `org_id = string (optional)` [Section titled “org\_id = string (optional)”](#org_id--string-optional) The ID of the Velociraptor organization. Defaults to `"root"`. ### `query = string (optional)` [Section titled “query = string (optional)”](#query--string-optional) The [VQL](https://docs.velociraptor.app/docs/vql) query string. ### `max_rows = int (optional)` [Section titled “max\_rows = int (optional)”](#max_rows--int-optional) The maxium number of rows to return in a the stream gRPC messages returned by the server. Defaults to `1000`. ### `subscribe = string (optional)` [Section titled “subscribe = string (optional)”](#subscribe--string-optional) Subscribes to a flow artifact. This option generates a larger VQL expression under the hood that creates one event per flow and artifact. The response contains a field `HuntResult` that contains the result of the hunt. ### `max_wait = duration (optional)` [Section titled “max\_wait = duration (optional)”](#max_wait--duration-optional) Controls how long to wait before releasing a partial result set. Defaults to `1s`. ### `profile = string (optional)` [Section titled “profile = string (optional)”](#profile--string-optional) Specifies the configuration profile for the Velociraptor instance. This enables connecting to multiple Velociraptor instances from the same Tenzir node. To use profiles, edit your `velociraptor.yaml` configuration like this, where `<config>` refers to the contents of the configuration file created by Velociraptor, and `<profile>` to the desired profile name. ```yaml --- title: before --- <config> --- title: after --- profiles: <profile>: <config> ``` If profiles are defined, the operator defaults to the first profile. ## Examples [Section titled “Examples”](#examples) ### Show all processes [Section titled “Show all processes”](#show-all-processes) ```tql from_velociraptor query="select * from pslist()" ``` ### Subscribe to a hunt flow containing the `Windows` artifact [Section titled “Subscribe to a hunt flow containing the Windows artifact”](#subscribe-to-a-hunt-flow-containing-the-windows-artifact) ```tql from_velociraptor subscribe="Windows" ```

# head

Limits the input to the first `n` events. ```tql head [n:int] ``` ## Description [Section titled “Description”](#description) Forwards the first `n` events and discards the rest. `head n` is a shorthand notation for [`slice end=n`](/reference/operators/slice). ### `n: int (optional)` [Section titled “n: int (optional)”](#n-int-optional) The number of events to keep. Defaults to `10`. ## Examples [Section titled “Examples”](#examples) ### Get the first 10 events [Section titled “Get the first 10 events”](#get-the-first-10-events) ```tql head ``` ### Get the first 5 events [Section titled “Get the first 5 events”](#get-the-first-5-events) ```tql head 5 ``` ## See Also [Section titled “See Also”](#see-also) [`slice`](/reference/operators/slice), [`tail`](/reference/operators/tail)

# http

Sends HTTP/1.1 requests and forwards the response. ```tql http url:string, [method=string, body=record|string|blob, encode=string, headers=record, response_field=field, metadata_field=field, error_field=field, paginate=record->string, paginate_delay=duration, parallel=int, tls=bool, certfile=string, keyfile=string, password=string, connection_timeout=duration, max_retry_count=int, retry_delay=duration { … }] ``` ## Description [Section titled “Description”](#description) The `http` operator issues HTTP/1.1 requests and forwards received responses as events. ### `url: string` [Section titled “url: string”](#url-string) URL to connect to. ### `method = string (optional)` [Section titled “method = string (optional)”](#method--string-optional) One of the following HTTP methods to use when using the client: * `get` * `head` * `post` * `put` * `del` * `connect` * `options` * `trace` Defaults to `get`, or `post` if `body` is specified. ### `body = blob|record|string (optional)` [Section titled “body = blob|record|string (optional)”](#body--blobrecordstring-optional) Body to send with the HTTP request. If the value is a `record`, then the body is encoded according to the `encode` option and an appropriate `Content-Type` is set for the request. ### `encode = string (optional)` [Section titled “encode = string (optional)”](#encode--string-optional) Specifies how to encode `record` bodies. Supported values: * `json` * `form` Defaults to `json`. ### `headers = record (optional)` [Section titled “headers = record (optional)”](#headers--record-optional) Record of headers to send with the request. ### `response_field = field (optional)` [Section titled “response\_field = field (optional)”](#response_field--field-optional) Field to insert the response into. Defaults to `this`. ### `metadata_field = field (optional)` [Section titled “metadata\_field = field (optional)”](#metadata_field--field-optional) Field to insert metadata into when using the parsing pipeline. The metadata has the following schema: | Field | Type | Description | | :-------- | :------- | :------------------------------------ | | `code` | `uint64` | The HTTP status code of the response. | | `headers` | `record` | The response headers. | ### `error_field = field (optional)` [Section titled “error\_field = field (optional)”](#error_field--field-optional) Field to insert the response body for HTTP error responses (status codes not in the 2xx or 3xx range). When set, any HTTP response with a status code outside the 200–399 range will have its body stored in this field as a `blob`. Otherwise, error responses, alongside the original event, are skipped and a warning is emitted. ### `paginate = record -> string (optional)` [Section titled “paginate = record -> string (optional)”](#paginate--record---string-optional) A lambda expression to evaluate against the result of the request (optionally parsed by the given pipeline). If the expression evaluation is successful and non-null, the resulting string is used as the URL for a new GET request with the same headers. ### `paginate_delay = duration (optional)` [Section titled “paginate\_delay = duration (optional)”](#paginate_delay--duration-optional) The duration to wait between consecutive pagination requests. Defaults to `0s`. ### `parallel = int (optional)` [Section titled “parallel = int (optional)”](#parallel--int-optional) Maximum amount of requests that can be in progress at any time. Defaults to `1`. ### `tls = bool (optional)` [Section titled “tls = bool (optional)”](#tls--bool-optional) Enables TLS. ### `certfile = string (optional)` [Section titled “certfile = string (optional)”](#certfile--string-optional) Path to the client certificate. ### `keyfile = string (optional)` [Section titled “keyfile = string (optional)”](#keyfile--string-optional) Path to the key for the client certificate. ### `password = string (optional)` [Section titled “password = string (optional)”](#password--string-optional) Password file for keyfile. ### `connection_timeout = duration (optional)` [Section titled “connection\_timeout = duration (optional)”](#connection_timeout--duration-optional) Timeout for the connection. Defaults to `5s`. ### `max_retry_count = int (optional)` [Section titled “max\_retry\_count = int (optional)”](#max_retry_count--int-optional) The maximum times to retry a failed request. Every request has its own retry count. Defaults to `0`. ### `retry_delay = duration (optional)` [Section titled “retry\_delay = duration (optional)”](#retry_delay--duration-optional) The duration to wait between each retry. Defaults to `1s`. ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) A pipeline that receives the response body as bytes, allowing parsing per request. This is especially useful in scenarios where the response body can be parsed into multiple events. If not provided, the operator will attempt to infer the parsing operator from the `Content-Type` header. Should this inference fail (e.g., unsupported or missing `Content-Type`), the operator raises a warning and skips the request. ## Examples [Section titled “Examples”](#examples) ### Make a GET request [Section titled “Make a GET request”](#make-a-get-request) Here we make a request to [urlscan.io](https://urlscan.io/docs/api#search) to search for scans for `tenzir.com` and get the first result. ```tql from {} http "https://urlscan.io/api/v1/search?q=tenzir.com" unroll results head 1 ``` ```tql { results: { submitter: { ... }, task: { ... }, stats: { ... }, page: { ... }, _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30", _score: null, sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ], result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/", screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png", }, total: 9, took: 296, has_more: false, } ``` ### Keeping input context [Section titled “Keeping input context”](#keeping-input-context) Frequently, the purpose of making real-time requests in a pipeline is to enrich the incoming data with additional context. In these cases, we want to keep the original event around. This can be done simply by specifying the `response_field` and `metadata_field` options as appropriate. E.g. in the above example, let’s assume we had some initial context that we want to keep around: ```tql from { ctx: {severity: "HIGH"}, domain: "tenzir.com", ip: 0.0.0.0 } http "https://urlscan.io/api/v1/search?q=" + domain, response_field=scan scan.results = scan.results[0] ``` ```tql { ctx: { severity: "HIGH", }, domain: "tenzir.com", ip: 0.0.0.0, scan: { results: { submitter: { ... }, task: { ... }, stats: { ... }, page: { ... }, _id: "0196edb1-521e-761f-9d62-1ca4cfad5b30", _score: null, sort: [ "1747744570133", "\"0196edb1-521e-761f-9d62-1ca4cfad5b30\"" ], result: "https://urlscan.io/api/v1/result/0196edb1-521e-761f-9d62-1ca4cfad5b30/", screenshot: "https://urlscan.io/screenshots/0196edb1-521e-761f-9d62-1ca4cfad5b30.png", }, total: 9, took: 88, has_more: false, }, } ``` ### Paginate an API [Section titled “Paginate an API”](#paginate-an-api) We can utilize the `sort` and `has_more` fields to get more pages from the API. ```tql let $URL = "https://urlscan.io/api/v1/search?q=example.com" from {} http $URL, paginate=(x => $URL + "&search_after=" + results.last().sort.first() + "," + results.last().sort.last().slice(begin=1, end=-1) if has_more?) head 10 ``` Here we construct the next url for pagination by extracting values from the responses. The query parameter `search_after` expects the two values from the `sort` key in the response to be joined with a `,`. Thus forming a URL like `https://urlscan.io/api/v1/search?q=example.com&search_after=1747796723608,0196f0cd-6fda-761a-81a6-ae1b18914e61`. The `if has_more?` ensures pagination only continues till we have a `has_more` field that is `true`. Additionally, we limit the maximum pages by a simple `head 10`. ## See Also [Section titled “See Also”](#see-also) [`from_http`](/reference/operators/from_http)

# import

Imports events into a Tenzir node. ```tql import ``` ## Description [Section titled “Description”](#description) The `import` operator persists events in a Tenzir node. This operator is the dual to [`export`](/reference/operators/export). ## Examples [Section titled “Examples”](#examples) ### Import Zeek connection logs in TSV format [Section titled “Import Zeek connection logs in TSV format”](#import-zeek-connection-logs-in-tsv-format) ```tql load_file "conn.log" read_zeek_tsv import ``` ## See Also [Section titled “See Also”](#see-also) [`export`](/reference/operators/export), [`publish`](/reference/operators/publish)

# load_amqp

Loads a byte stream via AMQP messages. ```tql load_amqp [url:str, channel=int, exchange=str, routing_key=str, queue=str, options=record, passive=bool, durable=bool, exclusive=bool, no_auto_delete=bool, no_local=bool, ack=bool] ``` ## Description [Section titled “Description”](#description) The `load_amqp` operator is an [AMQP](https://www.amqp.org/) 0-9-1 client to receive messages from a queue. ### `url: str (optional)` [Section titled “url: str (optional)”](#url-str-optional) A URL that specifies the AMQP server. The URL must have the following format: ```plaintext amqp://[USERNAME[:PASSWORD]@]HOSTNAME[:PORT]/[VHOST] ``` When the URL is present, it will overwrite the corresponding values of the configuration options. ### `channel = int (optional)` [Section titled “channel = int (optional)”](#channel--int-optional) The channel number to use. Defaults to `1`. ### `exchange = str (optional)` [Section titled “exchange = str (optional)”](#exchange--str-optional) The exchange to interact with. Defaults to `"amq.direct"`. ### `routing_key = str (optional)` [Section titled “routing\_key = str (optional)”](#routing_key--str-optional) The name of the routing key to bind a queue to an exchange. Defaults to the empty string. ### `options = record (optional)` [Section titled “options = record (optional)”](#options--record-optional) An option record for RabbitMQ , e.g., `{max_channels: 42, frame_size: 1024, sasl_method: "external"}`. Available options are: ```yaml hostname: 127.0.0.1 port: 5672 ssl: false vhost: / max_channels: 2047 frame_size: 131072 heartbeat: 0 sasl_method: plain username: guest password: guest ``` ### `queue = str (optional)` [Section titled “queue = str (optional)”](#queue--str-optional) The name of the queue to declare and then bind to. Defaults to the empty string, resulting in auto-generated queue names, such as `"amq.gen-XNTLF0FwabIn9FFKKtQHzg"`. ### `passive = bool (optional)` [Section titled “passive = bool (optional)”](#passive--bool-optional) If `true`, the server will reply with OK if an exchange already exists with the same name, and raise an error otherwise. Defaults to `false`. ### `durable = bool (optional)` [Section titled “durable = bool (optional)”](#durable--bool-optional) If `true` when creating a new exchange, the exchange will be marked as durable. Durable exchanges remain active when a server restarts. Non-durable exchanges (transient exchanges) are purged if/when a server restarts. Defaults to `false`. ### `exclusive = bool (optional)` [Section titled “exclusive = bool (optional)”](#exclusive--bool-optional) If `true`, marks the queue as exclusive. Exclusive queues may only be accessed by the current connection, and are deleted when that connection closes. Passive declaration of an exclusive queue by other connections are not allowed. Defaults to `false`. ### `no_auto_delete = bool (optional)` [Section titled “no\_auto\_delete = bool (optional)”](#no_auto_delete--bool-optional) If `true`, the exchange will *not* be deleted when all queues have finished using it. Defaults to `false`. ### `no_local = bool (optional)` [Section titled “no\_local = bool (optional)”](#no_local--bool-optional) If `true`, the server will not send messages to the connection that published them. Defaults to `false`. ### `ack = bool (optional)` [Section titled “ack = bool (optional)”](#ack--bool-optional) If `true`, the server expects acknowledgements for messages. Otherwise, when a message is delivered to the client the server assumes the delivery will succeed and immediately dequeues it. This functionality may decrease performance, while improving reliability. Without this flag, messages can get lost if a client dies before they are delivered to the application. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Consume a message from a specified AMQP queue [Section titled “Consume a message from a specified AMQP queue”](#consume-a-message-from-a-specified-amqp-queue) ```tql load_amqp "amqp://admin:pass@0.0.0.1:5672/vhost", queue="foo" read_json ``` ## See Also [Section titled “See Also”](#see-also) [`save_amqp`](/reference/operators/save_amqp)

# load_azure_blob_storage

Loads bytes from Azure Blob Storage. ```tql load_azure_blob_storage uri:string, [account_key=string] ``` ## Description [Section titled “Description”](#description) The `load_azure_blob_storage` operator loads bytes from an Azure Blob Storage. By default, authentication is handled by the Azure SDK’s credential chain which may read from multiple environment variables, such as: * `AZURE_TENANT_ID` * `AZURE_CLIENT_ID` * `AZURE_CLIENT_SECRET` * `AZURE_AUTHORITY_HOST` * `AZURE_CLIENT_CERTIFICATE_PATH` * `AZURE_FEDERATED_TOKEN_FILE` ### `uri: string` [Section titled “uri: string”](#uri-string) A URI identifying the blob to load from. Supported URI formats: 1. `abfs[s]://[:<password>@]<account>.blob.core.windows.net[/<container>[/<path>]]` 2. `abfs[s]://<container>[:<password>]@<account>.dfs.core.windows.net[/path]` 3. `abfs[s]://[<account[:<password>]@]<host[.domain]>[<:port>][/<container>[/path]]` 4. `abfs[s]://[<account[:<password>]@]<container>[/path]` (1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs 1, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2). ### `account_key = string (optional)` [Section titled “account\_key = string (optional)”](#account_key--string-optional) Account key to authenticate with. ## Examples [Section titled “Examples”](#examples) ### Write JSON [Section titled “Write JSON”](#write-json) Read JSON from a blob `obj.json` in the blob container `container`, using the `tenzirdev` user: ```tql load_azure_blob_storage "abfss://tenzirdev@container/obj.json" read_json ``` ## See Also [Section titled “See Also”](#see-also) [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage)

# load_balance

Routes the data to one of multiple subpipelines. ```tql load_balance over:list { … } ``` ## Description [Section titled “Description”](#description) The `load_balance` operator spawns a nested pipeline for each element in the given list. Incoming events are distributed to exactly one of the nested pipelines. This operator may reorder the event stream. ### `over: list` [Section titled “over: list”](#over-list) This must be a `$`-variable, previously declared with `let`. For example, to load balance over a list of ports, use `let $cfg = [8080, 8081, 8082]` followed by `load_balance $cfg { … }`. ### `{ … }` [Section titled “{ … }”](#-) The nested pipeline to spawn. This pipeline can use the same variable as passed to `over`, which will be resolved to one of the list items. The following example spawns three nested pipelines, where `$port` is bound to `8080`, `8081` and `8082`, respectively. ```tql let $cfg = [8080, 8081, 8082] load_balance $cfg { let $port = $cfg // … continue here } ``` The given subpipeline must end with a sink. This limitation might be removed in future versions. ## Examples [Section titled “Examples”](#examples) ### Route data to multiple TCP ports [Section titled “Route data to multiple TCP ports”](#route-data-to-multiple-tcp-ports) ```tql let $cfg = ["192.168.0.30:8080", "192.168.0.30:8081"] subscribe "input" load_balance $cfg { write_json save_tcp $cfg } ``` ### Route data to multiple Splunk endpoints [Section titled “Route data to multiple Splunk endpoints”](#route-data-to-multiple-splunk-endpoints) ```tql let $cfg = [{ ip: 192.168.0.30, token: "example-token-1234", }, { ip: 192.168.0.31, token: "example-token-5678", }] subscribe "input" load_balance $cfg { let $endpoint = string($cfg.ip) + ":8080" to_splunk $endpoint, hec_token=$cfg.token } ```

# load_file

Loads the contents of the file at `path` as a byte stream. ```tql load_file path:string, [follow=bool, mmap=bool, timeout=duration] ``` ## Description [Section titled “Description”](#description) The `file` loader acquires raw bytes from a file. ### `path: string` [Section titled “path: string”](#path-string) The file path to load from. When `~` is the first character, it will be substituted with the value of the `$HOME` environment variable. ### `follow = bool (optional)` [Section titled “follow = bool (optional)”](#follow--bool-optional) Do not stop when the end of file is reached, but rather to wait for additional data to be appended to the input. ### `mmap = bool (optional)` [Section titled “mmap = bool (optional)”](#mmap--bool-optional) Use the `mmap(2)` system call to map the file and produce only one single chunk of bytes, instead of producing data piecemeal via `read(2)`. This option effectively gives the downstream parser full control over reads. <!-- TODO: Add this back once they are ported. For the [`feather`](TODO) and [`parquet`](TODO) parsers, this significantly reduces memory usage and improves performance. --> ### `timeout = duration (optional)` [Section titled “timeout = duration (optional)”](#timeout--duration-optional) Wait at most for the provided duration when performing a blocking system call. This flags comes in handy in combination with `follow=true` to produce a steady pulse of input in the pipeline execution, as input (even if empty) drives the processing forward. ## Examples [Section titled “Examples”](#examples) ### Load the raw contents of a file [Section titled “Load the raw contents of a file”](#load-the-raw-contents-of-a-file) ```tql load_file "example.txt" ``` ## See Also [Section titled “See Also”](#see-also) [`files`](/reference/operators/files), [`from_file`](/reference/operators/from_file), [`load_stdin`](/reference/operators/load_stdin), [`save_file`](/reference/operators/save_file)

# load_ftp

Loads a byte stream via FTP. ```tql load_ftp url:str [tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled “Description”](#description) Loads a byte stream via FTP. ### `url: str` [Section titled “url: str”](#url-str) The URL to request from. The `ftp://` scheme can be omitted. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ```tql load_ftp "ftp.example.org" ```

# load_gcs

Loads bytes from a Google Cloud Storage object. ```tql load_gcs uri:string, [anonymous=bool] ``` ## Description [Section titled “Description”](#description) The `load_gcs` operator connects to a GCS bucket to acquire raw bytes from a GCS object. The connector tries to retrieve the appropriate credentials using Google’s [Application Default Credentials](https://google.aip.dev/auth/4110). ### `uri: string` [Section titled “uri: string”](#uri-string) The path to the GCS object. The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist: > For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`. ### `anonymous = bool (optional)` [Section titled “anonymous = bool (optional)”](#anonymous--bool-optional) Ignore any predefined credentials and try to use anonymous credentials. ## Examples [Section titled “Examples”](#examples) Read JSON from an object `log.json` in the folder `logs` in `bucket`. ```tql load_gcs "gs://bucket/logs/log.json" read_json ``` ## See Also [Section titled “See Also”](#see-also) [`save_gcs`](/reference/operators/save_gcs)

# load_google_cloud_pubsub

Subscribes to a Google Cloud Pub/Sub subscription and obtains bytes. ```tql load_google_cloud_pubsub project_id=string, subscription_id=string, [timeout=duration] ``` ## Description [Section titled “Description”](#description) The operator acquires raw bytes from a Google Cloud Pub/Sub subscription. ### `project_id = string` [Section titled “project\_id = string”](#project_id--string) The project to connect to. Note that this is the project id, not the display name. ### `subscription_id = string` [Section titled “subscription\_id = string”](#subscription_id--string) The subscription to subscribe to. ### `timeout = duration (optional)` [Section titled “timeout = duration (optional)”](#timeout--duration-optional) How long to wait for messages before ending the connection. A duration of zero means the operator will run forever. The default value is `0s`. ## URI support & integration with `from` [Section titled “URI support & integration with from”](#uri-support--integration-with-from) The `load_google_cloud_pubsub` operator can also be used from the [`from`](/reference/operators/from) operator. For this, the `gcps://` scheme can be used. The URI is then translated: ```tql from "gcps://my_project/my_subscription" ``` ```tql load_google_cloud_pubsub project_id="my_project", subscription_id="my_subscription" ``` ## Examples [Section titled “Examples”](#examples) ### Read JSON messages from a subscription [Section titled “Read JSON messages from a subscription”](#read-json-messages-from-a-subscription) Subscribe to `my-subscription` in the project `amazing-project-123456` and parse the messages as JSON: ```tql load_google_cloud_pubsub project_id="amazing-project-123456", subscription_id="my-subscription" read_json ``` ## See Also [Section titled “See Also”](#see-also) [`save_google_cloud_pubsub`](/reference/operators/save_google_cloud_pubsub)

# load_http

Loads a byte stream via HTTP. ```tql load_http url:string, [data=record, params=record, headers=record, method=string, form=bool, chunked=bool, multipart=bool, parallel=int, tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled “Description”](#description) The `load_http` operator performs a HTTP request and returns the response. ### `url: string` [Section titled “url: string”](#url-string) The URL to request from. The `http://` scheme can be omitted. ### `method = string (optional)` [Section titled “method = string (optional)”](#method--string-optional) The HTTP method, such as `POST` or `GET`. The default is `"GET"`. ### `params = record (optional)` [Section titled “params = record (optional)”](#params--record-optional) The query parameters for the request. ### `headers = record (optional)` [Section titled “headers = record (optional)”](#headers--record-optional) The headers for the request. ### `data = record (optional)` [Section titled “data = record (optional)”](#data--record-optional) The request body as a record of key-value pairs. The body is encoded as JSON unless `form=true` has been set. ### `form = bool (optional)` [Section titled “form = bool (optional)”](#form--bool-optional) Submits the HTTP request body as form-encoded data. This automatically sets the `Content-Type` header to `application/x-www-form-urlencoded`. Defaults to `false`. ### `chunked = bool (optional)` [Section titled “chunked = bool (optional)”](#chunked--bool-optional) Whether to enable [chunked transfer encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding). This is equivalent to manually setting the header `Transfer-Encoding: chunked`. Defaults to `false`. ### `multipart = bool (optional)` [Section titled “multipart = bool (optional)”](#multipart--bool-optional) Whether to encode the HTTP request body as [multipart message](https://en.wikipedia.org/wiki/MIME#Multipart_messages). This automatically sets the `Content-Type` header to `application/form-multipart; X` where `X` contains the MIME part boundary. Defaults to `false`. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Perform an API call and get the response [Section titled “Perform an API call and get the response”](#perform-an-api-call-and-get-the-response) ```tql load_http "example.org/api", headers={"X-API-Token": "0000-0000-0000"} ``` ## See Also [Section titled “See Also”](#see-also) [`save_http`](/reference/operators/save_http)

# load_kafka

Loads a byte stream from an Apache Kafka topic. ```tql load_kafka topic:string, [count=int, exit=bool, offset=int|string, options=record, aws_iam=record, commit_batch_size=int, commit_timeout=duration] ``` ## Description [Section titled “Description”](#description) The `load_kafka` operator reads bytes from a Kafka topic. The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`. The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them: * `bootstrap.servers`: `localhost` * `client.id`: `tenzir` * `group.id`: `tenzir` * `enable.auto.commit`: `false` (This option cannot be changed) ### `topic: string` [Section titled “topic: string”](#topic-string) The Kafka topic to use. ### `count = int (optional)` [Section titled “count = int (optional)”](#count--int-optional) Exit successfully after having consumed `count` messages. ### `exit = bool (optional)` [Section titled “exit = bool (optional)”](#exit--bool-optional) Exit successfully after having received the last message. Without this option, the operator waits for new messages after consuming the last one. ### `offset = int|string (optional)` [Section titled “offset = int|string (optional)”](#offset--intstring-optional) The offset to start consuming from. Possible values are: * `"beginning"`: first offset * `"end"`: last offset * `"stored"`: stored offset * `<value>`: absolute offset * `-<value>`: relative offset from end The default is `"stored"`. <!-- - `s@<value>`: timestamp in ms to start at - `e@<value>`: timestamp in ms to stop at (not included) --> ### `options = record (optional)` [Section titled “options = record (optional)”](#options--record-optional) A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"auto.offset.reset" : "earliest", "enable.partition.eof": true}`. The `load_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs. We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `load_kafka` arguments. ### `commit_batch_size = int (optional)` [Section titled “commit\_batch\_size = int (optional)”](#commit_batch_size--int-optional) The operator commits offsets after receiving `commit_batch_size` messages to improve throughput. If you need to ensure exactly-once semantics for your pipeline, set this option to `1` to commit every message individually. Defaults to `1000`. ### `commit_timeout = duration (optional)` [Section titled “commit\_timeout = duration (optional)”](#commit_timeout--duration-optional) A timeout after which the operator commits messages, even if it accepted fewer than `commit_batch_size`. This helps with long-running, low-volume pipelines. Defaults to `10s`. ### `aws_iam = record (optional)` [Section titled “aws\_iam = record (optional)”](#aws_iam--record-optional) If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified. Available keys: * `region`: Region of the MSK Clusters. Must be specified when using IAM. * `assume_role`: Optional role ARN to assume. * `session_name`: Optional session name to use when assuming a role. * `external_id`: Optional external id to use when assuming a role. The operator tries to get credentials in the following order: 1. Checks your environment variables for AWS Credentials. 2. Checks your `$HOME/.aws/credentials` file for a profile and credentials 3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`. 4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that isn’t directly supported by AWS. 5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set. 6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON. ## Examples [Section titled “Examples”](#examples) ### Read 100 JSON messages from the topic `tenzir` [Section titled “Read 100 JSON messages from the topic tenzir”](#read-100-json-messages-from-the-topic-tenzir) ```tql load_kafka "tenzir", count=100 read_json ``` ### Read Zeek Streaming JSON logs starting at the beginning [Section titled “Read Zeek Streaming JSON logs starting at the beginning”](#read-zeek-streaming-json-logs-starting-at-the-beginning) ```tql load_kafka "zeek", offset="beginning" read_zeek_json ``` ## See Also [Section titled “See Also”](#see-also) [`save_kafka`](/reference/operators/save_kafka)

# load_nic

Loads bytes from a network interface card (NIC). ```tql load_nic iface:str, [snaplen=int, emit_file_headers=bool] ``` ## Description [Section titled “Description”](#description) The `load_nic` operator uses libpcap to acquire packets from a network interface and packs them into blocks of bytes that represent PCAP packet records. The received first packet triggers also emission of PCAP file header such that downstream operators can treat the packet stream as valid PCAP capture file. ### `iface: str` [Section titled “iface: str”](#iface-str) The interface to load bytes from. ### `snaplen = int (optional)` [Section titled “snaplen = int (optional)”](#snaplen--int-optional) Sets the snapshot length of the captured packets. This value is an upper bound on the packet size. Packets larger than this size get truncated to `snaplen` bytes. Defaults to `262144`. ### `emit_file_headers = bool (optional)` [Section titled “emit\_file\_headers = bool (optional)”](#emit_file_headers--bool-optional) Creates PCAP file headers for every flushed batch. The operator emits chunk of bytes that represent a stream of packets. When setting `emit_file_headers` every chunk gets its own PCAP file header, as opposed to just the very first. This yields a continuous stream of concatenated PCAP files. Our [`read_pcap`](/reference/operators/read_pcap) operator can handle such concatenated traces, and optionally re-emit thes file headers as separate events. ## Examples [Section titled “Examples”](#examples) ### Read PCAP packets from `eth0` [Section titled “Read PCAP packets from eth0”](#read-pcap-packets-from-eth0) ```tql load_nic "eth0" read_pcap ``` ### Perform the equivalent of `tcpdump -i en0 -w trace.pcap` [Section titled “Perform the equivalent of tcpdump -i en0 -w trace.pcap”](#perform-the-equivalent-of-tcpdump--i-en0--w-tracepcap) ```tql load_nic "en0" read_pcap write_pcap save_file "trace.pcap" ``` ## See Also [Section titled “See Also”](#see-also) [`read_pcap`](/reference/operators/read_pcap), [`nics`](/reference/operators/nics), [`write_pcap`](/reference/operators/write_pcap)

# load_s3

Loads from an Amazon S3 object. ```tql load_s3 uri:str, [anonymous=bool, role=string, external_id=string] ``` ## Description [Section titled “Description”](#description) The `load_s3` operator connects to an S3 bucket to acquire raw bytes from an S3 object. The connector tries to retrieve the appropriate credentials using AWS’s [default credentials provider chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). If a config file `<prefix>/etc/tenzir/plugin/s3.yaml` or `~/.config/tenzir/plugin/s3.yaml` exists, it is always preferred over the default AWS credentials. The configuration file must have the following format: ```yaml access-key: your-access-key secret-key: your-secret-key session-token: your-session-token (optional) ``` ### `uri: str` [Section titled “uri: str”](#uri-str) The path to the S3 object. The syntax is `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)`. Options can be appended to the path as query parameters, as per [Arrow](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri): > For S3, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`. ### `anonymous = bool (optional)` [Section titled “anonymous = bool (optional)”](#anonymous--bool-optional) Whether to ignore any predefined credentials and try to load with anonymous credentials. ### `role = string (optional)` [Section titled “role = string (optional)”](#role--string-optional) A role to assume when writing to S3. ### `external_id = string (optional)` [Section titled “external\_id = string (optional)”](#external_id--string-optional) The external ID to use when assuming the `role`. Defaults to no ID. ## Examples [Section titled “Examples”](#examples) Read CSV from an object `obj.csv` in the bucket `examplebucket`: ```tql load_s3 "s3://examplebucket/obj.csv" read_csv ``` Read JSON from an object `test.json` in the bucket `examplebucket`, but using a different, S3-compatible endpoint: ```tql load_s3 "s3://examplebucket/test.json?endpoint_override=s3.us-west.mycloudservice.com" read_json ``` ## See Also [Section titled “See Also”](#see-also) [`save_s3`](/reference/operators/save_s3)

# load_sqs

Loads bytes from [Amazon SQS](https://docs.aws.amazon.com/sqs/) queues. ```tql load_sqs queue:str, [poll_time=duration] ``` ## Description [Section titled “Description”](#description) [Amazon Simple Queue Service (Amazon SQS)](https://docs.aws.amazon.com/sqs/) is a fully managed message queuing service to decouple and scale microservices, distributed systems, and serverless applications. The `load_sqs` operator reads bytes from messages of an SQS queue. The `load_sqs` operator uses long polling, which helps reduce your cost of using SQS by reducing the number of empty responses when there are no messages available to return in reply to a message request. Use the `poll_time` option to adjust the timeout. The operator requires the following AWS permissions: * `sqs:GetQueueUrl` * `sqs:ReceiveMessage` * `sqs:DeleteMessage` ### `queue: str` [Section titled “queue: str”](#queue-str) The name of the queue to use. ### `poll_time = duration (optional)` [Section titled “poll\_time = duration (optional)”](#poll_time--duration-optional) The long polling timeout per request. The value must be between 1 and 20 seconds. Defaults to `3s`. ## Examples [Section titled “Examples”](#examples) Read JSON messages from the SQS queue `tenzir`: ```tql load_sqs "tenzir" ``` Read JSON messages with a 20-second long poll timeout: ```tql load_sqs "tenzir", poll_time=20s ``` ## See Also [Section titled “See Also”](#see-also) [`save_sqs`](/reference/operators/save_sqs)

# load_stdin

Accepts bytes from standard input. ```tql load_stdin ``` ## Description [Section titled “Description”](#description) Accepts bytes from standard input. This is mostly useful when using the `tenzir` executable as part of a shell script. ## Examples [Section titled “Examples”](#examples) ### Pipe text into `tenzir` [Section titled “Pipe text into tenzir”](#pipe-text-into-tenzir) ```sh echo "Hello World" | tenzir ``` ```tql load_stdin read_lines ``` ```tql { line: "Hello World", } ``` ## See Also [Section titled “See Also”](#see-also) [`save_stdout`](/reference/operators/save_stdout), [`load_file`](/reference/operators/load_file)

# load_tcp

Loads bytes from a TCP or TLS connection. ```tql load_tcp endpoint:string, [parallel=int, peer_field=field, tls=bool, cacert=string, certifle=string, max_buffered_chunks=int { … }] ``` ## Description [Section titled “Description”](#description) Reads bytes from the given endpoint via TCP or TLS. ### `endpoint: string` [Section titled “endpoint: string”](#endpoint-string) The endpoint at which the server will listen. Must be of the form `[tcp://]<hostname>:<port>`. Use the hostname `0.0.0.0` to accept connections on all interfaces. ### `parallel = int (optional)` [Section titled “parallel = int (optional)”](#parallel--int-optional) Number of threads to use for reading from connections. Defaults to 1. ### `peer_field = field (optional)` [Section titled “peer\_field = field (optional)”](#peer_field--field-optional) Write a record with the fields `ip`, `port`, and `hostname` resembling the peer endpoint of the respective TCP connection into the specified field at the end of the nested pipeline. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ### `max_buffered_chunks = int (optional)` [Section titled “max\_buffered\_chunks = int (optional)”](#max_buffered_chunks--int-optional) Maximum number of buffered chunks per connection. Defaults to 10. ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) The pipeline to run for each individual TCP connection. If none is specified, no transformations are applied to the output streams. Unless you are sure that there is at most one active connection at a time, it is recommended to specify a pipeline that parses the individual connection streams into events, for instance `{ read_json }`. Otherwise, the output can be interleaved. ## Examples [Section titled “Examples”](#examples) ### Listen for incoming Syslog over TCP [Section titled “Listen for incoming Syslog over TCP”](#listen-for-incoming-syslog-over-tcp) Listen on all network interfaces, parsing each individual connection as syslog: ```tql load_tcp "0.0.0.0:8090" { read_syslog } ``` ### Connect to a remote endpoint and read JSON [Section titled “Connect to a remote endpoint and read JSON”](#connect-to-a-remote-endpoint-and-read-json) ```tql // We know that there is only one connection, so we do not specify a pipeline. load_tcp "example.org:8090", connect=true read_json ``` ### Listen on localhost with TLS enabled [Section titled “Listen on localhost with TLS enabled”](#listen-on-localhost-with-tls-enabled) Wait for connections on localhost with TLS enabled, parsing incoming JSON streams according to the schema `"my_schema"`, forwarding no more than 20 events per individual connection: ```tql load_tcp "127.0.0.1:4000", tls=true, certfile="key_and_cert.pem", keyfile="key_and_cert.pem" { read_json schema="my_schema" head 20 } ``` This example may use a self-signed certificate that can be generated like this: ```bash openssl req -x509 -newkey rsa:2048 -keyout key_and_cert.pem -out key_and_cert.pem -days 365 -nodes ``` You can test the endpoint locally by issuing a TLS connection: ```bash openssl s_client 127.0.0.1:4000 ```

# load_udp

Loads bytes from a UDP socket. ```tql load_udp endpoint:str, [connect=bool, insert_newlines=bool] ``` ## Description [Section titled “Description”](#description) Loads bytes from a UDP socket. The operator defaults to creating a socket in listening mode. Use `connect=true` if the operator should initiate the connection instead. When you have a socket in listening mode, use `0.0.0.0` to accept connections on all interfaces. The [`nics`](/reference/operators/nics) operator lists all all available interfaces. ### `endpoint: str` [Section titled “endpoint: str”](#endpoint-str) The address of the remote endpoint to load bytes from. Must be of the format: `[udp://]host:port`. ### `connect = bool (optional)` [Section titled “connect = bool (optional)”](#connect--bool-optional) Connect to `endpoint` instead of listening at it. Defaults to `false`. ### `insert_newlines = bool (optional)` [Section titled “insert\_newlines = bool (optional)”](#insert_newlines--bool-optional) Append a newline character (`\n`) at the end of every datagram. This option comes in handy in combination with line-based parsers downstream, such as NDJSON. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Import JSON via UDP by listenting on localhost [Section titled “Import JSON via UDP by listenting on localhost”](#import-json-via-udp-by-listenting-on-localhost) ```tql load_udp "127.0.0.1:56789" import ``` ## See Also [Section titled “See Also”](#see-also) [`save_udp`](/reference/operators/save_udp)

# load_zmq

Receives ZeroMQ messages. ```tql load_zmq [endpoint:str, filter=str, listen=bool, connect=bool, insert_separator=string, monitor=bool] ``` ## Description [Section titled “Description”](#description) The `load_zmq` operator processes the bytes in a ZeroMQ message received by a `SUB` socket. Indpendent of the socket type, the `load_zmq` operator supports specfiying the direction of connection establishment with `listen` and `connect`. This can be helpful to work around firewall restrictions and fit into broader set of existing ZeroMQ applications. With the `monitor` option, you can activate message buffering for TCP sockets that hold off sending messages until *at least one* remote peer has connected. This can be helpful when you want to delay publishing until you have one connected subscriber, e.g., when the publisher spawns before any subscriber exists. ### `endpoint: str (optional)` [Section titled “endpoint: str (optional)”](#endpoint-str-optional) The endpoint for connecting to or listening on a ZeroMQ socket. Defaults to `tcp://127.0.0.1:5555`. ### `filter = str (optional)` [Section titled “filter = str (optional)”](#filter--str-optional) Installs a filter for the ZeroMQ `SUB` socket at the source. Filting in ZeroMQ means performing a prefix-match on the raw bytes of the entire message. Defaults to the empty string, which is equivalent to no filtering. ### `listen = bool (optional)` [Section titled “listen = bool (optional)”](#listen--bool-optional) Bind to the ZeroMQ socket. Defaults to `false`. ### `connect = bool (optional)` [Section titled “connect = bool (optional)”](#connect--bool-optional) Connect to the ZeroMQ socket. Defaults to `true`. ### `insert_separator = string (optional)` [Section titled “insert\_separator = string (optional)”](#insert_separator--string-optional) A separator string to append to each received ZeroMQ message. Defaults to no separator. ### `monitor = bool (optional)` [Section titled “monitor = bool (optional)”](#monitor--bool-optional) Monitors a 0mq socket over TCP until the remote side establishes a connection. ## Examples [Section titled “Examples”](#examples) ### Interpret ZeroMQ messages as JSON [Section titled “Interpret ZeroMQ messages as JSON”](#interpret-zeromq-messages-as-json) ```tql load_zmq "1.2.3.4:56789" read_json ``` ## See Also [Section titled “See Also”](#see-also) [`save_zmq`](/reference/operators/save_zmq)

# local

Forces a pipeline to run locally. ```tql local { … } ``` ## Description [Section titled “Description”](#description) The `local` operator takes a pipeline as an argument and forces it to run at a client process. This operator has no effect when running a pipeline through the API or Tenzir Platform. ## Examples [Section titled “Examples”](#examples) ### Do an expensive sort locally [Section titled “Do an expensive sort locally”](#do-an-expensive-sort-locally) ```tql export where @name.starts_with("suricata") local { sort timestamp } write_ndjson save_file "eve.json" ``` ## See Also [Section titled “See Also”](#see-also) [`remote`](/reference/operators/remote)

# measure

Replaces the input with metrics describing the input. ```tql measure [real_time=bool, cumulative=bool] ``` ## Description [Section titled “Description”](#description) The `measure` operator yields metrics for each received batch of events or bytes using the following schema, respectively: Events Metrics ```text type tenzir.measure.events = record{ timestamp: time, events: uint64, schema_id: string, schema: string, } ``` Bytes Metrics ```text type tenzir.measure.bytes = record{ timestamp: time, bytes: uint64, } ``` ### `real_time = bool (optional)` [Section titled “real\_time = bool (optional)”](#real_time--bool-optional) Whether to emit metrics immediately with every batch, rather than buffering until the upstream operator stalls, i.e., is idle or waiting for further input. The is especially useful when `measure` should emit data without latency. ### `cumulative = bool (optional)` [Section titled “cumulative = bool (optional)”](#cumulative--bool-optional) Whether to emit running totals for the `events` and `bytes` fields rather than per-batch statistics. ## Examples [Section titled “Examples”](#examples) ### Get the number of bytes read incrementally for a file [Section titled “Get the number of bytes read incrementally for a file”](#get-the-number-of-bytes-read-incrementally-for-a-file) ```tql load_file "input.json" measure ``` ```tql {timestamp: 2023-04-28T10:22:10.192322, bytes: 16384} {timestamp: 2023-04-28T10:22:10.223612, bytes: 16384} {timestamp: 2023-04-28T10:22:10.297169, bytes: 16384} {timestamp: 2023-04-28T10:22:10.387172, bytes: 16384} {timestamp: 2023-04-28T10:22:10.408171, bytes: 8232} ``` ### Get the number of events read incrementally from a file [Section titled “Get the number of events read incrementally from a file”](#get-the-number-of-events-read-incrementally-from-a-file) ```tql load_file "eve.json" read_suricata measure ``` ```tql { timestamp: 2023-04-28T10:26:45.159885, events: 65536, schema_id: "d49102998baae44a", schema: "suricata.dns" } { timestamp: 2023-04-28T10:26:45.812321, events: 412, schema_id: "d49102998baae44a", schema: "suricata.dns" } ``` ### Get the total number of events in a file, grouped by schema [Section titled “Get the total number of events in a file, grouped by schema”](#get-the-total-number-of-events-in-a-file-grouped-by-schema) ```tql load_file "eve.json" read_suricata measure summarize schema, events=sum(events) ``` ```tql {schema: "suricata.dns", events: 65948} ```

# metrics

Retrieves metrics events from a Tenzir node. ```tql metrics [name:string, live=bool, retro=bool] ``` ## Description [Section titled “Description”](#description) The `metrics` operator retrieves metrics events from a Tenzir node. Metrics events are collected every second. ### `name: string (optional)` [Section titled “name: string (optional)”](#name-string-optional) Show only metrics with the specified name. For example, `metrics "cpu"` only shows CPU metrics. ### `live = bool (optional)` [Section titled “live = bool (optional)”](#live--bool-optional) Work on all metrics events as they are generated in real-time instead of on metrics events persisted at a Tenzir node. ### `retro = bool (optional)` [Section titled “retro = bool (optional)”](#retro--bool-optional) Work on persisted diagnostic events (first), even when `live` is given. ## Schemas [Section titled “Schemas”](#schemas) Tenzir collects metrics with the following schemas. ### `tenzir.metrics.api` [Section titled “tenzir.metrics.api”](#tenzirmetricsapi) Contains information about all accessed API endpoints, emitted once per second. | Field | Type | Description | | :-------------- | :--------- | :----------------------------------------------------- | | `timestamp` | `time` | The time at which the API request was received. | | `request_id` | `string` | The unique request ID assigned by the Tenzir Platform. | | `method` | `double` | The HTTP method used to access the API. | | `path` | `double` | The path of the accessed API endpoint. | | `response_time` | `duration` | The time the API endpoint took to respond. | | `status_code` | `uint64` | The HTTP status code of the API response. | | `params` | `record` | The API endpoints parameters passed inused. | The schema of the record `params` depends on the API endpoint used. Refer to the [API documentation](/reference/node/api) to see the available parameters per endpoint. ### `tenzir.metrics.caf` [Section titled “tenzir.metrics.caf”](#tenzirmetricscaf) Contains metrics about the CAF (C++ Actor Framework) runtime system. Aimed at Developers CAF metrics primarily exist for debugging purposes. Actor names and other details contained in these metrics are documented only in source code, and we may change them without notice. Do not rely on specific actor names or metrics in production systems. | Field | Type | Description | | :---------- | :------------- | :---------------------------------------- | | `system` | `record` | Metrics about the CAF actor system. | | `middleman` | `record` | Metrics about CAF’s network layer. | | `actors` | `list<record>` | Per-actor metrics for all running actors. | The record `system` has the following schema: | Field | Type | Description | | :----------------------- | :------- | :--------------------------------------------------------------------- | | `running_actors` | `int64` | Number of currently running actors. | | `running_actors_by_name` | `list` | Number of running actors, grouped by actor name. | | `all_messages` | `record` | Information about the total message metrics. | | `messages_by_actor` | `list` | Information about the message metrics, grouped by receiving actor name | The `running_actors_by_name` field is a `list` of `record`s with the following schema: | Field | Type | Description | | :------ | :------- | :------------------------------------------------- | | `name` | `string` | Actor name. | | `count` | `int64` | Number of actors with this name currently running. | The `all_messages` field has the following schema: | Field | Type | Description | | :---------- | :------ | :---------------------------- | | `processed` | `int64` | Number of processed messages. | | `rejected` | `int64` | Number of rejected messages. | The `messages_by_actor` field is a `list` of `record`s with the following schema: | Field | Type | Description | | :---------- | :------- | :-------------------------------------------------------------------------------------- | | `name` | `string` | Name of the receiving actor. This may be null for messages without an associated actor. | | `processed` | `int64` | Number of processed messages. | | `rejected` | `int64` | Number of rejected messages. | The record `middleman` has the following schema: | Field | Type | Description | | :----------------------- | :--------- | :---------------------------------------------------- | | `inbound_messages_size` | `int64` | Size of received messages in bytes since last metric. | | `outbound_messages_size` | `int64` | Size of sent messages in bytes since last metric. | | `serialization_time` | `duration` | Time spent serializing messages since last metric. | | `deserialization_time` | `duration` | Time spent deserializing messages since last metric. | Each record in the `actors` list has the following schema: | Field | Type | Description | | :---------------- | :--------- | :------------------------------------------------ | | `name` | `string` | Name of the actor. | | `processing_time` | `duration` | Time spent processing messages since last metric. | | `mailbox_time` | `duration` | Time messages spent in mailbox since last metric. | | `mailbox_size` | `int64` | Current number of messages in actor’s mailbox. | ### `tenzir.metrics.buffer` [Section titled “tenzir.metrics.buffer”](#tenzirmetricsbuffer) Contains information about the `buffer` operator’s internal buffer. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `buffer` operator in the pipeline. | | `used` | `uint64` | The number of events stored in the buffer. | | `free` | `uint64` | The remaining capacity of the buffer. | | `dropped` | `uint64` | The number of events dropped by the buffer. | ### `tenzir.metrics.cpu` [Section titled “tenzir.metrics.cpu”](#tenzirmetricscpu) Contains a measurement of CPU utilization. | Field | Type | Description | | :------------ | :------- | :------------------------------------------ | | `timestamp` | `time` | The time at which this metric was recorded. | | `loadavg_1m` | `double` | The load average over the last minute. | | `loadavg_5m` | `double` | The load average over the last 5 minutes. | | `loadavg_15m` | `double` | The load average over the last 15 minutes. | ### `tenzir.metrics.disk` [Section titled “tenzir.metrics.disk”](#tenzirmetricsdisk) Contains a measurement of disk space usage. | Field | Type | Description | | :------------ | :------- | :--------------------------------------------------------------------------------- | | `timestamp` | `time` | The time at which this metric was recorded. | | `path` | `string` | The byte measurements below refer to the filesystem on which this path is located. | | `total_bytes` | `uint64` | The total size of the volume, in bytes. | | `used_bytes` | `uint64` | The number of bytes occupied on the volume. | | `free_bytes` | `uint64` | The number of bytes still free on the volume. | ### `tenzir.metrics.enrich` [Section titled “tenzir.metrics.enrich”](#tenzirmetricsenrich) Contains a measurement of the `enrich` operator, emitted once every second. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `enrich` operator in the pipeline. | | `context` | `string` | The name of the context the associated operator is using. | | `events` | `uint64` | The amount of input events that entered the `enrich` operator since the last metric. | | `hits` | `uint64` | The amount of successfully enriched events since the last metric. | ### `tenzir.metrics.export` [Section titled “tenzir.metrics.export”](#tenzirmetricsexport) Contains a measurement of the `export` operator, emitted once every second per schema. Note that internal events like metrics or diagnostics do not emit metrics themselves. | Field | Type | Description | | :-------------- | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `export` operator in the pipeline. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were imported. | | `queued_events` | `uint64` | The total amount of events that are enqueued in the export. | ### `tenzir.metrics.import` [Section titled “tenzir.metrics.import”](#tenzirmetricsimport) Contains a measurement the `import` operator, emitted once every second per schema. Note that internal events like metrics or diagnostics do not emit metrics themselves. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `import` operator in the pipeline. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were imported. | ### `tenzir.metrics.ingest` [Section titled “tenzir.metrics.ingest”](#tenzirmetricsingest) Contains a measurement of all data ingested into the database, emitted once per second and schema. | Field | Type | Description | | :---------- | :------- | :------------------------------------------ | | `timestamp` | `time` | The time at which this metric was recorded. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were ingested. | ### `tenzir.metrics.lookup` [Section titled “tenzir.metrics.lookup”](#tenzirmetricslookup) Contains a measurement of the `lookup` operator, emitted once every second. | Field | Type | Description | | :---------------- | :------- | :------------------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `lookup` operator in the pipeline. | | `context` | `string` | The name of the context the associated operator is using. | | `live` | `record` | Information about the live lookup. | | `retro` | `record` | Information about the retroactive lookup. | | `context_updates` | `uint64` | The amount of times the underlying context has been updated while the associated lookup is active. | The record `live` has the following schema: | Field | Type | Description | | :------- | :------- | :------------------------------------------------------------------------- | | `events` | `uint64` | The amount of input events used for the live lookup since the last metric. | | `hits` | `uint64` | The amount of live lookup matches since the last metric. | The record `retro` has the following schema: | Field | Type | Description | | :-------------- | :------- | :-------------------------------------------------------------------- | | `events` | `uint64` | The amount of input events used for the lookup since the last metric. | | `hits` | `uint64` | The amount of lookup matches since the last metric. | | `queued_events` | `uint64` | The total amount of events that were in the queue for the lookup. | ### `tenzir.metrics.memory` [Section titled “tenzir.metrics.memory”](#tenzirmetricsmemory) Contains a measurement of the available memory on the host. | Field | Type | Description | | :------------ | :------- | :------------------------------------------ | | `timestamp` | `time` | The time at which this metric was recorded. | | `total_bytes` | `uint64` | The total available memory, in bytes. | | `used_bytes` | `uint64` | The amount of memory used, in bytes. | | `free_bytes` | `uint64` | The amount of free memory, in bytes. | ### `tenzir.metrics.operator` [Section titled “tenzir.metrics.operator”](#tenzirmetricsoperator) Contains input and output measurements over some amount of time for a single operator instantiation. Deprecation Notice Operator metrics are deprecated and will be removed in a future release. Use [pipeline metrics](#tenzirmetricspipeline) instead. While they offered great insight into the performance of operators, they were not as useful as pipeline metrics for understanding the overall performance of a pipeline, and were too expensive to collect and store. | Field | Type | Description | | :-------------------- | :--------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time when this event was emitted (immediately after the collection period). | | `operator_id` | `uint64` | The ID of the operator inside the pipeline referenced above. | | `source` | `bool` | True if this is the first operator in the pipeline. | | `transformation` | `bool` | True if this is neither the first nor the last operator. | | `sink` | `bool` | True if this is the last operator in the pipeline. | | `internal` | `bool` | True if the data flow is considered to internal to Tenzir. | | `duration` | `duration` | The timespan over which this data was collected. | | `starting_duration` | `duration` | The time spent to start the operator. | | `processing_duration` | `duration` | The time spent processing the data. | | `scheduled_duration` | `duration` | The time that the operator was scheduled. | | `running_duration` | `duration` | The time that the operator was running. | | `paused_duration` | `duration` | The time that the operator was paused. | | `input` | `record` | Measurement of the incoming data stream. | | `output` | `record` | Measurement of the outgoing data stream. | The records `input` and `output` have the following schema: | Field | Type | Description | | :------------- | :------- | :-------------------------------------------------------------- | | `unit` | `string` | The type of the elements, which is `void`, `bytes` or `events`. | | `elements` | `uint64` | Number of elements that were seen during the collection period. | | `approx_bytes` | `uint64` | An approximation for the number of bytes transmitted. | | `batches` | `uint64` | The number of batches included in this metric. | ### `tenzir.metrics.pipeline` [Section titled “tenzir.metrics.pipeline”](#tenzirmetricspipeline) Contains measurements of data flowing through pipelines, emitted once every 10 seconds. | Field | Type | Description | | :------------ | :------- | :---------------------------------------------- | | `timestamp` | `time` | The time at which this metric was recorded. | | `pipeline_id` | `string` | The ID of the pipeline these metrics represent. | | `ingress` | `record` | Measurement of data entering the pipeline. | | `egress` | `record` | Measurement of data exiting the pipeline. | The records `ingress` and `egress` have the following schema: | Field | Type | Description | | :--------- | :--------- | :------------------------------------------------------- | | `duration` | `duration` | The timespan over which this data was collected. | | `events` | `uint64` | Number of events that passed through during this period. | | `bytes` | `uint64` | Approximate number of bytes that passed through. | | `batches` | `uint64` | Number of batches that passed through. | | `internal` | `bool` | True if the data flow is considered internal to Tenzir. | ### `tenzir.metrics.platform` [Section titled “tenzir.metrics.platform”](#tenzirmetricsplatform) Signals whether the connection to the Tenzir Platform is working from the node’s perspective. Emitted once per second. | Field | Type | Description | | :---------- | :----- | :------------------------------------------ | | `timestamp` | `time` | The time at which this metric was recorded. | | `connected` | `bool` | The connection status. | ### `tenzir.metrics.process` [Section titled “tenzir.metrics.process”](#tenzirmetricsprocess) Contains a measurement of the amount of memory used by the `tenzir-node` process. | Field | Type | Description | | :--------------------- | :------- | :-------------------------------------------------------------------------------- | | `timestamp` | `time` | The time at which this metric was recorded. | | `current_memory_usage` | `uint64` | The memory currently used by this process. | | `peak_memory_usage` | `uint64` | The peak amount of memory, in bytes. | | `swap_space_usage` | `uint64` | The amount of swap space, in bytes. Only available on Linux systems. | | `open_fds` | `uint64` | The amount of open file descriptors by the node. Only available on Linux systems. | ### `tenzir.metrics.publish` [Section titled “tenzir.metrics.publish”](#tenzirmetricspublish) Contains a measurement of the `publish` operator, emitted once every second per schema. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `publish` operator in the pipeline. | | `topic` | `string` | The topic name. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were published to the `topic`. | ### `tenzir.metrics.rebuild` [Section titled “tenzir.metrics.rebuild”](#tenzirmetricsrebuild) Contains a measurement of the partition rebuild process, emitted once every second. | Field | Type | Description | | :------------------ | :------- | :-------------------------------------------------------- | | `timestamp` | `time` | The time at which this metric was recorded. | | `partitions` | `uint64` | The number of partitions currently being rebuilt. | | `queued_partitions` | `uint64` | The number of partitions currently queued for rebuilding. | ### `tenzir.metrics.subscribe` [Section titled “tenzir.metrics.subscribe”](#tenzirmetricssubscribe) Contains a measurement of the `subscribe` operator, emitted once every second per schema. | Field | Type | Description | | :------------ | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `subscribe` operator in the pipeline. | | `topic` | `string` | The topic name. | | `schema` | `string` | The schema name of the batch. | | `schema_id` | `string` | The schema ID of the batch. | | `events` | `uint64` | The amount of events that were retrieved from the `topic`. | ### `tenzir.metrics.tcp` [Section titled “tenzir.metrics.tcp”](#tenzirmetricstcp) Contains measurements about the number of read calls and the received bytes per TCP connection. | Field | Type | Description | | :-------------- | :------- | :----------------------------------------------------------------------------------------- | | `pipeline_id` | `string` | The ID of the pipeline where the associated operator is from. | | `run` | `uint64` | The number of the run, starting at 1 for the first run. | | `hidden` | `bool` | Indicates whether the corresponding pipeline is hidden from the list of managed pipelines. | | `timestamp` | `time` | The time at which this metric was recorded. | | `operator_id` | `uint64` | The ID of the `publish` operator in the pipeline. | | `native` | `string` | The native handle of the connection (unix: file descriptor). | | `reads` | `uint64` | The number of attempted reads since the last metric. | | `writes` | `uint64` | The number of attempted writes since the last metric. | | `bytes_read` | `uint64` | The number of bytes received since the last metrics. | | `bytes_written` | `uint64` | The number of bytes written since the last metrics. | ## Examples [Section titled “Examples”](#examples) ### Sort pipelines by total ingress in bytes [Section titled “Sort pipelines by total ingress in bytes”](#sort-pipelines-by-total-ingress-in-bytes) ```tql metrics "pipeline" summarize pipeline_id, ingress=sum(ingress.bytes if not ingress.internal) sort -ingress ``` ```tql {pipeline_id: "demo-node/m57-suricata", ingress: 59327586} {pipeline_id: "demo-node/m57-zeek", ingress: 43291764} ``` ### Show the CPU usage over the last hour [Section titled “Show the CPU usage over the last hour”](#show-the-cpu-usage-over-the-last-hour) ```tql metrics "cpu" where timestamp > now() - 1h select timestamp, percent=loadavg_1m ``` ```tql {timestamp: 2023-12-21T12:00:32.631102, percent: 0.40478515625} {timestamp: 2023-12-21T11:59:32.626043, percent: 0.357421875} {timestamp: 2023-12-21T11:58:32.620327, percent: 0.42578125} {timestamp: 2023-12-21T11:57:32.614810, percent: 0.50390625} {timestamp: 2023-12-21T11:56:32.609896, percent: 0.32080078125} {timestamp: 2023-12-21T11:55:32.605871, percent: 0.5458984375} ``` ### Get the current memory usage [Section titled “Get the current memory usage”](#get-the-current-memory-usage) ```tql metrics "memory" sort -timestamp tail 1 select current_memory_usage ``` ```tql {current_memory_usage: 1083031552} ``` ### Show the total pipeline ingress in bytes [Section titled “Show the total pipeline ingress in bytes”](#show-the-total-pipeline-ingress-in-bytes) Show the inggress for every day over the last week, excluding pipelines that run in the Explorer: ```tql metrics "operator" where timestamp > now() - 1week where source and not hidden timestamp = floor(timestamp, 1day) summarize timestamp, bytes=sum(output.approx_bytes) ``` ```tql {timestamp: 2023-11-08T00:00:00.000000, bytes: 79927223} {timestamp: 2023-11-09T00:00:00.000000, bytes: 51788928} {timestamp: 2023-11-10T00:00:00.000000, bytes: 80740352} {timestamp: 2023-11-11T00:00:00.000000, bytes: 75497472} {timestamp: 2023-11-12T00:00:00.000000, bytes: 55497472} {timestamp: 2023-11-13T00:00:00.000000, bytes: 76546048} {timestamp: 2023-11-14T00:00:00.000000, bytes: 68643200} ``` ### Show the operators that produced the most events [Section titled “Show the operators that produced the most events”](#show-the-operators-that-produced-the-most-events) Show the three operator instantiations that produced the most events in total and their pipeline IDs: ```tql metrics "operator" where output.unit == "events" summarize pipeline_id, operator_id, events=max(output.elements) sort -events head 3 ``` ```tql {pipeline_id: "70a25089-b16c-448d-9492-af5566789b99", operator_id: 0, events: 391008694 } {pipeline_id: "7842733c-06d6-4713-9b80-e20944927207", operator_id: 0, events: 246914949 } {pipeline_id: "6df003be-0841-45ad-8be0-56ff4b7c19ef", operator_id: 1, events: 83013294 } ``` ### Get the disk usage over time [Section titled “Get the disk usage over time”](#get-the-disk-usage-over-time) ```tql metrics "disk" sort timestamp select timestamp, used_bytes ``` ```tql {timestamp: 2023-12-21T12:52:32.900086, used_bytes: 461834444800} {timestamp: 2023-12-21T12:53:32.905548, used_bytes: 461834584064} {timestamp: 2023-12-21T12:54:32.910918, used_bytes: 461840302080} {timestamp: 2023-12-21T12:55:32.916200, used_bytes: 461842751488} ``` ### Get the memory usage over time [Section titled “Get the memory usage over time”](#get-the-memory-usage-over-time) ```tql metrics "memory" sort timestamp select timestamp, used_bytes ``` ```tql {timestamp: 2023-12-21T13:08:32.982083, used_bytes: 48572645376} {timestamp: 2023-12-21T13:09:32.986962, used_bytes: 48380682240} {timestamp: 2023-12-21T13:10:32.992494, used_bytes: 48438878208} {timestamp: 2023-12-21T13:11:32.997889, used_bytes: 48491839488} {timestamp: 2023-12-21T13:12:33.003323, used_bytes: 48529952768} ``` ### Get inbound TCP traffic over time [Section titled “Get inbound TCP traffic over time”](#get-inbound-tcp-traffic-over-time) ```tql metrics "tcp" sort timestamp select timestamp, port, handle, reads, bytes ``` ```tql { timestamp: 2024-09-04T15:43:38.011350, port: 10000, handle: "12", reads: 884, writes: 0, bytes_read: 10608, bytes_written: 0 } { timestamp: 2024-09-04T15:43:39.013575, port: 10000, handle: "12", reads: 428, writes: 0, bytes_read: 5136, bytes_written: 0 } { timestamp: 2024-09-04T15:43:40.015376, port: 10000, handle: "12", reads: 429, writes: 0, bytes_read: 5148, bytes_written: 0 } ``` ## See Also [Section titled “See Also”](#see-also) [`diagnostics`](/reference/operators/diagnostics)

# move

Moves values from one field to another, removing the original field. ```tql move to=from, … ``` ## Description [Section titled “Description”](#description) Moves from the field `from` to the field `to`. ### `to: field` [Section titled “to: field”](#to-field) The field to move into. ### `from: field` [Section titled “from: field”](#from-field) The field to move from. ## Examples [Section titled “Examples”](#examples) ```tql from {x: 1, y: 2} move z=y, w.x=x ``` ```tql { z: 2, w: { x: 1, }, } ``` ## See Also [Section titled “See Also”](#see-also) [`set`](/reference/operators/set)

# nics

Shows a snapshot of available network interfaces. ```tql nics ``` ## Description [Section titled “Description”](#description) The `nics` operator shows a snapshot of all available network interfaces. ## Schemas [Section titled “Schemas”](#schemas) Tenzir emits network interface card information with the following schema. ### `tenzir.nic` [Section titled “tenzir.nic”](#tenzirnic) Contains detailed information about the network interface. | Field | Type | Description | | :------------ | :------- | :--------------------------------------------------------------------------- | | `name` | `string` | The name of the network interface. | | `description` | `string` | A brief note or explanation about the network interface. | | `addresses` | `list` | A list of IP addresses assigned to the network interface. | | `loopback` | `bool` | Indicates if the network interface is a loopback interface. | | `up` | `bool` | Indicates if the network interface is up and can transmit data. | | `running` | `bool` | Indicates if the network interface is running and operational. | | `wireless` | `bool` | Indicates if the network interface is a wireless interface. | | `status` | `record` | A record containing detailed status information about the network interface. | The record `status` has the following schema: | Field | Type | Description | | :--------------- | :----- | :---------------------------------------------------- | | `unknown` | `bool` | Indicates if the network interface status is unknown. | | `connected` | `bool` | Indicates if the network interface is connected. | | `disconnected` | `bool` | Indicates if the network interface is disconnected. | | `not_applicable` | `bool` | Indicates if the network interface is not applicable. | ## Examples [Section titled “Examples”](#examples) ### List all connected network interfaces [Section titled “List all connected network interfaces”](#list-all-connected-network-interfaces) ```tql nics where status.connected ``` ## See Also [Section titled “See Also”](#see-also) [`load_nic`](/reference/operators/load_nic)

# ocsf::apply

Casts incoming events to their OCSF type. ```tql ocsf::apply [preserve_variants=bool] ``` ## Description [Section titled “Description”](#description) The `ocsf::apply` operator casts incoming events to their corresponding OCSF event class type. The resulting type is determined by four fields: `metadata.version`, `metadata.profiles`, `metadata.extensions` and `class_uid`. Events sharing the same values for these fields are cast to the same type. Tenzir supports all OCSF versions (including `-dev` versions), all profiles, and all event classes. Extensions are currently limited to those versioned with OCSF, including the `win` and `linux` extensions. To this end, the operator performs the following steps: * Add optional fields that are not present in the original event with a `null` value * Emit a warning for extra fields that should not be there and drop them * Encode free-form objects (such as `unmapped`) using their JSON representation * Assign `@name` depending on the class name, for example: `ocsf.dns_activity` The types used for OCSF events are slightly adjusted. For example, timestamps use the native `time` type instead of an integer representing the number of milliseconds since the Unix epoch. Furthermore, some fields that would lead to infinite recursion are currently left out. We plan to support recursion up to a certain depth in the future. Furthermore, this operator will likely be extended with additional features, such as the ability to drop all optional fields, or to automatically assign OCSF enumerations based on their sibling ID. ### `preserve_variants = bool` [Section titled “preserve\_variants = bool”](#preserve_variants--bool) Setting this option to `true` preserves free-form objects such as `unmapped` as-is, instead of being JSON-encoded. Note that this means the resulting event schema is no longer consistent across events of the same class, as changes to these free-form objects lead to different schemas. For schema-consistency and performance reasons, we recommend keeping this option `false` and instead using `unmapped.parse_json()` to extract fields on-demand. ## Examples [Section titled “Examples”](#examples) ### Cast a single pre-defined event [Section titled “Cast a single pre-defined event”](#cast-a-single-pre-defined-event) ```tql from { class_uid: 4001, class_name: "Network Activity", metadata: { version: "1.5.0", }, unmapped: { foo: 1, bar: 2, }, // … some more fields } ocsf::apply ``` ```tql { class_uid: 4001, class_name: "Network Activity", metadata: { version: "1.5.0", // … all other metadata fields set to `null` }, unmapped: "{\"foo\": 1, \"bar\": 2}", // … other fields (with `null` if they didn't exist before) } ``` ### Preserve `unmapped` as a record [Section titled “Preserve unmapped as a record”](#preserve-unmapped-as-a-record) ```tql from { class_uid: 4001, class_name: "Network Activity", metadata: { version: "1.5.0", }, unmapped: { foo: 1, bar: 2, }, } ocsf::apply preserve_variants=true select unmapped ``` ```tql { unmapped: { foo: 1, bar: 2, }, } ``` ### Filter, transform and send events to ClickHouse [Section titled “Filter, transform and send events to ClickHouse”](#filter-transform-and-send-events-to-clickhouse) ```tql subscribe "ocsf" where class_name == "Network Activity" and metadata.version == "1.5.0" ocsf::apply to_clickhouse table="network_activity" ``` ## See Also [Section titled “See Also”](#see-also) [`ocsf::derive`](/reference/operators/ocsf/derive), [`ocsf::trim`](/reference/operators/ocsf/trim)

# ocsf::derive

Automatically assigns enum strings from their integer counterparts and vice versa. ```tql ocsf::derive ``` ## Description [Section titled “Description”](#description) The `ocsf::derive` operator performs bidirectional enum derivation for OCSF events by automatically assigning enum string values based on their integer counterparts and vice versa. In the future, this operator will also assign automatically assign computable values such as `type_uid` based on what is available in the event. ## Examples [Section titled “Examples”](#examples) ### Integer to string [Section titled “Integer to string”](#integer-to-string) ```tql from { activity_id: 1, class_uid: 1001, metadata: { version: "1.5.0", }, } ocsf::derive ``` ```tql { activity_id: 1, activity_name: "Create", class_name: "File System Activity", class_uid: 1001, metadata: { version: "1.5.0", }, } ``` ### String to Integer [Section titled “String to Integer”](#string-to-integer) ```tql from { activity_name: "Read", class_uid: 1001, metadata: { version: "1.5.0", }, } ocsf::derive ``` ```tql { activity_id: 2, activity_name: "Read", class_name: "File System Activity", class_uid: 1001, metadata: { version: "1.5.0", }, } ``` ### Bidirectional enum validation [Section titled “Bidirectional enum validation”](#bidirectional-enum-validation) ```tql from { activity_id: 1, activity_name: "Delete", // Inconsistent with activity_id=1 class_uid: 1001, metadata: { version: "1.5.0", }, } ocsf::derive ``` ```tql { activity_id: 1, activity_name: "Delete", class_name: "File System Activity", class_uid: 1001, metadata: { version: "1.5.0", }, } ``` This will emit a warning about inconsistent values and preserve both original values without modification, allowing the user to decide how to handle the conflict. ```plaintext warning: found inconsistency between `activity_id` and `activity_name` --> <input>:9:1 | 9 | ocsf::derive | ~~~~~~~~~~~~ | = note: got 1 ("Create") and "Delete" (4) ``` ## See Also [Section titled “See Also”](#see-also) [`ocsf::apply`](/reference/operators/ocsf/apply), [`ocsf::trim`](/reference/operators/ocsf/trim)

# ocsf::trim

Drops fields from OCSF events to reduce their size. ```tql ocsf::trim [drop_optional=bool, drop_recommended=bool] ``` ## Description [Section titled “Description”](#description) The `ocsf::trim` operator uses intelligent analysis to determine which fields to remove from OCSF events, optimizing data size while preserving essential information. ### `drop_optional = bool` [Section titled “drop\_optional = bool”](#drop_optional--bool) If specified, explicitly controls whether to remove fields marked as optional in the OCSF schema. Otherwise, this decision is left to the operator itself. ### `drop_recommended = bool` [Section titled “drop\_recommended = bool”](#drop_recommended--bool) If specified, explicitly controls whether to remove fields marked as recommended in the OCSF schema. Otherwise, this decision is left to the operator itself. ## Examples [Section titled “Examples”](#examples) ### Use intelligent field selection (default behavior) [Section titled “Use intelligent field selection (default behavior)”](#use-intelligent-field-selection-default-behavior) ```tql from { class_uid: 3002, class_name: "Authentication", // will be removed metadata: { version: "1.5.0", }, user: { name: "alice", uid: "1000", display_name: "Alice", // will be removed }, auth_protocol: "Kerberos", status: "Success", status_id: 1, } ocsf::trim ``` ```tql { class_uid: 3002, metadata: { version: "1.5.0", }, user: { name: "alice", uid: "1000", }, auth_protocol: "Kerberos", status: "Success", status_id: 1, } ``` ### Explicitly remove optional fields only [Section titled “Explicitly remove optional fields only”](#explicitly-remove-optional-fields-only) ```tql from { class_uid: 1001, class_name: "File System Activity", metadata: {version: "1.5.0"}, file: { name: "document.txt", path: "/home/user/document.txt", size: 1024, // optional: will be removed type: "Regular File", // optional: also removed }, activity_id: 1, } ocsf::trim drop_optional=true, drop_recommended=false ``` ```tql { class_uid: 1001, metadata: { version: "1.5.0", }, file: { name: "document.txt", path: "/home/user/document.txt", }, activity_id: 1, } ``` ### Only keep required fields to minimize event size [Section titled “Only keep required fields to minimize event size”](#only-keep-required-fields-to-minimize-event-size) ```tql from { class_uid: 4001, class_name: "Network Activity", metadata: {version: "1.5.0"}, src_endpoint: { ip: "192.168.1.100", port: 443, hostname: "client.local", }, severity: "Critical", severity_id: 5, } ocsf::trim drop_optional=true, drop_recommended=true ``` ```tql { class_uid: 4001, metadata: { version: "1.5.0", }, severity_id: 5, } ``` ## See Also [Section titled “See Also”](#see-also) [`ocsf::apply`](/reference/operators/ocsf/apply), [`ocsf::derive`](/reference/operators/ocsf/derive)

# openapi

Shows the node’s OpenAPI specification. ```tql openapi ``` ## Description [Section titled “Description”](#description) The `openapi` operator shows the current Tenzir node’s [OpenAPI specification](/reference/node/api) for all available REST endpoint plugins. ## Examples [Section titled “Examples”](#examples) ### Render the OpenAPI specification as YAML [Section titled “Render the OpenAPI specification as YAML”](#render-the-openapi-specification-as-yaml) ```tql openapi write_yaml ``` ## See Also [Section titled “See Also”](#see-also) [`api`](/reference/operators/api), [`serve`](/reference/operators/serve)

# package::add

Installs a package. ```tql package::add [package_path:string, inputs=record] ``` ## Description [Section titled “Description”](#description) The `package::add` operator installs all operators, pipelines, and contexts from a package. ### `package_path : string (optional)` [Section titled “package\_path : string (optional)”](#package_path--string-optional) The path to a package located on the file system. ### `inputs = record (optional)` [Section titled “inputs = record (optional)”](#inputs--record-optional) A record of optional package inputs that configure the package. ## Examples [Section titled “Examples”](#examples) ### Add a package from the Community Library [Section titled “Add a package from the Community Library”](#add-a-package-from-the-community-library) ```tql package::add "suricata-ocsf" ``` ### Add a local package with inputs [Section titled “Add a local package with inputs”](#add-a-local-package-with-inputs) ```tql package::add "/mnt/config/tenzir/library/zeek", inputs={format: "tsv", "log-directory": "/opt/tenzir/logs"} ``` ## See Also [Section titled “See Also”](#see-also) [`list`](/reference/operators/package/list), [`remove`](/reference/operators/package/remove)

# package::list

Shows installed packages. ```tql package::list [format=string] ``` ## Description [Section titled “Description”](#description) The `package::list` operator returns the list of all installed packages. ### `format = string (optional)` [Section titled “format = string (optional)”](#format--string-optional) Controls the output format. Valid options are `compact` and `extended`. Defaults to `compact`. ## Schemas [Section titled “Schemas”](#schemas) The `package::list` operator produces two output formats, controlled by the `format` option: * `compact`: succinct output in a human-readable format * `extended`: verbose output in a machine-readable format The formats generate the following schemas below. ### `tenzir.package.compact` [Section titled “tenzir.package.compact”](#tenzirpackagecompact) The compact format prints the package information according to the following schema: | Field | Type | Description | | :------------ | :------- | :-------------------------------------- | | `id` | `string` | The unique package id. | | `name` | `string` | The name of this package. | | `author` | `string` | The package author. | | `description` | `string` | The description of this package. | | `config` | `record` | The user-provided package configuration | ### `tenzir.package.extended` [Section titled “tenzir.package.extended”](#tenzirpackageextended) The `extended` format is mainly intended for use by non-human consumers, like shell scripts or frontend code. It contains all available information about a package. | Field | Type | Description | | :------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------ | | `package_definition` | `record` | The original package definition object asa found in the library. | | `resolved_package` | `record` | The effective package definition that was produced by applying all inputs and overrides from the `config` section and removing all disabled pipelines and contexts. | | `config` | `record` | The user-provided package configuration. | | `package_status` | `record` | Run-time information about the package provided the package manager. | The `config` object has the following schema, where all fields are optional: | Field | Type | Description | | :---------- | :------- | :------------------------------------------------------------ | | `version` | `string` | The package version. | | `source` | `record` | The upstream location of the package definition. | | `inputs` | `record` | User-provided values for the package inputs. | | `overrides` | `record` | User-provided overrides for fields in the package definition. | | `metadata` | `record` | An opaque record that can be set during installation. | The `package_status` object has the following schema: | Field | Type | Description | | :------------------- | :------- | :---------------------------------------------------------------------------------------------- | | `install_state` | `string` | The install state of this package. One of `installing`, `installed`, `removing` or `zombie`. | | `from_configuration` | `bool` | Whether the package was installed from the `package add` operator or from a configuration file. | ## Examples [Section titled “Examples”](#examples) ### Show all installed packages [Section titled “Show all installed packages”](#show-all-installed-packages) ```tql package::list ``` ```tql { "id": "suricata-ocsf", "name": "Suricata OCSF Mappings", "author": "Tenzir", "description": "[Suricata](https://suricata.io) is an open-source network monitor and\nthreat detection tool.\n\nThis package converts all Suricata events published on the topic `suricata` to\nOCSF and publishes the converted events on the topic `ocsf`.\n", "config": { "inputs": {}, "overrides": {} } } ``` ## See Also [Section titled “See Also”](#see-also) [`list`](/reference/operators/pipeline/list), [`package::add`](/reference/operators/package/add), [`package::remove`](/reference/operators/package/remove)

# package::remove

Uninstalls a package. ```tql package::remove package_id:string ``` ## Description [Section titled “Description”](#description) The `package::remove` operator uninstalls a previously installed package. ### `package_id : string` [Section titled “package\_id : string”](#package_id--string) The unique ID of the package as in the package definition. ## Examples [Section titled “Examples”](#examples) ### Remove an installed package [Section titled “Remove an installed package”](#remove-an-installed-package) ```tql package::remove "suricata-ocsf" ``` ## See Also [Section titled “See Also”](#see-also) [`package::add`](/reference/operators/package/add)

# partitions

Retrieves metadata about events stored at a node. ```tql partitions [predicate:expr] ``` ## Description [Section titled “Description”](#description) The `partitions` operator shows a summary of candidate partitions at a node. ### `predicate: expr (optional)` [Section titled “predicate: expr (optional)”](#predicate-expr-optional) Show only partitions which would be considered for pipelines of the form `export | where <expr>` instead of returning all data. ## Schemas [Section titled “Schemas”](#schemas) Tenzir emits partition information with the following schema: ### `tenzir.partition` [Section titled “tenzir.partition”](#tenzirpartition) Contains detailed information about a partition. | Field | Type | Description | | :---------------- | :------- | :----------------------------------------------------------------------------------- | | `uuid` | `string` | The unique ID of the partition in the UUIDv4 format. | | `memusage` | `uint64` | The memory usage of the partition in bytes. | | `diskusage` | `uint64` | The disk usage of the partition in bytes. | | `events` | `uint64` | The number of events contained in the partition. | | `min_import_time` | `time` | The time at which the first event of the partition arrived at the `import` operator. | | `max_import_time` | `time` | The time at which the last event of the partition arrived at the `import` operator. | | `version` | `uint64` | The version number of the internal partition storage format. | | `schema` | `string` | The schema name of the events contained in the partition. | | `schema_id` | `string` | A unique identifier for the physical layout of the partition. | | `store` | `record` | Resource information about the partition’s store. | | `indexes` | `record` | Resource information about the partition’s indexes. | | `sketches` | `record` | Resource information about the partition’s sketches. | The records `store`, `indexes`, and `sketches` have the following schema: | Field | Type | Description | | :----- | :------- | :------------------------ | | `url` | `string` | The URL of the resource. | | `size` | `uint64` | The size of the resource. | ## Examples [Section titled “Examples”](#examples) ### Get memory and disk requirements of all stored data [Section titled “Get memory and disk requirements of all stored data”](#get-memory-and-disk-requirements-of-all-stored-data) ```tql partitions summarize schema, events=sum(events), diskusage=sum(diskusage), memusage=sum(memusage) sort schema ``` ### Get an upper bound of events that have a field `src_ip` with 127.0.0.1 [Section titled “Get an upper bound of events that have a field src\_ip with 127.0.0.1”](#get-an-upper-bound-of-events-that-have-a-field-src_ip-with-127001) ```tql partitions src_ip == 127.0.0.1 summarize candidates=sum(events) ``` ### See how many partitions contain a non-null value for the field `hostname` [Section titled “See how many partitions contain a non-null value for the field hostname”](#see-how-many-partitions-contain-a-non-null-value-for-the-field-hostname) ```tql partitions hostname != null ```

# pass

Does nothing with the input. ```tql pass ``` ## Description [Section titled “Description”](#description) The `pass` operator relays the input without any modification. Outside of testing and debugging, it is only used when an empty pipeline needs to created, as `{}` is a record, while `{ pass }` is a pipeline. ## Examples [Section titled “Examples”](#examples) ### Forward the input without any changes [Section titled “Forward the input without any changes”](#forward-the-input-without-any-changes) ```tql pass ``` ### Do nothing every 10s [Section titled “Do nothing every 10s”](#do-nothing-every-10s) ```tql every 10s { pass } ```

# pipeline::activity

Summarizes the activity of pipelines. ```tql pipeline::activity range=duration, interval=duration ``` ## Description [Section titled “Description”](#description) Internal Operator This operator is only for internal usage by the Tenzir Platform. It can change without notice. ### `range = duration` [Section titled “range = duration”](#range--duration) The range for which the activity should be fetched. Note that the individual rates returned by this operator typically represent a larger range because they are aligned with the interval. ### `interval = duration` [Section titled “interval = duration”](#interval--duration) The interval used to summarize the individual throughout rates. Needs to be a multiple of the built-in storage interval, which is typically `10min`. Also needs to cleanly divide `range`. ## Schemas [Section titled “Schemas”](#schemas) ### `tenzir.activity` [Section titled “tenzir.activity”](#tenziractivity) | Field | Type | Description | | :---------- | :------------- | :-------------------------------------------------------- | | `first` | `time` | The time of the first throughput rate in the lists below. | | `last` | `time` | The time of the last throughput rate in the lists below. | | `pipelines` | `list<record>` | The activity for individual pipelines. | The records in `pipelines` have the following schema: | Field | Type | Description | | :-------- | :------- | :----------------------------------------------------------------- | | `id` | `string` | The ID uniquely identifying the pipeline this activity belongs to. | | `ingress` | `record` | The activity at the source of the pipeline. | | `egress` | `record` | The activity at the destination of the pipeline. | The records `ingress` and `egress` have the following schema: | Field | Type | Description | | :--------- | :------------- | :------------------------------------------------------- | | `internal` | `bool` | Whether this end of the pipeline is considered internal. | | `bytes` | `uint64` | The total number of bytes over the range. | | `rates` | `list<uint64>` | The throughput in bytes/second over time. | You can derive the time associated with a given throughput rate with the formula `first + index*interval`, except the last value, which is associated with `last`. The recommended way to chart these values is to show a sliding window over `[last - range, last]`. The value in `bytes` is an approximation for the total number of bytes inside that window. ## Examples [Section titled “Examples”](#examples) ### Show the activity over the last 20s [Section titled “Show the activity over the last 20s”](#show-the-activity-over-the-last-20s) ```tql pipeline::activity range=20s, interval=20s ``` ```tql { first: 2025-05-07T08:33:40.000Z, last: 2025-05-07T08:34:10.000Z, pipelines: [ { id: "3b43d497-5f4d-47f4-b191-5f432644d5ba", ingress: { internal: true, bytes: 289800, rates: [ 14490, 14490, 14490, ], }, egress: { internal: true, bytes: 292360, rates: [ 14721.75, 14514.25, 14488.8, ], }, }, ], } ```

# pipeline::detach

Starts a pipeline in the node. ```tql pipeline::detach { … }, [id=string] ``` ## Description [Section titled “Description”](#description) The `pipeline::detach` operator starts a hidden managed pipeline in the node, and returns as soon as the pipeline has started. Subject to Change This operator primarily exists for testing purposes, where it is often required to run pipelines in the background, but to be able to wait until the pipeline has started. The operator may change without further notice. ### `id = string (optional)` [Section titled “id = string (optional)”](#id--string-optional) Sets the pipeline’s ID explicitly, instead of assigning a random ID. This corresponds to the `id` field in the output of `pipeline::list`, and the `pipeline_id` field in the output of `metrics` and `diagnostics`. ## Examples [Section titled “Examples”](#examples) ### Run a pipeline in the background [Section titled “Run a pipeline in the background”](#run-a-pipeline-in-the-background) ```tql pipeline::detach { every 1min { version } select version write_lines save_stdout } ``` ## See also [Section titled “See also”](#see-also) [`run`](/reference/operators/pipeline/run)

# pipeline::list

Shows managed pipelines. ```tql pipeline::list ``` ## Description [Section titled “Description”](#description) The `pipeline::list` operator returns the list of all managed pipelines. Managed pipelines are pipelines created through the [`/pipeline` API](/reference/node/api), which includes all pipelines run through the Tenzir Platform. ## Examples [Section titled “Examples”](#examples) ### Count pipelines per state [Section titled “Count pipelines per state”](#count-pipelines-per-state) ```tql pipeline::list top state ``` ```tql { "state": "running", "count": 31 } { "state": "failed", "count": 4 } { "state": "stopped", "count": 2 } ``` ### Show pipelines per package [Section titled “Show pipelines per package”](#show-pipelines-per-package) ```tql pipeline::list summarize package, names=collect(name) ``` ```tql { "package": "suricata-ocsf", "names": [ "Suricata Flow to OCSF Network Activity", "Suricata DNS to OCSF DNS Activity", "Suricata SMB to OCSF SMB Activity", // … ] } ``` ## See Also [Section titled “See Also”](#see-also) [`list`](/reference/operators/package/list), [`run`](/reference/operators/pipeline/run)

# pipeline::run

Starts a pipeline in the node and waits for it to complete. ```tql pipeline::run { … }, [id=string] ``` ## Description [Section titled “Description”](#description) The `pipeline::run` operator starts a hidden managed pipeline in the node, and returns when the pipeline has finished. Note that pipelines may emit diagnostics after they have finished. Subject to Change This operator primarily exists for testing purposes, where it is often required to run pipelines with an explicitly specified pipeline id. ### `{ … }` [Section titled “{ … }”](#-) The pipeline to execute. This pipeline runs as a separate managed pipeline within the node. ### `id = string (optional)` [Section titled “id = string (optional)”](#id--string-optional) Sets the pipeline’s ID explicitly, instead of assigning a random ID. This corresponds to the `id` field in the output of `pipeline::list`, and the `pipeline_id` field in the output of `metrics` and `diagnostics`. ## Examples [Section titled “Examples”](#examples) ### Run a pipeline in the background and wait for it to complete [Section titled “Run a pipeline in the background and wait for it to complete”](#run-a-pipeline-in-the-background-and-wait-for-it-to-complete) ```tql pipeline::run { every 1min { version } select version write_lines save_stdout } ``` ## See Also [Section titled “See Also”](#see-also) [`pipeline::detach`](/reference/operators/pipeline/detach), [`pipeline::list`](/reference/operators/pipeline/list)

# plugins

Shows all available plugins and built-ins. ```tql plugins ``` ## Description [Section titled “Description”](#description) The `plugins` operator shows all available plugins and built-ins. Tenzir is built on a modular monolith architecture. Most features are available as plugins and extensible by developers. Tenzir comes with a set of built-ins and bundled plugins. The former use the plugin API but are available as part of the core library, and the latter are plugins shipped with Tenzir. ## Schemas [Section titled “Schemas”](#schemas) Tenzir emits plugin information with the following schema. ### `tenzir.plugin` [Section titled “tenzir.plugin”](#tenzirplugin) Contains detailed information about the available plugins. | Field | Type | Description | | :------------- | :------------- | :------------------------------------------------------------------------------------------ | | `name` | `string` | The unique, case-insensitive name of the plugin. | | `version` | `string` | The version identifier of the plugin, or `bundled` if the plugin has no version of its own. | | `kind` | `string` | The kind of plugin. One of `builtin`, `static`, or `dynamic`. | | `types` | `list<string>` | The interfaces implemented by the plugin, e.g., `operator` or `function`. | | `dependencies` | `list<string>` | Plugins that must be loaded for this plugin to function. | ## Examples [Section titled “Examples”](#examples) ### Show all currently available functions [Section titled “Show all currently available functions”](#show-all-currently-available-functions) ```tql plugins where "function" in types summarize functions=collect(name) ```

# processes

Shows a snapshot of running processes. ```tql processes ``` ## Description [Section titled “Description”](#description) The `processes` operator shows a snapshot of all currently running processes. ## Schemas [Section titled “Schemas”](#schemas) Tenzir emits process information with the following schema. ### `tenzir.process` [Section titled “tenzir.process”](#tenzirprocess) Contains detailed information about the process. | Field | Type | Description | | :------------- | :------------- | :----------------------------------------------------------- | | `name` | `string` | The process name. | | `command_line` | `list<string>` | The command line of the process. | | `pid` | `uint64` | The process identifier. | | `ppid` | `uint64` | The parent process identifier. | | `uid` | `uint64` | The user identifier of the process owner. | | `gid` | `uint64` | The group identifier of the process owner. | | `ruid` | `uint64` | The real user identifier of the process owner. | | `rgid` | `uint64` | The real group identifier of the process owner. | | `priority` | `string` | The priority level of the process. | | `startup` | `time` | The time when the process was started. | | `vsize` | `uint64` | The virtual memory size of the process. | | `rsize` | `uint64` | The resident set size (physical memory used) of the process. | | `swap` | `uint64` | The amount of swap memory used by the process. | | `peak_mem` | `uint64` | Peak memory usage of the process. | | `open_fds` | `uint64` | The number of open file descriptors by the process. | | `utime` | `duration` | The user CPU time consumed by the process. | | `stime` | `duration` | The system CPU time consumed by the process. | ## Examples [Section titled “Examples”](#examples) ### Show running processes by runtime [Section titled “Show running processes by runtime”](#show-running-processes-by-runtime) ```tql processes sort -startup ``` ### Show the top five running processes by name [Section titled “Show the top five running processes by name”](#show-the-top-five-running-processes-by-name) ```tql processes top name head 5 ``` ## See Also [Section titled “See Also”](#see-also) [`files`](/reference/operators/files), [`sockets`](/reference/operators/sockets)

# publish

Publishes events to a channel with a topic. ```tql publish [topic:string] ``` ## Description [Section titled “Description”](#description) The `publish` operator publishes events at a node in a channel with the specified topic. All [`subscribers`](/reference/operators/subscribe) of the channel operator receive the events immediately. ### `topic: string (optional)` [Section titled “topic: string (optional)”](#topic-string-optional) An optional topic for publishing events under. If unspecified, the operator publishes events to the topic `main`. ## Examples [Section titled “Examples”](#examples) ### Publish Zeek connection logs under the fixed topic `zeek` [Section titled “Publish Zeek connection logs under the fixed topic zeek”](#publish-zeek-connection-logs-under-the-fixed-topic-zeek) ```tql from "conn.log.gz" { decompress_gzip read_zeek_tsv } publish "zeek" ``` ### Publish Suricata events under a dynamic topic depending on their event type [Section titled “Publish Suricata events under a dynamic topic depending on their event type”](#publish-suricata-events-under-a-dynamic-topic-depending-on-their-event-type) ```tql from "eve.json" { read_suricata } publish f"suricata.{event_type}" ``` ## See Also [Section titled “See Also”](#see-also) [`import`](/reference/operators/import), [`subscribe`](/reference/operators/subscribe)

# python

Executes Python code against each event of the input. ```tql python code:string, [requirements=string] python file=string, [requirements=string] ``` ## Description [Section titled “Description”](#description) The `python` operator executes user-provided Python code against each event of the input. By default, the Tenzir node executing the pipeline creates a virtual environment into which the `tenzir` Python package is installed. This behavior can be turned off in the node configuration using the `plugin.python.create-venvs` boolean option. ### `code: string` [Section titled “code: string”](#code-string) The provided Python code describes an event-to-event transformation, i.e., it is executed once for each input event and produces exactly output event. An implicitly defined `self` variable represents the event. Modify it to alter the output of the operator. Fields of the event can be accessed with the dot notation. For example, if the input event contains fields `a` and `b` then the Python code can access and modify them using `self.a` and `self.b`. Similarly, new fields are added by assigning to `self.fieldname` and existing fields can be removed by deleting them from `self`. When new fields are added, it is required that the new field has the same type for every row of the event. ### `file: string` [Section titled “file: string”](#file-string) Instead of providing the code inline, the `file` option allows for passing a path to a file containing the code the operator executes per event. ### `requirements = string (optional)` [Section titled “requirements = string (optional)”](#requirements--string-optional) The `requirements` flag can be used to pass additional package dependencies in the pip format. When it is used, the argument is passed on to `pip install` in a dedicated virtual environment. The string is passed verbatim to `pip install`. To add multiple dependencies, separate them with a space: `requirements="foo bar"`. ## Secrets [Section titled “Secrets”](#secrets) By default, the `python` operator does not accept secrets. If you want to allow usage of secrets in the `code` argument, you can enable the configuration option `tenzir.allow-secrets-in-escape-hatches`. ## Examples [Section titled “Examples”](#examples) ### Insert or modify a field [Section titled “Insert or modify a field”](#insert-or-modify-a-field) Set field `x` to `"hello, world"` ```tql python "self.x = 'hello, world'" ``` ### Remove all fields from an event [Section titled “Remove all fields from an event”](#remove-all-fields-from-an-event) Clear the contents of `self` to remove the implicit input values from the output: ```tql python " self.clear() " ``` ### Insert a new field [Section titled “Insert a new field”](#insert-a-new-field) Define a new field `x` as the square root of the field `y`, and remove `y` from the output: ```tql python " import math self.x = math.sqrt(self.y) del self.y " ``` ### Make use of third party packages [Section titled “Make use of third party packages”](#make-use-of-third-party-packages) ```tql python r#" import requests requests.post("http://imaginary.api/receive", data=self) "#, requirements="requests=^2.30" ``` ## See Also [Section titled “See Also”](#see-also) [`shell`](/reference/operators/shell)

# rare

Shows the least common values. ```tql rare x:field ``` ## Description [Section titled “Description”](#description) Shows the least common values for a given field. For each unique value, a new event containing its count will be produced. In general, `rare x` is equivalent to: This operator is the dual to [`top`](/reference/operators/top). ```tql summarize x, count=count() sort count ``` ### `x: field` [Section titled “x: field”](#x-field) The name of the field to find the least common values for. ## Examples [Section titled “Examples”](#examples) ### Find the least common values [Section titled “Find the least common values”](#find-the-least-common-values) ```tql from {x: "B"}, {x: "A"}, {x: "A"}, {x: "B"}, {x: "A"}, {x: "D"}, {x: "C"}, {x: "C"} rare x ``` ```tql {x: "D", count: 1} {x: "C", count: 2} {x: "B", count: 2} {x: "A", count: 3} ``` ### Show the five least common values for `id.orig_h` [Section titled “Show the five least common values for id.orig\_h”](#show-the-five-least-common-values-for-idorig_h) ```tql rare id.orig_h head 5 ``` ## See Also [Section titled “See Also”](#see-also) [`summarize`](/reference/operators/summarize), [`sort`](/reference/operators/sort), [`top`](/reference/operators/top)

# read_all

Parses an incoming bytes stream into a single event. ```tql read_all [binary=bool] ``` ## Description [Section titled “Description”](#description) The `read_all` operator takes its input bytes and produces a single event that contains everything. This is useful if the entire stream is needed for further processing at once. The resulting events have a single field called `data`. ### `binary = bool (optional)` [Section titled “binary = bool (optional)”](#binary--bool-optional) Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`. ## Examples [Section titled “Examples”](#examples) ### Read an entire text file into a single event [Section titled “Read an entire text file into a single event”](#read-an-entire-text-file-into-a-single-event) ```tql load_file "data.txt" read_all ``` ```tql {data: "<file contents>"} ``` ### Read an entire binary file into a single event [Section titled “Read an entire binary file into a single event”](#read-an-entire-binary-file-into-a-single-event) ```tql load_file "data.bin" read_all binary=true ``` ```tql {data: b"<file contents>"} ``` ## See Also [Section titled “See Also”](#see-also) [`read_delimited`](/reference/operators/read_delimited), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_lines`](/reference/operators/read_lines),

# read_bitz

Parses bytes as *BITZ* format. ```tql read_bitz ``` ## Description [Section titled “Description”](#description) BITZ is short for **Bi**nary **T**en**z**ir and is our internal wire format. Use BITZ when you need high-throughput structured data exchange with minimal overhead. BITZ is a thin wrapper around Arrow’s record batches. That is, BITZ lays out data in a (compressed) columnar fashion that makes it conducive for analytical workloads. Since it’s padded and byte-aligned, it is portable and doesn’t induce any deserialization cost, making it suitable for write-once-read-many use cases. Internally, BITZ uses Arrow’s IPC format for serialization and deserialization, but prefixes each message with a 64 bit size prefix to support changing schemas between batches—something that Arrow’s IPC format does not support on its own. ## See Also [Section titled “See Also”](#see-also) [`read_bitz`](/reference/operators/read_bitz), [`read_feather`](/reference/operators/write_feather), [`read_parquet`](/reference/operators/write_parquet), [`write_bitz`](/reference/operators/write_bitz)

# read_cef

Parses an incoming Common Event Format (CEF) stream into events. ```tql read_cef [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) The [Common Event Format (CEF)](https://community.microfocus.com/cfs-file/__key/communityserver-wikis-components-files/00-00-00-00-23/3731.CommonEventFormatV25.pdf) is a text-based event format that originally stems from ArcSight. It is line-based and human readable. The first 7 fields of a CEF event are always the same, and the 8th *extension* field is an optional list of key-value pairs: ```plaintext CEF:Version|Device Vendor|Device Product|Device Version|Device Event Class ID|Name|Severity|[Extension] ``` Here is a real-world example: ```plaintext CEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8| externalId=6 clientId=2251997 scanGroupId=3 scanGroupName=Manually Installed Agents sev=High duser=tikasrv01\\administrator cat=END-POINT Alert dhost=TikaSrv01 src=172.31.5.93 filePath=c:\\windows\\temp\\javac.exe fname=javac.exe rt=3/30/2022 10:55:34 AM fileHash=2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD rtUtc=Mar 30 2022 10:55:34.688 dtUtc=Mar 30 2022 10:55:32.458 hostLS=2022-03-30 10:55:34 GMT+00:00 osVer=Windows Server 2016 Datacenter x64 1607 epsVer=4.5.5.6845 confVer=637842168250000000 prUser=tikasrv01\\administrator pParams="C:\\Windows\\Temp\\javac.exe" sign=Not signed pct=2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609 pFileHash=1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7 pprUser=tikasrv01\\administrator ppParams=C:\\Windows\\Explorer.EXE pssdeep=49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3 pSign=Signed and has certificate info gpFileHash=CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F gpprUser=tikasrv01\\administrator gpParams=C:\\Windows\\system32\\userinit.exe gpssdeep=384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu gpSign=Signed actRem=Kill, Rename ``` The [CEF specification](https://community.microfocus.com/cfs-file/__key/communityserver-wikis-components-files/00-00-00-00-23/3731.CommonEventFormatV25.pdf) pre-defines several extension field key names and data types for the corresponding values. Tenzir’s parser does not enforce the strict definitions and instead tries to infer the type from the provided values. Tenzir translates the `extension` field to a nested record, where the key-value pairs of the extensions map to record fields. Here is an example of the above event: Output (shortened) ```json { "cef_version": 0, "device_vendor": "Cynet", "device_product": "Cynet 360", "device_version": "4.5.4.22139", "signature_id": "0", "name": "Memory Pattern - Cobalt Strike Beacon ReflectiveLoader", "severity": "8", "extension": { "externalId": 6, "clientId": 2251997, "scanGroupId": 3, ... "gpssdeep": "384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu", "gpSign": "Signed", "actRem": "Kill, Rename" } } ``` ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_cef`](/reference/functions/parse_cef), [`print_cef`](/reference/functions/print_cef), [`read_leef`](/reference/operators/read_leef)

# read_csv

Read CSV (Comma-Separated Values) from a byte stream. ```tql read_csv [list_separator=string, null_value=string, comments=bool, header=string, quotes=string, auto_expand=bool, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) The `read_csv` operator transforms a byte stream into a event stream by parsing the bytes as [CSV](https://en.wikipedia.org/wiki/Comma-separated_values). ### `auto_expand = bool (optional)` [Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `comments = bool (optional)` [Section titled “comments = bool (optional)”](#comments--bool-optional) Treat lines beginning with ”#” as comments. ### `header = list<string>|string (optional)` [Section titled “header = list\<string>|string (optional)”](#header--liststringstring-optional) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header. ### `list_separator = string (optional)` [Section titled “list\_separator = string (optional)”](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `;`. ### `null_value = string (optional)` [Section titled “null\_value = string (optional)”](#null_value--string-optional) The `string` denoting an absent value. Defaults to empty string (`""`). ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Read a CSV file with header [Section titled “Read a CSV file with header”](#read-a-csv-file-with-header) input.csv ```txt message,count,ip some text,42,"1.1.1.1" more text,100,"1.1.1.2" ``` ```tql load "input.csv" read_csv ``` ```tql {message: "some text", count: 42, ip: 1.1.1.1} {message: "more text", count: 100, ip: 1.1.1.2} ``` ### Manually specify a header [Section titled “Manually specify a header”](#manually-specify-a-header) input\_no\_header.csv ```txt some text,42,"1.1.1.1" more text,100,"1.1.1.2" ``` ```tql load "input_no_header.csv" read_csv header="message,count,ip" ``` ```tql {message: "some text", count: 42, ip: 1.1.1.1} {message: "more text", count: 100, ip: 1.1.1.2} ``` ## See Also [Section titled “See Also”](#see-also) [`parse_csv`](/reference/functions/parse_csv), [`print_csv`](/reference/functions/print_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv), [`write_csv`](/reference/operators/write_csv)

# read_delimited

Parses an incoming bytes stream into events using a string as delimiter. ```tql read_delimited separator:string|blob, [binary=bool, include_separator=bool] ``` ## Description [Section titled “Description”](#description) The `read_delimited` operator takes its input bytes and splits it using the provided string as a delimiter. This is useful for parsing data that uses simple string delimiters instead of regular expressions or standard newlines. The resulting events have a single field called `data`. ### `separator: string|blob (required)` [Section titled “separator: string|blob (required)”](#separator-stringblob-required) The string or blob to use as delimiter. The operator will split the input whenever this exact sequence is matched. When a blob literal is provided (e.g., `b"\x00\x01"`), the `binary` option defaults to `true`. ### `binary = bool (optional)` [Section titled “binary = bool (optional)”](#binary--bool-optional) Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`. ### `include_separator = bool (optional)` [Section titled “include\_separator = bool (optional)”](#include_separator--bool-optional) When enabled, includes the matched separator string in the output events. By default, the separator is excluded from the results. ## Examples [Section titled “Examples”](#examples) ### Split on a simple delimiter [Section titled “Split on a simple delimiter”](#split-on-a-simple-delimiter) ```tql load_file "data.txt" read_delimited "||" ``` ### Parse CSV-like data with custom delimiter [Section titled “Parse CSV-like data with custom delimiter”](#parse-csv-like-data-with-custom-delimiter) ```tql load_file "custom.csv" read_delimited ";;;" ``` ### Include the separator in the output [Section titled “Include the separator in the output”](#include-the-separator-in-the-output) ```tql load_file "data.txt" read_delimited "||", include_separator=true ``` ### Parse binary data with blob delimiters [Section titled “Parse binary data with blob delimiters”](#parse-binary-data-with-blob-delimiters) ```tql load_file "binary.dat" read_delimited b"\x00\x01" ``` ### Use blob separator with include\_separator [Section titled “Use blob separator with include\_separator”](#use-blob-separator-with-include_separator) ```tql load_file "data.txt" read_delimited b"||", include_separator=true ``` ### Parse binary data with string delimiters [Section titled “Parse binary data with string delimiters”](#parse-binary-data-with-string-delimiters) ```tql load_file "binary.dat" read_delimited "\x00\x01", binary=true ``` ## See Also [Section titled “See Also”](#see-also) [`read_all`](/reference/operators/read_all), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_lines`](/reference/operators/read_lines), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_delimited_regex

Parses an incoming bytes stream into events using a regular expression as delimiter. ```tql read_delimited_regex regex:string|blob, [binary=bool, include_separator=bool] ``` ## Description [Section titled “Description”](#description) The `read_delimited_regex` operator takes its input bytes and splits it using the provided regular expression as a delimiter. This is useful for parsing data that uses custom delimiters or patterns instead of standard newlines. The regular expression flavor is Perl compatible and documented [here](https://www.boost.org/doc/libs/1_88_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html). The resulting events have a single field called `data`. ### `regex: string|blob (required)` [Section titled “regex: string|blob (required)”](#regex-stringblob-required) The regular expression pattern to use as delimiter. This can be provided as a string or blob literal. The operator will split the input whenever this pattern is matched. When a blob literal is provided (e.g., `b"\\x00\\x01"`), the `binary` option defaults to `true`. ### `binary = bool (optional)` [Section titled “binary = bool (optional)”](#binary--bool-optional) Treat the input as binary data instead of UTF-8 text. When enabled, invalid UTF-8 sequences will not cause warnings, and the resulting `data` field will be of type `blob` instead of `string`. ### `include_separator = bool (optional)` [Section titled “include\_separator = bool (optional)”](#include_separator--bool-optional) When enabled, includes the matched separator pattern in the output events. By default, the separator is excluded from the results. ## Examples [Section titled “Examples”](#examples) ### Split Syslog-like events without newline terminators from a TCP input [Section titled “Split Syslog-like events without newline terminators from a TCP input”](#split-syslog-like-events-without-newline-terminators-from-a-tcp-input) ```tql load_tcp "0.0.0.0:514" read_delimited_regex "(?=<[0-9]+>)" this = data.parse_syslog() ``` ### Parse log entries separated by timestamps [Section titled “Parse log entries separated by timestamps”](#parse-log-entries-separated-by-timestamps) ```tql load_file "application.log" read_delimited_regex "(?=\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})" ``` ### Split on multiple possible delimiters [Section titled “Split on multiple possible delimiters”](#split-on-multiple-possible-delimiters) ```tql load_file "mixed_delimiters.txt" read_delimited_regex "[;|]" ``` ### Include the separator in the output [Section titled “Include the separator in the output”](#include-the-separator-in-the-output) ```tql load_file "data.txt" read_delimited_regex "\\|\\|", include_separator=true ``` ### Parse binary data with blob patterns [Section titled “Parse binary data with blob patterns”](#parse-binary-data-with-blob-patterns) ```tql load_file "binary.dat" read_delimited_regex b"\\x00\\x01" ``` ### Use blob pattern with include\_separator for binary delimiters [Section titled “Use blob pattern with include\_separator for binary delimiters”](#use-blob-pattern-with-include_separator-for-binary-delimiters) ```tql load_file "protocol.dat" read_delimited_regex b"\\xFF\\xFE", include_separator=true ``` ## See Also [Section titled “See Also”](#see-also) [`read_all`](/reference/operators/read_all), [`read_delimited`](/reference/operators/read_delimited), [`read_lines`](/reference/operators/read_lines), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_feather

Parses an incoming Feather byte stream into events. ```tql read_feather ``` ## Description [Section titled “Description”](#description) Transforms the input [Feather](https://arrow.apache.org/docs/python/feather.html) (a thin wrapper around [Apache Arrow’s IPC](https://arrow.apache.org/docs/python/ipc.html) wire format) byte stream to event stream. ## Examples [Section titled “Examples”](#examples) ### Publish a feather logs file [Section titled “Publish a feather logs file”](#publish-a-feather-logs-file) ```tql load_file "logs.feather" read_feather pulish "log" ``` ## See Also [Section titled “See Also”](#see-also) [`read_parquet`](/reference/operators/read_parquet), [`write_feather`](/reference/operators/write_feather)

# read_gelf

Parses an incoming GELF stream into events. ```tql read_gelf [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) Parses an incoming [GELF](https://go2docs.graylog.org/current/getting_in_log_data/gelf.html) stream into events. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Read a GELF stream from a TCP socket [Section titled “Read a GELF stream from a TCP socket”](#read-a-gelf-stream-from-a-tcp-socket) ```tql load_tcp "0.0.0.0:54321" read_gelf ```

# read_grok

Parses lines of input with a grok pattern. ```tql read_grok pattern:string, [pattern_definitions=record|string, indexed_captures=bool, include_unnamed=bool, schema=string, selector=string, schema_only=bool, merge=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) `read_grok` uses a regular expression based parser similar to the [Logstash `grok` plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) in Elasticsearch. Tenzir ships with the same built-in patterns as Elasticsearch, found [here](https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns/ecs-v1). In short, `pattern` consists of replacement fields, that look like `%{SYNTAX[:SEMANTIC[:CONVERSION]]}`, where: * `SYNTAX` is a reference to a pattern, either built-in or user-defined through the `pattern_defintions` option. * `SEMANTIC` is an identifier that names the field in the parsed record. * `CONVERSION` is either `infer` (default), `string` (default with `raw=true`), `int`, or `float`. The supported regular expression syntax is the one supported by [Boost.Regex](https://www.boost.org/doc/libs/1_81_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html), which is effectively Perl-compatible. ### `pattern: string` [Section titled “pattern: string”](#pattern-string) The `grok` pattern used for matching. Must match the input in its entirety. ### `pattern_definitions = record|string (optional)` [Section titled “pattern\_definitions = record|string (optional)”](#pattern_definitions--recordstring-optional) New pattern definitions to use. This may be a record of the form ```tql { pattern_name: "pattern" } ``` For example, the built-in pattern `INT` would be defined as ```tql { INT: "(?:[+-]?(?:[0-9]+))" } ``` Alternatively, this may be a user-defined newline-delimited list of patterns, where a line starts with the pattern name, followed by a space, and the `grok`-pattern for that pattern. For example, the built-in pattern `INT` is defined as follows: ```plaintext INT (?:[+-]?(?:[0-9]+)) ``` ### `indexed_captures = bool (optional)` [Section titled “indexed\_captures = bool (optional)”](#indexed_captures--bool-optional) All subexpression captures are included in the output, with the `SEMANTIC` used as the field name if possible, and the capture index otherwise. ### `include_unnamed = bool (optional)` [Section titled “include\_unnamed = bool (optional)”](#include_unnamed--bool-optional) By default, only fields that were given a name with `SEMANTIC`, or with the regular expression named capture syntax `(?<name>...)` are included in the resulting record. With `include_unnamed=true`, replacement fields without a `SEMANTIC` are included in the output, using their `SYNTAX` value as the record field name. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Parse a fictional HTTP request log [Section titled “Parse a fictional HTTP request log”](#parse-a-fictional-http-request-log) ```tql // Input: 55.3.244.1 GET /index.html 15824 0.043 let $pattern = "%{IP:client} %{WORD} %{URIPATHPARAM:req} %{NUMBER:bytes} %{NUMBER:dur}" read_grok $pattern ``` ```tql { client: 55.3.244.1, req: "/index.html", bytes: 15824, dur: 0.043 } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_grok`](/reference/functions/parse_grok)

# read_json

Parses an incoming JSON stream into events. ```tql read_json [schema=string, selector=string, schema_only=bool, merge=bool, raw=bool, unflatten_separator=string, arrays_of_objects=bool] ``` ## Description [Section titled “Description”](#description) Parses an incoming JSON byte stream into events. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ### `arrays_of_objects = bool (optional)` [Section titled “arrays\_of\_objects = bool (optional)”](#arrays_of_objects--bool-optional) Default: `false`. Parse arrays of objects, with every object in the outermost arrays resulting in one event each. This is particularly useful when interfacing with REST APIs, which often yield large arrays of objects instead of newline-delimited JSON objects. ## Examples [Section titled “Examples”](#examples) ### Read a JSON file [Section titled “Read a JSON file”](#read-a-json-file) input.json ```json { "product": "Tenzir", "version.major": 4, "version.minor": 22 } { "product": "Tenzir", "version.major": 4, "version.minor": 21, "version.dirty": true } ``` Pipeline ```tql load_file "events.json" read_json unflatten="." ``` Output ```json { "product": "Tenzir", "version": { "major": 4, "minor": 22 } } { "product": "Tenzir", "version": { "major": 4, "minor": 21, "dirty": true } } ``` ### Read a JSON array [Section titled “Read a JSON array”](#read-a-json-array) [JA4+](https://ja4db.com/) provides fingerprints via a REST API, which returns a single JSON array. You can easily ingest this into Tenzir using Pipeline ```tql load "https://ja4db.com/api/read/" read_json arrays_of_objects=true ``` Example Output ```json { "application": "SemrushBot", "library": null, "device": null, "os": "Other", "user_agent_string": null, "certificate_authority": null, "observation_count": 449, "verified": false, "notes": null, "ja4_fingerprint": "t13d301000_01455d0db58d_5ac7197df9d2", "ja4_fingerprint_string": null, "ja4s_fingerprint": null, "ja4h_fingerprint": "ge11nn100000_c910c42e1704_e3b0c44298fc_e3b0c44298fc", "ja4x_fingerprint": null, "ja4t_fingerprint": null, "ja4ts_fingerprint": null, "ja4tscan_fingerprint": null }, { "application": null, "library": null, "device": "Epson Printer", "os": null, "user_agent_string": null, "certificate_authority": null, "observation_count": 1, "verified": true, "notes": null, "ja4_fingerprint": null, "ja4s_fingerprint": null, "ja4h_fingerprint": null, "ja4x_fingerprint": null, "ja4t_fingerprint": null, "ja4ts_fingerprint": null, "ja4tscan_fingerprint": "28960_2-4-8-1-3_1460_3_1-4-8-16" } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_json`](/reference/functions/parse_json), [`read_ndjson`](/reference/operators/read_ndjson)

# read_kv

Read Key-Value pairs from a byte stream. ```tql read_kv [field_split=string, value_split=string, merge=bool, raw=bool, quotes=string, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) The `read_kv` operator transforms a byte stream into a event stream by parsing the bytes as Key-Value pairs. Incoming strings are first split into fields according to `field_split`. This can be a regular expression. For example, the input `foo: bar, baz: 42` can be split into `foo: bar` and `baz: 42` with the regular expression `r",\s*"` (a comma, followed by any amount of whitespace) as the field splitter. Note that the matched separators are removed when splitting a string. Afterwards, the extracted fields are split into their key and value by `value_split`, which can again be a regular expression. In our example, `r":\s*"` could be used to split `foo: bar` into the key `foo` and its value `bar`, and similarly `baz: 42` into `baz` and `42`. The result would thus be `{"foo": "bar", "baz": 42}`. If the regex matches multiple substrings, only the first match is used. If no match is found, the “field” is considered an extension of the previous fields value. The supported regular expression syntax is [RE2](https://github.com/google/re2/wiki/Syntax). In particular, this means that lookahead `(?=...)` and lookbehind `(?<=...)` are not supported by `read_kv` at the moment. However, if the regular expression has a capture group, it is assumed that only the content of the capture group shall be used as the separator. This means that unsupported regular expressions such as `(?=foo)bar(?<=baz)` can be effectively expressed as `foo(bar)baz` instead. ### Quoted Values [Section titled “Quoted Values”](#quoted-values) The parser is aware of double-quotes (`"`). If the `field_split` or `value_split` are found within enclosing quotes, they are not considered matches. This means that both the key and the value may be enclosed in double-quotes. For example, given `field_split` `\s*,\s*` and `value_split` `=`, the input ```plaintext "key"="nested = value",key2="value, and more" ``` will parse as ```tql { key: "nested = value", key2: "value, and more", } ``` ### `field_split = string (optional)` [Section titled “field\_split = string (optional)”](#field_split--string-optional) The regular expression used to separate individual fields. Defaults to `r"\s"`. ### `value_split = string (optional)` [Section titled “value\_split = string (optional)”](#value_split--string-optional) The regular expression used to separate a key from its value. Defaults to `"="`. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Read comma-separated key-value pairs [Section titled “Read comma-separated key-value pairs”](#read-comma-separated-key-value-pairs) Input ```txt surname:"John Norman", family_name:Smith, date_of_birth: 1995-05-26 ``` ```tql read_kv field_split=r"\s*,\s*", value_split=r"\s*:\s*" ``` ```tql { surname: "John Norman", family_name: "Smith", date_of_birth: 1995-05-26, } ``` ### Extract key-value pairs with more complex rules [Section titled “Extract key-value pairs with more complex rules”](#extract-key-value-pairs-with-more-complex-rules) Input ```txt PATH: C:\foo INPUT_MESSAGE: hello world PATH: D:\bar VALUE: 42 INFO: Great ``` ```tql read_kv field_split=r"(\s+)[A-Z][A-Z_]+:", value_split=r":\s*" ``` ```tql { PATH: "C:\\foo", INPUT_MESSAGE: "hello world", } { PATH: "D:\\bar", VALUE: 42, INFO: "Great", } ``` This requires lookahead because not every whitespace acts as a field separator. Instead, we only want to split if the whitespace is followed by `[A-Z][A-Z_]+:`, i.e., at least two uppercase characters followed by a colon. We can express this as `"(\s+)[A-Z][A-Z_]+:"`, which yields `PATH: C:\foo` and `INPUT_MESSAGE: hello world`. We then split the key from its value with `":\s*"`. Since only the first match is used to split key and value, this leaves the path intact. ### Fields without a `value_split` [Section titled “Fields without a value\_split”](#fields-without-a-value_split) Input ```txt x=1 y=2 z=3 4 5 a=6 ``` ```tql read_kv ``` ```tql { x: 1, y: 2, z: "3 4 5", a: 6, } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_kv`](/reference/functions/parse_kv), [`write_kv`](/reference/operators/write_kv)

# read_leef

Parses an incoming [LEEF](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) stream into events. ```tql read_leef [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) The [Log Event Extended Format (LEEF)](https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components) is an event representation popularized by IBM QRadar. Many tools send LEEF over [Syslog](/reference/operators/read_syslog). LEEF is a line-based format and every line begins with a *header* that is followed by *attributes* in the form of key-value pairs. LEEF v1.0 defines 5 header fields and LEEF v2.0 has an additional field to customize the key-value pair separator, which can be a single character or the hex value prefixed by `0x` or `x`: ```plaintext LEEF:1.0|Vendor|Product|Version|EventID| LEEF:2.0|Vendor|Product|Version|EventID|DelimiterCharacter| ``` For LEEF v1.0, the tab (`\t`) character is hard-coded as attribute separator. Here are some real-world LEEF events: ```plaintext LEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1 dst=2.10.20.20 spt=1200 LEEF:2.0|Lancope|StealthWatch|1.0|41|^|src=10.0.1.8^dst=10.0.0.5^sev=5^srcPort=81^dstPort=21 ``` Tenzir translates the event attributes into a nested record, where the key-value pairs map to record fields. Here is an example of the parsed events from above: ```tql { leef_version: "1.0", vendor: "Microsoft", product_name: "MSExchange", product_version: "2016", event_class_id: "15345", attributes: { src: 10.50.1.1, dst: 2.10.20.20, spt: 1200, } } { leef_version: "2.0", vendor: "Lancope", product_name: "StealthWatch", product_version: "1.0", event_class_id: "41", attributes: { src: 10.0.1.8, dst: 10.0.0.5, sev: 5, srcPort: 81, dstPort: 21 } } ``` ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_leef`](/reference/functions/parse_leef), [`print_leef`](/reference/functions/print_leef), [`read_cef`](/reference/operators/read_cef), [`read_syslog`](/reference/operators/read_syslog), [`write_syslog`](/reference/operators/write_syslog)

# read_lines

Parses an incoming bytes stream into events. ```tql read_lines [skip_empty=bool, split_at_null=bool, split_at_regex=string] ``` ## Description [Section titled “Description”](#description) The `read_lines` operator takes its input bytes and splits it at a newline character. Newline characters include: * `\n` * `\r\n` The resulting events have a single field called `line`. ### `skip_empty = bool (optional)` [Section titled “skip\_empty = bool (optional)”](#skip_empty--bool-optional) Ignores empty lines in the input. ### `split_at_null = bool (optional)` [Section titled “split\_at\_null = bool (optional)”](#split_at_null--bool-optional) Deprecated This option is deprecated. Use [`read_delimited`](/reference/operators/read_delimited) instead. Use null byte (`\0`) as the delimiter instead of newline characters. ### `split_at_regex = string (optional)` [Section titled “split\_at\_regex = string (optional)”](#split_at_regex--string-optional) Deprecated This option is deprecated. Use [`read_delimited_regex`](/reference/operators/read_delimited_regex) instead. Use the specified regex as the delimiter instead of newline characters. The regex flavor is Perl compatible and documented [here](https://www.boost.org/doc/libs/1_88_0/libs/regex/doc/html/boost_regex/syntax/perl_syntax.html). ## Examples [Section titled “Examples”](#examples) ### Reads lines from a file [Section titled “Reads lines from a file”](#reads-lines-from-a-file) ```tql load_file "events.log" read_lines is_error = line.starts_with("error:") ``` ### Split Syslog-like events without newline terminators from a TCP input [Section titled “Split Syslog-like events without newline terminators from a TCP input”](#split-syslog-like-events-without-newline-terminators-from-a-tcp-input) Consider using [`read_delimited_regex`](/reference/operators/read_delimited_regex) for regex-based splitting: ```tql load_tcp "0.0.0.0:514" read_delimited_regex "(?=<[0-9]+>)" this = line.parse_syslog() ``` ```tql load_tcp "0.0.0.0:514" read_lines split_at_regex="(?=<[0-9]+>)" this = line.parse_syslog() ``` ## See Also [Section titled “See Also”](#see-also) [`read_all`](/reference/operators/read_all), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv), [`read_delimited_regex`](/reference/operators/read_delimited_regex), [`read_xsv`](/reference/operators/read_xsv), [`write_lines`](/reference/operators/write_lines)

# read_ndjson

Parses an incoming NDJSON (newline-delimited JSON) stream into events. ```tql read_ndjson [schema=string, selector=string, schema_only=bool, merge=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) Parses an incoming NDJSON byte stream into events. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Read a newline-delimited JSON file [Section titled “Read a newline-delimited JSON file”](#read-a-newline-delimited-json-file) versions.json ```json {"product": "Tenzir", "version.major": 4, "version.minor": 22} {"product": "Tenzir", "version.major": 4, "version.minor": 21} ``` ```tql load_file "versions.json" read_ndjson unflatten="." ``` ```tql { product: "Tenzir", version: { major: 4, minor: 22, } } { product: "Tenzir", version: { major: 4, minor: 21, } } ``` ## See Also [Section titled “See Also”](#see-also) [`read_json`](/reference/operators/read_json)

# read_parquet

Reads events from a Parquet byte stream. ```tql read_parquet ``` ## Description [Section titled “Description”](#description) Reads events from a [Parquet](https://parquet.apache.org/) byte stream. [Apache Parquet](https://parquet.apache.org/) is a columnar storage format that a variety of data tools support. Limitation Tenzir currently assumes that all Parquet files use metadata recognized by Tenzir. We plan to lift this restriction in the future. ## Examples [Section titled “Examples”](#examples) Read a Parquet file: ```tql load_file "/tmp/data.prq", mmap=true read_parquet ``` ## See Also [Section titled “See Also”](#see-also) [`read_feather`](/reference/operators/read_feather), [`to_hive`](/reference/operators/to_hive), [`write_parquet`](/reference/operators/write_parquet)

# read_pcap

Reads raw network packets in PCAP file format. ```tql read_pcap [emit_file_headers=bool] ``` ## Description [Section titled “Description”](#description) The `read_pcap` operator converts raw bytes representing a [PCAP](https://datatracker.ietf.org/doc/id/draft-gharris-opsawg-pcap-00.html) file into events. ### `emit_file_headers = bool (optional)` [Section titled “emit\_file\_headers = bool (optional)”](#emit_file_headers--bool-optional) Emit a `pcap.file_header` event that represents the PCAP file header. If present, the parser injects this additional event before the subsequent stream of packets. Emitting this extra event makes it possible to seed the [`write_pcap`](/reference/operators/write_pcap) operator with a file header from the input. This allows for controlling the timestamp formatting (microseconds vs. nanosecond granularity) and byte order in the packet headers. When the PCAP parser processes a concatenated stream of PCAP files, specifying `emit_file_headers` will also re-emit every intermediate file header as separate event. Use this option when you would like to reproduce the identical trace file layout of the PCAP input. ## Schemas [Section titled “Schemas”](#schemas) The operator emits events with the following schema. ### `pcap.packet` [Section titled “pcap.packet”](#pcappacket) Contains information about all accessed API endpoints, emitted once per second. | Field | Type | Description | | :----------------------- | :------- | :------------------------------------ | | `timestamp` | `time` | The time of capturing the packet. | | `linktype` | `uint64` | The linktype of the captured packet. | | `original_packet_length` | `uint64` | The length of the original packet. | | `captured_packet_length` | `uint64` | The length of the captured packet. | | `data` | `blob` | The captured packet’s data as a blob. | ## Examples [Section titled “Examples”](#examples) ### Read packets from a PCAP file [Section titled “Read packets from a PCAP file”](#read-packets-from-a-pcap-file) ```tql load_file "/tmp/trace.pcap" read_pcap ``` ### Read packets from the [network interface](/reference/operators/load_nic) `eth0` [Section titled “Read packets from the network interface eth0”](#read-packets-from-the-network-interface-eth0) ```tql load_nic "eth0" read_pcap ``` ## See Also [Section titled “See Also”](#see-also) [`load_nic`](/reference/operators/load_nic), [`write_pcap`](/reference/operators/write_pcap)

# read_ssv

Read SSV (Space-Separated Values) from a byte stream. ```tql read_ssv [list_separator=string, null_value=string, comments=bool, header=string, quotes=string, auto_expand=bool, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) The `read_ssv` operator transforms a byte stream into a event stream by parsing the bytes as SSV. ### `auto_expand = bool (optional)` [Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `comments = bool (optional)` [Section titled “comments = bool (optional)”](#comments--bool-optional) Treat lines beginning with ”#” as comments. ### `header = list<string>|string (optional)` [Section titled “header = list\<string>|string (optional)”](#header--liststringstring-optional) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header. ### `list_separator = string (optional)` [Section titled “list\_separator = string (optional)”](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `,`. ### `null_value = string (optional)` [Section titled “null\_value = string (optional)”](#null_value--string-optional) The `string` denoting an absent value. Defaults to `-`. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Parse an SSV file [Section titled “Parse an SSV file”](#parse-an-ssv-file) input.ssv ```txt message count ip text 42 "1.1.1.1" "longer string" 100 1.1.1.2 ``` ```tql load "input.ssv" read_ssv ``` ```tql {message: "text", count: 42, ip: 1.1.1.1} {message: "longer string", count: 100, ip: 1.1.1.2} ``` ## See Also [Section titled “See Also”](#see-also) [`parse_ssv`](/reference/functions/parse_ssv), [`read_csv`](/reference/operators/read_csv), [`read_tsv`](/reference/operators/read_tsv), [`read_xsv`](/reference/operators/read_xsv)

# read_suricata

Parse an incoming [Suricata EVE JSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html) stream into events. ```tql read_suricata [schema_only=bool, raw=bool] ``` ## Description [Section titled “Description”](#description) The [Suricata](https://suricata.io) network security monitor converts network traffic into a stream of metadata events and provides a rule matching engine to generate alerts. Suricata emits events in the [EVE JSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html) format. The output is a single stream of events where the `event_type` field disambiguates the event type. Tenzir’s [`JSON`](/reference/operators/read_json) can handle EVE JSON correctly, but for the schema names to match the value from the `event_type` field, you need to pass the option `selector=event_type:suricata`. The `suricata` parser does this by default. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. This means that JSON numbers will be parsed as numbers, but every JSON string remains a string, unless the field is in the `schema`. ## Examples [Section titled “Examples”](#examples) ### Parse a Suricata EVE JSON log file [Section titled “Parse a Suricata EVE JSON log file”](#parse-a-suricata-eve-json-log-file) Here’s an `eve.log` sample: ```json {"timestamp":"2011-08-12T14:52:57.716360+0200","flow_id":1031464864740687,"pcap_cnt":83,"event_type":"alert","src_ip":"147.32.84.165","src_port":1181,"dest_ip":"78.40.125.4","dest_port":6667,"proto":"TCP","alert":{"action":"allowed","gid":1,"signature_id":2017318,"rev":4,"signature":"ET CURRENT_EVENTS SUSPICIOUS IRC - PRIVMSG *.(exe|tar|tgz|zip) download command","category":"Potentially Bad Traffic","severity":2},"flow":{"pkts_toserver":27,"pkts_toclient":35,"bytes_toserver":2302,"bytes_toclient":4520,"start":"2011-08-12T14:47:24.357711+0200"},"payload":"UFJJVk1TRyAjemFyYXNhNDggOiBzbXNzLmV4ZSAoMzY4KQ0K","payload_printable":"PRIVMSG #zarasa48 : smss.exe (368)\r\n","stream":0,"packet":"AB5J2xnDCAAntbcZCABFAABMGV5AAIAGLlyTIFSlTih9BASdGgvw0QvAxUWHdVAY+rCL4gAAUFJJVk1TRyAjemFyYXNhNDggOiBzbXNzLmV4ZSAoMzY4KQ0K","packet_info":{"linktype":1}} {"timestamp":"2011-08-12T14:55:22.154618+0200","flow_id":2247896271051770,"pcap_cnt":775,"event_type":"dns","src_ip":"147.32.84.165","src_port":1141,"dest_ip":"147.32.80.9","dest_port":53,"proto":"UDP","dns":{"type":"query","id":553,"rrname":"irc.freenode.net","rrtype":"A","tx_id":0}} {"timestamp":"2011-08-12T16:59:22.181050+0200","flow_id":472067367468746,"pcap_cnt":25767,"event_type":"fileinfo","src_ip":"74.207.254.18","src_port":80,"dest_ip":"147.32.84.165","dest_port":1046,"proto":"TCP","http":{"hostname":"www.nmap.org","url":"/","http_user_agent":"Mozilla/4.0 (compatible)","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":301,"redirect":"http://nmap.org/","length":301},"app_proto":"http","fileinfo":{"filename":"/","magic":"HTML document, ASCII text","gaps":false,"state":"CLOSED","md5":"70041821acf87389e40ddcb092004184","sha1":"10395ab3566395ca050232d2c1a0dbad69eb5fd2","sha256":"2e4c462b3424afcc04f43429d5f001e4ef9a28143bfeefb9af2254b4df3a7c1a","stored":true,"file_id":1,"size":301,"tx_id":0}} ``` Import it as follows: ```tql read_file "eve.log" read_suricata import ``` ### Read Suricata EVE JSON from a Unix domain socket [Section titled “Read Suricata EVE JSON from a Unix domain socket”](#read-suricata-eve-json-from-a-unix-domain-socket) Instead of writing to a file, Suricata can also log to a Unix domain socket that Tenzir can then read from. This saves a filesystem round-trip. This requires the following settings in your `suricata.yaml`: ```yaml outputs: - eve-log: enabled: yes filetype: unix_stream filename: eve.sock ``` Suricata creates `eve.sock` upon startup. Thereafter, you can read from the socket: ```tql load_file "eve.sock" read_suricata ```

# read_syslog

Parses an incoming Syslog stream into events. ```tql read_syslog [octet_counting=bool, merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) [Syslog](https://en.wikipedia.org/wiki/Syslog) is a standard format for message logging. Tenzir supports reading syslog messages in both the standardized “Syslog Protocol” format ([RFC 5424](https://tools.ietf.org/html/rfc5424)), and the older “BSD syslog Protocol” format ([RFC 3164](https://tools.ietf.org/html/rfc3164)). Depending on the syslog format, the result can be different. Here’s an example of a syslog message in RFC 5424 format: ```plaintext <165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource="Application" eventID="1011"] Event log entry ``` With this input, the parser will produce the following output, with the schema name `syslog.rfc5424`: ```tql { input: "<165>8 2023-10-11T22:14:15.003Z mymachineexamplecom evntslog 1370 ID47 [exampleSDID@32473 eventSource=\"Application\" eventID=\"1011\"] Event log entry", output: { facility: 20, severity: 5, version: 8, timestamp: 2023-10-11T22:14:15.003Z, hostname: "mymachineexamplecom", app_name: "evntslog", process_id: "1370", message_id: "ID47", structured_data: { "exampleSDID@32473": { eventSource: "Application", eventID: 1011, }, }, message: "Event log entry", }, } ``` Here’s an example of a syslog message in RFC 3164 format: ```plaintext <34>Nov 16 14:55:56 mymachine PROGRAM: Freeform message ``` With this input, the parser will produce the following output, with the schema name `syslog.rfc3164`: ```json { "facility": 4, "severity": 2, "timestamp": "Nov 16 14:55:56", "hostname": "mymachine", "app_name": "PROGRAM", "process_id": null, "content": "Freeform message" } ``` ### `octet_counting = bool (optional)` [Section titled “octet\_counting = bool (optional)”](#octet_counting--bool-optional) Employs “octet counting” according to [RFC6587](https://datatracker.ietf.org/doc/html/rfc6587#section-3.4.1) to determine message boundaries instead of the parsing heuristic for multi-line messages. Defaults to `false`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Read in the `auth.log` [Section titled “Read in the auth.log”](#read-in-the-authlog) Pipeline ```tql load_file "/var/log/auth.log" read_syslog ``` ```tql { facility: null, severity: null, timestamp: 2024-10-14T07:15:01.348027, hostname: "tenzirs-magic-machine", app_name: "CRON", process_id: "895756", content: "pam_unix(cron:session): session opened for user root(uid=0) by root(uid=0)", } { facility: null, severity: null, timestamp: 2024-10-14T07:15:01.349838, hostname: "tenzirs-magic-machine", app_name: "CRON", process_id: "895756", content: "pam_unix(cron:session): session closed for user root" } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_syslog`](/reference/functions/parse_syslog), [`write_syslog`](/reference/operators/write_syslog)

# read_tsv

Read TSV (Tab-Separated Values) from a byte stream. ```tql read_tsv [list_separator=string, null_value=string, comments=bool, header=string, quotes=string, auto_expand=bool, schema=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) The `read_tsv` operator transforms a byte stream into a event stream by parsing the bytes as [TSV](https://en.wikipedia.org/wiki/Tab-separated_values). ### `auto_expand = bool (optional)` [Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `comments = bool (optional)` [Section titled “comments = bool (optional)”](#comments--bool-optional) Treat lines beginning with ”#” as comments. ### `header = list<string>|string (optional)` [Section titled “header = list\<string>|string (optional)”](#header--liststringstring-optional) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header. ### `list_separator = string (optional)` [Section titled “list\_separator = string (optional)”](#list_separator--string-optional) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. Defaults to `,`. ### `null_value = string (optional)` [Section titled “null\_value = string (optional)”](#null_value--string-optional) The `string` denoting an absent value. Defaults to `-`. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Parse a TSV file [Section titled “Parse a TSV file”](#parse-a-tsv-file) input.tsv ```txt message count ip text 42 "1.1.1.1" "longer string" 100 "1.1.1.2" ``` ```tql load "input.tsv" read_tsv ``` ```tql {message: "text", count: 42, ip: 1.1.1.1} {message: "longer string", count: 100, ip: 1.1.1.2} ``` ## See Also [Section titled “See Also”](#see-also) [`parse_tsv`](/reference/functions/parse_tsv), [`read_csv`](/reference/operators/read_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_xsv`](/reference/operators/read_xsv)

# read_xsv

Read XSV from a byte stream. ```tql read_xsv field_separator=string, list_separator=string, null_value=string, [comments=bool, header=string, auto_expand=bool, quotes=string, selector=string, schema_only=bool, raw=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) The `read_xsv` operator transforms a byte stream into a event stream by parsing the bytes as [XSV](https://en.wikipedia.org/wiki/Delimiter-separated_values), a generalization of CSV with a more flexible separator specification. The following table lists existing XSV configurations: | Format | Field Separator | List Separator | Null Value | | -------------------------------------- | :-------------: | :------------: | :--------: | | [`csv`](/reference/operators/read_csv) | `,` | `;` | empty | | [`ssv`](/reference/operators/read_ssv) | `<space>` | `,` | `-` | | [`tsv`](/reference/operators/read_tsv) | `\t` | `,` | `-` | ### `field_separator = string` [Section titled “field\_separator = string”](#field_separator--string) The string separating different fields. ### `list_separator = string` [Section titled “list\_separator = string”](#list_separator--string) The `string` separating the elements *inside* a list. If this string is found outside of quotes in a field, that field will become a list. If this string is empty, list parsing is disabled. ### `null_value = string` [Section titled “null\_value = string”](#null_value--string) The string denoting an absent value. ### `auto_expand = bool (optional)` [Section titled “auto\_expand = bool (optional)”](#auto_expand--bool-optional) Automatically add fields to the schema when encountering events with too many values instead of dropping the excess values. ### `comments = bool (optional)` [Section titled “comments = bool (optional)”](#comments--bool-optional) Treat lines beginning with `#` as comments. ### `header = list<string>|string (optional)` [Section titled “header = list\<string>|string (optional)”](#header--liststringstring-optional) A list of strings to be used as the column names, or a `string` to be parsed as the `header` for the parsed values. If unspecified, the first line of the input is used as the header. ### `quotes = string (optional)` [Section titled “quotes = string (optional)”](#quotes--string-optional) A string of not escaped characters that are supposed to be considered as quotes. Defaults to the characters `"'`. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_xsv`](/reference/functions/parse_xsv), [`read_csv`](/reference/operators/read_csv), [`read_ssv`](/reference/operators/read_ssv), [`read_tsv`](/reference/operators/read_tsv)

# read_yaml

Parses an incoming YAML stream into events. ```tql read_yaml [merge=bool, raw=bool, schema=string, selector=string, schema_only=bool, unflatten_separator=string] ``` ## Description [Section titled “Description”](#description) Parses an incoming [YAML](https://en.wikipedia.org/wiki/YAML) stream into events. ### `merge = bool (optional)` Merges all incoming events into a single schema\* that converges over time. This option is usually the fastest *for reading* highly heterogeneous data, but can lead to huge schemas filled with nulls and imprecise results. Use with caution. \*: In selector mode, only events with the same selector are merged. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. ### `schema = string (optional)` Provide the name of a schema to be used by the parser. If a schema with a matching name is installed, the result will always have all fields from that schema. * Fields that are specified in the schema, but did not appear in the input will be null. * Fields that appear in the input, but not in the schema will also be kept. Use `schema_only=true` to reject fields that are not in the schema. If the given schema does not exist, this option instead assigns the output schema name only. The `schema` option is incompatible with the `selector` option. ### `selector = string (optional)` Designates a field value as schema name with an optional dot-separated prefix. The string is parsed as `<fieldname>[:<prefix>]`. The `prefix` is optional and will be prepended to the field value to generate the schema name. For example, the Suricata EVE JSON format includes a field `event_type` that contains the event type. Setting the selector to `event_type:suricata` causes an event with the value `flow` for the field `event_type` to map onto the schema `suricata.flow`. The `selector` option is incompatible with the `schema` option. ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. If the schema name is obtained via a `selector` and it does not exist, this has no effect. This option requires either `schema` or `selector` to be set. ### `unflatten_separator = string (optional)` A delimiter that, if present in keys, causes values to be treated as values of nested records. A popular example of this is the [Zeek JSON](/reference/operators/read_zeek_json) format. It includes the fields `id.orig_h`, `id.orig_p`, `id.resp_h`, and `id.resp_p` at the top-level. The data is best modeled as an `id` record with four nested fields `orig_h`, `orig_p`, `resp_h`, and `resp_p`. Without an unflatten separator, the data looks like this: Without unflattening ```json { "id.orig_h": "1.1.1.1", "id.orig_p": 10, "id.resp_h": "1.1.1.2", "id.resp_p": 5 } ``` With the unflatten separator set to `.`, Tenzir reads the events like this: With 'unflatten' ```json { "id": { "orig_h": "1.1.1.1", "orig_p": 10, "resp_h": "1.1.1.2", "resp_p": 5 } } ``` ## Examples [Section titled “Examples”](#examples) ### Parse a YAML file [Section titled “Parse a YAML file”](#parse-a-yaml-file) input.yaml ```yaml --- name: yaml version: bundled kind: builtin types: - parser - printer dependencies: [] ... ``` ```tql load_file "input.yaml" read_yaml ``` ```tql { name: "yaml", version: "bundled", kind: "builtin", types: [ "parser", "printer", ], dependencies: [], } ``` *** ## title: See Also [Section titled “title: See Also”](#title-see-also) [`parse_yaml`](/reference/functions/parse_yaml), [`print_yaml`](/reference/functions/print_yaml), [`write_yaml`](/reference/operators/write_yaml)

# read_zeek_json

Parse an incoming Zeek JSON stream into events. ```tql read_zeek_json [schema_only=bool, raw=bool] ``` ## Description [Section titled “Description”](#description) ### `schema_only = bool (optional)` When working with an existing schema, this option will ensure that the output schema has *only* the fields from that schema. ### `raw = bool (optional)` Use only the raw types that are native to the parsed format. Fields that have a type specified in the chosen `schema` will still be parsed according to the schema. This means that JSON numbers will be parsed as numbers, but every JSON string remains a string, unless the field is in the `schema`. ## Examples [Section titled “Examples”](#examples) ### Load a Zeek connection log [Section titled “Load a Zeek connection log”](#load-a-zeek-connection-log) zeek.json ```json {"__name":"sensor_10_0_0_2","_write_ts":"2020-02-26T04:00:03.734769Z","ts":"2020-02-26T03:40:03.724911Z","uid":"Cx3bf12iVwo5m7Gkd1","id.orig_h":"193.10.255.99","id.orig_p":6667,"id.resp_h":"141.9.40.50","id.resp_p":21,"proto":"tcp","duration":1196.975041,"orig_bytes":0,"resp_bytes":0,"conn_state":"S1","local_orig":false,"local_resp":true,"missed_bytes":0,"history":"Sh","orig_pkts":194,"orig_ip_bytes":7760,"resp_pkts":191,"resp_ip_bytes":8404} {"_path":"_0_0_2","_write_ts":"2020-02-11T03:48:57.477193Z","ts":"2020-02-11T03:48:57.477193Z","uid":"Cpk0Nl33Zb5ZWLP1tc","id.orig_h":"185.100.59.59","id.orig_p":6667,"id.resp_h":"141.9.255.157","id.resp_p":8080,"proto":"tcp","note":"LongConnection::found","msg":"185.100.59.59 -> 141.9.255.157:8080/tcp remained alive for longer than 19m55s","sub":"1194.62","src":"185.100.59.59","dst":"141.9.255.157","p":8080,"peer_descr":"worker-02","actions":["Notice::ACTION_LOG"],"suppress_for":3600} ``` ```tql load "zeek.json" read_zeek_json ``` ```tql { __name: "sensor_10_0_0_2", _write_ts: 2020-02-26T04:00:03.734769, ts: 2020-02-26T03:40:03.724911, uid: "Cx3bf12iVwo5m7Gkd1", id: { orig_h: 193.10.255.99, orig_p: 6667, resp_h: 141.9.40.50, resp_p: 21, }, proto: "tcp", duration: 1196.975041, orig_bytes: 0, resp_bytes: 0, conn_state: "S1", local_orig: false, local_resp: true, missed_bytes: 0, history: "Sh", orig_pkts: 194, orig_ip_bytes: 7760, resp_pkts: 191, resp_ip_bytes: 8404, } { _write_ts: 2020-02-11T03:48:57.477193, ts: 2020-02-11T03:48:57.477193, uid: "Cpk0Nl33Zb5ZWLP1tc", id: { orig_h: 185.100.59.59, orig_p: 6667, resp_h: 141.9.255.157, resp_p: 8080, }, proto: "tcp", _path: "_0_0_2", note: "LongConnection::found", msg: "185.100.59.59 -> 141.9.255.157:8080/tcp remained alive for longer than 19m55s", sub: "1194.62", src: 185.100.59.59, dst: 141.9.255.157, p: 8080, peer_descr: "worker-02", actions: [ Notice::ACTION_LOG ], suppress_for: 3600, } ``` ## See Also [Section titled “See Also”](#see-also) [`read_zeek_tsv`](/reference/operators/read_zeek_tsv), [`write_zeek_tsv`](/reference/operators/write_zeek_tsv)

# read_zeek_tsv

Parses an incoming `Zeek TSV` stream into events. ```tql read_zeek_tsv ``` ## Description [Section titled “Description”](#description) The [Zeek](https://zeek.org) network security monitor comes with its own tab-separated value (TSV) format for representing logs. This format includes additional header fields with field names, type annotations, and additional metadata. The `read_zeek_tsv` operator processes this metadata to extract a schema for the subsequent log entries. The Zeek types `count`, `real`, and `addr` map to the respective Tenzir types `uint64`, `double`, and `ip`. Here’s an example of a typical Zeek `conn.log` in TSV form: ```txt #separator \x09 #set_separator , #empty_field (empty) #unset_field - #path conn #open 2014-05-23-18-02-04 #fields ts uid id.orig_h id.orig_p id.resp_h id.resp_p proto service duration …orig_bytes resp_bytes conn_state local_orig missed_bytes history orig_pkts …orig_ip_bytes resp_pkts resp_ip_bytes tunnel_parents #types time string addr port addr port enum string interval count coun…t string bool count string count count count count table[string] 1258531221.486539 Pii6cUUq1v4 192.168.1.102 68 192.168.1.1 67 udp - 0.163820 …301 300 SF - 0 Dd 1 329 1 328 (empty) 1258531680.237254 nkCxlvNN8pi 192.168.1.103 137 192.168.1.255 137 udp dns 3.7801…25 350 0 S0 - 0 D 7 546 0 0 (empty) 1258531693.816224 9VdICMMnxQ7 192.168.1.102 137 192.168.1.255 137 udp dns 3.7486…47 350 0 S0 - 0 D 7 546 0 0 (empty) 1258531635.800933 bEgBnkI31Vf 192.168.1.103 138 192.168.1.255 138 udp - 46.72538…0 560 0 S0 - 0 D 3 644 0 0 (empty) 1258531693.825212 Ol4qkvXOksc 192.168.1.102 138 192.168.1.255 138 udp - 2.248589… 348 0 S0 - 0 D 2 404 0 0 (empty) 1258531803.872834 kmnBNBtl96d 192.168.1.104 137 192.168.1.255 137 udp dns 3.7488…93 350 0 S0 - 0 D 7 546 0 0 (empty) 1258531747.077012 CFIX6YVTFp2 192.168.1.104 138 192.168.1.255 138 udp - 59.05289…8 549 0 S0 - 0 D 3 633 0 0 (empty) 1258531924.321413 KlF6tbPUSQ1 192.168.1.103 68 192.168.1.1 67 udp - 0.044779 …303 300 SF - 0 Dd 1 331 1 328 (empty) 1258531939.613071 tP3DM6npTdj 192.168.1.102 138 192.168.1.255 138 udp - - - - S0… - 0 D 1 229 0 0 (empty) 1258532046.693816 Jb4jIDToo77 192.168.1.104 68 192.168.1.1 67 udp - 0.002103 …311 300 SF - 0 Dd 1 339 1 328 (empty) 1258532143.457078 xvWLhxgUmj5 192.168.1.102 1170 192.168.1.1 53 udp dns 0.0685…11 36 215 SF - 0 Dd 1 64 1 243 (empty) 1258532203.657268 feNcvrZfDbf 192.168.1.104 1174 192.168.1.1 53 udp dns 0.1709…62 36 215 SF - 0 Dd 1 64 1 243 (empty) 1258532331.365294 aLsTcZJHAwa 192.168.1.1 5353 224.0.0.251 5353 udp dns 0.1003…81 273 0 S0 - 0 D 2 329 0 0 (empty) ``` ## Examples [Section titled “Examples”](#examples) ### Read a Zeek connection log from a file [Section titled “Read a Zeek connection log from a file”](#read-a-zeek-connection-log-from-a-file) ```tql load_file "/tmp/conn.log" read_zeek_tsv ``` ## See Also [Section titled “See Also”](#see-also) [`read_zeek_json`](/reference/operators/read_zeek_json), [`write_zeek_tsv`](/reference/operators/write_zeek_tsv)

# remote

Forces a pipeline to run remotely at a node. ```tql remote { … } ``` ## Description [Section titled “Description”](#description) The `remote` operator takes a pipeline as an argument and forces it to run at a Tenzir Node. This operator has no effect when running a pipeline through the API or Tenzir Platform. ## Examples [Section titled “Examples”](#examples) ### Get the version of a node [Section titled “Get the version of a node”](#get-the-version-of-a-node) ```tql remote { version } ``` ## See Also [Section titled “See Also”](#see-also) [`local`](/reference/operators/local)

# repeat

Repeats the input a number of times. ```tql repeat [count:int] ``` ## Description [Section titled “Description”](#description) The `repeat` operator relays the input without any modification, and repeats its inputs a specified number of times. It is primarily used for testing and when working with generated data. ### `count: int (optional)` [Section titled “count: int (optional)”](#count-int-optional) The number of times to repeat the input data. If not specified, the operator repeats its input indefinitely. ## Examples [Section titled “Examples”](#examples) ### Repeat input indefinitely [Section titled “Repeat input indefinitely”](#repeat-input-indefinitely) Given the following events: ```tql {number: 1, "text": "one"} {number: 2, "text": "two"} ``` The `repeat` operator will repeat them indefinitely, in order: ```tql repeat ``` ```tql {number: 1, "text": "one"} {number: 2, "text": "two"} {number: 1, "text": "one"} {number: 2, "text": "two"} {number: 1, "text": "one"} {number: 2, "text": "two"} // … ``` ### Repeat the first event 5 times [Section titled “Repeat the first event 5 times”](#repeat-the-first-event-5-times) ```tql head 1 repeat 5 ``` ```tql {number: 1, "text": "one"} {number: 1, "text": "one"} {number: 1, "text": "one"} {number: 1, "text": "one"} {number: 1, "text": "one"} ```

# replace

Replaces all occurrences of a value with another value. ```tql replace [path:field...], what=any, with=any ``` ## Description [Section titled “Description”](#description) The `replace` operator scans all fields of each input event and replaces every occurrence of a value equal to `what` with the value specified by `with`. ### `path: field... (optional)` [Section titled “path: field... (optional)”](#path-field-optional) An optional set of paths to restrict replacements to. ### `what: any` [Section titled “what: any”](#what-any) The value to search for and replace. ### `with: any` [Section titled “with: any”](#with-any) The value to replace in place of `what`. ## Examples [Section titled “Examples”](#examples) ### Replace all occurrences of 42 with null [Section titled “Replace all occurrences of 42 with null”](#replace-all-occurrences-of-42-with-null) ```tql from { count: 42, data: {value: 42, other: 100}, list: [42, 24, 42] } replace what=42, with=null ``` ```tql { count: null, data: {value: null, other: 100}, list: [42, 24, 42] } ``` ### Replace only within specific fields [Section titled “Replace only within specific fields”](#replace-only-within-specific-fields) ```tql from { count: 42, data: {value: 42, other: 100}, } replace data, what=42, with=null ``` ```tql { count: 42, data: {value: null, other: 100}, } ``` ### Replace a specific IP address with a redacted value [Section titled “Replace a specific IP address with a redacted value”](#replace-a-specific-ip-address-with-a-redacted-value) ```tql from { src_ip: 192.168.1.1, dst_ip: 10.0.0.1, metadata: {source: 192.168.1.1} } replace what=192.168.1.1, with="REDACTED" ``` ```tql { src_ip: "REDACTED", dst_ip: 10.0.0.1, metadata: { source: "REDACTED", }, } ``` ## See Also [Section titled “See Also”](#see-also) [`replace`](/reference/functions/replace)

# reverse

Reverses the event order. ```tql reverse ``` ## Description [Section titled “Description”](#description) `reverse` is a shorthand notation for [`slice stride=-1`](/reference/operators/slice). ## Examples [Section titled “Examples”](#examples) ### Reverse a stream of events [Section titled “Reverse a stream of events”](#reverse-a-stream-of-events) ```tql from {x: 1}, {x: 2}, {x: 3} reverse ``` ```tql {x: 3} {x: 2} {x: 1} ``` ## See Also [Section titled “See Also”](#see-also) [`sort`](/reference/operators/sort)

# sample

Dynamically samples events from a event stream. ```tql sample [period:duration, mode=string, min_events=int, max_rate=int, max_samples=int] ``` ## Description [Section titled “Description”](#description) Dynamically samples input data from a stream based on the frequency of receiving events for streams with varying load. The operator counts the number of events received in the `period` and applies the specified function on the count to calculate the sampling rate for the next period. ### `period: duration (optional)` [Section titled “period: duration (optional)”](#period-duration-optional) The duration to count events in, i.e., how often the sample rate is computed. The sampling rate for the first window is `1:1`. Defaults to `30s`. ### `mode = string (optional)` [Section titled “mode = string (optional)”](#mode--string-optional) The function used to compute the sampling rate: * `"ln"` (default) * `"log2"` * `"log10"` * `"sqrt"` ### `min_events = int (optional)` [Section titled “min\_events = int (optional)”](#min_events--int-optional) The minimum number of events that must be received during the previous sampling period for the sampling mode to be applied in the current period. If the number of events in a sample group falls below this threshold, a `1:1` sample rate is used instead. Defaults to `30`. ### `max_rate = int (optional)` [Section titled “max\_rate = int (optional)”](#max_rate--int-optional) The sampling rate is capped to this value if the computed rate is higher than this. ### `max_samples = int (optional)` [Section titled “max\_samples = int (optional)”](#max_samples--int-optional) The maximum number of events to emit per `period`. ## Examples [Section titled “Examples”](#examples) ### Sample the input every 30s dynamically [Section titled “Sample the input every 30s dynamically”](#sample-the-input-every-30s-dynamically) Sample a feed `log-stream` every 30s dynamically, only changing rate when more than 50 events (`min_events`) are received. Additionally, cap the max sampling rate to `1:500`, i.e., 1 sample for every 500 events or more (`max_rate`). ```tql subscribe "log-stream" sample 30s, min_events=50, max_rate=500 ``` ### Sample metrics every hour [Section titled “Sample metrics every hour”](#sample-metrics-every-hour) Sample some `metrics` every hour, limiting the max samples per period to 5,000 samples (`max_samples`) and limiting the overall sample count to 100,000 samples ([`head`](/reference/operators/head)). ```tql subscribe "metrics" sample 1h, max_samples=5k head 100k ``` ## See Also [Section titled “See Also”](#see-also) [`deduplicate`](/reference/operators/deduplicate)

# save_amqp

Saves a byte stream via AMQP messages. ```tql save_amqp [url:str, channel=int, exchange=str, routing_key=str, options=record, mandatory=bool, immediate=bool] ``` ## Description [Section titled “Description”](#description) The `save_amqp` operator is an [AMQP](https://www.amqp.org/) 0-9-1 client to send messages to an exchange. ### `url: str (optional)` [Section titled “url: str (optional)”](#url-str-optional) A URL that specifies the AMQP server. The URL must have the following format: ```plaintext amqp://[USERNAME[:PASSWORD]@]HOSTNAME[:PORT]/[VHOST] ``` When the URL is present, it will overwrite the corresponding values of the configuration options. ### `channel = int (optional)` [Section titled “channel = int (optional)”](#channel--int-optional) The channel number to use. Defaults to `1`. ### `exchange = str (optional)` [Section titled “exchange = str (optional)”](#exchange--str-optional) The exchange to interact with. Defaults to `"amq.direct"`. ### `routing_key = str (optional)` [Section titled “routing\_key = str (optional)”](#routing_key--str-optional) The routing key to publish messages with. Defaults to the empty string. ### `options = record (optional)` [Section titled “options = record (optional)”](#options--record-optional) An option record for RabbitMQ , e.g., `{max_channels: 42, frame_size: 1024, sasl_method: "external"}`. Available options are: ```yaml hostname: 127.0.0.1 port: 5672 ssl: false vhost: / max_channels: 2047 frame_size: 131072 heartbeat: 0 sasl_method: plain username: guest password: guest ``` ### `mandatory = bool (optional)` [Section titled “mandatory = bool (optional)”](#mandatory--bool-optional) This flag tells the server how to react if the message cannot be routed to a queue. If `true`, the server will return an unroutable message with a Return method. Otherwise the server silently drops the message. Defaults to `false`. ### `immediate = bool (optional)` [Section titled “immediate = bool (optional)”](#immediate--bool-optional) This flag tells the server how to react if the message cannot be routed to a queue consumer immediately. If `true`, the server will return an undeliverable message with a Return method. If `false`, the server will queue the message, but with no guarantee that it will ever be consumed. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Send the list of plugins as [JSON](/reference/operators/write_json) [Section titled “Send the list of plugins as JSON”](#send-the-list-of-plugins-as-json) ```tql plugins write_json save_amqp ``` ## See Also [Section titled “See Also”](#see-also) [`load_amqp`](/reference/operators/load_amqp)

# save_azure_blob_storage

Saves bytes to Azure Blob Storage. ```tql save_azure_blob_storage uri:string, [account_key=string] ``` ## Description [Section titled “Description”](#description) The `save_azure_blob_storage` operator writes bytes to a blob in an Azure Blob Store. By default, authentication is handled by the Azure SDK’s credential chain which may read from multiple environment variables, such as: * `AZURE_TENANT_ID` * `AZURE_CLIENT_ID` * `AZURE_CLIENT_SECRET` * `AZURE_AUTHORITY_HOST` * `AZURE_CLIENT_CERTIFICATE_PATH` * `AZURE_FEDERATED_TOKEN_FILE` ### `uri: string` [Section titled “uri: string”](#uri-string) An URI identifying the blob to save to. If the blob and/or do not exist, they will be created. Supported URI formats: 1. `abfs[s]://[:<password>@]<account>.blob.core.windows.net[/<container>[/<path>]]` 2. `abfs[s]://<container>[:<password>]@<account>.dfs.core.windows.net[/path]` 3. `abfs[s]://[<account[:<password>]@]<host[.domain]>[<:port>][/<container>[/path]]` 4. `abfs[s]://[<account[:<password>]@]<container>[/path]` (1) and (2) are compatible with the Azure Data Lake Storage Gen2 URIs 1, (3) is for Azure Blob Storage compatible service including Azurite, and (4) is a shorter version of (1) and (2). ### `account_key = string (optional)` [Section titled “account\_key = string (optional)”](#account_key--string-optional) Account key to authenticate with. ## Examples [Section titled “Examples”](#examples) ### Write JSON [Section titled “Write JSON”](#write-json) Write to blob `obj.json` in the blob container `container`, using the `tenzirdev` user: ```tql write_json save_azure_blob_storage "abfss://tenzirdev@container/obj.json" ``` ## See Also [Section titled “See Also”](#see-also) [`load_azure_blob_storage`](/reference/operators/load_azure_blob_storage)

# save_email

Saves bytes through an SMTP server. ```tql save_email recipient:str, [endpoint=str, from=str, subject=str, username=str, password=str, authzid=str, authorization=str, tls=bool, skip_peer_verification=bool, cacert=string, certfile=string, keyfile=string, mime=bool] ``` ## Description [Section titled “Description”](#description) The `save_email` operator establishes a SMTP(S) connection to a mail server and sends bytes as email body. ### `recipient: str` [Section titled “recipient: str”](#recipient-str) The recipient of the mail. The expected format is either `Name <user@example.org>` with the email in angle brackets, or a plain email adress, such as `user@example.org`. ### `endpoint = str (optional)` [Section titled “endpoint = str (optional)”](#endpoint--str-optional) The endpoint of the mail server. To choose between SMTP and SMTPS, provide a URL with with the corresponding scheme. For example, `smtp://127.0.0.1:25` will establish an unencrypted connection, whereas `smtps://127.0.0.1:25` an encrypted one. If you specify a server without a schema, the protocol defaults to SMTPS. Defaults to `smtp://localhost:25`. ### `from = str (optional)` [Section titled “from = str (optional)”](#from--str-optional) The `From` header. If you do not specify this parameter, an empty address is sent to the SMTP server which might cause the email to be rejected. ### `subject = str (optional)` [Section titled “subject = str (optional)”](#subject--str-optional) The `Subject` header. ### `username = str (optional)` [Section titled “username = str (optional)”](#username--str-optional) The username in an authenticated SMTP connection. ### `password = str (optional)` [Section titled “password = str (optional)”](#password--str-optional) The password in an authenticated SMTP connection. ### `authzid = str (optional)` [Section titled “authzid = str (optional)”](#authzid--str-optional) The authorization identity in an authenticated SMTP connection. This option is only applicable to the PLAIN SASL authentication mechanism where it is optional. When not specified only the authentication identity (`authcid`) as specified by the username is sent to the server, along with the password. The server derives an `authzid` from the `authcid` when not provided, which it then uses internally. When the `authzid` is specified it can be used to access another user’s inbox, that the user has been granted access to, or a shared mailbox. ### `authorization = str (optional)` [Section titled “authorization = str (optional)”](#authorization--str-optional) The authorization options for an authenticated SMTP connection. This login option defines the preferred authentication mechanism, e.g., `AUTH=PLAIN`, `AUTH=LOGIN`, or `AUTH=*`. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ### `mime = bool (optional)` [Section titled “mime = bool (optional)”](#mime--bool-optional) Whether to wrap the chunk into a MIME part. The operator uses the metadata of the byte chunk for the `Content-Type` MIME header. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) Send the Tenzir version string as CSV to `user@example.org`: ```tql version write_csv save_email "user@example.org" ``` Send the email body as MIME part: ```tql version write_json save_email "user@example.org", mime=true ``` This may result in the following email body: ```plaintext --------------------------s89ecto6c12ILX7893YOEf Content-Type: application/json Content-Transfer-Encoding: quoted-printable { "version": "4.10.4+ge0a060567b-dirty", "build": "ge0a060567b-dirty", "major": 4, "minor": 10, "patch": 4 } --------------------------s89ecto6c12ILX7893YOEf-- ```

# save_file

Writes a byte stream to a file. ```tql save_file path:string, [append=bool, real_time=bool, uds=bool] ``` ## Description [Section titled “Description”](#description) Writes a byte stream to a file. ### `path: string` [Section titled “path: string”](#path-string) The file path to write to. If intermediate directories do not exist, they will be created. When `~` is the first character, it will be substituted with the value of the `$HOME` environment variable. ### `append = bool (optional)` [Section titled “append = bool (optional)”](#append--bool-optional) If `true`, appends to the file instead of overwriting it. ### `real_time = bool (optional)` [Section titled “real\_time = bool (optional)”](#real_time--bool-optional) If `true`, immediately synchronizes the file with every chunk of bytes instead of buffering bytes to batch filesystem write operations. ### `uds = bool (optional)` [Section titled “uds = bool (optional)”](#uds--bool-optional) If `true`, creates a Unix Domain Socket instead of a normal file. Cannot be combined with `append=true`. ## Examples [Section titled “Examples”](#examples) ### Save bytes to a file [Section titled “Save bytes to a file”](#save-bytes-to-a-file) ```tql save_file "/tmp/out.txt" ``` ## See Also [Section titled “See Also”](#see-also) [`files`](/reference/operators/files), [`load_file`](/reference/operators/load_file), [`save_stdout`](/reference/operators/save_stdout)

# save_ftp

Saves a byte stream via FTP. ```tql save_ftp url:str [tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled “Description”](#description) Saves a byte stream via FTP. ### `url: str` [Section titled “url: str”](#url-str) The URL to request from. The `ftp://` scheme can be omitted. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ```tql save_ftp "ftp.example.org" ```

# save_gcs

Saves bytes to a Google Cloud Storage object. ```tql save_gcs uri:string, [anonymous=bool] ``` ## Description [Section titled “Description”](#description) The `save_gcs` operator connects to a GCS bucket to save raw bytes to a GCS object. The connector tries to retrieve the appropriate credentials using Google’s [Application Default Credentials](https://google.aip.dev/auth/4110). ### `uri: string` [Section titled “uri: string”](#uri-string) The path to the GCS object. The syntax is `gs://<bucket-name>/<full-path-to-object>(?<options>)`. The `<options>` are query parameters. Per the [Arrow documentation](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri), the following options exist: > For GCS, the supported parameters are `scheme`, `endpoint_override`, and `retry_limit_seconds`. ### `anonymous = bool (optional)` [Section titled “anonymous = bool (optional)”](#anonymous--bool-optional) Ignore any predefined credentials and try to use anonymous credentials. ## Examples [Section titled “Examples”](#examples) Write JSON to an object `test.json` in `bucket`, but using a different GCS-compatible endpoint: ```tql write_json save_gcs "gs://bucket/test.json?endpoint_override=gcs.mycloudservice.com" ``` ## See Also [Section titled “See Also”](#see-also) [`load_gcs`](/reference/operators/load_gcs)

# save_google_cloud_pubsub

Publishes to a Google Cloud Pub/Sub topic. ```tql save_google_cloud_pubsub project_id=string, topic_id=string ``` ## Description [Section titled “Description”](#description) The operator publishes bytes to a Google Cloud Pub/Sub topic. ### `project_id = string` [Section titled “project\_id = string”](#project_id--string) The project to connect to. Note that this is the project\_id, not the display name. ### `topic_id = string` [Section titled “topic\_id = string”](#topic_id--string) The topic to publish to. ## URI support & integration with `from` [Section titled “URI support & integration with from”](#uri-support--integration-with-from) The `save_google_cloud_pubsub` operator can also be used from the [`to`](/reference/operators/to) operator. For this, the `gcps://` scheme can be used. The URI is then translated: ```tql to "gcps://my_project/my_topic" ``` ```tql save_google_cloud_pubsub project_id="my_project", topic_id="my_topic" ``` ## Examples [Section titled “Examples”](#examples) ### Publish alerts to a given topic [Section titled “Publish alerts to a given topic”](#publish-alerts-to-a-given-topic) Publish `suricata.alert` events as JSON to `alerts-topic`: ```tql export where @name = "suricata.alert" write_json save_google_cloud_pubsub project_id="amazing-project-123456", topic_id="alerts-topic" ``` ## See Also [Section titled “See Also”](#see-also) [`load_google_cloud_pubsub`](/reference/operators/load_google_cloud_pubsub)

# save_http

Sends a byte stream via HTTP. ```tql save_http url:string, [params=record, headers=record, method=string, parallel=int, tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled “Description”](#description) The `save_http` operator performs a HTTP request with the request body being the bytes provided by the previous operator. ### `url: string` [Section titled “url: string”](#url-string) The URL to write to. The `http://` scheme can be omitted. ### `method = string (optional)` [Section titled “method = string (optional)”](#method--string-optional) The HTTP method, such as `POST` or `GET`. The default is `"POST"`. ### `params = record (optional)` [Section titled “params = record (optional)”](#params--record-optional) The query parameters for the request. ### `headers = record (optional)` [Section titled “headers = record (optional)”](#headers--record-optional) The headers for the request. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Call a webhook with pipeline data [Section titled “Call a webhook with pipeline data”](#call-a-webhook-with-pipeline-data) ```tql save_http "example.org/api", headers={"X-API-Token": "0000-0000-0000"} ``` ## See Also [Section titled “See Also”](#see-also) [`load_http`](/reference/operators/load_http)

# save_kafka

Saves a byte stream to a Apache Kafka topic. ```tql save_kafka topic:string, [key=string, timestamp=time, options=record, aws_iam=record] ``` ## Description [Section titled “Description”](#description) Deprecated The `save_kafka` operator does not respect event boundaries and can combine multiple events into a single message, causing issues for consumers. Consider using `to_kafka` instead. The `save_kafka` operator saves bytes to a Kafka topic. The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`. The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them: * `bootstrap.servers`: `localhost` * `client.id`: `tenzir` * `group.id`: `tenzir` ### `topic: string` [Section titled “topic: string”](#topic-string) The Kafka topic to use. ### `key = string (optional)` [Section titled “key = string (optional)”](#key--string-optional) Sets a fixed key for all messages. ### `timestamp = time (optional)` [Section titled “timestamp = time (optional)”](#timestamp--time-optional) Sets a fixed timestamp for all messages. ### `options = record (optional)` [Section titled “options = record (optional)”](#options--record-optional) A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"auto.offset.reset" : "earliest", "enable.partition.eof": true}`. The `save_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs. We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `save_kafka` arguments. ### `aws_iam = record (optional)` [Section titled “aws\_iam = record (optional)”](#aws_iam--record-optional) If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified. Available keys: * `region`: Region of the MSK Clusters. Must be specified when using IAM. * `assume_role`: Optional Role ARN to assume. * `session_name`: Optional session name to use when assuming a role. * `external_id`: Optional external id to use when assuming a role. The operator will try to get credentials in the following order: 1. Checks your environment variables for AWS Credentials. 2. Checks your `$HOME/.aws/credentials` file for a profile and credentials 3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`. 4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that are not directly supported by AWS. 5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set. 6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON. ## Examples [Section titled “Examples”](#examples) ### Write the Tenzir version to topic `tenzir` with timestamp from the past [Section titled “Write the Tenzir version to topic tenzir with timestamp from the past”](#write-the-tenzir-version-to-topic-tenzir-with-timestamp-from-the-past) ```tql version write_json save_kafka "tenzir", timestamp=1984-01-01 ``` ### Follow a CSV file and publish it to topic `data` [Section titled “Follow a CSV file and publish it to topic data”](#follow-a-csv-file-and-publish-it-to-topic-data) ```tql load_file "/tmp/data.csv" read_csv write_json save_kafka "data" ``` ## See Also [Section titled “See Also”](#see-also) [`load_kafka`](/reference/operators/load_kafka)

# save_s3

Saves bytes to an Amazon S3 object. ```tql save_s3 uri:str, [anonymous=bool, role=string, external_id=string] ``` ## Description [Section titled “Description”](#description) The `save_s3` operator writes bytes to an S3 object in an S3 bucket. The connector tries to retrieve the appropriate credentials using AWS’s [default credentials provider chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). If a config file `<prefix>/etc/tenzir/plugin/s3.yaml` or `~/.config/tenzir/plugin/s3.yaml` exists, it is always preferred over the default AWS credentials. The configuration file must have the following format: ```yaml access-key: your-access-key secret-key: your-secret-key session-token: your-session-token (optional) ``` ### `uri: str` [Section titled “uri: str”](#uri-str) The path to the S3 object. The syntax is `s3://[<access-key>:<secret-key>@]<bucket-name>/<full-path-to-object>(?<options>)`. Options can be appended to the path as query parameters, as per [Arrow](https://arrow.apache.org/docs/r/articles/fs.html#connecting-directly-with-a-uri): > For S3, the options that can be included in the URI as query parameters are `region`, `scheme`, `endpoint_override`, `allow_bucket_creation`, and `allow_bucket_deletion`. ### `anonymous = bool (optional)` [Section titled “anonymous = bool (optional)”](#anonymous--bool-optional) Whether to ignore any predefined credentials and try to save with anonymous credentials. ### `role = string (optional)` [Section titled “role = string (optional)”](#role--string-optional) A role to assume when writing to S3. ### `external_id = string (optional)` [Section titled “external\_id = string (optional)”](#external_id--string-optional) The external ID to use when assuming the `role`. Defaults to no ID. ## Examples [Section titled “Examples”](#examples) Read CSV from an object `obj.csv` in the bucket `examplebucket` and save it as YAML to another bucket `examplebucket2`: ```tql load_s3 "s3://examplebucket/obj.csv" read_csv write_yaml save_s3 "s3://examplebucket2/obj.yaml" ``` ## See Also [Section titled “See Also”](#see-also) [`load_s3`](/reference/operators/load_s3), [`to_amazon_security_lake`](/reference/operators/to_amazon_security_lake)

# save_sqs

Saves bytes to [Amazon SQS](https://docs.aws.amazon.com/sqs/) queues. ```tql save_sqs queue:str, [poll_time=duration] ``` ## Description [Section titled “Description”](#description) [Amazon Simple Queue Service (Amazon SQS)](https://docs.aws.amazon.com/sqs/) is a fully managed message queuing service to decouple and scale microservices, distributed systems, and serverless applications. The `save_sqs` operator writes bytes as messages into an SQS queue. The `save_sqs` operator uses long polling, which helps reduce your cost of using SQS by reducing the number of empty responses when there are no messages available to return in reply to a message request. Use the `poll_time` option to adjust the timeout. The operator requires the following AWS permissions: * `sqs:GetQueueUrl` * `sqs:SendMessage` ### `queue: str` [Section titled “queue: str”](#queue-str) The name of the queue to use. ### `poll_time = duration (optional)` [Section titled “poll\_time = duration (optional)”](#poll_time--duration-optional) The long polling timeout per request. The value must be between 1 and 20 seconds. Defaults to `3s`. ## Examples [Section titled “Examples”](#examples) Write JSON messages from a source feed to the SQS queue `tenzir`: ```tql subscribe "to-sqs" write_json save_sqs "tenzir" ``` ## See Also [Section titled “See Also”](#see-also) [`load_sqs`](/reference/operators/load_sqs)

# save_stdout

Writes a byte stream to standard output. ```tql save_stdout ``` ## Description [Section titled “Description”](#description) Writes a byte stream to standard output. This is mostly useful when using the `tenzir` executable as part of a shell script. ## Examples [Section titled “Examples”](#examples) ### Write colored, compact TQL-style [Section titled “Write colored, compact TQL-style”](#write-colored-compact-tql-style) ```tql from {x: "Hello World"} write_tql compact=true, color=true save_stdout ``` ```tql {x: "Hello World"} ``` ## See Also [Section titled “See Also”](#see-also) [`load_stdin`](/reference/operators/load_stdin), [`save_file`](/reference/operators/save_file)

# save_tcp

Saves bytes to a TCP or TLS connection. ```tql save_tcp endpoint:string, [retry_delay=duration, max_retry_count=int, tls=bool, cacert=string, certifle=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled “Description”](#description) Saves bytes to the given endpoint via TCP or TLS. Attempts to reconnect automatically for `max_retry_count` in case of recoverable connection errors. ### `endpoint: string` [Section titled “endpoint: string”](#endpoint-string) The endpoint to which the server will connect. Must be of the form `[tcp://]<hostname>:<port>`. You can also use an IANA service name instead of a numeric port. ### `retry_delay = duration (optional)` [Section titled “retry\_delay = duration (optional)”](#retry_delay--duration-optional) The amount of time to wait before attempting to reconnect in case a connection attempt fails and the error is deemed recoverable. Defaults to `30s`. ### \`max\_retry\_count = int (optional) [Section titled “\`max\_retry\_count = int (optional)”](#max_retry_count--int-optional) The number of retries to attempt in case of connection errors before transitioning into the error state. Defaults to `10`. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Transform incoming Syslog to BITZ and save over TCP [Section titled “Transform incoming Syslog to BITZ and save over TCP”](#transform-incoming-syslog-to-bitz-and-save-over-tcp) ```tql load_tcp "0.0.0.0:8090" { read_syslog } write_bitz save_tcp "127.0.0.1:4000" ``` ### Save to localhost with TLS [Section titled “Save to localhost with TLS”](#save-to-localhost-with-tls) ```tql subscribe "feed" write_json save_tcp "127.0.0.1:4000", tls=true, skip_peer_verification=true ```

# save_udp

Saves bytes to a UDP socket. ```tql save_udp endpoint:str ``` ## Description [Section titled “Description”](#description) Saves bytes to a UDP socket. ### `endpoint: str` [Section titled “endpoint: str”](#endpoint-str) The address of the remote endpoint to load bytes from. Must be of the format: `[udp://]host:port`. ## Examples [Section titled “Examples”](#examples) Send the Tenzir version as CSV file to a remote endpoint via UDP: ```tql version write_csv save_udp "127.0.0.1:56789" ``` Use `nc -ul 127.0.0.1 56789` to spin up a UDP server to test the above pipeline. ## See Also [Section titled “See Also”](#see-also) [`load_udp`](/reference/operators/load_udp)

# save_zmq

Sends bytes as ZeroMQ messages. ```tql save_zmq [endpoint:str, listen=bool, connect=bool, monitor=bool] ``` ## Description [Section titled “Description”](#description) The `save_zmq` operator sends bytes as a ZeroMQ message via a `PUB` socket. Indpendent of the socket type, the `save_zmq` operator supports specfiying the direction of connection establishment with `listen` and `connect`. This can be helpful to work around firewall restrictions and fit into broader set of existing ZeroMQ applications. With the `monitor` option, you can activate message buffering for TCP sockets that hold off sending messages until *at least one* remote peer has connected. This can be helpful when you want to delay publishing until you have one connected subscriber, e.g., when the publisher spawns before any subscriber exists. ### `endpoint: str (optional)` [Section titled “endpoint: str (optional)”](#endpoint-str-optional) The endpoint for connecting to or listening on a ZeroMQ socket. Defaults to `tcp://127.0.0.1:5555`. ### `listen = bool (optional)` [Section titled “listen = bool (optional)”](#listen--bool-optional) Bind to the ZeroMQ socket. Defaults to `true`. ### `connect = bool (optional)` [Section titled “connect = bool (optional)”](#connect--bool-optional) Connect to the ZeroMQ socket. Defaults to `false`. ### `monitor = bool (optional)` [Section titled “monitor = bool (optional)”](#monitor--bool-optional) Monitors a 0mq socket over TCP until the remote side establishes a connection. ## Examples [Section titled “Examples”](#examples) ### Publish events by connecting to a PUB socket [Section titled “Publish events by connecting to a PUB socket”](#publish-events-by-connecting-to-a-pub-socket) ```tql from {x: 42} write_csv save_zmq connect=true ``` ## See Also [Section titled “See Also”](#see-also) [`load_zmq`](/reference/operators/load_zmq)

# schemas

Retrieves all schemas for events stored at a node. ```tql schemas ``` ## Description [Section titled “Description”](#description) The `schemas` operator shows all schemas of all events stored at a node. Note that there may be multiple schema definitions with the same name, but a different set of fields, e.g., because the imported data’s schema changed over time. ## Examples [Section titled “Examples”](#examples) ### See all available definitions for a given schema [Section titled “See all available definitions for a given schema”](#see-all-available-definitions-for-a-given-schema) ```tql schemas where name == "suricata.alert" ```

# select

Selects some values and discards the rest. ```tql select (field|assignment)... ``` ## Description [Section titled “Description”](#description) This operator keeps only the provided fields and drops the rest. ### `field` [Section titled “field”](#field) The field to keep. If it does not exist, it’s given the value `null` and a warning is emitted. ### `assignment` [Section titled “assignment”](#assignment) An assignment of the form `<field>=<expr>`. ## Examples [Section titled “Examples”](#examples) ### Select and create columns [Section titled “Select and create columns”](#select-and-create-columns) Keep `a` and introduce `y` with the value of `b`: ```tql from {a: 1, b: 2, c: 3} select a, y=b ``` ```tql {a: 1, y: 2} ``` A more complex example with expressions and selection through records: ```tql from { name: "foo", pos: { x: 1, y: 2, }, state: "active", } select id=name.to_upper(), pos.x, added=true ``` ```tql { id: "FOO", pos: { x: 1, }, added: true, } ``` ## See Also [Section titled “See Also”](#see-also) [`drop`](/reference/operators/drop), [`where`](/reference/operators/where)

# serve

Make events available under the `/serve` REST API endpoint ```tql serve id:string, [buffer_size=int] ``` ## Description [Section titled “Description”](#description) The `serve` operator bridges between pipelines and the corresponding `/serve` [REST API endpoint](/reference/node/api). ![Serve Operator](/_astro/serve.excalidraw.CC8LHhCr_19DKCs.svg) Pipelines ending with the `serve` operator exit when all events have been delivered over the corresponding endpoint. ### `id: string` [Section titled “id: string”](#id-string) An identifier that uniquely identifies the operator. The `serve` operator errors when receiving a duplicate serve id. ### `buffer_size = int (optional)` [Section titled “buffer\_size = int (optional)”](#buffer_size--int-optional) The buffer size specifies the maximum number of events to keep in the `serve` operator to make them instantly available in the corresponding endpoint before throttling the pipeline execution. Defaults to `1Ki`. ## Examples [Section titled “Examples”](#examples) ### Make the input available as REST API [Section titled “Make the input available as REST API”](#make-the-input-available-as-rest-api) Read a Zeek `conn.log` and make it available as `zeek-conn-logs`: ```tql load_file "path/to/conn.log" read_zeek_tsv serve "zeek-conn-logs"' ``` Then fetch the first 100 events from the `/serve` endpoint: ```bash curl \ -X POST \ -H "Content-Type: application/json" \ -d '{"serve_id": "zeek-conn-logs", "continuation_token": null, "timeout": "1s", "max_events": 100}' \ http://localhost:5160/api/v0/serve ``` This will return up to 100 events, or less if the specified timeout of 1 second expired. Subsequent results for further events must specify a continuation token. The token is included in the response under `next_continuation_token` if there are further events to be retrieved from the endpoint. ### Wait for the first event [Section titled “Wait for the first event”](#wait-for-the-first-event) This pipeline will produce 10 events after 3 seconds of doing nothing. ```tql shell "sleep 3; jq --null-input '{foo: 1}'" read_json repeat 10 serve "slow-events" ``` ```bash curl \ -X POST \ -H "Content-Type: application/json" \ -d '{"serve_id": "slow-events", "continuation_token": null, "timeout": "5s", "min_events": 1}' \ http://localhost:5160/api/v0/serve ``` The call to `/serve` will wait up to 5 seconds for the first event from the pipeline arriving at the serve operator, and return immediately once the first event arrives. ## See Also [Section titled “See Also”](#see-also) [`api`](/reference/operators/api), [`from_http`](/reference/operators/from_http), [`openapi`](/reference/operators/openapi)

# set

Assigns a value to a field, creating it if necessary. ```tql field = expr set field=expr... ``` ## Description [Section titled “Description”](#description) Assigns a value to a field, creating it if necessary. If the field does not exist, it is appended to the end. If the field name is a path such as `foo.bar.baz`, records for `foo` and `bar` will be created if they do not exist yet. Within assignments, the `move` keyword in front of a field causes a field to be removed from the input after evaluation. ## Examples [Section titled “Examples”](#examples) ### Append a new field [Section titled “Append a new field”](#append-a-new-field) ```tql from {a: 1, b: 2} c = a + b ``` ```tql {a: 1, b: 2, c: 3} ``` ### Update an existing field [Section titled “Update an existing field”](#update-an-existing-field) ```tql from {a: 1, b: 2} a = "Hello" ``` ```tql {a: "Hello", b: 2} ``` ### Move a field [Section titled “Move a field”](#move-a-field) ```tql from {a: 1} b = move a ``` ```tql {b: 1} ``` ## See Also [Section titled “See Also”](#see-also) [`move`](/reference/operators/move)

# shell

Executes a system command and hooks its stdin and stdout into the pipeline. ```tql shell cmd:string ``` ## Description [Section titled “Description”](#description) The `shell` operator executes the provided command by spawning a new process. The input of the operator is forwarded to the child’s standard input. Similarly, the child’s standard output is forwarded to the output of the operator. ### `cmd: string` [Section titled “cmd: string”](#cmd-string) The command to execute and hook into the pipeline processing. It is interpreted by `/bin/sh -c`. ## Secrets [Section titled “Secrets”](#secrets) By default, the `shell` operator does not accept secrets. If you want to allow usage of secrets in the `cmd` argument, you can enable the configuration option `tenzir.allow-secrets-in-escape-hatches`. ## Examples [Section titled “Examples”](#examples) ### Show a live log from the `tenzir-node` service [Section titled “Show a live log from the tenzir-node service”](#show-a-live-log-from-the-tenzir-node-service) ```tql shell "journalctl -u tenzir-node -f" read_json ``` ## See Also [Section titled “See Also”](#see-also) [`python`](/reference/operators/python)

# sigma

Filter the input with Sigma rules and output matching events. ```tql sigma path:string, [refresh_interval=duration] ``` ## Description [Section titled “Description”](#description) The `sigma` operator executes [Sigma rules](https://github.com/SigmaHQ/sigma) on its input. If a rule matches, the operator emits a `tenzir.sigma` event that wraps the input record into a new record along with the matching rule. The operator discards all events that do not match the provided rules. Sigma uses [value modifiers](https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md#value-modifiers) to select a concrete relational operator for given search predicate. Without a modifier, Sigma uses equality comparison (`==`) of field and value. For example, the `contains` modifier changes the relational operator to substring search, and the `re` modifier switches to a regular expression match. The table below shows what modifiers the `sigma` operator supports, where ✅ means implemented, 🚧 not yet implemented but possible, and ❌ not yet supported: | Modifier | Use | sigmac | Tenzir | | ---------------- | -------------------------------------------------------- | :----: | :----: | | `contains` | perform a substring search with the value | ✅ | ✅ | | `startswith` | match the value as a prefix | ✅ | ✅ | | `endswith` | match the value as a suffix | ✅ | ✅ | | `base64` | encode the value with Base64 | ✅ | ✅ | | `base64offset` | encode value as all three possible Base64 variants | ✅ | ✅ | | `utf16le`/`wide` | transform the value to UTF16 little endian | ✅ | 🚧 | | `utf16be` | transform the value to UTF16 big endian | ✅ | 🚧 | | `utf16` | transform the value to UTF16 | ✅ | 🚧 | | `re` | interpret the value as regular expression | ✅ | ✅ | | `cidr` | interpret the value as a IP CIDR | ❌ | ✅ | | `all` | changes the expression logic from OR to AND | ✅ | ✅ | | `lt` | compare less than (`<`) the value | ❌ | ✅ | | `lte` | compare less than or equal to (`<=`) the value | ❌ | ✅ | | `gt` | compare greater than (`>`) the value | ❌ | ✅ | | `gte` | compare greater than or equal to (`>=`) the value | ❌ | ✅ | | `expand` | expand value to placeholder strings, e.g., `%something%` | ❌ | ❌ | ### `path: string` [Section titled “path: string”](#path-string) The rule to match. If `path` points to a rule, the operator transpiles the rule file at the time of pipeline creation. If this points to a directory, the operator watches it and attempts to parse each contained file as a Sigma rule. The `sigma` operator matches if *any* of the contained rules match, effectively creating a disjunction of all rules inside the directory. ### `refresh_interval = duration (optional)` [Section titled “refresh\_interval = duration (optional)”](#refresh_interval--duration-optional) How often the `sigma` operator looks at the specified rule or directory of rules to update its internal state. Defaults to `5s`. ## Examples [Section titled “Examples”](#examples) ### Apply a Sigma rule to an EVTX file [Section titled “Apply a Sigma rule to an EVTX file”](#apply-a-sigma-rule-to-an-evtx-file) The tool [`evtx_dump`](https://github.com/omerbenamram/evtx) turns an EVTX file into a JSON object. On the command line, use the `tenzir` binary to pipe the `evtx_dump` output to a Tenzir pipeline using the `sigma` operator: ```bash evtx_dump -o jsonl file.evtx | tenzir 'read_json | sigma "rule.yaml"' ``` ### Run a Sigma rule on historical data [Section titled “Run a Sigma rule on historical data”](#run-a-sigma-rule-on-historical-data) Apply a Sigma rule over historical data in a node from the last day: ```tql export where ts > now() - 1d sigma "rule.yaml" ``` ### Stream a file and apply a set of Sigma rules to it [Section titled “Stream a file and apply a set of Sigma rules to it”](#stream-a-file-and-apply-a-set-of-sigma-rules-to-it) Watch a directory of Sigma rules and apply all of them on a continuous stream of Suricata events: ```tql load_file "eve.json", follow=true read_suricata sigma "/tmp/rules/" ``` When you add a new file to `/tmp/rules`, the `sigma` operator transpiles it and will match it on all subsequent inputs.

# slice

Keeps a range of events within the interval `[begin, end)` stepping by `stride`. ```tql slice [begin=int, end=int, stride=int] ``` ## Description [Section titled “Description”](#description) The `slice` operator selects a range of events from the input. The semantics of the operator match Python’s array slicing. ### `begin = int (optional)` [Section titled “begin = int (optional)”](#begin--int-optional) The beginning (inclusive) of the range to keep. Use a negative number to count from the end. ### `end = int (optional)` [Section titled “end = int (optional)”](#end--int-optional) The end (exclusive) of the range to keep. Use a negative number to count from the end. ### `stride = int (optional)` [Section titled “stride = int (optional)”](#stride--int-optional) The number of elements to advance before the next element. Use a negative number to count from the end, effectively reversing the stream. ## Examples [Section titled “Examples”](#examples) ### Get the second 100 events [Section titled “Get the second 100 events”](#get-the-second-100-events) ```tql slice begin=100, end=200 ``` ### Get the last 5 events [Section titled “Get the last 5 events”](#get-the-last-5-events) ```tql slice begin=-5 ``` ### Skip the last 10 events [Section titled “Skip the last 10 events”](#skip-the-last-10-events) ```tql slice end=-10 ``` ### Return the last 50 events, except for the last 2 [Section titled “Return the last 50 events, except for the last 2”](#return-the-last-50-events-except-for-the-last-2) ```tql slice begin=-50, end=-2 ``` ### Skip the first and the last event [Section titled “Skip the first and the last event”](#skip-the-first-and-the-last-event) ```tql slice begin=1, end=-1 ``` ### Return every second event starting from the tenth [Section titled “Return every second event starting from the tenth”](#return-every-second-event-starting-from-the-tenth) ```tql slice begin=9, stride=2 ``` ### Return all but the last five events in reverse order [Section titled “Return all but the last five events in reverse order”](#return-all-but-the-last-five-events-in-reverse-order) ```tql slice end=-5, stride=-1 ``` ## See Also [Section titled “See Also”](#see-also) [`head`](/reference/operators/head), [`tail`](/reference/operators/tail)

# sockets

Shows a snapshot of open sockets. ```tql sockets ``` ## Description [Section titled “Description”](#description) The `sockets` operator shows a snapshot of all currently open sockets. ## Schemas [Section titled “Schemas”](#schemas) Tenzir emits socket information with the following schema. ### `tenzir.socket` [Section titled “tenzir.socket”](#tenzirsocket) Contains detailed information about the socket. | Field | Type | Description | | :------------ | :------- | :------------------------------------------------- | | `pid` | `uint64` | The process identifier. | | `process` | `string` | The name of the process involved. | | `protocol` | `uint64` | The protocol used for the communication. | | `local_addr` | `ip` | The local IP address involved in the connection. | | `local_port` | `port` | The local port number involved in the connection. | | `remote_addr` | `ip` | The remote IP address involved in the connection. | | `remote_port` | `port` | The remote port number involved in the connection. | | `state` | `string` | The current state of the connection. | ## Examples [Section titled “Examples”](#examples) ### Show process ID, local, and remote IP address of all sockets [Section titled “Show process ID, local, and remote IP address of all sockets”](#show-process-id-local-and-remote-ip-address-of-all-sockets) ```tql sockets select pid, local_addr, remote_addr ``` ## See Also [Section titled “See Also”](#see-also) [`files`](/reference/operators/files), [`processes`](/reference/operators/processes)

# sort

Sorts events by the given expressions. ```tql sort [-]expr... ``` ## Description [Section titled “Description”](#description) Sorts events by the given expressions, putting all `null` values at the end. If multiple expressions are specified, the sorting happens lexicographically, that is: Later expressions are only considered if all previous expressions evaluate to equal values. This operator performs a stable sort (preserves relative ordering when all expressions evaluate to the same value). ### `[-]expr` [Section titled “\[-\]expr”](#-expr) An expression that is evaluated for each event. Normally, events are sorted in ascending order. If the expression starts with `-`, descending order is used instead. In both cases, `null` is put last. ## Examples [Section titled “Examples”](#examples) ### Sort by a field in ascending order [Section titled “Sort by a field in ascending order”](#sort-by-a-field-in-ascending-order) ```tql sort timestamp ``` ### Sort by a field in descending order [Section titled “Sort by a field in descending order”](#sort-by-a-field-in-descending-order) ```tql sort -timestamp ``` ### Sort by multiple fields [Section titled “Sort by multiple fields”](#sort-by-multiple-fields) Sort by a field `src_ip` and, in case of matching values, sort by `dest_ip`: ```tql sort src_ip, dest_ip ``` Sort by the field `src_ip` in ascending order and by the field `dest_ip` in descending order. ```tql sort src_ip, -dest_ip ``` ## See Also [Section titled “See Also”](#see-also) [`rare`](/reference/operators/rare), [`reverse`](/reference/operators/reverse), [`top`](/reference/operators/top)

# strict

Treats all warnings as errors. ```tql strict { … } ``` ## Description [Section titled “Description”](#description) The `strict` operator takes a pipeline as an argument and treats all warnings emitted by the execution of the pipeline as errors. This is useful when you want to stop a pipeline on warnings or unexpected diagnostics. ## Examples [Section titled “Examples”](#examples) ### Stop the pipeline on any warnings when sending logs [Section titled “Stop the pipeline on any warnings when sending logs”](#stop-the-pipeline-on-any-warnings-when-sending-logs) ```tql subscribe "log-feed" strict { to_google_cloud_logging … } ``` ## See Also [Section titled “See Also”](#see-also) [`assert`](/reference/operators/assert)

# subscribe

Subscribes to events from a channel with a topic. ```tql subscribe [topic:string...] ``` ## Description [Section titled “Description”](#description) The `subscribe` operator subscribes to events from a channel with the specified topic. Multiple `subscribe` operators with the same topic receive the same events. Subscribers propagate back pressure to publishers. If a subscribing pipeline fails to keep up, all publishers will slow down as well to a matching speed to avoid data loss. This mechanism is disabled for pipelines that are not visible on the overview page on [app.tenzir.com](https://app.tenzir.com), which drop data rather than slow down their publishers. ### `topic: string... (optional)` [Section titled “topic: string... (optional)”](#topic-string-optional) Optional channel names to subscribe to. If unspecified, the operator subscribes to the topic `main`. ## Examples [Section titled “Examples”](#examples) ### Subscribe to the events under a topic [Section titled “Subscribe to the events under a topic”](#subscribe-to-the-events-under-a-topic) ```tql subscribe "zeek-conn" ``` ### Subscribe to the multiple topics [Section titled “Subscribe to the multiple topics”](#subscribe-to-the-multiple-topics) ```tql subscribe "alerts", "notices", "critical" ``` ## See Also [Section titled “See Also”](#see-also) [`export`](/reference/operators/export), [`publish`](/reference/operators/publish)

# summarize

Groups events and applies aggregate functions to each group. ```tql summarize (group|aggregation)... ``` ## Description [Section titled “Description”](#description) The `summarize` operator groups events according to certain fields and applies [aggregation functions](/reference/functions#aggregation) to each group. The operator consumes the entire input before producing any output, and may reorder the event stream. The order of the output fields follows the sequence of the provided arguments. Unspecified fields are dropped. ### `group` [Section titled “group”](#group) To group by a certain field, use the syntax `<field>` or `<field>=<field>`. For each unique combination of the `group` fields, a single output event will be returned. ### `aggregation` [Section titled “aggregation”](#aggregation) The [aggregation functions](/reference/functions#aggregation) applied to each group are specified with `f(…)` or `<field>=f(…)`, where `f` is the name of an aggregation function (see below) and `<field>` is an optional name for the result. The aggregation function will produce a single result for each group. If no name is specified, the aggregation function call will automatically generate one. If processing continues after `summarize`, we strongly recommend to specify a custom name. ## Examples [Section titled “Examples”](#examples) ### Compute the sum of a field over all events [Section titled “Compute the sum of a field over all events”](#compute-the-sum-of-a-field-over-all-events) ```tql from {x: 1}, {x: 2} summarize x=sum(x) ``` ```tql {x: 3} ``` Group over `y` and compute the sum of `x` for each group: ```tql from {x: 0, y: 0, z: 1}, {x: 1, y: 1, z: 2}, {x: 1, y: 1, z: 3} summarize y, x=sum(x) ``` ```tql {y: 0, x: 0} {y: 1, x: 2} ``` ### Gather unique values in a list [Section titled “Gather unique values in a list”](#gather-unique-values-in-a-list) Group the input by `src_ip` and aggregate all unique `dest_port` values into a list: ```tql summarize src_ip, distinct(dest_port) ``` Same as above, but produce a count of the unique number of values instead of a list: ```tql summarize src_ip, count_distinct(dest_port) ``` ### Compute min and max of a group [Section titled “Compute min and max of a group”](#compute-min-and-max-of-a-group) Compute minimum and maximum of the `timestamp` field per `src_ip` group: ```tql summarize min(timestamp), max(timestamp), src_ip ``` Compute minimum and maximum of the `timestamp` field over all events: ```tql summarize min(timestamp), max(timestamp) ``` ### Check if any value of a group is true [Section titled “Check if any value of a group is true”](#check-if-any-value-of-a-group-is-true) Create a boolean flag `originator` that is `true` if any value in the `src_ip` group is `true`: ```tql summarize src_ip, originator=any(is_orig) ``` ### Create 1-hour time buckets [Section titled “Create 1-hour time buckets”](#create-1-hour-time-buckets) Create 1-hour groups and produce a summary of network traffic between host pairs: ```tql ts = round(ts, 1h) summarize ts, src_ip, dest_ip, sum(bytes_in), sum(bytes_out) ``` ## See Also [Section titled “See Also”](#see-also) [`rare`](/reference/operators/rare), [`top`](/reference/operators/top)

# tail

Limits the input to the last `n` events. ```tql tail [n:int] ``` ## Description [Section titled “Description”](#description) Forwards the last `n` events and discards the rest. `tail n` is a shorthand notation for [`slice begin=-n`](/reference/operators/slice). ### `n: int (optional)` [Section titled “n: int (optional)”](#n-int-optional) The number of events to keep. Defaults to `10`. ## Examples [Section titled “Examples”](#examples) ### Get the last 10 results [Section titled “Get the last 10 results”](#get-the-last-10-results) ```tql export tail ``` ### Get the last 5 results [Section titled “Get the last 5 results”](#get-the-last-5-results) ```tql export tail 5 ``` ## See Also [Section titled “See Also”](#see-also) [`head`](/reference/operators/head), [`slice`](/reference/operators/slice)

# taste

Limits the input to `n` events per unique schema. ```tql taste [n:int] ``` ## Description [Section titled “Description”](#description) Forwards the first `n` events per unique schema and discards the rest. The `taste` operator provides an exemplary overview of the “shape” of the data described by the pipeline. This helps to understand the diversity of the result, especially when interactively exploring data. ### `n: int (optional)` [Section titled “n: int (optional)”](#n-int-optional) The number of events to keep per schema. Defaults to `10`. ## Examples [Section titled “Examples”](#examples) ### Retrieve at most 10 events of each unique schema [Section titled “Retrieve at most 10 events of each unique schema”](#retrieve-at-most-10-events-of-each-unique-schema) ```tql export taste ``` ### Get only one sample for every unique event type [Section titled “Get only one sample for every unique event type”](#get-only-one-sample-for-every-unique-event-type) ```tql export taste 1 ```

# throttle

Limits the bandwidth of a pipeline. ```tql throttle bandwidth:int, [within=duration] ``` ## Description [Section titled “Description”](#description) The `throttle` operator limits the amount of data flowing through it to a bandwidth. ### `bandwidth: int` [Section titled “bandwidth: int”](#bandwidth-int) The maximum bandwidth that is enforced for this pipeline, in bytes per the specified interval. ### `within = duration (optional)` [Section titled “within = duration (optional)”](#within--duration-optional) The duration over which to measure the maximum bandwidth. Defaults to `1s`. ## Examples [Section titled “Examples”](#examples) ### Read a byte stream at 1 byte per second [Section titled “Read a byte stream at 1 byte per second”](#read-a-byte-stream-at-1-byte-per-second) Read a TCP stream at a rate of 1 character per second: ```tql load_tcp "tcp://0.0.0.0:4000" throttle 1 ``` ### Set a throughput limit for a given time window [Section titled “Set a throughput limit for a given time window”](#set-a-throughput-limit-for-a-given-time-window) Load a sample input data file at a speed of at most 1MiB every 10s and import it into the node: ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" throttle 1Mi, within=10s decompress "zstd" read_zeek_tsv import ```

# timeshift

Adjusts timestamps relative to a given start time, with an optional speedup. ```tql timeshift field:time, [start=time, speed=double] ``` ## Description [Section titled “Description”](#description) The `timeshift` operator adjusts a series of time values by anchoring them around a given `start` time. With `speed`, you can adjust the relative speed of the time series induced by `field` with a multiplicative factor. This has the effect of making the time series “faster” for values great than 1 and “slower” for values less than 1. ![Timeshift](/_astro/timeshift.excalidraw.BVaAeL1C_19DKCs.svg) ### `field: time` [Section titled “field: time”](#field-time) The field containing the timestamp values. ### `start = time (optional)` [Section titled “start = time (optional)”](#start--time-optional) The timestamp to anchor the time values around. Defaults to the first non-null timestamp in `field`. ### `speed = double (optional)` [Section titled “speed = double (optional)”](#speed--double-optional) A constant factor to be divided by the inter-arrival time. For example, 2.0 decreases the event gaps by a factor of two, resulting a twice as fast dataflow. A value of 0.1 creates dataflow that spans ten times the original time frame. Defaults to `1.0`. ## Examples [Section titled “Examples”](#examples) ### Reset events to begin at Jan 1, 1984 [Section titled “Reset events to begin at Jan 1, 1984”](#reset-events-to-begin-at-jan-1-1984) ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" decompress "zstd" read_zeek_tsv timeshift ts, start=1984-01-01 ``` ### Scale inter-arrival times by 100x [Section titled “Scale inter-arrival times by 100x”](#scale-inter-arrival-times-by-100x) As above, but also make the time span of the trace 100 times longer: ```tql load_http "https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst" decompress "zstd" read_zeek_tsv timeshift ts, start=1984-01-01, speed=0.01 ``` ## See Also [Section titled “See Also”](#see-also) [`delay`](/reference/operators/delay)

# to

Saves to an URI, inferring the destination, compression and format. ```tql to uri:string, [saver_args… { … }] ``` ## Description [Section titled “Description”](#description) The `to` operator is an easy way to get data out of Tenzir into It will try to infer the connector, compression and format based on the given URI. ### `uri: string` [Section titled “uri: string”](#uri-string) The URI to load from. ### `saver_args… (optional)` [Section titled “saver\_args… (optional)”](#saver_args-optional) An optional set of arguments passed to the saver. This can be used to e.g. pass credentials to a connector: ```tql to "https://example.org/file.json", headers={Token: "XYZ"} ``` ### `{ … } (optional)` [Section titled “{ … } (optional)”](#---optional) The optional pipeline argument allows for explicitly specifying how `to` compresses and writes data. By default, the pipeline is inferred based on a set of [rules](#explanation). If inference is not possible, or not sufficient, this argument can be used to control compression and writing. Providing this pipeline disables the inference. ## Explanation [Section titled “Explanation”](#explanation) Saving Tenzir data into some resource consists of three steps: * [**Writing**](#writing) events as bytes according to some format * [**compressing**](#compressing) (optional) * [**Saving**](#saving) saving the bytes to some location The `to` operator tries to infer all three steps from the given URI. ### Writing [Section titled “Writing”](#writing) The format to write inferred from the file-ending. Supported file formats are the common file endings for our [`read_*` operators](/reference/operators#parsing). If you want to provide additional arguments to the writer, you can use the [pipeline argument](#---optional) to specify the parsing manually. ### Compressing [Section titled “Compressing”](#compressing) The compression, just as the format, is inferred from the “file-ending” in the URI. Under the hood, this uses the [`decompress_*` operators](/reference/operators#encode--decode). Supported compressions can be found in the [list of compression extensions](#compression). The compression step is optional and will only happen if a compression could be inferred. If you want to write with specific compression settings, you can use the [pipeline argument](#---optional) to specify the decompression manually. ### Saving [Section titled “Saving”](#saving) The connector is inferred based on the URI `scheme://`. If no scheme is present, the connector attempts to save to the local filesystem. ## Supported Deductions [Section titled “Supported Deductions”](#supported-deductions) ### URI schemes [Section titled “URI schemes”](#uri-schemes) | Scheme | Operator | Example | | :--------------------------------- | :-------------------------------------------------------------------------- | :--------------------------------------------- | | `abfs`,`abfss` | [`save_azure_blob_storage`](/reference/operators/save_azure_blob_storage) | `to "abfs://path/to/file.json"` | | `amqp` | [`save_amqp`](/reference/operators/save_amqp) | `to "amqp://…` | | `elasticsearch` | [`to_opensearch`](/reference/operators/to_opensearch) | `to "elasticsearch://…` | | `file` | [`save_file`](/reference/operators/save_file) | `to "file://path/to/file.json"` | | `fluent-bit` | [`to_fluent_bit`](/reference/operators/to_fluent_bit) | `to "fluent-bit://elasticsearch"` | | `ftp`, `ftps` | [`save_ftp`](/reference/operators/save_ftp) | `to "ftp://example.com/file.json"` | | `gcps` | [`save_google_cloud_pubsub`](/reference/operators/save_google_cloud_pubsub) | `to "gcps://project_id/topic_id" { … }` | | `gs` | [`save_gcs`](/reference/operators/save_gcs) | `to "gs://bucket/object.json"` | | `http`, `https` | [`save_http`](/reference/operators/save_http) | `to "http://example.com/file.json"` | | `inproc` | [`save_zmq`](/reference/operators/save_zmq) | `to "inproc://127.0.0.1:56789" { write_json }` | | `kafka` | [`save_kafka`](/reference/operators/save_kafka) | `to "kafka://topic" { write_json }` | | `opensearch` | [`to_opensearch`](/reference/operators/to_opensearch) | `to "opensearch://…` | | `s3` | [`save_s3`](/reference/operators/save_s3) | `to "s3://bucket/file.json"` | | `sqs` | [`save_sqs`](/reference/operators/save_sqs) | `to "sqs://my-queue" { write_json }` | | `tcp` | [`save_tcp`](/reference/operators/save_tcp) | `to "tcp://127.0.0.1:56789" { write_json }` | | `udp` | [`save_udp`](/reference/operators/save_udp) | `to "udp://127.0.0.1:56789" { write_json }` | | `zmq` | [`save_zmq`](/reference/operators/save_zmq) | `to "zmq://127.0.0.1:56789" { write_json }` | | `smtp`, `smtps`, `mailto`, `email` | [`save_email`](/reference/operators/save_email) | `to "smtp://john@example.com"` | Please see the respective operator pages for details on the URI’s locator format. ### File extensions [Section titled “File extensions”](#file-extensions) #### Format [Section titled “Format”](#format) The `to` operator can deduce the file format based on these file-endings: | Format | File Endings | Operator | | :------ | :------------------- | :---------------------------------------------------- | | CSV | `.csv` | [`write_csv`](/reference/operators/write_csv) | | Feather | `.feather`, `.arrow` | [`write_feather`](/reference/operators/write_feather) | | JSON | `.json` | [`write_json`](/reference/operators/write_json) | | NDJSON | `.ndjson`, `.jsonl` | [`write_ndjson`](/reference/operators/write_ndjson) | | Parquet | `.parquet` | [`write_parquet`](/reference/operators/write_parquet) | | Pcap | `.pcap` | [`write_pcap`](/reference/operators/write_pcap) | | SSV | `.ssv` | [`write_ssv`](/reference/operators/write_ssv) | | TSV | `.tsv` | [`write_tsv`](/reference/operators/write_tsv) | | YAML | `.yaml` | [`write_yaml`](/reference/operators/write_yaml) | #### Compression [Section titled “Compression”](#compression) The `to` operator can deduce the following compressions based on these file-endings: | Compression | File Endings | | :---------- | :--------------- | | Brotli | `.br`, `.brotli` | | Bzip2 | `.bz2` | | Gzip | `.gz`, `.gzip` | | LZ4 | `.lz4` | | Zstd | `.zst`, `.zstd` | #### Example transformation: [Section titled “Example transformation:”](#example-transformation) to operator ```tql to "myfile.json.gz" ``` Effective pipeline ```tql write_json compress_gzip save_file "myfile.json.gz" ``` ## Examples [Section titled “Examples”](#examples) ### Save to a local file [Section titled “Save to a local file”](#save-to-a-local-file) ```tql to "path/to/my/output.csv" ``` ### Save to a compressed file [Section titled “Save to a compressed file”](#save-to-a-compressed-file) ```tql to "path/to/my/output.csv.bz2" ``` ## See Also [Section titled “See Also”](#see-also) [from](/reference/operators/from)

# to_amazon_security_lake

Sends OCSF events to Amazon Security Lake. ```tql to_amazon_security_lake s3_uri:string, region=string, account_id=string, [timeout=duration, role=string, external_id=string] ``` ## Description [Section titled “Description”](#description) The `to_amazon_security_lake` operator sends OCSF events to [Amazon Security Lake](https://aws.amazon.com/security-lake/), AWS’s centralized security data repository that normalizes and stores security data from multiple sources. The operator automatically handles Amazon Security Lake’s partitioning requirements and file size constraints, but does not validate the OCSF schema of the events. Consider [`ocsf::apply`](/reference/operators/ocsf/apply) in your pipeline to ensure schema compliance. For a list of OCSF event classes supported by Amazon Security Lake, see the [AWS documentation](https://docs.aws.amazon.com/security-lake/latest/userguide/adding-custom-sources.html#ocsf-eventclass). The operator generates random UUID (v7) file names with a `.parquet` extension. ### `s3_uri: string` [Section titled “s3\_uri: string”](#s3_uri-string) The base URI for the S3 storage backing the lake in the form ```plaintext s3://<bucket>/ext/<custom-source-name> ``` Replace the placeholders as follows: * `<bucket>`: the bucket associated with your lake * `<custom-source-name>`: the name of your custom Amazon Security Lake source You can copy this URI directly from the AWS Security Lake custom source interface. ### `region = string` [Section titled “region = string”](#region--string) The region for partitioning. ### `account_id = string` [Section titled “account\_id = string”](#account_id--string) The AWS account ID or external ID you chose when creating the Amazon Security Lake custom source. ### `timeout = duration (optional)` [Section titled “timeout = duration (optional)”](#timeout--duration-optional) A duration after which the operator will write to Amazon Security Lake, regardless of file size. Amazon Security Lake requires this to be between `5min` and `1d`. Defaults to `5min`. ### `role = string (optional)` [Section titled “role = string (optional)”](#role--string-optional) A role to assume when writing to S3. When not specified, the operator automatically uses the standard Amazon Security Lake provider role based on your configuration: `arn:aws:iam::<account_id>:role/AmazonSecurityLake-Provider-<custom-source-name>-<region>` The operator extracts the custom source name from the provided S3 URI. For example, given: * `account_id`: `"123456789012"` * `s3_uri`: `"s3://aws-security-data-lake-…/ext/tnz-ocsf-4001/"` * `region`: `"eu-west-1"` The operator will use: `arn:aws:iam::123456789012:role/AmazonSecurityLake-Provider-tnz-ocsf-4001-eu-west-1` When defaulted, the operator requires an `external_id` to use the role. You can explicitly disable role authorization by setting `role=null`. ### `external_id = string (optional)` [Section titled “external\_id = string (optional)”](#external_id--string-optional) The external ID to use when assuming the `role`. This is required when using the default role for the custom source. Defaults to no ID. ## Examples [Section titled “Examples”](#examples) ### Send OCSF Network Activity events to Amazon Security Lake [Section titled “Send OCSF Network Activity events to Amazon Security Lake”](#send-ocsf-network-activity-events-to-amazon-security-lake) This example shows how to send OCSF Network Activity events to an AWS Security Lake running on `eu-west-2` with a custom source called `tenzir_network_activity` and account ID `123456789012`: ```tql let $s3_uri = "s3://aws-security-data-lake-eu-west-2-lake-abcdefghijklmnopqrstuvwxyz1234/ext/tnz-ocsf-4001/" subscribe "ocsf" where @name == "ocsf.network_activity" ocsf::apply to_amazon_security_lake $s3_uri, region="eu-west-2", account_id="123456789012" ``` ## See Also [Section titled “See Also”](#see-also) [`ocsf::apply`](/reference/operators/ocsf/apply), [`save_s3`](/reference/operators/save_s3)

# to_azure_log_analytics

Sends events to the Microsoft Azure Logs Ingestion API. ```tql to_azure_log_analytics tenant_id=string, client_id=string, client_secret=string, dce=string, dcr=string, stream=string, [batch_timeout=duration] ``` ## Description [Section titled “Description”](#description) Sends events to the Microsoft [Azure Logs Ingestion API](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview). The `to_azure_log_analytics` operator makes it possible to upload events to [supported tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview#supported-tables) or to [custom tables](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/create-custom-table?tabs=azure-portal-1%2Cazure-portal-2%2Cazure-portal-3#create-a-custom-table) in Microsoft Azure. The operator handles access token retrievals by itself and updates that token automatically, if needed. ### `tenant_id = string` [Section titled “tenant\_id = string”](#tenant_id--string) The Microsoft Directory (tenant) ID, written as `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`. ### `client_id = string` [Section titled “client\_id = string”](#client_id--string) The Microsoft Application (client) ID, written as `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`. ### `client_secret = string` [Section titled “client\_secret = string”](#client_secret--string) The client secret. ### `dce = string` [Section titled “dce = string”](#dce--string) The data collection endpoint URL. ### `dcr = string` [Section titled “dcr = string”](#dcr--string) The data collection rule ID, written as `dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`. ### `stream = string` [Section titled “stream = string”](#stream--string) The stream to upload events to. ### `batch_timeout = duration` [Section titled “batch\_timeout = duration”](#batch_timeout--duration) Maximum duration to wait for new events before sending a batch. Defaults to `5s`. ## Examples [Section titled “Examples”](#examples) ### Upload `custom.mydata` events to the stream `Custom-MyData` [Section titled “Upload custom.mydata events to the stream Custom-MyData”](#upload-custommydata-events-to-the-stream-custom-mydata) ```tql export where @name == "custom.mydata" to_azure_log_analytics tenant_id="00a00a00-0a00-0a00-00aa-000aa0a0a000", client_id="000a00a0-0aa0-00a0-0000-00a000a000a0", client_secret="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", dce="https://my-stuff-a0a0.westeurope-1.ingest.monitor.azure.com", dcr="dcr-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", stream="Custom-MyData" ```

# to_clickhouse

Sends events to a ClickHouse table. ```tql to_clickhouse table=string, [host=string, port=int, user=string, password=string, mode=string, primary=field, tls=bool, cacert=string, certfile=string, keyfile=string, skip_peer_verification=bool, skip_host_verification=bool] ``` ## Description [Section titled “Description”](#description) ### `table = string` [Section titled “table = string”](#table--string) The name of the table you want to write to. When giving a plain table name, it will use the `default` database, otherwise `database.table` can be specified. ### `host = string (optional)` [Section titled “host = string (optional)”](#host--string-optional) The hostname for the ClickHouse server. Defaults to `"localhost"`. ### `port = int (optional)` [Section titled “port = int (optional)”](#port--int-optional) The port for the ClickHouse server. Defaults to `9000` without TLS and `9440` with TLS. ### `user = string (optional)` [Section titled “user = string (optional)”](#user--string-optional) The user to use for authentication. Defaults to `"default"`. ### `password = string (optional)` [Section titled “password = string (optional)”](#password--string-optional) The password for the given user. Defaults to `""`. ### `mode = string (optional)` [Section titled “mode = string (optional)”](#mode--string-optional) * `"create"` if you want to create a table and fail if it already exists * `"append"` to append to an existing table * `"create_append"` to create a table if it does not exist and append to it otherwise. Defaults to `"create_append"`. ### `primary = field (optional)` [Section titled “primary = field (optional)”](#primary--field-optional) The primary key to use when creating a table. Required for `mode = "create"` as well as for `mode = "create_append"` if the table does not yet exist. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. Path to the key for the client certificate. ## Types [Section titled “Types”](#types) Tenzir uses ClickHouse’s [clickhouse-cpp](https://github.com/ClickHouse/clickhouse-cpp) client library to communicate with ClickHouse. The below table explains the translation from Tenzir’s types to ClickHouse: | Tenzir | ClickHouse | Comment | | :--------- | :----------------------------- | :------------------------------------------------------------------------------------------------ | | `bool` | `UInt8` | | | `int64` | `Int64` | | | `uint64` | `UInt64` | | | `double` | `Float64` | | | `ip` | `IPv6` | | | `subnet` | `Tuple(ip IPv6, length UInt8)` | | | `time` | `DateTime64(9)` | | | `duration` | `Int64` | Converted as `nanoseconds(duration)` | | `record` | `Tuple(...)` | Fields in the tuple will be named with the field name. The record must have at least one element. | | `list<T>` | `Array(T)` | | | `blob` | `Array(UInt8)` | Blobs that are `null` will be represented by an empty array | Tenzir also supports `Nullable` versions of the above types (or their nested types). If a `list` itself is `null`, it will be represented by an empty `Array`. If a `record` is `null`, all elements of the `Tuple` will be null, if possible. Otherwise the event will be dropped. ### Table Creation [Section titled “Table Creation”](#table-creation) When a ClickHouse table is created from Tenzir, all columns except the `primary` will be created as `Nullable`. For example, a column of type `ip` will be created as `Nullable(IPv6)`, while a `list<int64>` will be created as `Array(Nullable(Int64))`. The table will be created from the first event the operator receives. Should this first event contain unsupported types/values, an error is raised. #### Untyped nulls [Section titled “Untyped nulls”](#untyped-nulls) Tenzir has both typed and untyped nulls. Typed nulls have a type, but no value. They are no problem for `to_clickhouse`. For untyped nulls, the type itself is `null`, which cannot be supported by the `to_clickhouse` operator when creating a table. Typed and Untyped Nulls in Tenzir ```tql from { typed_null: int(null), untyped_null: null, } ``` Untyped nulls are usually directly caused by nulls in the input, such as in a JSON file: ```json { "value": null } ``` If your input format has untyped nulls, but you know the type, you can either define an a schema and use that when parsing the input, or you can explicitly cast the columns to their desired type: ```tql from ( { value: null }, { value: 42 }, ) value = int(value) // explicit cast turns untyped into typed nulls to_clickhouse "example_table", primary=value ``` #### Empty records [Section titled “Empty records”](#empty-records) Empty records cannot be send to ClickHouse. Should an empty record appear in the first event, an error is raised. ## Examples [Section titled “Examples”](#examples) ### Send CSV file to a local ClickHouse instance, without TLS [Section titled “Send CSV file to a local ClickHouse instance, without TLS”](#send-csv-file-to-a-local-clickhouse-instance-without-tls) ```tql from "my_file.csv" to_clickhouse table="my_table", tls=false ``` ### Create a new table with multiple fields [Section titled “Create a new table with multiple fields”](#create-a-new-table-with-multiple-fields) ```tql from { i: 42, d: 10.0, b: true, l: [42], r:{ s:"string" } } to_clickhouse table="example", primary=i ``` This creates the following table: ```plaintext ┌─name─┬─type────────────────────┐ 1. │ i │ Int64 │ 2. │ d │ Nullable(Float64) │ 3. │ b │ Nullable(UInt8) │ 4. │ l │ Array(Nullable(Int64)) │ 5. │ r │ Tuple( ↴│ │ │↳ s Nullable(String)) │ └──────┴─────────────────────────┘ ```

# to_fluent_bit

Sends events via Fluent Bit. ```tql to_fluent_bit plugin:string, [options=record, fluent_bit_options=record, tls=bool, cacert=string, certfile=string, keyfile=string, skip_peer_verification=bool] ``` ## Description [Section titled “Description”](#description) The `to_fluent_bit` operator acts as a bridge into the [Fluent Bit](https://docs.fluentbit.io) ecosystem, making it possible to send events to Fluent Bit [output plugin](https://docs.fluentbit.io/manual/pipeline/outputs). An invocation of the `fluent-bit` commandline utility ```bash fluent-bit -o plugin -p key1=value1 -p key2=value2 -p… ``` translates to our `to_fluent_bit` operator as follows: ```tql to_fluent_bit "plugin", options={key1: value1, key2:value2, …} ``` ### `plugin: string` [Section titled “plugin: string”](#plugin-string) The name of the Fluent Bit plugin. Run `fluent-bit -h` and look under the **Outputs** section of the help text for available plugin names. The web documentation often comes with an example invocation near the bottom of the page, which also provides a good idea how you could use the operator. ### `options = record (optional)` [Section titled “options = record (optional)”](#options--record-optional) Sets plugin configuration properties. The key-value pairs in this record are equivalent to `-p key=value` for the `fluent-bit` executable. ### `fluent_bit_options = record (optional)` [Section titled “fluent\_bit\_options = record (optional)”](#fluent_bit_options--record-optional) Sets global properties of the Fluent Bit service. E.g., `fluent_bit_options={flush:1, grace:3}`. Consult the list of available [key-value pairs](https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file#config_section) to configure Fluent Bit according to your needs. We recommend factoring these options into the plugin-specific `fluent-bit.yaml` so that they are independent of the `fluent-bit` operator arguments. ### `tls = bool (optional)` Enables TLS. Defaults to `false`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ## URI support & integration with `from` [Section titled “URI support & integration with from”](#uri-support--integration-with-from) The `to_fluent_bit` operator can also be used from the [`to`](/reference/operators/to) operator. For this, the `fluentbit://` scheme can be used. The URI is then translated: ```tql to "fluentbit://plugin" ``` ```tql to_fluent_bit "plugin" ``` ## Examples [Section titled “Examples”](#examples) ### Slack [Section titled “Slack”](#slack) Send events to [Slack](https://docs.fluentbit.io/manual/pipeline/outputs/slack): ```tql let $slack_hook = "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX" to_fluent_bit "slack", options={webhook: $slack_hook} ```

# to_google_cloud_logging

Sends events to Google Cloud Logging. ```tql to_google_cloud_logging log_id=string, [project=string, organization=string, billing_account=string, folder=string,] [resource_type=string, resource_labels=record, payload=string, severity=string, timestamp=time, service_credentials=string, batch_timeout=duration, max_batch_size=int] ``` ## Description [Section titled “Description”](#description) Sends events to [Google Cloud Logging](https://cloud.google.com/logging). ### `log_id = string` [Section titled “log\_id = string”](#log_id--string) ID to associated the ingested logs with. It must be less than 512 characters long and can only include the following characters: upper and lower case alphanumeric characters, forward-slash, underscore, hyphen, and period. ### `project = string (optional)` [Section titled “project = string (optional)”](#project--string-optional) A project id to associated the ingested logs with. ### `organization = string (optional)` [Section titled “organization = string (optional)”](#organization--string-optional) An organization id to associated the ingested logs with. ### `billing_account = string (optional)` [Section titled “billing\_account = string (optional)”](#billing_account--string-optional) A billing account id to associated the ingested logs with. ### `folder = string (optional)` [Section titled “folder = string (optional)”](#folder--string-optional) A folder id to associated the ingested logs with. ### `resource_type = string (optional)` [Section titled “resource\_type = string (optional)”](#resource_type--string-optional) The type of the [monitored resource](https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource). All available types with their associated labels are listed [here](https://cloud.google.com/logging/docs/api/v2/resource-list). Defaults to `global`. ### `resource_labels = record (optional)` [Section titled “resource\_labels = record (optional)”](#resource_labels--record-optional) Record of associated labels for the resource. Values of the record must be of type `string`. Consult the [official docs](https://cloud.google.com/logging/docs/api/v2/resource-list) for available types with their associated labels. ### `payload = string (optional)` [Section titled “payload = string (optional)”](#payload--string-optional) The log entry payload. If unspecified, the incoming event is serialised as JSON and sent. ### `service_credentials = string (optional)` [Section titled “service\_credentials = string (optional)”](#service_credentials--string-optional) JSON credentials to use if using a service account. ### `severity = string (optional)` [Section titled “severity = string (optional)”](#severity--string-optional) Severity of the event. Consult the [official docs](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogSeverity) for available severity levels. Defaults to `default`. ### `timestamp = time (optional)` [Section titled “timestamp = time (optional)”](#timestamp--time-optional) Timestamp of the event. ### `batch_timeout = duration (optional)` [Section titled “batch\_timeout = duration (optional)”](#batch_timeout--duration-optional) Maximum interval between sending the events. Defaults to `5s`. ### `max_batch_size = int (optional)` [Section titled “max\_batch\_size = int (optional)”](#max_batch_size--int-optional) Maximum events to batch before sending. Defaults to `1k`. ## Example [Section titled “Example”](#example) ## Send logs, authenticating automatically via ADC [Section titled “Send logs, authenticating automatically via ADC”](#send-logs-authenticating-automatically-via-adc) ```tql from { content: "log message", timestamp: now(), } to_google_cloud_logging log_id="LOG_ID", project="PROJECT_ID" ``` ## Send logs using a service account [Section titled “Send logs using a service account”](#send-logs-using-a-service-account) ```tql from { content: "totally not a made up log", timestamp: now(), resource: "global", } to_google_cloud_logging log_id="LOG_ID", project="PROJECT_ID" resource_type=resource, service_credentials=file_contents("/path/to/credentials.json") ``` ## See Also [Section titled “See Also”](#see-also) [`to_google_secops`](/reference/operators/to_google_secops)

# to_google_secops

Sends unstructured events to a Google SecOps Chronicle instance. ```tql to_google_secops customer_id=string, private_key=string, client_email=string, log_type=string, log_text=string, [region=string, timestamp=time, labels=record, namespace=string, max_request_size=int, batch_timeout=duration] ``` ## Description [Section titled “Description”](#description) The `to_google_secops` operator makes it possible to ingest events via the [Google SecOps Chronicle unstructured logs ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api#unstructuredlogentries). ### `customer_id = string` [Section titled “customer\_id = string”](#customer_id--string) The customer UUID to use. ### `private_key = string` [Section titled “private\_key = string”](#private_key--string) The private key to use for authentication. This corresponds to the `private_key` in the SecOps collector config. ### `client_email = string` [Section titled “client\_email = string”](#client_email--string) The user email to use for authentication. This corresponds to the `client_email` in the SecOps collector config. ### `log_type = string` [Section titled “log\_type = string”](#log_type--string) The log type of the events. ### `log_text = string` [Section titled “log\_text = string”](#log_text--string) The log text to send. ### `region = string (optional)` [Section titled “region = string (optional)”](#region--string-optional) [Regional prefix](https://cloud.google.com/chronicle/docs/reference/ingestion-api#regional_endpoints) for the Ingestion endpoint (`malachiteingestion-pa.googleapis.com`). ### `timestamp = time (optional)` [Section titled “timestamp = time (optional)”](#timestamp--time-optional) Optional timestamp field to attach to logs. ### `labels = record (optional)` [Section titled “labels = record (optional)”](#labels--record-optional) A record of labels to attach to the logs. For example, `{node: "Configured Tenzir Node"}`. ### `namespace = string (optional)` [Section titled “namespace = string (optional)”](#namespace--string-optional) The namespace to use when ingesting. Defaults to `tenzir`. ### `max_request_size = int (optional)` [Section titled “max\_request\_size = int (optional)”](#max_request_size--int-optional) The maximum number of bytes in the request payload. Defaults to `1M`. ### `batch_timeout = duration (optional)` [Section titled “batch\_timeout = duration (optional)”](#batch_timeout--duration-optional) The maximum duration to wait for new events before sending the request. Defaults to `5s`. ## Examples [Section titled “Examples”](#examples) ```tql from {log: "31-Mar-2025 01:35:02.187 client 0.0.0.0#4238: query: tenzir.com IN A + (255.255.255.255)"} to_google_secops \ customer_id="00000000-0000-0000-00000000000000000", private_key=secret("my_secops_key"), client_email="somebody@example.com", log_text=log, log_type="BIND_DNS", region="europe" ``` ## See Also [Section titled “See Also”](#see-also) [`to_google_cloud_logging`](/reference/operators/to_google_cloud_logging)

# to_hive

Writes events to a URI using hive partitioning. ```tql to_hive uri:string, partition_by=list<field>, format=string, [timeout=duration, max_size=int] ``` ## Description [Section titled “Description”](#description) Hive partitioning is a partitioning scheme where a set of fields is used to partition events. For each combination of these fields, a directory is derived under which all events with the same field values will be stored. For example, if the events are partitioned by the fields `year` and `month`, then the files in the directory `/year=2024/month=10` will contain all events where `year == 2024` and `month == 10`. Files within each partition directory are named using UUIDv7 for guaranteed uniqueness and natural time-based ordering. This prevents filename conflicts when multiple processes write to the same partition simultaneously. ### `uri: string` [Section titled “uri: string”](#uri-string) The base URI for all partitions. ### `partition_by = list<field>` [Section titled “partition\_by = list\<field>”](#partition_by--listfield) A list of fields that will be used for partitioning. Note that these fields will be elided from the output, as their value is already specified by the path. ### `format = string` [Section titled “format = string”](#format--string) The name of the format that will be used for writing, for example `json` or `parquet`. This will also be used for the file extension. ### `timeout = duration (optional)` [Section titled “timeout = duration (optional)”](#timeout--duration-optional) The time after which a new file will be opened for the same partition group. Defaults to `5min`. ### `max_size = int (optional)` [Section titled “max\_size = int (optional)”](#max_size--int-optional) The total file size after which a new file will be opened for the same partition group. Note that files will typically be slightly larger than this limit, because it opens a new file when only after it is exceeded. Defaults to `100M`. ### `compression = string (optional)` [Section titled “compression = string (optional)”](#compression--string-optional) Compress the output files with the given compression algorithm. See docs for the `compress` operator for supported compression algorithms. ## Examples [Section titled “Examples”](#examples) ### Partition by a single field into local JSON files [Section titled “Partition by a single field into local JSON files”](#partition-by-a-single-field-into-local-json-files) ```tql from {a: 0, b: 0}, {a: 0, b: 1}, {a: 1, b: 2} to_hive "/tmp/out/", partition_by=[a], format="json" // This pipeline produces two files: // -> /tmp/out/a=0/<uuid>.json: // {"b": 0} // {"b": 1} // -> /tmp/out/a=1/<uuid>.json: // {"b": 2} ``` ### Write a Parquet file into Azure Blob Store [Section titled “Write a Parquet file into Azure Blob Store”](#write-a-parquet-file-into-azure-blob-store) Write as Parquet into the Azure Blob Filesystem, partitioned by year, month and day. ```tql to_hive "abfs://domain/bucket", partition_by=[year, month, day], format="parquet" // -> abfs://domain/bucket/year=<year>/month=<month>/day=<day>/<uuid>.parquet ``` ### Write partitioned JSON into an S3 bucket [Section titled “Write partitioned JSON into an S3 bucket”](#write-partitioned-json-into-an-s3-bucket) Write JSON into S3, partitioned by year and month, opening a new file after 1 GB. ```tql year = ts.year() month = ts.month() to_hive "s3://my-bucket/some/subdirectory", partition_by=[year, month], format="json", max_size=1G // -> s3://my-bucket/some/subdirectory/year=<year>/month=<month>/<uuid>.json ``` ## See Also [Section titled “See Also”](#see-also) [`read_parquet`](/reference/operators/read_parquet), [`write_bitz`](/reference/operators/write_bitz), [`write_feather`](/reference/operators/write_feather), [`write_parquet`](/reference/operators/write_parquet)

# to_kafka

Sends messages to an Apache Kafka topic. ```tql to_kafka topic:string, [message=blob|string, key=string, timestamp=time, options=record, aws_iam=record] ``` ## Description [Section titled “Description”](#description) The `to_kafka` operator sends one message per event to a Kafka topic. The implementation uses the official [librdkafka](https://github.com/confluentinc/librdkafka) from Confluent and supports all [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md). You can specify them via `options` parameter as `{key: value, ...}`. The operator injects the following default librdkafka configuration values in case no configuration file is present, or when the configuration does not include them: * `bootstrap.servers`: `localhost` * `client.id`: `tenzir` ### `topic: string` [Section titled “topic: string”](#topic-string) The Kafka topic to send messages to. ### `message = blob|string (optional)` [Section titled “message = blob|string (optional)”](#message--blobstring-optional) An expression that evaluates to the message content for each row. Defaults to `this.print_json()` when not specified. ### `key = string (optional)` [Section titled “key = string (optional)”](#key--string-optional) Sets a fixed key for all messages. ### `timestamp = time (optional)` [Section titled “timestamp = time (optional)”](#timestamp--time-optional) Sets a fixed timestamp for all messages. ### `options = record (optional)` [Section titled “options = record (optional)”](#options--record-optional) A record of key-value configuration options for [librdkafka](https://github.com/confluentinc/librdkafka), e.g., `{"acks": "all", "batch.size": 16384}`. The `to_kafka` operator passes the key-value pairs directly to [librdkafka](https://github.com/confluentinc/librdkafka). Consult the list of available [configuration options](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) to configure Kafka according to your needs. We recommend factoring these options into the plugin-specific `kafka.yaml` so that they are independent of the `to_kafka` arguments. ### `aws_iam = record (optional)` [Section titled “aws\_iam = record (optional)”](#aws_iam--record-optional) If specified, enables using AWS IAM Authentication for MSK. The keys must be non-empty when specified. Available keys: * `region`: Region of the MSK Clusters. Must be specified when using IAM. * `assume_role`: Optional Role ARN to assume. * `session_name`: Optional session name to use when assuming a role. * `external_id`: Optional external id to use when assuming a role. The operator will try to get credentials in the following order: 1. Checks your environment variables for AWS Credentials. 2. Checks your `$HOME/.aws/credentials` file for a profile and credentials 3. Contacts and logs in to a trusted identity provider. The login information to these providers can either be on the environment variables: `AWS_ROLE_ARN`, `AWS_WEB_IDENTITY_TOKEN_FILE`, `AWS_ROLE_SESSION_NAME` or on a profile in your `$HOME/.aws/credentials`. 4. Checks for an external method set as part of a profile on `$HOME/.aws/config` to generate or look up credentials that are not directly supported by AWS. 5. Contacts the ECS Task Role to request credentials if Environment variable `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` has been set. 6. Contacts the EC2 Instance Metadata service to request credentials if `AWS_EC2_METADATA_DISABLED` is NOT set to ON. ## Examples [Section titled “Examples”](#examples) ### Send JSON-formatted events to topic `events` (using default) [Section titled “Send JSON-formatted events to topic events (using default)”](#send-json-formatted-events-to-topic-events-using-default) Stream security events to a Kafka topic with automatic JSON formatting: ```tql subscribe "security-alerts" where severity >= "high" select timestamp, source_ip, alert_type, details to_kafka "events" ``` This pipeline subscribes to security alerts, filters for high-severity events, selects relevant fields, and sends them to Kafka as JSON. Each event is automatically formatted using `this.print_json()`, producing messages like: ```json { "timestamp": "2024-03-15T10:30:00.000000", "source_ip": "192.168.1.100", "alert_type": "brute_force", "details": "Multiple failed login attempts detected" } ``` ### Send JSON-formatted events with explicit message [Section titled “Send JSON-formatted events with explicit message”](#send-json-formatted-events-with-explicit-message) ```tql subscribe "logs" to_kafka "events", message=this.print_json() ``` ### Send specific field values with a timestamp [Section titled “Send specific field values with a timestamp”](#send-specific-field-values-with-a-timestamp) ```tql subscribe "logs" to_kafka "alerts", message=alert_msg, timestamp=2024-01-01T00:00:00 ``` ### Send data with a fixed key for partitioning [Section titled “Send data with a fixed key for partitioning”](#send-data-with-a-fixed-key-for-partitioning) ```tql metrics to_kafka "metrics", message=this.print_json(), key="server-01" ``` ## See Also [Section titled “See Also”](#see-also) [`load_kafka`](/reference/operators/load_kafka), [`save_kafka`](/reference/operators/save_kafka)

# to_opensearch

Sends events to an OpenSearch-compatible Bulk API. ```tql to_opensearch url:string, action=string, [index=string, id=string, doc=record, user=string, passwd=string, tls=bool, skip_peer_verification=bool, cacert=string, certfile=string, keyfile=string, include_nulls=bool, max_content_length=int, buffer_timeout=duration, compress=bool] ``` ## Description [Section titled “Description”](#description) The `to_opensearch` operator sends events to a [OpenSearch-compatible Bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/) such as [ElasticSearch](https://www.elastic.co/elasticsearch). The operator accumulates multiple events before sending them as a single request. You can control the maximum request size via the `max_content_length` and the timeout before sending all accumulated events via the `send_timeout` option. ### `url: string` [Section titled “url: string”](#url-string) The URL of the API endpoint. ### `action = string` [Section titled “action = string”](#action--string) An expression for the action that evaluates to a `string`. Supported actions: * `create`: Creates a document if it doesn’t already exist and returns an error otherwise. * `delete`: Deletes a document if it exists. * `index`: Creates a document if it doesn’t yet exist and replace the document if it already exists. * `update`: Updates existing documents and returns an error if the document doesn’t exist. * `upsert`: If a document exists, it is updated; if it does not exist, a new document is indexed. ### `index = string (optional)` [Section titled “index = string (optional)”](#index--string-optional) An optional expression for the index that evaluates to a `string`. Must be provided if the `url` does not have an index. ### `id = string (optional)` [Section titled “id = string (optional)”](#id--string-optional) The `id` of the document to act on. Must be provided when using the `delete` and `update` actions. ### `doc = record (optional)` [Section titled “doc = record (optional)”](#doc--record-optional) The document to serialize. Defaults to `this`. ### `user = string (optional)` [Section titled “user = string (optional)”](#user--string-optional) Optional user for HTTP Basic Authentication. ### `passwd = string (optional)` [Section titled “passwd = string (optional)”](#passwd--string-optional) Optional password for HTTP Basic Authentication. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ### `include_nulls = bool (optional)` [Section titled “include\_nulls = bool (optional)”](#include_nulls--bool-optional) Include fields with null values in the transmitted event data. By default, the operator drops all null values to save space. Defaults to `false`. ### `max_content_length = int (optional)` [Section titled “max\_content\_length = int (optional)”](#max_content_length--int-optional) The maximum size of the message uncompressed body in bytes. A message may consist of multiple events. If a single event is larger than this limit, it is dropped and a warning is emitted. Defaults to `5Mi`. ### `buffer_timeout = duration (optional)` [Section titled “buffer\_timeout = duration (optional)”](#buffer_timeout--duration-optional) The maximum amount of time for which the operator accumulates messages before sending them out to the HEC endpoint as a single message. Defaults to `5s`. ### `compress = bool (optional)` [Section titled “compress = bool (optional)”](#compress--bool-optional) Whether to compress the message body using standard gzip. Defaults to `true`. ## Examples [Section titled “Examples”](#examples) ### Send events from a JSON file [Section titled “Send events from a JSON file”](#send-events-from-a-json-file) ```tql from "example.json" to_opensearch "localhost:9200", action="create", index="main" ``` ## See Also [Section titled “See Also”](#see-also) [`from_opensearch`](/reference/operators/from_opensearch)

# to_sentinelone_data_lake

Sends security events to SentinelOne Singularity Data Lake via REST API. ```tql to_sentinelone_data_lake url:string, token=string, [session_info=record, timeout=duration] ``` ## Description [Section titled “Description”](#description) The `to_sentinelone_data_lake` operator sends incoming events to the [SentinelOne Data Lake REST API](https://support.sentinelone.com/hc/en-us/articles/360004195934-SentinelOne-API-Guide) as structured data, using the `addEvents` endpoint. The operator accumulates multiple events before sending them as a single request, respecting the API’s limits. If events are OCSF events, the `time` and `severity_id` fields are automatically extracted and added to the events meta information. The OCSF `severity_id` is mapped to the SentinelOne Data Lake `sev` property according to this table: | OCSF `severity_id` | SentinelOne severity | | :----------------: | :------------------: | | 0 (Unknown) | 3 (info) | | 1 (Informational) | 1 (finer) | | 2 (Low) | 2 (fine) | | 3 (Medium) | 3 (info) | | 4 (High) | 4 (warn) | | 5 (Critical) | 5 (error) | | 6 (Fatal) | 6 (fatal) | | 99 (Other) | 3 (info) | ### `url: string` [Section titled “url: string”](#url-string) The ingest URL for the Data Lake. Please note that using the wrong ingestion endpoint, such as an incorrect region, may silently fail, as the SentinelOne API responds with 200 OK, even for some erroneous requests. ### `token = string` [Section titled “token = string”](#token--string) The token to use for authorization. ### `session_info = record (optional)` [Section titled “session\_info = record (optional)”](#session_info--record-optional) Some additional sessionInfo to send with each batch of events, as the `sessionInfo` field in the request body. If this option is used, it is recommended that it contains a field `serverHost` to identify the Node. This can also contain a field `parser`, which names a SentinelOne parser that will be applied to the data field `message`. This can be used to ingest unstructured data. ### `timeout = duration (optional)` [Section titled “timeout = duration (optional)”](#timeout--duration-optional) The delay after which events are sent, even if this results in fewer events sent per message. Defaults to `1min`. ## Examples [Section titled “Examples”](#examples) ### Send events to SentinelOne Data Lake [Section titled “Send events to SentinelOne Data Lake”](#send-events-to-sentinelone-data-lake) ```tql to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("sentinelone-token") ``` ### Send additional session information [Section titled “Send additional session information”](#send-additional-session-information) ```tql to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("sentinelone-token"), session_info={ serverHost: "Node 42", serverType: "Tenzir Node", region: "Planet Earth", } ``` ### Send ‘unstructured’ data [Section titled “Send ‘unstructured’ data”](#send-unstructured-data) The operator can also be used to send unstructured data to be parsed by SentinelOne. For this, the operators input must contain a field `message` and a `parser` must be specified in the `session_info`: ```tql select message = this.print_ndjson(); // Format the entire event as JSON to_sentinelone_data_lake "https://ingest.eu1.sentinelone.net", token=secret("sentinelone-token"), session_info={ serverHost: "Node 42", parser: "json", // Have SentinelOne parse the data } ``` Ingest Costs SentinelOne charges per ingested *value* byte in the events. This means that you get charged for all bytes in `message`, including the keys, structural elements and whitespace. If you already have structured data in Tenzir, prefer sending structured data, as you will only be charged for the values and one byte per key, as opposed to the full keys and structural characters in `message`.

# to_snowflake

Sends events to a Snowflake database. ```tql to_snowflake account_identifier=string, user_name=string, password=string, snowflake_database=string snowflake_schema=string table=string, [ingest_mode=string] ``` ## Description [Section titled “Description”](#description) The `to_snowflake` operator makes it possible to send events to a [Snowflake](https://www.snowflake.com/) database. It uploads the events via bulk-ingestion under the hood and then copies them into the target table. The operator supports nested types as [Snowflake semi-structured types](https://docs.snowflake.com/en/sql-reference/data-types-semistructured). Alternatively, you can use the [`flatten`](/reference/functions/flatten) function operator beforehand. ### `account_identifier = string` [Section titled “account\_identifier = string”](#account_identifier--string) The [Snowflake account identifier](https://docs.snowflake.com/en/user-guide/admin-account-identifier) to use. ### `user_name = string` [Section titled “user\_name = string”](#user_name--string) The Snowflake user name. The user must have the [`CREATE STAGE`](https://docs.snowflake.com/en/sql-reference/sql/create-stage#access-control-requirements) privilege on the given schema. ### `password = string` [Section titled “password = string”](#password--string) The password for the user. ### `database = string` [Section titled “database = string”](#database--string) The [Snowflake database](https://docs.snowflake.com/en/sql-reference/ddl-database) to write to. The user must be allowed to access it. ### `schema = string` [Section titled “schema = string”](#schema--string) The [Snowflake schema](https://docs.snowflake.com/en/sql-reference/ddl-database) to use. The user be allowed to access it. ### `table = string` [Section titled “table = string”](#table--string) The name of the table that should be used/created. The user must have the required permissions to create/write to it. Table columns that are not in the event will be null, while event fields that are not in the table will be dropped. Type mismatches between the table and events are a hard error. ### `ingest_mode = string (optional)` [Section titled “ingest\_mode = string (optional)”](#ingest_mode--string-optional) You can set the ingest mode to one of three options: * `"create_append"`: Creates the table if it does not exist, otherwise appends to it. * `"create"`: creates the table, causing an error if it already exists. * `"append"`: appends to the table, causing an error if it does not exist. In case the operator creates the table it will use the the first event to infer the columns. Default to `"create_append"`. ## Examples [Section titled “Examples”](#examples) ### Send an event to a Snowflake table [Section titled “Send an event to a Snowflake table”](#send-an-event-to-a-snowflake-table) Upload `suricata.alert` events to a table `TENZIR` in `MY_DB@SURICATA_ALERT`: ```tql export where @name == "suricata.alert" to_snowflake \ account_identifier="asldyuf-xgb47555", user_name="tenzir_user", password="password1234", database="MY_DB", schema="SURICATA_ALERT", table="TENZIR" ```

# to_splunk

Sends events to a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector). ```tql to_splunk url:string, hec_token=string, [event=any, host=string, source=string, sourcetype=expr, index=expr, tls=bool, cacert=string, certfile=string, keyfile=string, skip_peer_verification=bool, print_nulls=bool, max_content_length=int, buffer_timeout=duration, compress=bool] ``` ## Description [Section titled “Description”](#description) The `to_splunk` operator sends events to a Splunk [HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector). The source type defaults to `_json` and the operator renders incoming events as JSON. You can specify a different source type via the `sourcetype` option. The operator accumulates multiple events before sending them as a single message to the HEC endpoint. You can control the maximum message size via the `max_content_length` and the timeout before sending all accumulated events via the `send_timeout` option. ### `url: string` [Section titled “url: string”](#url-string) The address of the Splunk indexer. ### `hec_token = string` [Section titled “hec\_token = string”](#hec_token--string) The [HEC token](https://docs.splunk.com/Documentation/Splunk/9.3.1/Data/UsetheHTTPEventCollector#Create_an_Event_Collector_token_on_Splunk_Cloud_Platform) for authentication. ### `event = any (optional)` [Section titled “event = any (optional)”](#event--any-optional) The event to send. Defaults to `this`, meaning the entire event is sent. ### `host = string (optional)` [Section titled “host = string (optional)”](#host--string-optional) An optional value for the [Splunk `host`](https://docs.splunk.com/Splexicon:Host). ### `source = string (optional)` [Section titled “source = string (optional)”](#source--string-optional) An optional value for the [Splunk `source`](https://docs.splunk.com/Splexicon:Source). ### `sourcetype = expr (optional)` [Section titled “sourcetype = expr (optional)”](#sourcetype--expr-optional) An optional expression for [Splunk’s `sourcetype`](https://docs.splunk.com/Splexicon:Sourcetype) that evaluates to a `string`. You can use this to set the `sourcetype` per event, by providing a field instead of a string. Regardless of the chosen `sourcetype`, the event itself is passed as a json object in `event` key of level object that is sent. Defaults to `_json`. ### `index = expr (optional)` [Section titled “index = expr (optional)”](#index--expr-optional) An optional expression for the [Splunk `index`](https://docs.splunk.com/Splexicon:Index) that evaluates to a `string`. If you do not provide this option, Splunk will use the default index. **NB**: HEC silently drops events with an invalid `index`. ### `tls = bool (optional)` Enables TLS. Defaults to `true`. ### `cacert = string (optional)` Path to the CA certificate used to verify the server’s certificate. Defaults to the Tenzir configuration value `tenzir.cacert`, which in turn defaults to a common cacert location for the system. ### `certfile = string (optional)` Path to the client certificate. ### `keyfile = string (optional)` Path to the key for the client certificate. ### `skip_peer_verification = bool (optional)` Toggles TLS certificate verification. Defaults to `false`. ### `include_nulls = bool (optional)` [Section titled “include\_nulls = bool (optional)”](#include_nulls--bool-optional) Include fields with null values in the transmitted event data. By default, the operator drops all null values to save space. ### `max_content_length = int (optional)` [Section titled “max\_content\_length = int (optional)”](#max_content_length--int-optional) The maximum size of the message uncompressed body in bytes. A message may consist of multiple events. If a single event is larger than this limit, it is dropped and a warning is emitted. This corresponds with Splunk’s [`max_content_length`](https://docs.splunk.com/Documentation/Splunk/9.3.1/Admin/Limitsconf#.5Bhttp_input.5D) option. Be aware that [Splunk Cloud has a default of `1MB`](https://docs.splunk.com/Documentation/SplunkCloud/9.2.2406/Service/SplunkCloudservice#Using_HTTP_Event_Collector_.28HEC.29) for `max_content_length`. Defaults to `5Mi`. ### `buffer_timeout = duration (optional)` [Section titled “buffer\_timeout = duration (optional)”](#buffer_timeout--duration-optional) The maximum amount of time for which the operator accumulates messages before sending them out to the HEC endpoint as a single message. Defaults to `5s`. ### `compress = bool (optional)` [Section titled “compress = bool (optional)”](#compress--bool-optional) Whether to compress the message body using standard gzip. Defaults to `true`. ## Examples [Section titled “Examples”](#examples) ### Send a JSON file to a HEC endpoint [Section titled “Send a JSON file to a HEC endpoint”](#send-a-json-file-to-a-hec-endpoint) ```tql load_file "example.json" read_json to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token") ``` ### Data Dependent Splunk framing [Section titled “Data Dependent Splunk framing”](#data-dependent-splunk-framing) By default, the `to_splunk` operator sends the entire event as the `event` field to the HEC, together with any optional Splunk “frame” fields such as `host`, `source`, `sourcetype` and `index`. These special properties can be set using the operators respective arguments, with an expression that is evaluated per event. However, this means that these special properties may be transmitted as both part of `event` and as part of the Splunk frame. This can be especially undesirable when the events are supposed to adhere to a specific schema, such as OCSF. In this case, you can specify the additional `event` option to specify which part of the incoming event should be sent as the event. ```tql from { host: "my-host", source: "my-source", a: 42, b: 0, message: "text", nested: { x: 0 }, } // move the entire event into `event` this = { event: this } // hoist the splunk specific fields back out, so they are no longer part of the // sent event move host = event.host, source = event.source to_splunk "https://localhost:8088", hec_token=secret("splunk-hec-token"), host=host, source=source, event=event ```

# top

Shows the most common values. ```tql top x:field ``` ## Description [Section titled “Description”](#description) Shows the most common values for a given field. For each value, a new event containing its count will be produced. In general, `top x` is equivalent to: ```tql summarize x, count=count() sort -count ``` This operator is the dual to [`rare`](/reference/operators/rare). ### `x: field` [Section titled “x: field”](#x-field) The field to find the most common values for. ## Examples [Section titled “Examples”](#examples) ### Find the most common values [Section titled “Find the most common values”](#find-the-most-common-values) ```tql from {x: "B"}, {x: "A"}, {x: "A"}, {x: "B"}, {x: "A"}, {x: "D"}, {x: "C"}, {x: "C"} top x ``` ```tql {x: "A", count: 3} {x: "B", count: 2} {x: "C", count: 2} {x: "D", count: 1} ``` ### Show the 5 top-most values [Section titled “Show the 5 top-most values”](#show-the-5-top-most-values) ```tql top id.orig_h head 5 ``` ## See Also [Section titled “See Also”](#see-also) [`summarize`](/reference/operators/summarize), [`rare`](/reference/operators/rare), [`sort`](/reference/operators/sort)

# unordered

Removes ordering assumptions from a pipeline. ```tql unordered { … } ``` ## Description [Section titled “Description”](#description) The `unordered` operator takes a pipeline as an argument and removes ordering assumptions from it. This causes some operators to run faster. Note that some operators implicitly remove ordering assumptions. For example, `sort` tells upstream operators that ordering does not matter. ## Examples [Section titled “Examples”](#examples) ### Parse JSON unordered [Section titled “Parse JSON unordered”](#parse-json-unordered) ```tql unordered { read_json } ```

# unroll

Returns a new event for each member of a list or a record in an event, duplicating the surrounding event. ```tql unroll [field:list|record] ``` ## Description [Section titled “Description”](#description) The `unroll` returns an event for each member of a specified list or record field, leaving the surrounding event unchanged. Drops events where the specified field is an empty record, an empty list, or null. ### `field: list|record` [Section titled “field: list|record”](#field-listrecord) Sets the name of the list or record field. ## Examples [Section titled “Examples”](#examples) ### Unroll a list [Section titled “Unroll a list”](#unroll-a-list) ```tql from {x: "a", y: [1, 2, 3]}, {x: "b", y: []}, {x: "c", y: [4]} unroll y ``` ```tql {x: "a", y: 1} {x: "a", y: 2} {x: "a", y: 3} {x: "c", y: 4} ``` ### Unroll a record [Section titled “Unroll a record”](#unroll-a-record) ```tql from {x: "a", y: {foo: 1, baz: 2}}, {x: "b", y: {foo: null, baz: 3}}, {x: "c", y: null} unroll y ``` ```tql {x: "a", y: {foo: 1}} {x: "a", y: {baz: 2}} {x: "b", y: {foo: null}} {x: "b", y: {baz: 3}} ```

# version

Shows the current version. ```tql version ``` ## Description [Section titled “Description”](#description) The `version` operator shows the current Tenzir version. ## Schemas [Section titled “Schemas”](#schemas) Tenzir emits version information with the following schema. ### `tenzir.version` [Section titled “tenzir.version”](#tenzirversion) Contains detailed information about the process version. | Field | Type | Description | | :------------- | :------------- | :--------------------------------------------------------------------------------- | | `version` | `string` | The formatted version string. | | `tag` | `string` | An optional identifier of the build. | | `major` | `uint64` | The major release version. | | `minor` | `uint64` | The minor release version. | | `patch` | `uint64` | The patch release version. | | `features` | `list<string>` | A list of feature flags that conditionally enable features in the Tenzir Platform. | | `build` | `record` | Build-time configuration options. | | `dependencies` | `list<record>` | A list of build-time dependencies and their versions. | The `build` record contains the following fields: | Field | Type | Description | | :----------- | :------- | :------------------------------------------------------------------------- | | `type` | `string` | The configured build type. One of `Release`, `Debug`, or `RelWithDebInfo`. | | `tree_hash` | `string` | A hash of all files in the source directory. | | `assertions` | `bool` | Whether potentially expensive run-time checks are enabled. | | `sanitizers` | `record` | Contains information about additional run-time checks from sanitizers. | The `build.sanitzers` record contains the following fields: | Field | Type | Description | | :------------------- | :----- | :--------------------------------------------------- | | `address` | `bool` | Whether the address sanitizer is enabled. | | `undefined_behavior` | `bool` | Whether the undefined behavior sanitizer is enabled. | The `dependencies` record contains the following fields: | Field | Type | Description | | :-------- | :------- | :----------------------------- | | `name` | `string` | The name of the dependency. | | `version` | `string` | THe version of the dependency. | ## Examples [Section titled “Examples”](#examples) ### Show the current version [Section titled “Show the current version”](#show-the-current-version) ```tql version drop dependencies ``` ```tql { version: "5.0.1+g847fcc6334", tag: "g847fcc6334", major: 5, minor: 0, patch: 1, features: [ "chart_limit", "modules", "tql2_from", "exact_schema", "tql2_only", ], build: { type: "Release", tree_hash: "ef28a81eb124cc46a646250d1fb17390", assertions: false, sanitizers: { address: false, undefined_behavior: false, }, }, } ```

# where

Keeps only events for which the given predicate is true. ```tql where predicate:bool ``` ## Description [Section titled “Description”](#description) The `where` operator only keeps events that match the provided predicate and discards all other events. Only events for which it evaluates to `true` pass. ## Examples [Section titled “Examples”](#examples) ### Keep only events where `src_ip` is `1.2.3.4` [Section titled “Keep only events where src\_ip is 1.2.3.4”](#keep-only-events-where-src_ip-is-1234) ```tql where src_ip == 1.2.3.4 ``` ### Use a nested field name and a temporal constraint on the `ts` field [Section titled “Use a nested field name and a temporal constraint on the ts field”](#use-a-nested-field-name-and-a-temporal-constraint-on-the-ts-field) ```tql where id.orig_h == 1.2.3.4 and ts > now() - 1h ``` ### Combine subnet, size and duration constraints [Section titled “Combine subnet, size and duration constraints”](#combine-subnet-size-and-duration-constraints) ```tql where src_ip in 10.10.5.0/25 and (orig_bytes > 1Mi or duration > 30min) ``` ## See Also [Section titled “See Also”](#see-also) [`assert`](/reference/operators/assert), [`drop`](/reference/operators/drop), [`select`](/reference/operators/select)

# write_bitz

Writes events in *BITZ* format. ```tql write_bitz ``` ## Description [Section titled “Description”](#description) BITZ is short for **Bi**nary **T**en**z**ir and is our internal wire format. Use BITZ when you need high-throughput structured data exchange with minimal overhead. BITZ is a thin wrapper around Arrow’s record batches. That is, BITZ lays out data in a (compressed) columnar fashion that makes it conducive for analytical workloads. Since it’s padded and byte-aligned, it is portable and doesn’t induce any deserialization cost, making it suitable for write-once-read-many use cases. Internally, BITZ uses Arrow’s IPC format for serialization and deserialization, but prefixes each message with a 64 bit size prefix to support changing schemas between batches—something that Arrow’s IPC format does not support on its own. ## See Also [Section titled “See Also”](#see-also) [`read_bitz`](/reference/operators/read_bitz), [`to_hive`](/reference/operators/to_hive), [`write_feather`](/reference/operators/write_feather), [`write_parquet`](/reference/operators/write_parquet)

# write_csv

Transforms event stream to CSV (Comma-Separated Values) byte stream. ```tql write_csv [list_separator=str, null_value=str, no_header=bool] ``` ## Description [Section titled “Description”](#description) The `write_csv` operator transforms an event stream into a byte stream by writing the events as CSV. ### `list_separator = str (optional)` [Section titled “list\_separator = str (optional)”](#list_separator--str-optional) The string separating different elements in a list within a single field. Defaults to `";"`. ### `null_value = str (optional)` [Section titled “null\_value = str (optional)”](#null_value--str-optional) The string denoting an absent value. Defaults to `" "`. ### `no_header = bool (optional)` [Section titled “no\_header = bool (optional)”](#no_header--bool-optional) Whether to not print a header line containing the field names. ## Examples [Section titled “Examples”](#examples) Write an event as CSV. ```tql from {x:1, y:true, z: "String"} write_csv ``` ```plaintext x,y,z 1,true,String ``` ## See Also [Section titled “See Also”](#see-also) [`parse_csv`](/reference/functions/parse_csv), [`print_csv`](/reference/functions/print_csv), [`read_csv`](/reference/operators/read_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_feather

Transforms the input event stream to Feather byte stream. ```tql write_feather [compression_level=int, compression_type=str, min_space_savings=double] ``` ## Description [Section titled “Description”](#description) Transforms the input event stream to [Feather](https://arrow.apache.org/docs/python/feather.html) (a thin wrapper around [Apache Arrow’s IPC](https://arrow.apache.org/docs/python/ipc.html) wire format) byte stream. ### `compression_level = int (optional)` [Section titled “compression\_level = int (optional)”](#compression_level--int-optional) An optional compression level for the corresponding compression type. This option is ignored if no compression type is specified. Defaults to the compression type’s default compression level. ### `compression_type = str (optional)` [Section titled “compression\_type = str (optional)”](#compression_type--str-optional) Supported options are `zstd` for [Zstandard](http://facebook.github.io/zstd/) compression and `lz4` for [LZ4 Frame](https://android.googlesource.com/platform/external/lz4/+/HEAD/doc/lz4_Frame_format.md) compression. ### `min_space_savings = double (optional)` [Section titled “min\_space\_savings = double (optional)”](#min_space_savings--double-optional) Minimum space savings percentage required for compression to be applied. This option is ignored if no compression is specified. The provided value must be between 0 and 1 inclusive. Space savings are calculated as `1.0 - compressed_size / uncompressed_size`. For example, with a minimum space savings rate of 0.1, a 100-byte body buffer will not be compressed if its expected compressed size exceeds 90 bytes. Defaults to `0`, i.e., always applying compression. ## Examples [Section titled “Examples”](#examples) ### Convert a JSON stream into a Feather file [Section titled “Convert a JSON stream into a Feather file”](#convert-a-json-stream-into-a-feather-file) ```tql load_file "input.json" read_json write_feather save_file "output.feather" ``` ## See Also [Section titled “See Also”](#see-also) [`read_bitz`](/reference/operators/read_bitz), [`read_feather`](/reference/operators/read_feather), [`to_hive`](/reference/operators/to_hive), [`write_bitz`](/reference/operators/write_bitz), [`write_parquet`](/reference/operators/write_parquet)

# write_json

Transforms the input event stream to a JSON byte stream. ```tql write_json [strip=bool, color=bool, arrays_of_objects=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool] ``` ## Description [Section titled “Description”](#description) Transforms the input event stream to a JSON byte stream. ### `strip = bool (optional)` [Section titled “strip = bool (optional)”](#strip--bool-optional) Enables all `strip_*` options. Defaults to `false`. ### `color = bool (optional)` [Section titled “color = bool (optional)”](#color--bool-optional) Colorize the output. Defaults to `false`. ### `arrays_of_objects = bool (optional)` [Section titled “arrays\_of\_objects = bool (optional)”](#arrays_of_objects--bool-optional) Prints the input as a single array of objects, instead of as separate objects. Defaults to `false`. ### `strip_null_fields = bool (optional)` [Section titled “strip\_null\_fields = bool (optional)”](#strip_null_fields--bool-optional) Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` [Section titled “strip\_nulls\_in\_lists = bool (optional)”](#strip_nulls_in_lists--bool-optional) Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` [Section titled “strip\_empty\_records = bool (optional)”](#strip_empty_records--bool-optional) Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` [Section titled “strip\_empty\_lists = bool (optional)”](#strip_empty_lists--bool-optional) Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Convert a YAML stream into a JSON file [Section titled “Convert a YAML stream into a JSON file”](#convert-a-yaml-stream-into-a-json-file) ```tql load_file "input.yaml" read_yaml write_json save_file "output.json" ``` ### Strip null fields [Section titled “Strip null fields”](#strip-null-fields) ```tql from { yes: 1, no: null} write_json strip_null_fields=true ``` ```json { "yes": 1 } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_json`](/reference/functions/parse_json), [`print_json`](/reference/functions/print_json), [`read_json`](/reference/operators/read_json), [`write_tql`](/reference/operators/write_tql)

# write_kv

Writes events in a Key-Value format. ```tql write_kv [field_separator=str, value_separator=str, list_separator=str, flatten_separator=str, null_value=str] ``` ## Description [Section titled “Description”](#description) Writes events in a Key-Value format, with one event per line. Nested data will be flattend, keys or values containing the given separators will be quoted and the special characters `\n`, `\r`, `\` and `"` will be escaped. ### `field_separator = str (optional)` [Section titled “field\_separator = str (optional)”](#field_separator--str-optional) A string that shall separate the key-value pairs. Must not be an empty string. Defaults to `" "`. ### `value_separator = str (optional)` [Section titled “value\_separator = str (optional)”](#value_separator--str-optional) A string that shall separate key and value within key-value pair. Must not be an empty string. Defaults to `"="`. ### `list_separator = str (optional)` [Section titled “list\_separator = str (optional)”](#list_separator--str-optional) Must not be an empty string. Defaults to `","`. ### `flatten_separator = str (optional)` [Section titled “flatten\_separator = str (optional)”](#flatten_separator--str-optional) A string to join the keys of nested records with. For example, given `flatten="."` Defaults to `"."`. ### `null_value = str (optional)` [Section titled “null\_value = str (optional)”](#null_value--str-optional) A string to represent null values. Defaults to the empty string. ## Examples [Section titled “Examples”](#examples) ### Write key-value pairs with quoted strings [Section titled “Write key-value pairs with quoted strings”](#write-key-value-pairs-with-quoted-strings) ```tql from {x: "hello world", y: "hello=world"} ``` ```txt x="hello world" y:"hello=world" ``` ### Write key-value pairs of nested records [Section titled “Write key-value pairs of nested records”](#write-key-value-pairs-of-nested-records) ```tql from {x: {y: {z:0}, y2:42}, a: "string" } write_kv ``` ```txt x.y.z=0 y.y2=42 a=string ``` ## See Also [Section titled “See Also”](#see-also) [`parse_kv`](/reference/functions/parse_kv), [`print_kv`](/reference/functions/print_kv), [`read_kv`](/reference/operators/read_kv), [`write_lines`](/reference/operators/write_lines)

# write_lines

Writes events as key-value pairsthe *values* of an event. ```tql write_lines ``` ## Description [Section titled “Description”](#description) Each event is printed on a new line, with fields separated by spaces, and nulls skipped. ## Examples [Section titled “Examples”](#examples) ### Write the values of an event [Section titled “Write the values of an event”](#write-the-values-of-an-event) ```tql from {x:1, y:true, z: "String"} write_lines ``` ```txt 1 true String ``` ## See Also [Section titled “See Also”](#see-also) [`read_lines`](/reference/operators/read_lines), [`write_csv`](/reference/operators/write_csv), [`write_kv`](/reference/operators/write_kv), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_ndjson

Transforms the input event stream to a Newline-Delimited JSON byte stream. ```tql write_ndjson [strip=bool, color=bool, arrays_of_objects=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool] ``` ## Description [Section titled “Description”](#description) Transforms the input event stream to a Newline-Delimited JSON byte stream. ### `strip = bool (optional)` [Section titled “strip = bool (optional)”](#strip--bool-optional) Enables all `strip_*` options. Defaults to `false`. ### `color = bool (optional)` [Section titled “color = bool (optional)”](#color--bool-optional) Colorize the output. Defaults to `false`. ### `arrays_of_objects = bool (optional)` [Section titled “arrays\_of\_objects = bool (optional)”](#arrays_of_objects--bool-optional) Prints the input as a single array of objects, instead of as separate objects. Defaults to `false`. ### `strip_null_fields = bool (optional)` [Section titled “strip\_null\_fields = bool (optional)”](#strip_null_fields--bool-optional) Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` [Section titled “strip\_nulls\_in\_lists = bool (optional)”](#strip_nulls_in_lists--bool-optional) Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` [Section titled “strip\_empty\_records = bool (optional)”](#strip_empty_records--bool-optional) Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` [Section titled “strip\_empty\_lists = bool (optional)”](#strip_empty_lists--bool-optional) Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Convert a YAML stream into a JSON file [Section titled “Convert a YAML stream into a JSON file”](#convert-a-yaml-stream-into-a-json-file) ```tql load_file "input.yaml" read_yaml write_ndjson save_file "output.json" ``` ### Strip null fields [Section titled “Strip null fields”](#strip-null-fields) ```tql from { yes: 1, no: null, } write_ndjson strip_null_fields=true ``` ```json { "yes": 1 } ``` ## See Also [Section titled “See Also”](#see-also) [`parse_json`](/reference/functions/parse_json), [`print_ndjson`](/reference/functions/print_ndjson), [`print_json`](/reference/functions/print_json), [`read_json`](/reference/operators/read_json), [`read_ndjson`](/reference/operators/read_ndjson), [`write_json`](/reference/operators/write_json)

# write_parquet

Transforms event stream to a Parquet byte stream. ```tql write_parquet [compression_level=int, compression_type=str] ``` ## Description [Section titled “Description”](#description) [Apache Parquet](https://parquet.apache.org/) is a columnar storage format that a variety of data tools support. ### `compression_level = int (optional)` [Section titled “compression\_level = int (optional)”](#compression_level--int-optional) An optional compression level for the corresponding compression type. This option is ignored if no compression type is specified. Defaults to the compression type’s default compression level. ### `compression_type = str (optional)` [Section titled “compression\_type = str (optional)”](#compression_type--str-optional) Specifies an optional compression type. Supported options are `zstd` for [Zstandard](http://facebook.github.io/zstd/) compression, `brotli` for [brotli](https://www.brotli.org) compression, `gzip` for [gzip](https://www.gzip.org) compression, and `snappy` for [snappy](https://google.github.io/snappy/) compression. ## Examples [Section titled “Examples”](#examples) Write a Parquet file: ```tql load_file "/tmp/data.json" read_json write_parquet ``` ## See Also [Section titled “See Also”](#see-also) [`read_bitz`](/reference/operators/read_bitz), [`read_parquet`](/reference/operators/read_parquet), [`to_hive`](/reference/operators/to_hive), [`write_bitz`](/reference/operators/write_bitz), [`write_feather`](/reference/operators/write_feather)

# write_pcap

Transforms event stream to PCAP byte stream. ```tql write_pcap ``` ## Description [Section titled “Description”](#description) Transforms event stream to [PCAP](https://datatracker.ietf.org/doc/id/draft-gharris-opsawg-pcap-00.html) byte stream. The structured representation of packets has the `pcap.packet` schema: ```yaml pcap.packet: record: - linktype: uint64 - time: timestamp: time - captured_packet_length: uint64 - original_packet_length: uint64 - data: string ``` ## Examples [Section titled “Examples”](#examples) ### Write packet events as a PCAP file [Section titled “Write packet events as a PCAP file”](#write-packet-events-as-a-pcap-file) ```tql subscribe "packets" write_pcap save_file "/logs/packets.pcap" ``` ## See Also [Section titled “See Also”](#see-also) [`load_nic`](/reference/operators/load_nic), [`read_pcap`](/reference/operators/read_pcap)

# write_ssv

Transforms event stream to SSV (Space-Separated Values) byte stream. ```tql write_ssv [list_separator=str, null_value=str, no_header=bool] ``` ## Description [Section titled “Description”](#description) The `write_ssv` operator transforms an event stream into a byte stream by writing the events as SSV. ### `list_separator = str (optional)` [Section titled “list\_separator = str (optional)”](#list_separator--str-optional) The string separating different elements in a list within a single field. Defaults to `","`. ### `null_value = str (optional)` [Section titled “null\_value = str (optional)”](#null_value--str-optional) The string denoting an absent value. Defaults to `"-"`. ### `no_header = bool (optional)` [Section titled “no\_header = bool (optional)”](#no_header--bool-optional) Whether to not print a header line containing the field names. ## Examples [Section titled “Examples”](#examples) Write an event as SSV. ```tql from {x:1, y:true, z: "String"} write_ssv ``` ```plaintext x y z 1 true String ``` ## See Also [Section titled “See Also”](#see-also) [`print_ssv`](/reference/functions/print_ssv), [`read_ssv`](/reference/operators/read_ssv), [`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_tsv`](/reference/operators/write_tsv), [`write_xsv`](/reference/operators/write_xsv)

# write_syslog

Writes events as syslog. ```tql write_syslog [facility=int, severity=int, timestamp=time, hostname=string, app_name=string, process_id=string, message_id=string, structured_data=record, message=string] ``` ## Description [Section titled “Description”](#description) Writes events as [RFC 5424](https://datatracker.ietf.org/doc/html/rfc5424) Syslog messages. All options to the operator try to get values for the respective fields from the same-named fields in the input events if unspecified. ### `facility = int (optional)` [Section titled “facility = int (optional)”](#facility--int-optional) Set the facility of the syslog. Defaults to `1` if `null`. ### `severity = int (optional)` [Section titled “severity = int (optional)”](#severity--int-optional) Set the severity of the syslog. Defaults to `6` if `null`. ### `timestamp = time (optional)` [Section titled “timestamp = time (optional)”](#timestamp--time-optional) Set the timestamp of the syslog. ### `hostname = string (optional)` [Section titled “hostname = string (optional)”](#hostname--string-optional) Set the hostname of the syslog. ### `app_name = string (optional)` [Section titled “app\_name = string (optional)”](#app_name--string-optional) Set the application name of the syslog. ### `process_id = string (optional)` [Section titled “process\_id = string (optional)”](#process_id--string-optional) Set the process id of the syslog. ### `message_id = string (optional)` [Section titled “message\_id = string (optional)”](#message_id--string-optional) Set the message id of the syslog. ### `structured_data = record (optional)` [Section titled “structured\_data = record (optional)”](#structured_data--record-optional) Set the structured data of the syslog. ### `message = string (optional)` [Section titled “message = string (optional)”](#message--string-optional) Set the message of the syslog. ## Examples [Section titled “Examples”](#examples) ### Create a syslog manually [Section titled “Create a syslog manually”](#create-a-syslog-manually) ```tql from { facility: 1, severity: 1, timestamp: now(), hostname: "localhost", structured_data: { origin: { key: "value", }, }, message: "Tenzir", } write_syslog ``` ```log <9>1 2025-03-31T13:28:55.971210Z localhost - - - [origin key="value"] Tenzir ``` ## See Also [Section titled “See Also”](#see-also) [`parse_syslog`](/reference/functions/parse_syslog), [`read_syslog`](/reference/operators/read_syslog)

# write_tql

Transforms the input event stream to a TQL notation byte stream. ```tql write_tql [strip=bool, color=bool, compact=bool, strip_null_fields=bool, strip_nulls_in_lists=bool, strip_empty_records=bool, strip_empty_lists=bool] ``` ## Description [Section titled “Description”](#description) Transforms the input event stream to a TQL notation byte stream. ### `strip = bool (optional)` [Section titled “strip = bool (optional)”](#strip--bool-optional) Enables all `strip_*` options. Defaults to `false`. ### `compact = bool (optional)` [Section titled “compact = bool (optional)”](#compact--bool-optional) Write one event per line, omitting linebreaks and indentation of records. Defaults to `false`. ### `color = bool (optional)` [Section titled “color = bool (optional)”](#color--bool-optional) Colorize the output. Defaults to `false`. ### `strip_null_fields = bool (optional)` [Section titled “strip\_null\_fields = bool (optional)”](#strip_null_fields--bool-optional) Strips all fields with a `null` value from records. Defaults to `false`. ### `strip_nulls_in_lists = bool (optional)` [Section titled “strip\_nulls\_in\_lists = bool (optional)”](#strip_nulls_in_lists--bool-optional) Strips all `null` values from lists. Defaults to `false`. ### `strip_empty_records = bool (optional)` [Section titled “strip\_empty\_records = bool (optional)”](#strip_empty_records--bool-optional) Strips empty records, including those that only became empty by stripping. Defaults to `false`. ### `strip_empty_lists = bool (optional)` [Section titled “strip\_empty\_lists = bool (optional)”](#strip_empty_lists--bool-optional) Strips empty lists, including those that only became empty by stripping. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Print an event as TQL [Section titled “Print an event as TQL”](#print-an-event-as-tql) ```tql from {activity_id: 16, activity_name: "Query", rdata: 31.3.245.133, dst_endpoint: {ip: 192.168.4.1, port: 53}} write_tql ``` ```tql { activity_id: 16, activity_name: "Query", rdata: 31.3.245.133, dst_endpoint: { ip: 192.168.4.1, port: 53, }, } ``` ### Strip null fields [Section titled “Strip null fields”](#strip-null-fields) ```tql from {yes: 1, no: null} write_tql strip_null_fields=true ``` ```tql { yes: 1, } ``` ## See Also [Section titled “See Also”](#see-also) [`write_json`](/reference/operators/write_json)

# write_tsv

Transforms event stream to TSV (Tab-Separated Values) byte stream. ```tql write_tsv [list_separator=str, null_value=str, no_header=bool] ``` ## Description [Section titled “Description”](#description) The `write_tsv` operator transforms an event stream into a byte stream by writing the events as TSV. ### `list_separator = str (optional)` [Section titled “list\_separator = str (optional)”](#list_separator--str-optional) The string separating different elements in a list within a single field. Defaults to `","`. ### `null_value = str (optional)` [Section titled “null\_value = str (optional)”](#null_value--str-optional) The string denoting an absent value. Defaults to `"-"`. ### `no_header = bool (optional)` [Section titled “no\_header = bool (optional)”](#no_header--bool-optional) Whether to not print a header line containing the field names. ## Examples [Section titled “Examples”](#examples) Write an event as TSV. ```tql from {x:1, y:true, z: "String"} write_tsv ``` ```plaintext x y z 1 true String ``` ## See Also [Section titled “See Also”](#see-also) [`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_xsv`](/reference/operators/write_xsv)

# write_xsv

Transforms event stream to XSV byte stream. ```tql write_xsv field_separator=str, list_separator=str, null_value=str, [no_header=bool] ``` ## Description [Section titled “Description”](#description) The [`xsv`](https://en.wikipedia.org/wiki/Delimiter-separated_values) format is a generalization of [comma-separated values (CSV)](https://en.wikipedia.org/wiki/Comma-separated_values) data in tabular form with a more flexible separator specification supporting tabs, commas, and spaces. The first line in an XSV file is the header that describes the field names. The remaining lines contain concrete values. One line corresponds to one event, minus the header. The following table lists existing XSV configurations: | Format | Field Separator | List Separator | Null Value | | --------------------------------------- | :-------------: | :------------: | :--------: | | [`csv`](/reference/operators/write_csv) | `,` | `;` | empty | | [`ssv`](/reference/operators/write_ssv) | `<space>` | `,` | `-` | | [`tsv`](/reference/operators/write_tsv) | `\t` | `,` | `-` | Note that nested records have dot-separated field names. ### `field_separator = str` [Section titled “field\_separator = str”](#field_separator--str) The string separating different fields. ### `list_separator = str` [Section titled “list\_separator = str”](#list_separator--str) The string separating different elements in a list within a single field. ### `null_value = str` [Section titled “null\_value = str”](#null_value--str) The string denoting an absent value. ### `no_header=bool (optional)` [Section titled “no\_header=bool (optional)”](#no_headerbool-optional) Whether to not print a header line containing the field names. ## Examples [Section titled “Examples”](#examples) ```tql from {x:1, y:true, z: "String"} write_xsv field_separator="/", list_separator=";", null_value="" ``` ```plaintext x/y/z 1/true/String ``` ## See Also [Section titled “See Also”](#see-also) [`print_xsv`](/reference/functions/print_xsv), [`write_csv`](/reference/operators/write_csv), [`write_lines`](/reference/operators/write_lines), [`write_ssv`](/reference/operators/write_ssv), [`write_tsv`](/reference/operators/write_tsv)

# write_yaml

Transforms the input event stream to YAML byte stream. ```tql write_yaml ``` ## Description [Section titled “Description”](#description) Transforms the input event stream to YAML byte stream. ## Examples [Section titled “Examples”](#examples) ### Convert a JSON file into a YAML file [Section titled “Convert a JSON file into a YAML file”](#convert-a-json-file-into-a-yaml-file) ```tql load_file "input.json" read_json write_yaml save_file "output.yaml" ``` ## See Also [Section titled “See Also”](#see-also) [`read_yaml`](/reference/operators/read_yaml), [`parse_yaml`](/reference/functions/parse_yaml), [`print_yaml`](/reference/functions/print_yaml)

# write_zeek_tsv

Transforms event stream into Zeek Tab-Separated Value byte stream. ```tql write_zeek_tsv [set_separator=str, empty_field=str, unset_field=str, disable_timestamp_tags=bool] ``` ## Description [Section titled “Description”](#description) The [Zeek](https://zeek.org) network security monitor comes with its own tab-separated value (TSV) format for representing logs. This format includes additional header fields with field names, type annotations, and additional metadata. The `write_zeek_tsv` operator (re)generates the TSV metadata based on Tenzir’s internal schema. Tenzir’s data model is a superset of Zeek’s, so the conversion into Zeek TSV may be lossy. The Zeek types `count`, `real`, and `addr` map to the respective Tenzir types `uint64`, `double`, and `ip`. ### `set_separator = str (optional)` [Section titled “set\_separator = str (optional)”](#set_separator--str-optional) Specifies the set separator. Defaults to `\x09`. ### `empty_field = str (optional)` [Section titled “empty\_field = str (optional)”](#empty_field--str-optional) Specifies the separator for empty fields. Defaults to `(empty)`. ### `unset_field = str (optional)` [Section titled “unset\_field = str (optional)”](#unset_field--str-optional) Specifies the separator for unset “null” fields. Defaults to `-`. ### `disable_timestamp_tags = bool (optional)` [Section titled “disable\_timestamp\_tags = bool (optional)”](#disable_timestamp_tags--bool-optional) Disables the `#open` and `#close` timestamp tags. Defaults to `false`. ## Examples [Section titled “Examples”](#examples) ### Write pipelines results in Zeek TSV format [Section titled “Write pipelines results in Zeek TSV format”](#write-pipelines-results-in-zeek-tsv-format) ```tql subscribe "zeek-logs" where duration > 2s and id.orig_p != 80 write_zeek_tsv save_file "filtered_conn.log" ``` ## See Also [Section titled “See Also”](#see-also) [`read_zeek_json`](/reference/operators/read_zeek_json), [`read_zeek_tsv`](/reference/operators/read_zeek_tsv)

# yara

Executes YARA rules on byte streams. ```tql yara rule:list<string>, [blockwise=bool, compiled_rules=bool, fast_scan=bool] ``` ## Description [Section titled “Description”](#description) The `yara` operator applies [YARA](https://virustotal.github.io/yara/) rules to an input of bytes, emitting rule context upon a match. ![YARA Operator](/_astro/yara-operator.excalidraw.CVYOkp4d_19DKCs.svg) We modeled the operator after the official [`yara` command-line utility](https://yara.readthedocs.io/en/stable/commandline.html) to enable a familiar experience for the command users. Similar to the official `yara` command, the operator compiles the rules by default, unless you provide the option `compiled_rules=true`. To quote from the above link: > This is a security measure to prevent users from inadvertently using compiled rules coming from a third-party. Using compiled rules from untrusted sources can lead to the execution of malicious code in your computer. The operator uses a YARA *scanner* under the hood that buffers blocks of bytes incrementally. Even though the input arrives in non-contiguous blocks of memories, the YARA scanner engine support matching across block boundaries. For continuously running pipelines, use the `blockwise=true` option that considers each block as a separate unit. Otherwise the scanner engine would simply accumulate blocks but never trigger a scan. ### `rule: list<string>` [Section titled “rule: list\<string>”](#rule-liststring) The path to the YARA rule(s). If the path is a directory, the operator attempts to recursively add all contained files as YARA rules. ### `blockwise = bool (optional)` [Section titled “blockwise = bool (optional)”](#blockwise--bool-optional) Whether to match on every byte chunk instead of triggering a scan when the input exhausted. This option makes sense for never-ending dataflows where each chunk of bytes constitutes a self-contained unit, such as a single file. ### `compiled_rules = bool (optional)` [Section titled “compiled\_rules = bool (optional)”](#compiled_rules--bool-optional) Whether to interpret the rules as compiled. When providing this flag, you must exactly provide one rule path as positional argument. ### `fast_scan = bool (optional)` [Section titled “fast\_scan = bool (optional)”](#fast_scan--bool-optional) Enable fast matching mode. ## Examples [Section titled “Examples”](#examples) The examples below show how you can scan a single file and how you can create a simple rule scanning service. ### Perform one-shot scanning of files [Section titled “Perform one-shot scanning of files”](#perform-one-shot-scanning-of-files) Scan a file with a set of YARA rules: ```tql load_file "evil.exe", mmap=true yara "rule.yara" ``` Let’s unpack a concrete example: ```plaintext rule test { meta: string = "string meta data" integer = 42 boolean = true strings: $foo = "foo" $bar = "bar" $baz = "baz" condition: ($foo and $bar) or $baz } ``` You can produce test matches by feeding bytes into the `yara` operator. You will get one `yara.match` per matching rule: ```tql { rule: { identifier: "test", namespace: "default", tags: [], meta: { string: "string meta data", integer: 42, boolean: true }, strings: { "$foo": "foo", "$bar": "bar", "$baz": "baz" } }, matches: { "$foo": [ { data: "Zm9v", base: 0, offset: 0, match_length: 3 } ], "$bar": [ { data: "YmFy", base: 0, offset: 4, match_length: 3 } ] } } ``` Each match has a `rule` field describing the rule and a `matches` record indexed by string identifier to report a list of matches per rule string.

# Platform command line interface

The Tenzir Platform command-line interface (CLI) allows you to interact with the Tenzir Platform from the command line to manage workspaces and nodes. ## Installation [Section titled “Installation”](#installation) Install the [`tenzir-platform`](https://pypi.org/project/tenzir-platform/) package from PyPI. ```text pip install tenzir-platform ``` ## Global Options [Section titled “Global Options”](#global-options) The following options are available for all `tenzir-platform` commands: * `-v, --verbose`: Enable verbose logging for additional error information and debugging output. ## Environment Variable Configuration [Section titled “Environment Variable Configuration”](#environment-variable-configuration) The Tenzir Platform CLI supports several environment variables to configure authentication and platform connection settings. All configuration variables for the CLI share the prefix `TENZIR_PLATFORM_CLI_`. ### Supported Settings [Section titled “Supported Settings”](#supported-settings) * `TENZIR_PLATFORM_CLI_API_ENDPOINT`: The URL of the Tenzir Platform API instance to connect to. Defaults to `https://rest.tenzir.app/production-v1`. * `TENZIR_PLATFORM_CLI_EXTRA_HEADERS`: Additional headers that the CLI should send with any request to the Tenzir Platform. Must provide as a JSON object. * `TENZIR_PLATFORM_CLI_ISSUER_URL`: The OIDC issuer URL for authentication. Defaults to the issuer URL that the public Tenzir Platform instance uses at <https://app.tenzir.com>. * `TENZIR_PLATFORM_CLI_CLIENT_ID`: The client ID for the CLI client. Defaults to the client id that the public Tenzir Platform instance uses at <https://app.tenzir.com>. * `TENZIR_PLATFORM_CLI_CLIENT_SECRET`: The client secret for the CLI client. When set, the CLI automatically attempts to use the client credentials flow instead of the device code flow for authentication. * `TENZIR_PLATFORM_CLI_CLIENT_SECRET_FILE`: Path to a file containing the client secret. Alternative to providing the secret directly via `TENZIR_PLATFORM_CLI_CLIENT_SECRET`. * `TENZIR_PLATFORM_CLI_ID_TOKEN`: If provided, skip the login workflow completely and use this token for authentication. * `TENZIR_PLATFORM_CLI_AUDIENCE`: Override the OIDC audience parameter. Defaults to the client ID. Required for non-interactive logins with some identity providers like Microsoft Entra. * `TENZIR_PLATFORM_CLI_SCOPE`: Override the default OIDC scopes. Defaults to `openid email` for device code flow and `openid` for client credentials flow. Use this to customize the requested permissions during authentication. ## Authentication [Section titled “Authentication”](#authentication) ### Synopsis [Section titled “Synopsis”](#synopsis) ```text tenzir-platform auth login tenzir-platform workspace list tenzir-platform workspace select <workspace_id> ``` ### Description [Section titled “Description”](#description) The `tenzir-platform auth login` command authenticates you with the platform. The `tenzir-platform workspace list` command shows all workspaces available to you. The `tenzir-platform workspace select` command selects a workspace for subsequent operations. #### `<workspace_id>` [Section titled “\<workspace\_id>”](#workspace_id) The unique ID of the workspace, as shown in `tenzir-platform workspace list`. ## Manage Nodes [Section titled “Manage Nodes”](#manage-nodes) ### Synopsis [Section titled “Synopsis”](#synopsis-1) ```text tenzir-platform node list tenzir-platform node ping <node_id> tenzir-platform node create [--name <node_name>] tenzir-platform node delete <node_id> tenzir-platform node run [--name <node_name>] [--image <container_image>] ``` ### Description [Section titled “Description”](#description-1) The following commands interact with the selected workspace: * `tenzir-platform node list` lists all nodes in the selected workspace, including their ID, name, and connection status. * `tenzir-platform node ping` pings the specified node. * `tenzir-platform node create` registers a new node at the platform. This command creates a new API key that a node can use to connect to the platform. It does not start or configure a node. * `tenzir-platform node delete` removes a node from the platform. This command does not stop the node; it only removes it from the platform. * `tenzir-platform node run` creates and registers an ad-hoc node, then starts it on the local host. This command requires Docker Compose. The platform deletes the temporary node when you stop the `run` command. #### `<node_id>` [Section titled “\<node\_id>”](#node_id) The unique ID of the node, as shown in `tenzir-platform node list`. #### `<node_name>` [Section titled “\<node\_name>”](#node_name) The name of the node as shown in the app. #### `<container_image>` [Section titled “\<container\_image>”](#container_image) The Docker image to use for the ad-hoc created node. We recommend using one of the following images: * `tenzir/tenzir:latest` to use the latest release. * `tenzir/tenzir:main` to use the current development version. * `tenzir/tenzir:v5` to pin to a major release. * `tenzir/tenzir:v5.1` to pin to a minor release. * `tenzir/tenzir:v5.1.3` to use a specific release. ## Manage Secrets [Section titled “Manage Secrets”](#manage-secrets) The Tenzir Platform provides secret storage that pipelines running on a Tenzir node can access. ### Synopsis [Section titled “Synopsis”](#synopsis-2) ```text tenzir-platform secret add <name> [--file=<file>] [--value=<value>] [--env] tenzir-platform secret update <secret_id> [--file=<file>] [--value=<value>] [--env] tenzir-platform secret delete <secret_id> tenzir-platform secret list [--json] ``` ### Description [Section titled “Description”](#description-2) The following commands manage secrets in the Tenzir Platform: * `tenzir-platform secret add` adds a new secret to the platform. You can provide the secret value directly via the `--value` option, read it from a file using the `--file` option, or source it from an environment variable using the `--env` flag. The platform identifies the secret by the provided `<name>`. * `tenzir-platform secret update` updates an existing secret identified by `<secret_id>`. Like adding a secret, you can provide the new value via `--value`, `--file`, or `--env`. * `tenzir-platform secret delete` removes a secret from the platform. The command identifies the secret to delete by its `<secret_id>`. * `tenzir-platform secret list` lists all secrets available in the platform. Use the `--json` flag to output the list in JSON format for easier integration with other tools. #### `<name>` [Section titled “\<name>”](#name) The unique name to identify the secret. Pipelines use this name when they access the secret. #### `<secret_id>` [Section titled “\<secret\_id>”](#secret_id) The unique identifier of the secret, as shown in the output of `tenzir-platform secret list`. #### `--file=<file>` [Section titled “--file=\<file>”](#--filefile) Specifies a file containing the secret value. The platform securely stores the file’s contents as the secret value. #### `--value=<value>` [Section titled “--value=\<value>”](#--valuevalue) Specifies the secret value directly as a command-line argument. #### `--env` [Section titled “--env”](#--env) Indicates that the command should source the secret value from an environment variable. You must set the environment variable before you run the command. #### `--json` [Section titled “--json”](#--json) Outputs the list of secrets in JSON format when used with the `tenzir-platform secret list` command. ## Manage external secret stores [Section titled “Manage external secret stores”](#manage-external-secret-stores) You can configure workspaces to use external secret stores instead of the Tenzir Platform’s built-in secret store. Currently, the platform only supports [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) as an external store. By default, the platform mounts external secret stores as read-only. You can’t add or update secrets from the CLI or web interface. Some external secret store implementations may offer write access options. ### Synopsis [Section titled “Synopsis”](#synopsis-3) ```text tenzir-platform secret store add aws --region=<region> --assumed-role-arn=<assumed_role_arn> [--name=<name>] [--prefix=<prefix>] [--access-key-id=<key_id>] [--secret-access-key=<key>] tenzir-platform secret store set-default <store_id> tenzir-platform secret store delete <store_id> tenzir-platform secret store list [--json] ``` ### Description [Section titled “Description”](#description-3) When a node accesses a secret using the `secret("foo")` function in a pipeline, the platform looks up the secret named `foo` in the workspace’s default secret store and returns the value to the node. When using an external secret store, the platform needs the necessary permissions to read secret values from that store. For AWS Secrets Manager, the Tenzir Platform uses [AWS Security Token Service (STS)](https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html) to assume a role with the necessary permissions. You must create this role and configure one of the following options: 1. Create an IAM user. Download access keys and pass them via the `--access-key-id` and `--secret-access-key` arguments. This approach is the easiest to set up but only acceptable for self-hosted testing or development instances of the Tenzir Platform because it stores long-term credentials in the Tenzir Platform. 2. Assign the permissions to the task role of the websocket gateway. We recommend this option if you’re running the Sovereign Edition of the Platform and deploying it to your own AWS account. 3. Assign the permission to assume the configured role to Tenzir’s AWS account with the ID `660178929208`. This option works only with our publicly hosted platform instance on `https://app.tenzir.com`. We plan to add an OIDC-based option as an alternative to options 1 and 3. ## Manage Alerts [Section titled “Manage Alerts”](#manage-alerts) ### Synopsis [Section titled “Synopsis”](#synopsis-4) ```text tenzir-platform alert add <node> <duration> <webhook_url> [<webhook_body>] tenzir-platform alert delete <alert_id> tenzir-platform alert list ``` ### Description [Section titled “Description”](#description-4) The following commands set up alerts for specific nodes. When a node remains disconnected for the configured duration, the alert triggers by sending a POST request to the configured webhook URL. #### `<node>` [Section titled “\<node>”](#node) The node to monitor. You can provide either a node ID or a node name, as long as the name is unambiguous. #### `<duration>` [Section titled “\<duration>”](#duration) The amount of time to wait between the node disconnect and triggering the alert. #### `<webhook_url>` [Section titled “\<webhook\_url>”](#webhook_url) The platform sends a POST request to this URL when the alert triggers. #### `<webhook_body>` [Section titled “\<webhook\_body>”](#webhook_body) The body to send with the webhook. Must be valid JSON. The body may contain `$NODE_NAME`, which the platform replaces with the name of the node that triggered the alert. Defaults to `{"text": "Node $NODE_NAME disconnected for more than {duration}s"}`, where the platform sets `$NODE_NAME` and `{duration}` dynamically from the CLI parameters. ### Example [Section titled “Example”](#example) Consider nodes like this: ```text $ tenzir-platform node list 🟢 Node-1 (n-w2tjezz3) 🟢 Node-2 (n-kzw21299) 🔴 Node-3 (n-ie2tdgca) ``` We want to receive a Slack notification whenever Node-3 is offline for more than three minutes. First, we create a webhook as described in the [Slack docs](https://api.slack.com/messaging/webhooks). Next, we configure the alert in the Tenzir Platform: ```text tenzir-platform alert add Node-3 3m "https://hooks.slack.com/services/XXXXX/YYYYY/ZZZZZ" '{"text": "Alert! Look after node $NODE_NAME"}' ``` If Node-3 doesn’t reconnect within three minutes, a message appears in the configured Slack channel. ## Manage workspaces [Section titled “Manage workspaces”](#manage-workspaces) On-premise setup required This CLI functionality requires an on-premise platform deployment, available with the [Sovereign Edition](https://tenzir.com/pricing). These CLI commands are available only to local platform administrators. The [`TENZIR_PLATFORM_OIDC_ADMIN_RULES` variable](/guides/platform-setup/configure-identity-provider) defines who’s an administrator in your platform deployment. ### Synopsis [Section titled “Synopsis”](#synopsis-5) ```text tenzir-platform admin list-global-workspaces tenzir-platform admin create-workspace <owner_namespace> <owner_id> [--name <workspace_name>] tenzir-platform admin delete-workspace <workspace_id> ``` ### Description [Section titled “Description”](#description-5) The `tenzir-platform admin list-global-workspaces`, `tenzir-platform admin create-workspace`, and `tenzir-platform admin delete-workspace` commands list, create, or delete workspaces, respectively. #### `<owner_namespace>` [Section titled “\<owner\_namespace>”](#owner_namespace) Either `user` or `organization`, depending on whether the workspace associates with a user or an organization. The selected namespace determines the *default* access rules for the workspace: * For a user workspace, the platform creates a single access rule that allows access to the user whose user ID matches the given `owner_id`. * For an organization workspace, the platform creates no rules by default. You must manually add them using the `add-auth-rule` subcommand described below. #### `<owner_id>` [Section titled “\<owner\_id>”](#owner_id) The unique ID of the workspace owner: * If `<owner_namespace>` is `user`, this matches the user’s `sub` claim in the OIDC token. * If `<owner_namespace>` is `organization`, this is an arbitrary string that uniquely identifies the organization the workspace belongs to. #### `--name <workspace_name>` [Section titled “--name \<workspace\_name>”](#--name-workspace_name) The name of the workspace as shown in the app. #### `<workspace_id>` [Section titled “\<workspace\_id>”](#workspace_id-1) The unique ID of the workspace, as shown in `tenzir-platform workspace list` or `tenzir-platform admin list-global-workspaces`. ## Configure access rules [Section titled “Configure access rules”](#configure-access-rules) On-premise setup required You can use this CLI functionality only with an on-premise platform deployment, which is available to users of the [Sovereign Edition](https://tenzir.com/pricing). These CLI commands are available only to local platform administrators. The [`TENZIR_PLATFORM_OIDC_ADMIN_RULES` variable](/guides/platform-setup/configure-identity-provider) defines who’s an administrator in your platform deployment. ### Synopsis [Section titled “Synopsis”](#synopsis-6) ```text tenzir-platform admin list-auth-rules <workspace_id> tenzir-platform admin add-auth-rule allow-all <workspace_id> tenzir-platform admin add-auth-rule user <workspace_id> <user_id> tenzir-platform admin add-auth-rule email-domain <workspace_id> <connection> <domain> tenzir-platform admin add-auth-rule organization-membership <workspace_id> <connection> <organization_claim> <organization> tenzir-platform admin add-auth-rule organization-role <workspace_id> <connection> <roles_claim> <role> <organization_claim> <organization> tenzir-platform admin delete-auth-rule <workspace_id> <auth_rule_index> ``` ### Description [Section titled “Description”](#description-6) You can use the `tenzir-platform admin list-auth-rules`, `tenzir-platform admin add-auth-rule`, and `tenzir-platform admin delete-auth-rule` commands to list, create, or delete authentication rules for all users, respectively, if you have admin permissions. Authentication rules allow you to access the workspace with the provided `<workspace_id>` if your `id_token` matches the configured rule. You gain access to a workspace if any configured rule allows access. The following rules exist: * **Email Suffix Rule**: `tenzir-platform admin add-auth-rule email-domain` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>` and an `email` field that ends with the configured `<domain>`. * **Organization Membership**: `tenzir-platform admin add-auth-rule organization-membership` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>` and an `<organization_claim>` field that exactly matches the provided `<organization>`. Note that you can freely choose the `<organization_claim>` and `<organization>`, so you can also repurpose this rule for generic claims that are not necessarily related to organizations. * **Organization Role Rule**: `tenzir-platform admin add-auth-rule organization-role` allows access if the `id_token` contains a `connection` field that exactly matches the provided `<connection>`, an `<organization_claim>` field that exactly matches the provided `<organization>`, and a `<roles_claim>` field that must be a list containing a value exactly matching `<role>`. We recommend using organization role rules to check if you have a specific role with an organization. * **User Rule**: `tenzir-platform admin add-auth-rule user` allows access if the `id_token` contains a `sub` field that exactly matches the provided `<user_id>`. * **Allow all rule**: `tenzir-platform admin add-auth-rule allow-all` allows access to every user. Use this rule to set up a workspace that all users of a platform instance can access.

# Platform Configuration

This page lists configuration settings for the Tenzir Platform. ## Environment variables [Section titled “Environment variables”](#environment-variables) The Tenzir Platform runs as a set of containers in a Docker Compose stack. Our [example files](https://github.com/tenzir/platform/tree/main/examples) pick up configuration parameters from environment variables. To configure the platform, create a `.env` file in the same directory as your `docker-compose.yaml` file and set the environment variables described below. ### General Settings [Section titled “General Settings”](#general-settings) You must configure these settings for every platform instance. ```sh # The docker image tag that you use for platform deployment. TENZIR_PLATFORM_VERSION=latest # By default, the Tenzir UI Frontend communicates directly with the Tenzir # Gateway to get the current status of all connected nodes. When you set this # to true, the UI backend proxies this communication instead. TENZIR_PLATFORM_USE_INTERNAL_WS_PROXY=false # When enabled, this setting allows users to spawn demo nodes that run inside # the same Docker Compose stack as the platform. TENZIR_PLATFORM_DISABLE_LOCAL_DEMO_NODES=true # The Docker image for running demo nodes. TENZIR_PLATFORM_DEMO_NODE_IMAGE=tenzir/tenzir-node:latest # Optional file defining the static workspace configuration for this platform. TENZIR_PLATFORM_CONFIG_FILE= # To configure administrators, provide a list of authentication rules. Every # user matching any of the provided rules becomes an administrator of this # platform instance and can use the `tenzir-platform admin` CLI commands. Use # the `tenzir-platform tools print-auth-rule` CLI command to get valid rules. TENZIR_PLATFORM_ADMIN_RULES=[] # A random string used to encrypt frontend cookies. # Generate with `openssl rand -hex 32`. TENZIR_PLATFORM_INTERNAL_AUTH_SECRET= # A random string used to generate user keys. # Generate with `openssl rand 32 | base64`. TENZIR_PLATFORM_INTERNAL_TENANT_TOKEN_ENCRYPTION_KEY= # A random string the app uses to access restricted API endpoints. # Generate with `openssl rand -hex 32`. TENZIR_PLATFORM_INTERNAL_APP_API_KEY= ``` ### External Connectivity [Section titled “External Connectivity”](#external-connectivity) These settings define the outward-facing interface of the Tenzir Platform. ```sh # The domain where users can reach the Tenzir UI, e.g., # `https://app.tenzir.example`. Route this to the `app` service through your # external HTTPS proxy. TENZIR_PLATFORM_UI_ENDPOINT= # The domain where the API is reachable, e.g., `https://api.tenzir.example`. # Route this to the `platform` service through your external HTTPS proxy. TENZIR_PLATFORM_API_ENDPOINT= # The endpoint where Tenzir nodes connect. Use a URL with `ws://` or `wss://` # scheme, e.g., `wss://nodes.tenzir.example`. Route this to the # `websocket-gateway` service through your external HTTPS proxy. TENZIR_PLATFORM_NODES_ENDPOINT= # The URL where the platform exposes blob storage, e.g., # `https://downloads.tenzir.example`. If you use the bundled blob storage, # route this to the `seaweed` service through your external HTTPS proxy. TENZIR_PLATFORM_DOWNLOADS_ENDPOINT= ``` ### Identity Provider [Section titled “Identity Provider”](#identity-provider) Create OAuth clients for the Tenzir Platform in your identity provider and fill in the values below to enable platform connectivity. ```sh # A short identifier for the OIDC provider (e.g., 'auth0', 'keycloak') TENZIR_PLATFORM_OIDC_PROVIDER_NAME= # The OIDC provider for platform authentication. TENZIR_PLATFORM_OIDC_PROVIDER_ISSUER_URL= # A JSON object (or array of objects) containing the OIDC issuer and audiences that the platform # accepts. Example: '{"issuer": "keycloak.example.org", "audiences": ["tenzir_platform"]}' TENZIR_PLATFORM_OIDC_TRUSTED_AUDIENCES= # The client ID for the Tenzir Platform CLI. TENZIR_PLATFORM_OIDC_CLI_CLIENT_ID= # The client ID and client secret for the Tenzir UI. TENZIR_PLATFORM_OIDC_UI_CLIENT_ID= TENZIR_PLATFORM_OIDC_UI_CLIENT_SECRET= ``` ### Database [Section titled “Database”](#database) You need to specify the following environment variables so the Tenzir Platform can connect to a postgres instance. ```sh TENZIR_PLATFORM_POSTGRES_USER= TENZIR_PLATFORM_POSTGRES_PASSWORD= TENZIR_PLATFORM_POSTGRES_DB= TENZIR_PLATFORM_POSTGRES_HOSTNAME= ``` ### Blob Storage [Section titled “Blob Storage”](#blob-storage) ```sh # When using S3 or another external blob storage, create the bucket and provide # a valid access key with read and write permissions. When using the bundled # Seaweed instance, set these values to arbitrary strings. TENZIR_PLATFORM_INTERNAL_BUCKET_NAME= TENZIR_PLATFORM_INTERNAL_ACCESS_KEY_ID= TENZIR_PLATFORM_INTERNAL_SECRET_ACCESS_KEY= ``` ## Configuration File [Section titled “Configuration File”](#configuration-file) Currently, the configuration file supports only static workspace configuration. ```yaml --- workspaces: static0: # The name of this workspace name: Tenzir # The category for this workspace in the workspace switcher. category: Statically Configured Workspaces # The icon to use for this workspace. icon-url: https://storage.googleapis.com/tenzir-public-data/icons/tenzir-logo-square.svg # Nodes use this token to connect to the workspace as ephemeral nodes. token: wsk_e9ee76d4faf4b213745dd5c99a9be11f501d7009ded63f2d5NmDS38vXR # - or - # token-file: /path/to/token # All users can access this workspace. auth-rules: - { "auth_fn": "auth_allow_all" } # Example dashboard definition. dashboards: dashboard1: name: Example Dashboard cells: - name: Dashboard 1 definition: | partitions where not internal summarize events=sum(events), schema sort -events type: table x: 0 y: 0 w: 12 h: 12 ```

# Test Framework

The [`tenzir-test`](https://github.com/tenzir/test) harness discovers and runs integration tests for pipelines, fixtures, and custom runners. Use this page as a reference for concepts, configuration, and CLI details. For step-by-step walkthroughs, see the guides for [writing tests](/guides/testing/write-tests), [creating fixtures](/guides/testing/create-fixtures), and [adding custom runners](/guides/testing/add-custom-runners). ## Install [Section titled “Install”](#install) `tenzir-test` ships as a Python package that requires Python 3.12 or later. Install it with [`uv`](https://docs.astral.sh/uv/) (or `pip`) and verify the console script: ```sh uv add tenzir-test uvx tenzir-test --help ``` ## Core concepts [Section titled “Core concepts”](#core-concepts) * **Project root** – Directory passed to `--root`; typically contains `fixtures/`, `inputs/`, `runners/`, and `tests/`. * **Mode** – Auto-detected as *project* or *package*. A `package.yaml` in the current directory (or its parent when you run from `<package>/tests`) switches to package mode. * **Test** – Any supported file under `tests/`; frontmatter controls execution. * **Runner** – Named strategy that executes a test (`tenzir`, `python`, custom entries). * **Fixture** – Reusable environment provider registered under `fixtures/` and requested via frontmatter. * **Suite** – Directory-owned group of tests that share fixtures and run sequentially. Declare it with `suite:` in a `test.yaml`; all descendants join automatically. * **Input** – Data accessed with `TENZIR_INPUTS`; defaults to `<root>/inputs` but you can override it per directory or per test with an `inputs:` setting. * **Scratch directory** – Ephemeral workspace exposed as `TENZIR_TMP_DIR` during each test run. * **Artifact / Baseline** – Runner output persisted next to the test; regenerate with `--update`. * **Configuration sources** – Frontmatter plus inherited `test.yaml` files; `tenzir.yaml` still configures the Tenzir binary. A typical project layout looks like this: ```text project-root/ ├── fixtures/ │ └── __init__.py ├── inputs/ │ └── sample.ndjson ├── runners/ │ └── __init__.py └── tests/ ├── alerts/ │ ├── sample.tql │ └── sample.txt └── python/ └── quick-check.py ``` For a package layout (with `package.yaml`), the structure may look like: ```text my-package/ ├── package.yaml ├── operators/ │ └── custom-op.tql ├── pipelines/ │ └── smoke.tql └── tests/ ├── inputs/ │ └── sample.ndjson ├── fixtures/ │ └── __init__.py ├── runners/ │ └── __init__.py └── pipelines/ ├── custom-op.tql └── custom-op.txt ``` ## Execution modes and packages [Section titled “Execution modes and packages”](#execution-modes-and-packages) * The harness treats `--root` as the project root. If that directory (or its parent when named `tests`) contains `package.yaml`, `tenzir-test` switches to **package mode** and exposes: * `TENZIR_PACKAGE_ROOT` – Absolute package directory. * `TENZIR_INPUTS` – `<package>/tests/inputs/` unless a directory `test.yaml` or the test frontmatter overrides it. * `--package-dirs=<package>` – Passed automatically to the `tenzir` binary. * Without a manifest the harness stays in **project mode**, recursively discovers tests under `tests/`, and applies global fixtures, runners, and inputs. ## CLI reference [Section titled “CLI reference”](#cli-reference) Run the tests from the project root: ```sh uvx tenzir-test ``` Useful options: * `--tenzir-binary /path/to/tenzir`: Override binary lookup. * `--tenzir-node-binary /path/to/tenzir-node`: Override node binary path. * `--update`: Rewrite reference artifacts next to each test. * `--purge`: Remove generated artifacts (diffs, text outputs) from previous runs. * `--jobs N`: Control concurrency (`4 * CPU cores` by default). * `--coverage` and `--coverage-source-dir`: Enable LLVM coverage. * `-k`, `--keep`: Preserve per-test scratch directories instead of deleting them (same as setting `TENZIR_KEEP_TMP_DIRS=1`). * `--debug`: Emit framework-level diagnostics (fixture lifecycle, discovery notes, comparison targets, etc.). The same mode is available via `TENZIR_TEST_DEBUG=1`. * `--summary`: Print the tabular breakdown and failure tree after each project. * `--diff/--no-diff`: Toggle unified diff output for failed comparisons. Diffs are shown by default; disable them when you only need aggregated statistics. * `--diff-stat/--no-diff-stat`: Show (or suppress) the per-file change counter, which summarises additions and deletions even when the diff body is hidden. * `-p`, `--passthrough`: Stream raw stdout/stderr to the terminal instead of comparing against reference artifacts. The harness forces single-job execution (overriding `--jobs` when necessary) and ignores `--update` while passthrough is active. * `-a`, `--all-projects`: Run the root project together with any satellites provided on the command line. Set `TENZIR_TEST_DEBUG=1` in CI when you want the same diagnostics without passing `--debug` on the command line. ## Selections [Section titled “Selections”](#selections) A *selection* is the ordered list of positional paths you pass after the CLI flags. Each element can point to a single test file, a directory, or an entire project. The harness resolves every element relative to the current working directory first and then relative to the root project. How you shape the selection determines which projects run: * No positional arguments → run every test in the root project. * Paths inside the root project → run only those targets (plus any explicitly named satellites). * Paths that resolve to satellite projects → run those satellites, skipping the root unless you also request it. Use `--all-projects` when you want the root project to execute alongside a selection that only names satellites. This keeps the CLI predictable: the selection lists the exact satellites you care about, and the flag opts the root back in without duplicating its path on the command line. ## Suites [Section titled “Suites”](#suites) Suites let you run several tests under one shared fixture lifecycle. Declare a suite in a directory-level `test.yaml`; the definition applies to every test under that directory, including nested subdirectories. tests/http/test.yaml ```yaml suite: smoke-http fixtures: [http] timeout: 45 ``` Key rules: * Suites are directory-owned. Once a `test.yaml` sets `suite`, all descendants belong to that suite. Put tests that should remain independent outside the suite directory or in a sibling directory with a different suite. * Per-test frontmatter may not declare `suite`. * Suite members inherit the directory defaults and can still override most keys on a per-file basis. The exceptions are `fixtures` and `retry`, which must be defined at the directory level once a suite is active so every member agrees on the shared lifecycle. Outside suites you can still set those keys directly in frontmatter. * The harness runs suite members sequentially in lexicographic order of their relative paths. Each suite occupies a single worker, but different suites can run in parallel when `--jobs` allows it. * The CLI executes all suites before any remaining standalone tests so shared fixtures start and stop predictably. * Run the directory that defines the suite (for example `tenzir-test tests/http`) when you want to focus on it. Selecting an individual member now raises an error so every run exercises the full lifecycle and shared fixtures stay in sync. ## Run a subset of tests [Section titled “Run a subset of tests”](#run-a-subset-of-tests) ```sh uvx tenzir-test tests/alerts/high-severity.tql ``` You can list multiple paths in a single invocation. `tenzir-test` wires every argument into the same runner and fixture registry, so you can mix scenarios from the project and external checkouts: ```sh uvx tenzir-test tests/alerts ../contrib/plugins/*/tests ``` ### Run multiple projects with one command [Section titled “Run multiple projects with one command”](#run-multiple-projects-with-one-command) Pass additional project directories after `--root` to execute several projects in one go. Include `--all-projects` so the root executes next to its satellites. The directory given to `--root` acts as the **root project**; all other directories are treated as **satellites**: ```sh uvx tenzir-test --root example-project --all-projects example-satellite ``` Key rules: * The root project provides the baseline configuration (fixtures, runners, `test.yaml` defaults, inputs). Satellites layer their own fixtures and runners on top; duplicate names raise an error so conflicts surface early. * Paths printed in the CLI summary are relative to the working directory. The harness announces each project before running it and lists the runner mix per project for quick insight. * You can target subsets inside each project with additional positional arguments (`tenzir-test --root main --all-projects secondary tests/smoke`). When you skip `--root` entirely and only list satellite directories, the harness runs those satellites in isolation. * Satellites keep their own `tests/`, `inputs/`, `fixtures/`, and `runners/` folders. A root project can host shared assets that satellites reuse without duplication—for example, the example repository includes an `example-satellite/` directory that consumes the `xxd` runner exported by the root project while defining a satellite-specific fixture. To regenerate baselines while targeting a specific binary and project root: ```sh TENZIR_BINARY=/opt/tenzir/bin/tenzir \ TENZIR_NODE_BINARY=/opt/tenzir/bin/tenzir-node \ uvx tenzir-test --root tests --update ``` ## Runners [Section titled “Runners”](#runners) | Runner | Command/behavior | Input extension | Artifact | | -------- | -------------------------------------- | --------------- | -------- | | `tenzir` | `tenzir -f <test>` | `.tql` | `.txt` | | `python` | Execute with the active Python runtime | `.py` | `.txt` | | `shell` | `sh -eu <test>` via the harness helper | `.sh` | varies | Selection flow: 1. The harness chooses the first registered runner that claimed the file extension. 2. Default suffix mapping applies when no runner explicitly claims an extension: `.tql → tenzir`, `.py → python`, `.sh → shell`. 3. A `runner: <name>` frontmatter entry overrides the automatic choice. 4. If no runner claims the extension and none is specified in frontmatter, the harness fails with an error instead of guessing. ### Shell runner [Section titled “Shell runner”](#shell-runner) Place scripts (for example under `tests/shell/`) with the `.sh` suffix to run them under `bash -eu` via the `shell` runner. The harness also prepends `<root>/_shell` to `PATH` so project-specific helper binaries become discoverable. The runner captures stdout and stderr (like `2>&1`) and compares the combined output with `<test>.txt`; run `tenzir-test --update path/to/test.sh` when you need to refresh the baseline. Register custom runners in `runners/__init__.py` via `tenzir_test.runners.register()` or the `@tenzir_test.runners.startup()` decorator. Use `replace=True` to override a bundled runner or `register_alias()` to publish alternate names. The [runner guide](/guides/testing/add-custom-runners) contains a full example (`XxdRunner`). ### Passthrough-aware subprocesses [Section titled “Passthrough-aware subprocesses”](#passthrough-aware-subprocesses) When passthrough mode is active the harness streams stdout/stderr directly to the terminal and skips reference comparisons. Runner implementations can respect this automatically by spawning processes through `tenzir_test.run.run_subprocess(...)`. The helper captures output when the harness needs it and inherits the parent descriptors otherwise. Pass `force_capture=True` when your runner must collect stdout even in passthrough mode. If you need to branch on the current behavior, call `tenzir_test.run.get_harness_mode()` or `tenzir_test.run.is_passthrough_enabled()`. The harness cycles between three internal modes: * `HarnessMode.COMPARE` – default behavior; compare actual output with stored baselines. * `HarnessMode.UPDATE` – engaged when you pass `--update`; runners should overwrite reference files. * `HarnessMode.PASSTHROUGH` – enabled via `-p/--passthrough`; stream output directly without touching baselines. `get_harness_mode()` returns the current enum value so custom runners can adapt logic if needed. ## Configuration and frontmatter [Section titled “Configuration and frontmatter”](#configuration-and-frontmatter) `tenzir-test` merges configuration sources in this order (later wins): 1. Project defaults (`test.yaml` files, applied per directory). 2. Per-test frontmatter (YAML for `.tql`/`.xxd`, `# key: value` comments for Python and shell scripts). Common frontmatter keys: | Key | Type | Default | Description | | ---------- | --------------- | --------- | ------------------------------------------------------ | | `runner` | string | by suffix | Runner name (`tenzir`, `python`, `shell`, custom). | | `fixtures` | list of strings | `[]` | Requested fixtures; use `fixture` for a single value. | | `timeout` | integer (s) | `30` | Command timeout. (`--coverage` multiplies it by five.) | | `error` | boolean | `false` | Expect a non-zero exit code. | | `skip` | string | unset | Mark the test as skipped (reason required). | | `inputs` | string | project | Override `TENZIR_INPUTS` for this directory or test. | | `retry` | integer | `1` | Total attempt budget for flaky tests (see below). | `test.yaml` files accept the same keys and apply recursively to child directories. A relative `inputs:` value resolves against the file that defines it, so `inputs: ../data` inside `tests/alerts/test.yaml` points at `tests/data/`. Frontmatter values follow the same rule and win over directory defaults. Adjacent `tenzir.yaml` files still configure the Tenzir binary; the harness appends `--config=<file>` automatically. The lookup keeps working even when you point the CLI at extra directories on the command line. `retry` represents the **total number of attempts** the harness should make before declaring the test failed. Intermediate attempts stay quiet; the final outcome line includes `attempts=N/M` whenever the budget exceeds one. Keep the value small and treat it as a temporary guardrail while you fix the underlying flakiness. ### Tenzir configuration files [Section titled “Tenzir configuration files”](#tenzir-configuration-files) * The harness inspects the directory that owns each test. If it finds `tenzir.yaml`, it appends `--config=<path>` to every invocation of the bundled `tenzir`/`tql`/`diff` runners. The path also seeds `TENZIR_CONFIG` unless you set that variable yourself. Custom runners that call the Tenzir binary should either use `run.get_test_env_and_config_args(test)` or honour the exported environment variables explicitly. * The built-in `node` fixture uses the same discovery process and starts `tenzir-node` from the directory that owns the test file, so relative paths inside `tenzir-node.yaml` resolve against the test location. See the [built-in node fixture](#built-in-node-fixture) section for precedence rules. * This lets you keep one config for CLI-driven scenarios while passing a different config to the embedded node, for example to tweak endpoints or data directories independently. ## Fixtures [Section titled “Fixtures”](#fixtures) ### Declaring fixtures [Section titled “Declaring fixtures”](#declaring-fixtures) * List fixture names in frontmatter (`fixtures: [node, http]`). Importing the project `fixtures` package is enough to register custom fixtures thanks to the side effects in `fixtures/__init__.py`. * The harness encodes requests in `TENZIR_TEST_FIXTURES` and exposes helper APIs in `tenzir_test.fixtures`: * `fixtures()` – Read-only view of active fixtures. Attribute access is supported, e.g. `fixtures().node` returns `True` if the fixture was requested and raises `AttributeError` otherwise. * `acquire_fixture("name")` – Manual controller for the named fixture. Use it as a context manager for automatic `start()`/`stop()` or call those methods explicitly to interleave lifecycle steps and optional hooks (for example `kill()` or `restart()`). * `require("name")` – Assert that a fixture was requested. * `Executor()` – Convenience wrapper that runs Tenzir commands with resolved binaries and timeout budget. Example use from a Python helper: ```python from tenzir_test.fixtures import Executor executor = Executor() result = executor.run("from_file 'inputs/events.ndjson' | where severity >= 5\n") assert result.returncode == 0 ``` ### Built-in node fixture [Section titled “Built-in node fixture”](#built-in-node-fixture) * Request the fixture with `fixtures: [node]`; the harness will start `tenzir-node` with the binaries discovered for the current test. * Configuration precedence: 1. `TENZIR_NODE_CONFIG` in the environment. 2. A `tenzir-node.yaml` placed next to the test file (exported automatically). 3. The Tenzir defaults (no config file). * The node process inherits the test directory as its current working directory, letting `tenzir-node.yaml` reference files with relative paths (for example `state/` or `schemas/`). * Each controller reuses its state and cache directories across `start()`/`stop()` cycles. By default they live under the per-test scratch directory (`TENZIR_TMP_DIR/tenzir-node-*`) and are removed once the fixture context ends. Starting a fresh controller (for example in another test run) yields a brand-new workspace. * The fixture reuses other inherited arguments (for example `--package-dirs=…`) but replaces any existing `--config=` flag so the node process always honours the chosen configuration file. * Tests can read `TENZIR_NODE_CLIENT_ENDPOINT`, `TENZIR_NODE_CLIENT_BINARY`, `TENZIR_NODE_CLIENT_TIMEOUT`, `TENZIR_NODE_STATE_DIRECTORY`, and `TENZIR_NODE_CACHE_DIRECTORY` from the environment to connect to the spawned node and inspect its working tree. * Pipelines launched by the bundled Tenzir runners automatically receive `--endpoint=<value>` when this fixture is active, so they talk to the transient node without additional wiring. * CLI and node configuration are independent: configure the CLI with `tenzir.yaml` and drop a `tenzir-node.yaml` (or set `TENZIR_NODE_CONFIG`) only when the node needs custom settings. ### Registering fixtures [Section titled “Registering fixtures”](#registering-fixtures) Implement fixtures in `fixtures/` and register them with `@tenzir_test.fixture()`. Decorate a generator function, yield the environment mapping, and handle cleanup in a `finally` block: ```python from tenzir_test import fixture @fixture() def http(): server = _start_server() try: yield {"HTTP_FIXTURE_URL": server.url} finally: server.stop() ``` `@fixture` also accepts regular callables returning dictionaries, context managers, or `FixtureHandle` instances for advanced scenarios. The [fixture guide](/guides/testing/create-fixtures) demonstrates an HTTP echo server that exposes `HTTP_FIXTURE_URL` and tears down cleanly. ## Environment variables [Section titled “Environment variables”](#environment-variables) `tenzir-test` recognises the following environment variables: * `TENZIR_TEST_ROOT` – Default test root when `--root` is omitted. * `TENZIR_BINARY` / `TENZIR_NODE_BINARY` – Override binary discovery. * `TENZIR_INPUTS` – Preferred data directory. Defaults to the project inputs folder but reflects any `inputs:` override from `test.yaml` or frontmatter. * `TENZIR_KEEP_TMP_DIRS` – Keep per-test scratch directories (equivalent to `--keep`). * `TENZIR_TEST_DEBUG` – Enable debug logging (equivalent to `--debug`). Fixtures often publish additional variables (for example `TENZIR_NODE_CLIENT_*`, `TENZIR_NODE_STATE_DIRECTORY`, `TENZIR_NODE_CACHE_DIRECTORY`, `HTTP_FIXTURE_URL`). During execution the harness also adds transient variables such as `TENZIR_TMP_DIR` so tests and fixtures can create temporary artifacts without polluting the repository. Combine it with `--keep` (or `TENZIR_KEEP_TMP_DIRS=1`) when you need to inspect the generated files after a run. ## Baselines and artifacts [Section titled “Baselines and artifacts”](#baselines-and-artifacts) Regenerate reference output whenever behavior changes intentionally: ```sh uvx tenzir-test --update ``` `--purge` removes stale artifacts (diffs, temporary files). Keep generated `.txt` files under version control so future runs can diff against them. ## Troubleshooting [Section titled “Troubleshooting”](#troubleshooting) * **Missing binaries** – Ensure `tenzir` and `tenzir-node` are on `PATH` or set `TENZIR_BINARY` / `TENZIR_NODE_BINARY` explicitly. * **Unexpected exits** – Set `error: true` in frontmatter when a non-zero exit is expected. * **Skipped tests** – Use `skip: reason` to document temporary skips; baseline files can stay empty. * **Noisy output** – Use `--jobs 1` to serialize worker logs, and enable `--debug` (or set `TENZIR_TEST_DEBUG=1`) when you need to trace comparisons and fixture activity. ## Further reading [Section titled “Further reading”](#further-reading) * [Write tests](/guides/testing/write-tests) * [Create fixtures](/guides/testing/create-fixtures) * [Add custom runners](/guides/testing/add-custom-runners)